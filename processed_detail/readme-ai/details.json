[
  {
    "id": "readme-ai:noxfile.py:function:install:chunk1",
    "text": "def install(session, groups=None, extras=None, root=True):\n    \"\"\"Install the project for the current session using Poetry.\"\"\"\n    groups = groups or []\n    extras = extras or []\n\n    if root and \"main\" not in groups:\n        groups.insert(0, \"main\")\n\n    only_arg = []\n    if groups:\n        only_arg = [f\"--only={','.join(groups)}\"]\n\n    extras_args = []\n    if extras:\n        for extra in extras:\n            extras_args.extend([\"--extras\", extra])\n\n    session.run_always(\n        \"poetry\",\n        \"install\",\n        \"--no-root\",\n        \"--sync\",\n        *only_arg,\n        *extras_args,\n        external=True,\n    )\n\n    if root:\n        session.install(\".\")",
    "repo": "readme-ai",
    "path": "noxfile.py",
    "type": "function",
    "name": "install",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:noxfile.py:function:tests:chunk1",
    "text": "def tests(session):\n    \"\"\"Run test suite against Python versions 3.9, 3.10, 3.11, and 3.12.\"\"\"\n    install(\n        session,\n        groups=[\"test\"],\n        extras=[\"anthropic\", \"google-generativeai\"],\n    )\n    session.install(\n        \"pytest\",\n        \"pytest-asyncio\",\n        \"pytest-cov\",\n        \"pytest-mock\",\n        \"pytest-pretty\",\n        \"pytest-randomly\",\n        \"pytest-sugar\",\n        \"pytest-xdist\",\n    )\n    session.run(\n        \"poetry\",\n        \"run\",\n        \"pytest\",\n        \"--config-file=pyproject.toml\",\n        \"--cov-config=pyproject.toml\",\n        external=True,\n    )",
    "repo": "readme-ai",
    "path": "noxfile.py",
    "type": "function",
    "name": "tests",
    "loc": 38,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\cli\\main.py:function:main:chunk1",
    "text": "def main(\n    align: str,\n    api: str,\n    badge_color: str,\n    badge_style: str,\n    base_url: str,\n    context_window: int,\n    emojis: bool,\n    header_style: str,\n    logo: str,\n    logo_size: str,\n    model: str,\n    navigation_style: str,\n    output: str,\n    rate_limit: int,\n    repository: str,\n    system_message: str,\n    temperature: float,\n    top_p: float,\n    tree_max_depth: int,\n) -> None:\n    \"\"\"Entry point for the readme-ai CLI application.\"\"\"\n    config.config.git = GitSettings(repository=repository)\n\n    config.config.llm = config.config.llm.model_copy(\n        update={\n            \"api\": api,\n            \"base_url\": base_url,\n            \"context_window\": context_window,\n            \"model\": model,\n            \"rate_limit\": rate_limit,\n            \"system_message\": system_message,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        },\n    )\n    config.config.md = config.config.md.model_copy(\n        update={\n            \"align\": align,\n            \"badge_color\": badge_color,\n            \"badge_style\": badge_style,\n            \"emojis\": emojis,\n            \"header_style\": header_style,\n            \"logo\": logo,\n            \"logo_size\": logo_size,\n            \"navigation_style\": navigation_style,\n            \"tree_max_depth\": tree_max_depth,\n        },\n    )\n\n    logger.debug(f\"Pydantic settings: {config.__dict__.keys()}\")\n    logger.debug(f\"Repository settings: {config.config.git.model_dump()}\")\n    logger.debug(f\"LLM API settings: {config.config.llm.model_dump()}\")\n\n    readme_agent(config=config, output_file=output)",
    "repo": "readme-ai",
    "path": "readmeai\\cli\\main.py",
    "type": "function",
    "name": "main",
    "loc": 42,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\cli\\options.py:function:prompt_for_logo:chunk1",
    "text": "def prompt_for_logo(\n    ctx: click.Context | None = None,\n    param: click.Parameter | None = None,\n    value: str | None = None,\n) -> str:\n    \"\"\"Manage user project logo selection.\"\"\"\n    if value is None:\n        return DefaultLogos.BLUE.value\n    if value == CustomLogos.CUSTOM.value:\n        return click.prompt(\"Provide an logo file path or URL\")\n    elif value == CustomLogos.LLM.value:\n        return CustomLogos.LLM.value\n    elif value in DefaultLogos.__members__:\n        return DefaultLogos[value].value\n    else:\n        raise click.BadParameter(f\"Invalid logo provided: {value}\")",
    "repo": "readme-ai",
    "path": "readmeai\\cli\\options.py",
    "type": "function",
    "name": "prompt_for_logo",
    "loc": 18,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\cli\\options.py:function:version_callback:chunk1",
    "text": "def version_callback(\n    ctx: click.Context | None = None,\n    param: click.Parameter | None = None,\n    value: str | None = None,\n) -> None:\n    \"\"\"Prints the version of readme-ai.\"\"\"\n    if not value or (ctx and ctx.resilient_parsing):\n        return\n    click.echo(f\"readmeai version {__version__}\")\n    if ctx is not None:\n        ctx.exit()",
    "repo": "readme-ai",
    "path": "readmeai\\cli\\options.py",
    "type": "function",
    "name": "version_callback",
    "loc": 36,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:FileSettings:chunk1",
    "text": "class FileSettings(BaseModel):\n    \"\"\"\n    File path resources for the readme-ai package.\n    \"\"\"\n\n    banners: str = Field(description=\"SVG banner templates.\")\n    dev_setup: str = Field(description=\"Development tool configurations.\")\n    dev_tools: str = Field(description=\"Development tools and utilities.\")\n    ignore_list: str = Field(description=\"List of files to ignore.\")\n    language_map: str = Field(description=\"Extension to language mappings.\")\n    parsers: str = Field(description=\"Common dependency file names.\")\n    prompts: str = Field(description=\"LLM API prompt templates.\")\n    quickstart_guides: str = Field(description=\"Quickstart guide templates.\")\n    shieldsio: str = Field(description=\"Shields.io svg icon badges.\")\n    skillicons: str = Field(description=\"Skill icon badges.\")\n    templates: str = Field(description=\"Custom README templates.\")",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "FileSettings",
    "loc": 53,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:GitSettings:chunk1",
    "text": "class GitSettings(BaseModel):\n    \"\"\"\n    User repository settings for a remote or local codebase.\n    \"\"\"\n\n    repository: Union[str, Path] = Field(..., description=\"Repository URL or path.\")\n    full_name: str = Field(default=\"\", description=\"Full repository name\")\n    host_domain: str = Field(default=\"\", description=\"Git host domain\")\n    host: str = Field(default=\"\", description=\"Git host name\")\n    name: str = Field(default=\"\", description=\"Repository name\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    @field_validator(\"repository\")\n    def validate_repository(cls, value: Union[str, Path]) -> Union[str, Path]:\n        \"\"\"Validates the repository path or Git URL.\"\"\"\n        if isinstance(value, Path) or (\n            isinstance(value, str) and Path(value).is_dir() and Path(value).exists()\n        ):\n            return value\n        try:\n            return str(GitURL.create(value).url)\n        except ValueError as exc:\n            raise GitValidationError(\n                f\"Invalid Git repository URL or path: {value}\",\n            ) from exc\n\n    @model_validator(mode=\"after\")\n    def set_git_attributes(self):\n        \"\"\"Parse and set Git repository attributes.\"\"\"\n        self.host_domain, self.host, self.name, self.full_name = parse_git_url(\n            str(self.repository)\n        )\n        return self",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "GitSettings",
    "loc": 71,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:MarkdownSettings:chunk1",
    "text": "class MarkdownSettings(BaseModel):\n    \"\"\"Markdown configuration settings for README.md file generation.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n        use_enum_values=True,\n    )\n\n    # Layout Settings\n    align: Literal[\"left\", \"center\", \"right\"] = Field(default=\"center\")\n    placeholder: str = Field(default=\"<code>❯ REPLACE-ME</code>\")\n    thematic_break: str = Field(default=\"---\\n\")\n\n    # Header Settings\n    logo: str = Field(default=DefaultLogos.PURPLE)\n    logo_size: str = Field(default=\"400px\")\n    header_style: HeaderStyles = Field(default=HeaderStyles.CLASSIC)\n\n    # Badge Settings\n    badge_color: Color = Field(default_factory=lambda: Color(\"blue\"))\n    badge_style: BadgeStyles = Field(default=BadgeStyles.DEFAULT)\n    shieldsio: str\n    skillicons: str\n    tech_stack_icons: str\n    tech_stack_description: str\n\n    # Navigation Settings\n    navigation_style: NavigationStyles = Field(default=NavigationStyles.BULLET)\n    top_anchor_markup: str = Field(default='<div id=\"top\">')\n    return_to_top_markup: str = Field(\n        default=\"\"\"<div align=\"left\"><a href=\"#top\">⬆ Return</a></div>\\n\"\"\"\n    )\n\n    # Content Settings\n    tagline: str = \"\"\n    overview: str = \"{0}\"\n    features: str = \"{0}\"\n    roadmap: str\n    contribute: str\n    license: str\n    acknowledgment: str\n    emojis: str = Field(\n        default=EmojiThemes.DEFAULT, description=\"Emoji theme header prefix\"\n    )\n    directory_structure: str = Field(\n        default=\"\"\"```sh\\n{0}\\n```\"\"\", description=\"Directory tree code block\"\n    )\n    tree_max_depth: PositiveInt = Field(\n        default=2, ge=1, le=5, description=\"Directory tree depth limit\"\n    )\n\n    @field_validator(\"badge_color\")\n    def set_color(cls, value: str) -> str:\n        \"\"\"Format badge color value to hexadecimal.\"\"\"\n        try:\n            return Color(value).as_hex(format=\"long\").lstrip(\"#\")\n        except ValueError as e:\n            _logger.error(f\"Invalid color provided: {value} - {e}\")\n            return cls.model_fields[\"badge_color\"].default\n\n    @field_validator(\"logo\")\n    def validate_logo(cls, value) -> str:\n        \"\"\"Validate the logo is not empty or None.\"\"\"\n        if not value or value in [None, \"\", DefaultLogos, CustomLogos]:\n            return DefaultLogos.PURPLE.value\n        return value\n\n    @model_validator(mode=\"after\")\n    def validate_emojis(self):\n        \"\"\"Validate the emoji theme is not empty or None.\"\"\"\n        if self.emojis == EmojiThemes.RANDOM.value:\n            self.emojis = random.choice(list(EmojiThemes))\n            _logger.info(f\"Random emoji theme selected: {self.emojis}\")\n        return self",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "MarkdownSettings",
    "loc": 107,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:ModelSettings:chunk1",
    "text": "class ModelSettings(BaseModel):\n    \"\"\"\n    LLM API service configuration and parameters.\n    \"\"\"\n\n    model_config = ConfigDict(\n        use_enum_values=True,\n        validate_assignment=True,\n    )\n\n    api: LLMProviders = LLMProviders.OFFLINE\n    encoder: str = \"cl100k_base\"\n    base_url: str = Field(\n        default=BaseURLs.OPENAI,\n        description=\"Base URL for the selected API service\",\n    )\n    context_window: PositiveInt = Field(default=3900, le=4096)\n    localhost: str = \"http://localhost:11434/\"\n    temperature: NonNegativeFloat = Field(default=0.1, le=2.0)\n    tokens: PositiveInt = Field(default=699, le=2048)\n    top_p: NonNegativeFloat = Field(default=0.9, le=1.0)\n    rate_limit: PositiveInt = Field(default=10, le=30)\n    resource: str = \"v1/chat/completions\"\n    system_message: str = (\n        \"You're a 10x Staff Software Engineering leader, with deep knowledge \"\n        \"across most tech stacks. You'll use your expertise to write robust \"\n        \"README markdown files for open-source projects. You're a master of \"\n        \"the craft, and you're here to help others succeed.\"\n    )\n    supported_models: Dict[str, List[str]] = Field(\n        default_factory=lambda: {\n            \"ollama\": [model.value for model in OllamaModels],\n            \"openai\": [model.value for model in OpenAIModels],\n            \"anthropic\": [model.value for model in AnthropicModels],\n            \"gemini\": [model.value for model in GeminiModels],\n        }\n    )\n    model: Union[OllamaModels, OpenAIModels, AnthropicModels, GeminiModels] = Field(\n        default=OpenAIModels.GPT35_TURBO,\n        description=\"Model for text generation\",\n    )\n\n    def get_supported_models(self) -> List[str]:\n        \"\"\"Get a list of supported models for a given LLM API.\"\"\"\n        return self.supported_models.get(self.api, [])\n\n    def validate_model(self, model: str) -> bool:\n        \"\"\"Validate if a LLM API supports a given model.\"\"\"\n        return model in self.get_supported_models()",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "ModelSettings",
    "loc": 184,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:Settings:chunk1",
    "text": "class Settings(BaseModel):\n    \"\"\"\n    Nested Pydantic model storing all configuration settings.\n    \"\"\"\n\n    files: FileSettings\n    git: GitSettings\n    llm: ModelSettings\n    md: MarkdownSettings\n\n    model_config = ConfigDict(\n        extra=\"allow\",\n        validate_assignment=True,\n    )\n\n    @model_validator(mode=\"after\")\n    def generate_banner(self) -> Self:\n        \"\"\"Generates a banner based on the selected header style.\"\"\"\n        header_style = self.md.header_style.lower()\n\n        if header_style == HeaderStyles.CONSOLE.value:\n            self.md.logo = ascii.generate_console_banner(self.git.name)\n            self.md.header_style = HeaderStyles.CONSOLE\n            return self\n\n        if header_style == HeaderStyles.ASCII.value:\n            self.md.logo = ascii.generate_banner(self.git.name)\n            self.md.header_style = HeaderStyles.ASCII\n\n        elif header_style == HeaderStyles.ASCII_BOX.value:\n            self.md.logo = ascii.generate_box_banner(self.git.name)\n            self.md.header_style = HeaderStyles.ASCII\n\n        elif header_style == HeaderStyles.BANNER.value:\n            self.md.logo = SVGBannerGenerator(\n                f\"config/settings/{self.files.banners}\"\n            ).generate_svg(\n                title=self.git.name,\n                # subtitle=self.md.placeholder,\n            )\n            self.md.header_style = HeaderStyles.BANNER\n        elif (\n            header_style\n            in [\n                HeaderStyles.CLASSIC.value,\n                HeaderStyles.COMPACT.value,\n                HeaderStyles.MODERN.value,\n            ]\n            or self.md.logo == CustomLogos.LLM.value\n        ):\n            self._set_header_style(header_style)\n\n        return self\n\n    def _set_header_style(self, header_style: str) -> None:\n        \"\"\"Helper method to set the header style.\"\"\"\n        style_map = {\n            HeaderStyles.CLASSIC.value: HeaderStyles.CLASSIC,\n            HeaderStyles.CLEAN.value: HeaderStyles.CLEAN,\n            HeaderStyles.COMPACT.value: HeaderStyles.COMPACT,\n            HeaderStyles.MODERN.value: HeaderStyles.MODERN,\n        }\n        if header_style in style_map:\n            self.md.header_style = style_map[header_style]",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "Settings",
    "loc": 235,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\config\\settings.py:class:ConfigLoader:chunk1",
    "text": "class ConfigLoader:\n    \"\"\"\n    Load all resources and settings for the readme-ai package.\n    \"\"\"\n\n    file_handler: FileHandler = FileHandler()\n    config_file: str = \"config.toml\"\n    module: str = \"readmeai.config\"\n    submodule: str = \"settings\"\n\n    def __init__(self) -> None:\n        if \"-V\" not in sys.argv and \"--version\" not in sys.argv:\n            self._load_config()\n            self._load_settings()\n\n    def _load_config(self) -> Settings:\n        \"\"\"Loads the base configuration file.\"\"\"\n        file_path = build_resource_path(\n            file_path=self.config_file,\n            submodule=self.submodule,\n        )\n        config_dict = self.file_handler.read(file_path)\n        self.config = Settings.model_validate(config_dict)\n        return self.config\n\n    def _load_settings(self) -> dict[str, dict]:\n        \"\"\"Loads all TOML config files from ./config/settings/*.toml.\"\"\"\n        settings = self.config.model_dump()\n\n        for key, file_path in settings[\"files\"].items():\n            if file_path.endswith(\".toml\"):\n                file_path = build_resource_path(file_path=file_path)\n                config_dict = self.file_handler.read(file_path)\n                settings[key] = config_dict\n                setattr(self, key, config_dict)\n                _logger.info(f\"Succesfully loaded cofing: {file_path.name}\")\n\n        themes_path = build_resource_path(\n            file_path=\"emojis.yaml\", submodule=f\"{self.submodule}/themes\"\n        )\n        themes_data = self.file_handler.read(themes_path)\n        self.themes = themes_data.get(\"themes\", {})\n        _logger.info(f\"Emoji themes loaded: {list(self.themes.keys())}\")\n\n        return settings",
    "repo": "readme-ai",
    "path": "readmeai\\config\\settings.py",
    "type": "class",
    "name": "ConfigLoader",
    "loc": 301,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:ReadmeAIError:chunk1",
    "text": "class ReadmeAIError(Exception):\n    \"\"\"\n    Base class for exceptions in this module.\n    \"\"\"\n\n    ...",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "ReadmeAIError",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:ReadmeGeneratorError:chunk1",
    "text": "class ReadmeGeneratorError(Exception):\n    \"\"\"\n    Raised when an error occurs during README generation.\n    \"\"\"\n\n    def __init__(self, message: str, *args: Tuple[Union[int, str, float], ...]):\n        self.message = f\"Error generating README: {message}\"\n        super().__init__(self.message)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "ReadmeGeneratorError",
    "loc": 14,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:CLIError:chunk1",
    "text": "class CLIError(ReadmeAIError):\n    \"\"\"Exceptions related to the CLI.\"\"\"\n\n    def __init__(self, message: str, *args: Tuple[Union[int, str, float], ...]):\n        super().__init__(f\"Invalid option provided to CLI: {message}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "CLIError",
    "loc": 27,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:FileSystemError:chunk1",
    "text": "class FileSystemError(ReadmeAIError):\n    \"\"\"\n    Exceptions related to file system operations.\n    \"\"\"\n\n    def __init__(self, message: str, *args: Tuple[Union[int, str, float], ...]):\n        super().__init__(f\"File system error: {message}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "FileSystemError",
    "loc": 37,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:FileReadError:chunk1",
    "text": "class FileReadError(FileSystemError):\n    \"\"\"\n    Raised when a file cannot be read.\n    \"\"\"\n\n    ...",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "FileReadError",
    "loc": 46,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:FileWriteError:chunk1",
    "text": "class FileWriteError(FileSystemError):\n    \"\"\"\n    Raised when a file cannot be written to.\n    \"\"\"\n\n    ...",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "FileWriteError",
    "loc": 54,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:GitValidationError:chunk1",
    "text": "class GitValidationError(ReadmeAIError):\n    \"\"\"\n    Base class errors validating Git repositories.\n    \"\"\"\n\n    ...",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "GitValidationError",
    "loc": 65,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:GitCloneError:chunk1",
    "text": "class GitCloneError(GitValidationError):\n    \"\"\"\n    Raised when a Git repository cannot be cloned.\n    \"\"\"\n\n    def __init__(self, repository: str, *args: Tuple[Union[int, str, float], ...]):\n        self.repository = repository\n        super().__init__(f\"Failed to clone repository: {repository}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "GitCloneError",
    "loc": 73,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:GitURLError:chunk1",
    "text": "class GitURLError(GitValidationError):\n    \"\"\"\n    Raised when an invalid Git repository URL is provided.\n    \"\"\"\n\n    def __init__(self, url: str, *args: Tuple[Union[int, str, float], ...]):\n        self.url = url\n        super().__init__(f\"Invalid Git repository URL: {url}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "GitURLError",
    "loc": 83,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:InvalidRepositoryError:chunk1",
    "text": "class InvalidRepositoryError(GitValidationError):\n    \"\"\"\n    Raised when an invalid repository is provided.\n    \"\"\"\n\n    def __init__(self, repository: str, *args: Tuple[Union[int, str, float], ...]):\n        self.repository = repository\n        super().__init__(f\"Invalid repository provided: {repository}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "InvalidRepositoryError",
    "loc": 93,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:UnsupportedGitHostError:chunk1",
    "text": "class UnsupportedGitHostError(GitValidationError):\n    \"\"\"\n    Raised when an unsupported Git host is provided.\n    \"\"\"\n\n    def __init__(self, host: str, *args: Tuple[Union[int, str, float], ...]):\n        self.host = host\n        super().__init__(f\"Unsupported Git host: {host}\", *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "UnsupportedGitHostError",
    "loc": 103,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:RepositoryProcessingError:chunk1",
    "text": "class RepositoryProcessingError(ReadmeAIError):\n    \"\"\"\n    Raised when an error occurs during repository processing.\n    \"\"\"\n\n    ...",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "RepositoryProcessingError",
    "loc": 116,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\errors.py:class:UnsupportedServiceError:chunk1",
    "text": "class UnsupportedServiceError(ReadmeAIError):\n    \"\"\"\n    Raised when an unsupported LLM service is provided.\n    \"\"\"\n\n    def __init__(self, message: str, *args: Tuple[Union[int, str, float], ...]):\n        super().__init__(message, *args)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\errors.py",
    "type": "class",
    "name": "UnsupportedServiceError",
    "loc": 127,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\logger.py:function:parse_env_bool:chunk1",
    "text": "def parse_env_bool(value: str) -> bool:\n    \"\"\"Parse a string environment variable into a boolean.\"\"\"\n    return value.lower() in (\"true\", \"1\", \"yes\")",
    "repo": "readme-ai",
    "path": "readmeai\\core\\logger.py",
    "type": "function",
    "name": "parse_env_bool",
    "loc": 28,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\logger.py:function:get_logger:chunk1",
    "text": "def get_logger(name: str) -> Logger:\n    \"\"\"Get a logger instance for the given name.\"\"\"\n    return Logger(name=name)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\logger.py",
    "type": "function",
    "name": "get_logger",
    "loc": 163,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\logger.py:class:LoggingConfig:chunk1",
    "text": "class LoggingConfig(BaseSettings):\n    \"\"\"\n    Logging configuration settings.\n    \"\"\"\n\n    log_level: Annotated[str, StringConstraints(to_upper=True)] = Field(default=\"DEBUG\")\n    indent: Annotated[int, Field(default=2, ge=2, le=50)] = 4\n    pad_event: int = Field(default=20, ge=0, le=50)\n    use_json: bool = Field(default=False)\n    log_prefix: str = Field(default=\"\")\n\n    model_config = SettingsConfigDict(\n        case_sensitive=False,\n        env_file=\"../.env\",\n        env_file_encoding=\"utf-8\",\n        env_prefix=\"LOG_\",\n        extra=\"ignore\",\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\core\\logger.py",
    "type": "class",
    "name": "LoggingConfig",
    "loc": 33,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\logger.py:class:CustomFormatter:chunk1",
    "text": "class CustomFormatter(logging.Formatter):\n    \"\"\"\n    Custom formatter adding colors and emojis to log messages.\n    \"\"\"\n\n    def __init__(self, log_prefix: str = \"\") -> None:\n        \"\"\"Initialize the formatter with a log prefix.\"\"\"\n        super().__init__(\n            fmt=\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n        self.log_prefix = log_prefix\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format the log record with color and emoji.\"\"\"\n        super().format(record)\n        emoji = LOG_LEVEL_EMOJIS.get(record.levelname, \"\")\n        color = LOG_LEVEL_COLORS.get(record.levelname, \"\")\n        prefix = f\"{self.log_prefix} | \" if self.log_prefix else \"\"\n        return (\n            f\"{color}{emoji} {record.levelname} | {record.asctime} | \"\n            f\"{prefix}{record.name} | {RESET_COLOR}{record.message}\"\n        )",
    "repo": "readme-ai",
    "path": "readmeai\\core\\logger.py",
    "type": "class",
    "name": "CustomFormatter",
    "loc": 53,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\logger.py:class:Logger:chunk1",
    "text": "class Logger:\n    \"\"\"\n    Logger that dynamically switches between JSON and console output.\n    \"\"\"\n\n    _instances: ClassVar[dict[str, \"Logger\"]] = {}\n\n    def __new__(cls, name: str) -> \"Logger\":\n        \"\"\"Singleton pattern to ensure only one logger per name.\"\"\"\n        if name not in cls._instances:\n            cls._instances[name] = super().__new__(cls)\n        return cls._instances[name]\n\n    def __init__(self, name: str) -> None:\n        \"\"\"Initialize the logger with the given name.\"\"\"\n        if not hasattr(self, \"_initialized\"):\n            self.name = name\n            self.config = LoggingConfig()\n            self._logger = self._setup_logger()\n            self._initialized = True\n\n    def _setup_logger(self) -> Union[logging.Logger, structlog.stdlib.BoundLogger]:\n        \"\"\"Initialize either structlog or standard logger based on config.\"\"\"\n        if self.config.use_json:\n            return self._setup_structlog()\n        return self._setup_custom_logger()\n\n    def _setup_structlog(self) -> structlog.stdlib.BoundLogger:\n        \"\"\"Configure and return a structured logger.\"\"\"\n        # Ensure root logger is configured\n        root_logger = logging.getLogger()\n        if not root_logger.handlers:\n            handler = logging.StreamHandler(sys.stderr)\n            root_logger.addHandler(handler)\n            root_logger.setLevel(self.config.log_level)\n\n        structlog.configure(\n            processors=[\n                structlog.stdlib.add_log_level,\n                structlog.stdlib.add_logger_name,\n                structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"),\n                structlog.processors.JSONRenderer(indent=self.config.indent),\n            ],\n            context_class=dict,\n            logger_factory=structlog.stdlib.LoggerFactory(),\n            wrapper_class=structlog.stdlib.BoundLogger,\n            cache_logger_on_first_use=True,\n        )\n\n        return structlog.get_logger(self.name)\n\n    def _setup_custom_logger(self) -> logging.Logger:\n        \"\"\"Configure and return a custom formatted logger.\"\"\"\n        logger = logging.getLogger(self.name)\n\n        if not logger.handlers:\n            handler = logging.StreamHandler(sys.stderr)\n            formatter = CustomFormatter(log_prefix=self.config.log_prefix)\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(self.config.log_level)\n            logger.propagate = False\n\n        return logger\n\n    def _log(self, level: str, msg: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Internal logging method.\"\"\"\n        getattr(self._logger, level)(msg, *args, **kwargs)\n\n    def info(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self._log(\"info\", msg, *args, **kwargs)\n\n    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self._log(\"debug\", msg, *args, **kwargs)\n\n    def warning(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self._log(\"warning\", msg, *args, **kwargs)\n\n    def error(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self._log(\"error\", msg, *args, **kwargs)\n\n    def critical(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self._log(\"critical\", msg, *args, **kwargs)",
    "repo": "readme-ai",
    "path": "readmeai\\core\\logger.py",
    "type": "class",
    "name": "Logger",
    "loc": 78,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\pipeline.py:function:error_handler:chunk1",
    "text": "def error_handler() -> Generator[None, None, None]:\n    \"\"\"Error handler wrapper for the README generation process.\"\"\"\n    try:\n        yield\n    except Exception as e:\n        raise ReadmeGeneratorError(str(e)) from e",
    "repo": "readme-ai",
    "path": "readmeai\\core\\pipeline.py",
    "type": "function",
    "name": "error_handler",
    "loc": 30,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\pipeline.py:function:readme_agent:chunk1",
    "text": "def readme_agent(config: ConfigLoader, output_file: str) -> None:\n    \"\"\"Wrap asyncronous README generation process with context manager.\"\"\"\n    with error_handler():\n        asyncio.run(readme_generator(config, output_file))",
    "repo": "readme-ai",
    "path": "readmeai\\core\\pipeline.py",
    "type": "function",
    "name": "readme_agent",
    "loc": 38,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\pipeline.py:function:should_generate_image:chunk1",
    "text": "def should_generate_image(config: ConfigLoader) -> bool:\n    \"\"\"Determines if the user enabled LLM image generation.\"\"\"\n    return (\n        config.config.md.logo == CustomLogos.LLM.value\n        and config.config.llm.api != LLMProviders.OFFLINE.value\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\core\\pipeline.py",
    "type": "function",
    "name": "should_generate_image",
    "loc": 101,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\pipeline.py:function:log_repository_context:chunk1",
    "text": "def log_repository_context(context: RepositoryContext) -> None:\n    \"\"\"Logs a snippet of the processed repository context data.\"\"\"\n    _logger.debug(f\"Total files analyzed: {len(context.files)}\")\n    _logger.debug(f\"Metadata extracted: {context.metadata}\")\n    _logger.debug(f\"Dependencies: {context.dependencies}\")\n    _logger.debug(f\"Languages: {context.language_counts}\")\n    _logger.debug(f\"Languages detected: {context.languages}\")\n    _logger.debug(f\"Extensions detected: {context.metadata}\")\n    _logger.debug(f\"Quickstart: {context.quickstart.model_dump()}\")",
    "repo": "readme-ai",
    "path": "readmeai\\core\\pipeline.py",
    "type": "function",
    "name": "log_repository_context",
    "loc": 109,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\core\\pipeline.py:function:log_process_completion:chunk1",
    "text": "def log_process_completion(output_file: str) -> None:\n    \"\"\"Logs the completion of the README generation process.\"\"\"\n    _logger.info(\"README.md file generated successfully.\")\n    _logger.info(f\"Output file saved @ {output_file}\")\n    _logger.info(\"Share with us @ github.com/eli64s/readme-ai/discussions\")",
    "repo": "readme-ai",
    "path": "readmeai\\core\\pipeline.py",
    "type": "function",
    "name": "log_process_completion",
    "loc": 120,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\analyzer.py:class:RepositoryAnalyzer:chunk1",
    "text": "class RepositoryAnalyzer:\n    \"\"\"\n    Processes a repository to extract dependencies and metadata.\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader):\n        self.config = config\n        self.file_processor = FileProcessor(config)\n        self.metadata_extractor = MetadataExtractor(config)\n        self.quickstart_generator = QuickStartGenerator(config)\n\n    async def process_repository(\n        self, repo_path: Path | str | None = None\n    ) -> RepositoryContext:\n        \"\"\"Process the repository and extract metadata.\"\"\"\n        repo_path = Path(str(repo_path))\n\n        file_contexts = self.file_processor.process_files(repo_path)\n        metadata = self.metadata_extractor.extract_metadata(file_contexts)\n        language_counts = self.file_processor.count_languages(file_contexts)\n        dependencies = self.file_processor.extract_dependencies(file_contexts)\n        extensions = self.file_processor.extract_extensions(file_contexts)\n        language_names = list({\n            file.language for file in file_contexts if file.language\n        })\n        quickstart = self.quickstart_generator.generate(language_counts, metadata)\n        dependencies_and_tools = (\n            [tool for tool_group in metadata.values() for tool in tool_group]\n            + language_names\n            + [dep.split(\".\")[0] for dep in dependencies]\n            + extensions\n        )\n        dependencies_and_tools.extend([\n            item\n            for _, items in metadata.get(\"dependencies\", {}).items()\n            if isinstance(items, list)\n            for item in items\n        ])\n        _logger.debug(f\"Extracted dependencies: {dependencies_and_tools}\")\n\n        return RepositoryContext(\n            files=file_contexts,\n            dependencies=dependencies_and_tools,\n            languages=language_names,\n            language_counts=language_counts,\n            metadata=metadata,\n            quickstart=quickstart,\n        )",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\analyzer.py",
    "type": "class",
    "name": "RepositoryAnalyzer",
    "loc": 13,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\dependencies.py:class:FileProcessor:chunk1",
    "text": "class FileProcessor:\n    \"\"\"\n    File processor class to process files in a repository.\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader):\n        self.config = config\n        self.document_cleaner = DocumentCleaner()\n        self.ignore_list = config.ignore_list.get(\"ignore_list\", [])\n        self.language_names = config.language_map.get(\"languages\", {})\n\n    def process_files(self, repo_path: Path) -> list[FileContext]:\n        \"\"\"Generate file info for the given repository path.\"\"\"\n        return [\n            self._create_file_context(file_path, repo_path)\n            for file_path in repo_path.rglob(\"*\")\n            if file_path.is_file()\n            and not is_excluded(self.ignore_list, file_path, repo_path)\n        ]\n\n    def count_languages(self, file_contexts: list[FileContext]) -> dict[str, int]:\n        \"\"\"Count the occurrences of each language.\"\"\"\n        return dict(Counter(file.ext for file in file_contexts if file.ext))\n\n    def extract_dependencies(self, file_contexts: list[FileContext]) -> list[str]:\n        \"\"\"Extract all dependencies from file contexts.\"\"\"\n\n        return list(set().union(*(file.dependencies for file in file_contexts)))\n\n    def extract_extensions(self, file_contexts: list[FileContext]) -> list[str]:\n        \"\"\"Extract all file extensions from file contexts.\"\"\"\n        return list(set(file.ext for file in file_contexts if file.ext))\n\n    def _create_file_context(self, file_path: Path, repo_path: Path) -> FileContext:\n        \"\"\"Create a file context object for the given file path.\"\"\"\n        relative_path = str(file_path.relative_to(repo_path))\n        content = file_path.read_text(errors=\"ignore\")\n        file_ext = file_path.suffix.lstrip(\".\")\n        return FileContext(\n            path=relative_path,\n            name=file_path.name,\n            ext=file_ext,\n            content=self.document_cleaner.clean(content),\n            language=self._map_language(file_ext, file_path.name),\n            dependencies=self._parse_dependencies(relative_path, content),\n        )\n\n    def _map_language(self, file_ext: str, file_name: str) -> str:\n        \"\"\"Map the file extension to the programming language name.\"\"\"\n        return self.language_names.get(file_ext, file_name)\n\n    def _parse_dependencies(self, file_path: str, content: str) -> list[str]:\n        \"\"\"Parse dependencies from the file content.\"\"\"\n        if parser := ParserFactory.create_parser(file_path):\n            try:\n                return parser.parse(content) or []\n            except Exception as e:\n                _logger.error(f\"Error parsing dependency file {file_path}: {e}\")\n        return []",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\dependencies.py",
    "type": "class",
    "name": "FileProcessor",
    "loc": 14,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\metadata.py:class:MetadataExtractor:chunk1",
    "text": "class MetadataExtractor:\n    \"\"\"\n    Enhanced metadata extractor class that uses the existing parser system.\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader) -> None:\n        self.config = config\n\n    def extract_metadata(\n        self, file_contexts: list[FileContext]\n    ) -> dict[str, dict[str, str | List[str]]]:\n        \"\"\"Extract metadata and dependencies from file contexts.\"\"\"\n        metadata_categories = {\n            \"cicd\": \"cicd\",\n            \"containers\": \"containers\",\n            \"documentation\": \"documentation\",\n            \"package_managers\": \"package_managers\",\n        }\n\n        # Get basic metadata\n        metadata = {\n            category: self._convert_to_string(\n                self._detect_tools(\n                    file_contexts, self.config.dev_tools.get(dev_setup, {})\n                )\n            )\n            for category, dev_setup in metadata_categories.items()\n        }\n\n        # Extract dependencies using the parser system\n        package_dependencies = self._extract_package_dependencies(\n            metadata.get(\"package_managers\", {}), file_contexts\n        )\n\n        if package_dependencies:\n            metadata[\"dependencies\"] = package_dependencies\n\n        return metadata\n\n    def _extract_package_dependencies(\n        self, package_managers: dict[str, str], file_contexts: list[FileContext]\n    ) -> dict[str, list[str]]:\n        \"\"\"Extract dependencies using the appropriate parser for each file.\"\"\"\n        dependencies: dict[str, set[str]] = {}\n\n        for manager, files_str in package_managers.items():\n            file_paths = [f.strip() for f in files_str.split(\",\")]\n\n            for file_path in file_paths:\n                file_name = Path(file_path).name\n                file_content = next(\n                    (f.content for f in file_contexts if f.path == file_path), None\n                )\n\n                if file_content:\n                    parser = ParserFactory.create_parser(file_name)\n                    parsed_deps = parser.parse(file_content)\n\n                    if parsed_deps:\n                        if manager not in dependencies:\n                            dependencies[manager] = set()\n                        dependencies[manager].update(parsed_deps)\n\n        return {manager: sorted(list(deps)) for manager, deps in dependencies.items()}\n\n    def _detect_tools(\n        self, files: list[FileContext], tool_definitions: dict[str, list[str]]\n    ) -> dict[str, list[str]]:\n        \"\"\"Detect tools based on file patterns.\"\"\"\n        detected_tools: dict[str, list[str]] = {}\n\n        for file in files:\n            for tool, patterns in tool_definitions.items():\n                if matching_files := [\n                    file.path\n                    for pattern in patterns\n                    if self._match_file_pattern(file.path, pattern)\n                ]:\n                    if tool not in detected_tools:\n                        detected_tools[tool] = []\n                    detected_tools[tool].extend(matching_files)\n\n        return detected_tools\n\n    def _match_file_pattern(self, file_path: str, pattern: str) -> bool:\n        \"\"\"Match a file path against a pattern.\"\"\"\n        if pattern.endswith(\"*\"):\n            return file_path.startswith(pattern.rstrip(\"*\"))\n        elif pattern.startswith(\"*\"):\n            return file_path.endswith(pattern[1:])\n        elif \"*\" in pattern:\n            return fnmatch.fnmatch(file_path, pattern)\n        return file_path.endswith(pattern)\n\n    def _convert_to_string(self, tool_files: dict[str, list[str]]) -> dict[str, str]:\n        \"\"\"Convert detected tools list of file paths into a single string.\"\"\"\n        return {tool: \", \".join(files) for tool, files in tool_files.items()}",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\metadata.py",
    "type": "class",
    "name": "MetadataExtractor",
    "loc": 13,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\models.py:class:QuickStart:chunk1",
    "text": "class QuickStart(BaseModel):\n    \"\"\"\n    Instructions for installation, usage, and testing a repository.\n    \"\"\"\n\n    primary_language: str | None = None\n    language_counts: dict[str, int] = Field(default_factory=dict)\n    package_managers: dict[str, str] = Field(default_factory=dict)\n    containers: dict[str, str] = Field(default_factory=dict)\n    install_commands: str = \"\"\n    usage_commands: str = \"\"\n    test_commands: str = \"\"",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\models.py",
    "type": "class",
    "name": "QuickStart",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\models.py:class:FileContext:chunk1",
    "text": "class FileContext(BaseModel):\n    \"\"\"\n    FileContext model for storing file information.\n    \"\"\"\n\n    path: str\n    name: str\n    ext: str\n    content: str\n    language: Annotated[str, StringConstraints(to_lower=True)]\n    dependencies: Annotated[list[str], Field(default_factory=list)]",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\models.py",
    "type": "class",
    "name": "FileContext",
    "loc": 20,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\models.py:class:RepositoryContext:chunk1",
    "text": "class RepositoryContext(BaseModel):\n    \"\"\"\n    RepositoryContext model for storing repository information\n    \"\"\"\n\n    files: list[FileContext]\n    dependencies: list[str]\n    languages: list[str]\n    language_counts: dict[str, int]\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    quickstart: QuickStart = Field(default_factory=QuickStart)",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\models.py",
    "type": "class",
    "name": "RepositoryContext",
    "loc": 33,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\tools.py:class:BadgeInfo:chunk1",
    "text": "class BadgeInfo(BaseModel):\n    \"\"\"\n    Model for storing badge information from repository files.\n    \"\"\"\n\n    name: str\n    description: str\n    url: str | None = None",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\tools.py",
    "type": "class",
    "name": "BadgeInfo",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\tools.py:class:RepositoryBadges:chunk1",
    "text": "class RepositoryBadges(BaseModel):\n    \"\"\"\n    Model for storing all badge information for a repository.\n    \"\"\"\n\n    badges: list[BadgeInfo] = Field(default_factory=list)",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\tools.py",
    "type": "class",
    "name": "RepositoryBadges",
    "loc": 16,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\tools.py:class:RepositoryContext:chunk1",
    "text": "class RepositoryContext(BaseModel):\n    \"\"\"\n    Extended RepositoryContext model with badges support\n    \"\"\"\n\n    files: list[FileContext]\n    dependencies: list[str]\n    languages: list[str]\n    language_counts: dict[str, int]\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    quickstart: QuickStart = Field(default_factory=QuickStart)\n    badges: RepositoryBadges = Field(default_factory=RepositoryBadges)",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\tools.py",
    "type": "class",
    "name": "RepositoryContext",
    "loc": 24,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\tools.py:class:BadgeExtractor:chunk1",
    "text": "class BadgeExtractor:\n    \"\"\"\n    Badge extractor class to detect and generate badges based on repository files.\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader) -> None:\n        self.config = config\n        self._badge_definitions = {\n            \"prettier\": {\n                \"files\": [\n                    \".prettierrc\",\n                    \".prettierrc.json\",\n                    \".prettierrc.js\",\n                    \".prettier.config.js\",\n                ],\n                \"description\": \"Code formatting with Prettier\",\n                \"url\": \"https://prettier.io\",\n            },\n            \"eslint\": {\n                \"files\": [\n                    \".eslintrc\",\n                    \".eslintrc.json\",\n                    \".eslintrc.js\",\n                    \".eslint.config.js\",\n                ],\n                \"description\": \"Code linting with ESLint\",\n                \"url\": \"https://eslint.org\",\n            },\n            \"typescript\": {\n                \"files\": [\"tsconfig.json\"],\n                \"description\": \"TypeScript enabled\",\n                \"url\": \"https://www.typescriptlang.org\",\n            },\n            \"docker\": {\n                \"files\": [\"Dockerfile\", \"docker-compose.yml\", \"docker-compose.yaml\"],\n                \"description\": \"Docker containerization\",\n                \"url\": \"https://www.docker.com\",\n            },\n            \"github-actions\": {\n                \"files\": [\".github/workflows/*.yml\", \".github/workflows/*.yaml\"],\n                \"description\": \"GitHub Actions CI/CD\",\n                \"url\": \"https://github.com/features/actions\",\n            },\n        }\n\n    def extract_badges(self, file_contexts: list[FileContext]) -> RepositoryBadges:\n        \"\"\"Extract badge information from file contexts.\"\"\"\n        badges = []\n        file_paths = {file.path for file in file_contexts}\n\n        for badge_name, badge_config in self._badge_definitions.items():\n            if self._has_badge_files(file_paths, badge_config[\"files\"]):\n                badges.append(\n                    BadgeInfo(\n                        name=badge_name,\n                        description=badge_config[\"description\"],\n                        url=badge_config[\"url\"],\n                    )\n                )\n\n        return RepositoryBadges(badges=badges)\n\n    def _has_badge_files(self, file_paths: set[str], badge_patterns: list[str]) -> bool:\n        \"\"\"Check if any of the badge file patterns match the repository files.\"\"\"\n        for pattern in badge_patterns:\n            if \"*\" in pattern:\n                # Handle glob patterns\n                base_path = pattern.split(\"*\")[0]\n                if any(path.startswith(base_path) for path in file_paths):\n                    return True\n            else:\n                # Direct file match\n                if pattern in file_paths:\n                    return True\n        return False",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\tools.py",
    "type": "class",
    "name": "BadgeExtractor",
    "loc": 38,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\extractors\\tools.py:class:RepositoryAnalyzer:chunk1",
    "text": "class RepositoryAnalyzer:\n    \"\"\"\n    Extended RepositoryAnalyzer with badge support\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader):\n        self.config = config\n        self.file_processor = FileProcessor(config)\n        self.metadata_extractor = MetadataExtractor(config)\n        self.quickstart_generator = QuickStartGenerator(config)\n        self.badge_extractor = BadgeExtractor(config)\n\n    async def process_repository(\n        self, repo_path: Path | str | None = None\n    ) -> RepositoryContext:\n        \"\"\"Process the repository and extract metadata including badges.\"\"\"\n        repo_path = Path(str(repo_path))\n\n        file_contexts = self.file_processor.process_files(repo_path)\n        metadata = self.metadata_extractor.extract_metadata(file_contexts)\n        language_counts = self.file_processor.count_languages(file_contexts)\n        dependencies = self.file_processor.extract_dependencies(file_contexts)\n        extensions = self.file_processor.extract_extensions(file_contexts)\n        language_names = list({\n            file.language for file in file_contexts if file.language\n        })\n\n        # Extract badges\n        badges = self.badge_extractor.extract_badges(file_contexts)\n\n        quickstart = self.quickstart_generator.generate(language_counts, metadata)\n\n        # Improve dependencies and tools extraction by including badge information\n        dependencies_and_tools = (\n            [tool for tool_group in metadata.values() for tool in tool_group]\n            + language_names\n            + [dep.split(\".\")[0] for dep in dependencies]\n            + extensions\n            + [badge.name for badge in badges.badges]  # Include badge names in tools\n        )\n\n        return RepositoryContext(\n            files=file_contexts,\n            dependencies=dependencies_and_tools,\n            languages=language_names,\n            language_counts=language_counts,\n            metadata=metadata,\n            quickstart=quickstart,\n            badges=badges,\n        )",
    "repo": "readme-ai",
    "path": "readmeai\\extractors\\tools.py",
    "type": "class",
    "name": "RepositoryAnalyzer",
    "loc": 115,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:build_code_metrics:chunk1",
    "text": "def build_code_metrics(\n    config: Settings,\n    full_name: str,\n    host: str,\n) -> str:\n    \"\"\"Build metadata badges using shields.io.\"\"\"\n    return config.md.shieldsio.format(\n        host=host,\n        full_name=full_name,\n        badge_color=config.md.badge_color,\n        badge_style=config.md.badge_style,\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "build_code_metrics",
    "loc": 17,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:build_tech_stack:chunk1",
    "text": "def build_tech_stack(\n    dependencies: list[str],\n    icons: dict[str, str],\n    style: str,\n) -> str:\n    \"\"\"Build HTML badges for project dependencies.\"\"\"\n    badges = [\n        icons[str(dependency).lower()]\n        for dependency in dependencies\n        if str(dependency).lower() in icons\n    ]\n    badges = sort_badges(badges)\n    badges = [badge[0].format(style) for badge in badges]\n    return format_badges(badges)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "build_tech_stack",
    "loc": 31,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:format_badges:chunk1",
    "text": "def format_badges(badges: list[str]) -> str:\n    \"\"\"Format SVG badge icons as HTML.\"\"\"\n    total = len(badges)\n    if badges is None or total == 0:\n        return \"\"\n\n    badges_per_line = total if total < 9 else (total // 2) + (total % 2)\n\n    lines = []\n    for i in range(0, total, badges_per_line):\n        line = \"\\n\".join(\n            [\n                f'<img src=\"{badge}\" alt=\"{badge.split(\"/badge/\")[1].split(\"-\")[0]}\">'\n                for badge in badges[i : i + badges_per_line]\n            ],\n        )\n        lines.append(\n            f\"{line}\\n<br>\" if i + badges_per_line < total else f\"{line}\\n\",\n        )\n\n    return \"\\n\".join(lines)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "format_badges",
    "loc": 47,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:sort_badges:chunk1",
    "text": "def sort_badges(badges: list[tuple[str, str]]) -> Sequence[tuple[str, str]]:\n    \"\"\"Sorts badges by color and then by name.\"\"\"\n    badges = [(badge[0], str(badge[1])) for badge in badges]\n    badges = list(set(badges))\n    return sorted(\n        badges,\n        key=lambda b: (*hex_to_hls(b[1]), b[0]) if b[1] else (float(\"inf\"), 0, 0, \"\"),\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "sort_badges",
    "loc": 70,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:shieldsio:chunk1",
    "text": "def shieldsio(\n    conf: Settings,\n    dependencies: list,\n    full_name: str,\n    git_host: str,\n) -> tuple[str, str]:\n    \"\"\"Generates badges for the README using shields.io icons.\"\"\"\n    icons_path = build_resource_path(\n        conf.files.shieldsio,\n        _package,\n        _submodule,\n    )\n    icons_dict = FileHandler().read(icons_path)\n\n    default_icons = build_code_metrics(conf, full_name, git_host)\n\n    tech_stack_icons = build_tech_stack(\n        dependencies,\n        icons_dict,\n        conf.md.badge_style,\n    )\n    tech_stack_icons = conf.md.tech_stack_icons.format(\n        align=conf.md.align,\n        tech_stack_icons=tech_stack_icons,\n    )\n\n    if (\n        conf.md.badge_style == BadgeStyles.DEFAULT.value\n        and git_host != GitHost.LOCAL.name\n    ):\n        conf.md.tech_stack_description = _comment\n        return (\n            default_icons,\n            _comment,\n        )\n\n    if git_host == GitHost.LOCAL.name:\n        return (\n            \"<!-- local repository, no metadata badges. -->\",\n            tech_stack_icons,\n        )\n\n    return default_icons, tech_stack_icons",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "shieldsio",
    "loc": 80,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\badges.py:function:skillicons:chunk1",
    "text": "def skillicons(conf: Settings, dependencies: list) -> str:\n    \"\"\"Build tech stack icon set using skillicons.\"\"\"\n    dependencies.extend([\"md\"])\n\n    icons_path = build_resource_path(\n        conf.files.skillicons,\n        _package,\n        _submodule,\n    )\n    icons_dict = FileHandler().read(icons_path)\n\n    icons = [icon for icon in icons_dict[\"icons\"][\"names\"] if icon in dependencies]\n\n    formatted_icons = icons_dict[\"url\"][\"base_url\"] + \",\".join(icons)\n\n    if conf.md.badge_style == \"skills-light\":\n        formatted_icons = f\"{formatted_icons}&theme=light\"\n\n    conf.md.skillicons = conf.md.skillicons.format(formatted_icons)\n\n    return conf.md.tech_stack_icons.format(\n        align=conf.md.align,\n        tech_stack_icons=conf.md.skillicons,\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\badges.py",
    "type": "function",
    "name": "skillicons",
    "loc": 125,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\builder.py:class:MarkdownBuilder:chunk1",
    "text": "class MarkdownBuilder:\n    \"\"\"\n    Generates each section for the README.md document.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_loader: ConfigLoader,\n        repo_context: RepositoryContext,\n        file_summaries: list[tuple[str, str]],\n        temp_dir: str,\n    ):\n        self.config_loader = config_loader\n        self.config = config_loader.config\n        self.deps = repo_context.dependencies\n        self.repo_context = repo_context\n        self.summaries = file_summaries\n        self.temp_dir = Path(temp_dir)\n        self.git = self.config.git\n        self.md = self.config.md\n        self.theme_map = self.config_loader.themes\n\n        self.repo_url = str(\n            self.git.repository\n            if self.git.host_domain != GitHost.LOCAL.name.lower()\n            else f\"../{self.git.name}\"\n        )\n        self.header_registry = HeaderRegistry(self.md.emojis, self.theme_map)\n        self.header_template = HeaderTemplate(style=self.md.header_style)\n        self.theme_manager = ThemeManager(self.config)\n        self.toc_template = NavigationTemplate(\n            style=self.md.navigation_style, header_registry=self.header_registry\n        )\n        self.thematic_break = self.md.thematic_break\n        self.themed_headers = self._get_themed_headers()\n\n    def _build_badges(self) -> tuple[str, str]:\n        \"\"\"Generate badge icon set to embed in the README header.\"\"\"\n        if BadgeStyles.SKILLS.value not in self.md.badge_style:\n            code_metrics, tech_stack = badges.shieldsio(\n                self.config,\n                self.deps,\n                self.git.full_name,\n                self.git.host,\n            )\n        else:\n            code_metrics = self.md.placeholder\n            tech_stack = badges.skillicons(self.config, self.deps)\n        return code_metrics, tech_stack\n\n    def _build_header(self) -> str:\n        \"\"\"Build the README header with badge icons.\"\"\"\n        code_metrics, tech_stack = self._build_badges()\n        header_data = {\n            \"align\": self.md.align,\n            \"logo\": self.md.logo,\n            \"logo_size\": self.md.logo_size,\n            \"repo_name\": self.git.name.upper()\n            if self.git.name\n            else self.md.placeholder,\n            \"tagline\": self.md.tagline,\n            \"shields_icons\": code_metrics,\n            \"tech_stack_icons\": tech_stack,\n            \"tech_stack_description\": self.md.tech_stack_description,\n        }\n\n        return self.header_template.render(header_data)\n\n    def _get_themed_headers(self) -> Dict[str, str]:\n        \"\"\"Get themed versions of all section headers.\"\"\"\n        base_headers = [\n            \"Table of Contents\",\n            \"Introduction\",\n            \"Overview\",\n            \"Features\",\n            \"Project Structure\",\n            \"Project Index\",\n            \"Getting Started\",\n            \"Prerequisites\",\n            \"Installation\",\n            \"Usage\",\n            \"Testing\",\n            \"Contributing\",\n            \"Roadmap\",\n            \"License\",\n            \"Acknowledgment\",\n        ]\n        headers = {}\n        for header in base_headers:\n            themed = self.theme_manager.apply_theme_to_section(header)\n            headers[header] = themed\n            if header.lower() == \"acknowledgment\":\n                headers[\"Acknowledgments\"] = themed\n            snake_key = header.lower().replace(\" \", \"_\")\n            headers[snake_key] = themed\n        return headers\n\n    @property\n    def header_and_badges(self) -> str:\n        \"\"\"Generate README header section with badges.\"\"\"\n        return self._build_header()\n\n    @property\n    def table_of_contents(self) -> str:\n        toc_with_theme = self.theme_manager.get_themed_toc()\n        _logger.debug(f\"Table of Contents: {toc_with_theme}\")\n        return self.toc_template.render(toc_with_theme)\n\n    @property\n    def tree(self) -> str:\n        \"\"\"Generate the project directory tree structure.\"\"\"\n        project_tree = tree.TreeGenerator(\n            repo_name=self.git.name,\n            root_dir=self.temp_dir,\n            repo_url=self.repo_url,\n            max_depth=self.md.tree_max_depth,\n        ).generate(self.temp_dir)\n        return self.md.directory_structure.format(project_tree)\n\n    @property\n    def file_summaries(self) -> str:\n        \"\"\"Generate formatted tables containing file summaries.\"\"\"\n        formatted_summaries = tables.format_code_summaries(\n            self.md.placeholder,\n            self.summaries,\n        )\n        return tables.generate_nested_module_tables(\n            formatted_summaries,\n            self.git.full_name,\n            self.repo_url,\n        )\n\n    @property\n    def quickstart_guide(self) -> str:\n        \"\"\"Generate a themed quickstart guide with sections.\"\"\"\n        quickstart = QuickStartBuilder(self.config_loader, self.repo_context)\n        content = quickstart.build()\n        _logger.info(f\"Quickstart (readmeai.generators): {content}\")\n        for section in [\n            \"Prerequisites\",\n            \"Installation\",\n            \"Usage\",\n            \"Testing\",\n        ]:\n            themed_header = self.theme_manager.apply_theme_to_section(\n                \"Getting Started\",\n                section,\n            )\n            content = content.replace(f\"### {section}\", f\"### {themed_header}\")\n\n        return content\n\n    @property\n    def contributing_guide(self) -> str:\n        \"\"\"Generate a themed contributing guide.\"\"\"\n        return self.md.contribute.format(\n            host=self.git.host,\n            host_domain=self.git.host_domain,\n            full_name=self.git.full_name,\n            repo_name=self.git.name,\n            repo_url=self.repo_url,\n        )\n\n    @property\n    def license(self) -> str:\n        \"\"\"Generate the license section.\"\"\"\n        return self.md.license.format(self.git.name.capitalize())\n\n    @property\n    def acknowledgment(self) -> str:\n        \"\"\"Generate the acknowledgment section.\"\"\"\n        return self.md.acknowledgment\n\n    def build(self) -> str:\n        \"\"\"Assembles each section of the README document in order.\"\"\"\n        sections = [\n            # -- HEADER -------------------------------------\n            self.md.top_anchor_markup,\n            self._build_header(),\n            # -- NAVIGATION ---------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Table of Contents')}\\n\\n{self.table_of_contents}\",\n            self.thematic_break,\n            # -- INTRODUCTION --------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Overview')}\\n\\n{self.md.overview}\\n\",\n            self.thematic_break,\n            # -- FEATURES -----------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Features')}\\n\\n{self.md.features}\\n\",\n            self.thematic_break,\n            # -- PROJECT STRUCTURE -------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Project Structure')}\\n\\n{self.tree}\\n\",\n            f\"### {self.theme_manager.apply_theme_to_section('Project Structure', 'Project Index')}\\n\\n{self.file_summaries}\\n\",\n            self.thematic_break,\n            # -- QUICKSTART GUIDE ----------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Getting Started')}\\n\\n{self.quickstart_guide}\",\n            self.thematic_break,\n            # -- ROADMAP ------------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Roadmap')}\\n\\n{self.md.roadmap}\",\n            self.thematic_break,\n            # -- CONTRIBUTING --------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Contributing')}\\n\\n{self.contributing_guide}\",\n            self.thematic_break,\n            # -- LICENSE ------------------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('License')}\\n\\n{self.license}\",\n            self.thematic_break,\n            # -- ACKNOWLEDGMENTS -----------------------------\n            f\"## {self.theme_manager.apply_theme_to_section('Acknowledgment')}\\n\\n{self.acknowledgment}\",\n            # -- FOOTER -------------------------------------\n            self.md.return_to_top_markup,\n            self.thematic_break,\n        ]\n        return \"\\n\".join(sections)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\builder.py",
    "type": "class",
    "name": "MarkdownBuilder",
    "loc": 20,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\emojis.py:function:normalize_section_name:chunk1",
    "text": "def normalize_section_name(name: str) -> str:\n    \"\"\"Normalize a section name for consistent matching.\n\n    This function standardizes section names by:\n    1. Converting to lowercase\n    2. Replacing underscores and multiple spaces with single spaces\n    3. Removing special characters\n    4. Trimming whitespace\n\n    Example: \"Project_Index\" -> \"project index\"\n    \"\"\"\n    if not name:\n        return \"\"\n    # Convert to lowercase and replace underscores\n    normalized = str(name).lower().replace(\"_\", \" \")\n    # Remove special characters except spaces\n    normalized = re.sub(r\"[^a-z0-9\\s]\", \"\", normalized)\n    # Replace multiple spaces with single space and trim\n    normalized = \" \".join(normalized.split())\n    return normalized",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\emojis.py",
    "type": "function",
    "name": "normalize_section_name",
    "loc": 50,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\emojis.py:class:Section:chunk1",
    "text": "class Section(BaseModel):\n    \"\"\"\n    Represents a section in the README with optional subsections.\n    \"\"\"\n\n    title: str\n    subsections: Optional[List[\"Section\"]] = None\n\n    def get_emoji(self) -> Optional[str]:\n        \"\"\"Extract emoji prefix from section title if present.\"\"\"\n        # Match emoji characters at the start of the title\n        emoji_pattern = re.compile(\n            r\"^[\"\n            \"\\U0001f300-\\U0001f5ff\"  # Symbols & pictographs\n            \"\\U0001f600-\\U0001f64f\"  # Emoticons\n            \"\\U0001f680-\\U0001f6ff\"  # Transport & map symbols\n            \"\\U0001f700-\\U0001f77f\"  # Alchemical symbols\n            \"\\U0001f780-\\U0001f7ff\"  # Geometric Shapes\n            \"\\U0001f800-\\U0001f8ff\"  # Supplemental Arrows-C\n            \"\\U0001f900-\\U0001f9ff\"  # Supplemental Symbols and Pictographs\n            \"\\U0001fa00-\\U0001fa6f\"  # Chess Symbols\n            \"\\U0001fa70-\\U0001faff\"  # Symbols and Pictographs Extended-A\n            \"\\U00002702-\\U000027b0\"  # Dingbats\n            \"]+\\\\s\"  # Must be followed by whitespace\n        )\n\n        match = emoji_pattern.search(self.title)\n        return match.group(0).strip() if match else None\n\n    def get_clean_title(self) -> str:\n        \"\"\"Get title without emoji prefix.\"\"\"\n        emoji = self.get_emoji()\n        return self.title[len(emoji) :].strip() if emoji else self.title",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\emojis.py",
    "type": "class",
    "name": "Section",
    "loc": 15,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\emojis.py:class:Theme:chunk1",
    "text": "class Theme(BaseModel):\n    \"\"\"Represents a complete emoji theme with metadata and sections.\"\"\"\n\n    name: str\n    description: str\n    sections: List[Section]\n\n    def get_section(self, section_name: str) -> Optional[Section]:\n        \"\"\"Find a section by its name with robust matching.\"\"\"\n        if not section_name:\n            return None\n\n        # Normalize the input section name\n        normalized_target = normalize_section_name(section_name)\n\n        # First try exact matching with normalized names\n        for section in self.sections:\n            normalized_section = normalize_section_name(section.get_clean_title())\n            if normalized_section == normalized_target:\n                return section\n\n        # If no exact match, try partial matching for special cases\n        # This helps match variations like \"project_index\" with \"Project Index\"\n        for section in self.sections:\n            normalized_section = normalize_section_name(section.get_clean_title())\n            if (\n                normalized_target in normalized_section\n                or normalized_section in normalized_target\n            ):\n                return section\n\n        return None\n\n    def get_subsection(\n        self, section_name: str, subsection_name: str\n    ) -> Optional[Section]:\n        \"\"\"Find a subsection within a specific section.\"\"\"\n        section = self.get_section(section_name)\n        if not section or not section.subsections:\n            return None\n\n        normalized_target = normalize_section_name(subsection_name)\n\n        for subsection in section.subsections:\n            normalized_subsection = normalize_section_name(subsection.get_clean_title())\n            if normalized_subsection == normalized_target:\n                return subsection\n\n        return None",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\emojis.py",
    "type": "class",
    "name": "Theme",
    "loc": 72,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\emojis.py:class:ThemeRegistry:chunk1",
    "text": "class ThemeRegistry(BaseModel):\n    \"\"\"Container for all available emoji themes.\"\"\"\n\n    themes: Dict[str, Theme] = Field(default_factory=dict)\n\n    def get_theme(self, theme_name: str) -> Theme:\n        \"\"\"Get a theme by name, falling back to default if not found.\"\"\"\n        return self.themes.get(theme_name, self.themes.get(\"default\"))\n\n    def list_themes(self) -> List[Dict[str, str]]:\n        \"\"\"List all available themes with their descriptions.\"\"\"\n        return [\n            {\n                \"id\": theme_id,\n                \"name\": theme.name,\n                \"description\": theme.description,\n            }\n            for theme_id, theme in self.themes.items()\n        ]",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\emojis.py",
    "type": "class",
    "name": "ThemeRegistry",
    "loc": 123,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\emojis.py:class:ThemeManager:chunk1",
    "text": "class ThemeManager:\n    \"\"\"Manages emoji themes for README documentation.\"\"\"\n\n    def __init__(self, config: Settings) -> None:\n        \"\"\"Initialize theme manager with configuration.\"\"\"\n        self.config = config\n        self.theme_registry = self._load_themes()\n        self.current_theme = self.theme_registry.get_theme(config.md.emojis)\n        self._header_cache: Dict[str, str] = {}\n\n    def _load_themes(self) -> ThemeRegistry:\n        \"\"\"Load all themes from the YAML configuration.\"\"\"\n        try:\n            theme_path = build_resource_path(\n                file_path=\"themes/emojis.yaml\",\n                module=\"readmeai.config\",\n                submodule=\"settings\",\n            )\n\n            with open(theme_path, encoding=\"utf-8\") as f:\n                theme_data = yaml.safe_load(f)\n\n            themes = {\n                theme_id: Theme(**theme_config)\n                for theme_id, theme_config in theme_data.get(\"themes\", {}).items()\n            }\n\n            return ThemeRegistry(themes=themes)\n\n        except Exception as e:\n            logger.error(f\"Error loading themes: {e}\")\n            return ThemeRegistry(\n                themes={\n                    \"default\": Theme(\n                        name=\"Default Theme\",\n                        description=\"Basic documentation theme without emojis\",\n                        sections=[\n                            Section(title=\"Table of Contents\"),\n                            Section(title=\"Overview\"),\n                            Section(title=\"Features\"),\n                            Section(\n                                title=\"Project Structure\",\n                                subsections=[Section(title=\"Project Index\")],\n                            ),\n                            Section(\n                                title=\"Getting Started\",\n                                subsections=[\n                                    Section(title=\"Prerequisites\"),\n                                    Section(title=\"Installation\"),\n                                    Section(title=\"Usage\"),\n                                    Section(title=\"Testing\"),\n                                ],\n                            ),\n                            Section(title=\"Roadmap\"),\n                            Section(title=\"Contributing\"),\n                            Section(title=\"License\"),\n                            Section(title=\"Acknowledgment\"),\n                        ],\n                    )\n                }\n            )\n\n    def apply_theme_to_headers(self, headers: Dict[str, str]) -> Dict[str, str]:\n        \"\"\"Apply the current theme to a set of section headers.\"\"\"\n        themed_headers = {}\n\n        for key, value in headers.items():\n            # Handle both direct section names and header keys\n            section_name = self._header_key_to_section(key)\n\n            if not section_name:\n                # If not a header key, try using the key directly as a section name\n                section_name = key\n\n            # Look up the themed version and cache it\n            cache_key = f\"header::{section_name}\"\n            if cache_key not in self._header_cache:\n                section = self.current_theme.get_section(section_name)\n                self._header_cache[cache_key] = section.title if section else value\n\n            themed_headers[key] = self._header_cache[cache_key]\n\n        return themed_headers\n\n    def apply_theme_to_section(\n        self, section_name: str, subsection_name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the themed title for a specific section/subsection.\"\"\"\n        cache_key = (\n            f\"section::{section_name}::{subsection_name}\"\n            if subsection_name\n            else f\"section::{section_name}\"\n        )\n\n        if cache_key not in self._header_cache:\n            if subsection_name:\n                section = self.current_theme.get_subsection(\n                    section_name, subsection_name\n                )\n            else:\n                section = self.current_theme.get_section(section_name)\n\n            self._header_cache[cache_key] = section.title if section else section_name\n\n        return self._header_cache[cache_key]\n\n    def get_themed_toc(self) -> Dict[str, list]:\n        \"\"\"Generate themed table of contents data.\"\"\"\n        return {\n            \"sections\": [\n                {\n                    \"title\": section.title,\n                    \"subsections\": [\n                        {\"title\": sub.title} for sub in (section.subsections or [])\n                    ]\n                    if section.subsections\n                    else None,\n                }\n                for section in self.current_theme.sections\n            ]\n        }\n\n    @staticmethod\n    def _header_key_to_section(key: str) -> Optional[str]:\n        \"\"\"Convert a header key to a section name.\"\"\"\n        # Special handling for known problematic sections\n        if key.lower() in {\"project_index\", \"acknowledgment\"}:\n            return key.replace(\"_\", \" \").title()\n\n        if key.endswith(\"_HEADER\"):\n            # Remove _HEADER suffix and convert to title case\n            return \" \".join(\n                word.capitalize() for word in key[:-7].lower().replace(\"_\", \" \").split()\n            )\n\n        return None",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\emojis.py",
    "type": "class",
    "name": "ThemeManager",
    "loc": 144,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:BadgeStyles:chunk1",
    "text": "class BadgeStyles(str, Enum):\n    \"\"\"\n    Badge icon styles for the project README.\n    \"\"\"\n\n    DEFAULT = \"default\"\n    FLAT = \"flat\"\n    FLAT_SQUARE = \"flat-square\"\n    FOR_THE_BADGE = \"for-the-badge\"\n    PLASTIC = \"plastic\"\n    SKILLS = \"skills\"\n    SKILLS_LIGHT = \"skills-light\"\n    SOCIAL = \"social\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "BadgeStyles",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:CustomLogos:chunk1",
    "text": "class CustomLogos(str, Enum):\n    \"\"\"\n    Options for custom/external project logo files.\n    \"\"\"\n\n    CUSTOM = \"CUSTOM\"\n    LLM = \"LLM\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "CustomLogos",
    "loc": 21,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:DefaultLogos:chunk1",
    "text": "class DefaultLogos(str, Enum):\n    \"\"\"\n    Predefined SVG project logo options.\n    \"\"\"\n\n    ANIMATED = \"readmeai/assets/logos/animated.svg\"\n    AURORA = \"readmeai/assets/logos/aurora.svg\"\n    BLUE = \"readmeai/assets/logos/blue.svg\"\n    GREEN = \"readmeai/assets/logos/green.svg\"\n    ICE = \"readmeai/assets/logos/ice.svg\"\n    METALLIC = \"readmeai/assets/logos/metallic.svg\"\n    MIDNIGHT = \"readmeai/assets/logos/midnight.svg\"\n    ORANGE = \"readmeai/assets/logos/orange.svg\"\n    PURPLE = \"readmeai/assets/logos/purple.svg\"\n    RAINBOW = \"readmeai/assets/logos/rainbow.svg\"\n    TERMINAL = \"readmeai/assets/logos/terminal.svg\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "DefaultLogos",
    "loc": 30,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:EmojiThemes:chunk1",
    "text": "class EmojiThemes(str, Enum):\n    \"\"\"\n    Emoji theme 'packs' for customizing header section titles.\n    \"\"\"\n\n    # -- Core\n    DEFAULT = \"default\"\n    MINIMAL = \"minimal\"\n    # OSS = \"oss\"\n\n    # -- Development\n    # API = \"api\"\n    # GAME = \"game\"\n    # MOBILE = \"mobile\"\n    # WEB = \"web\"\n\n    # -- Infrastructure\n    # CLOUD = \"cloud\"\n    # CYBER = \"cyber\"\n    # IOT = \"iot\"\n\n    # -- Data and AI\n    # DATA = \"data\"\n    # ML = \"ml\"\n\n    # -- Geometric and Abstract\n    ASCENSION = \"ascension\"\n    FIBONACCI = \"fibonacci\"\n    HARMONY = \"harmony\"\n    PRISM = \"prism\"\n    QUANTUM = \"quantum\"\n\n    # -- Monochrome and Unicode\n    MONOCHROME = \"monochrome\"\n    UNICODE = \"unicode\"\n\n    # -- Nature and Elements\n    ATOMIC = \"atomic\"\n    COSMIC = \"cosmic\"\n    CRYSTAL = \"crystal\"\n    EARTH = \"earth\"\n    FIRE = \"fire\"\n    FOREST = \"forest\"\n    NATURE = \"nature\"\n    RAINBOW = \"rainbow\"\n    SOLAR = \"solar\"\n    WATER = \"water\"\n\n    # -- Miscellaneous\n    FUN = \"fun\"\n    VINTAGE = \"vintage\"\n    ZEN = \"zen\"\n\n    # -- Random\n    RANDOM = \"random\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "EmojiThemes",
    "loc": 48,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:HeaderStyles:chunk1",
    "text": "class HeaderStyles(str, Enum):\n    \"\"\"\n    Header style template options for the README file.\n    \"\"\"\n\n    ASCII = \"ascii\"\n    ASCII_BOX = \"ascii_box\"\n    BANNER = \"banner\"\n    CLASSIC = \"classic\"\n    CLEAN = \"clean\"\n    COMPACT = \"compact\"\n    CONSOLE = \"console\"\n    MODERN = \"modern\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "HeaderStyles",
    "loc": 105,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\enums.py:class:NavigationStyles:chunk1",
    "text": "class NavigationStyles(str, Enum):\n    \"\"\"\n    Navigation menu styles for the README file.\n    \"\"\"\n\n    ACCORDION = \"accordion\"\n    BULLET = \"bullet\"\n    NUMBER = \"number\"\n    ROMAN = \"roman\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\enums.py",
    "type": "class",
    "name": "NavigationStyles",
    "loc": 120,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:function:normalize_section_name:chunk1",
    "text": "def normalize_section_name(name: str) -> str:\n    \"\"\"Normalize section name for header generation.\"\"\"\n    normalized = name.lower()\n    normalized = re.sub(r\"[^a-z0-9_]\", \"_\", normalized)\n    normalized = re.sub(r\"_+\", \"_\", normalized)\n    return normalized.strip(\"_\")",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "function",
    "name": "normalize_section_name",
    "loc": 208,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:class:SectionType:chunk1",
    "text": "class SectionType(str, Enum):\n    \"\"\"\n    Standard section types for documentation.\n    \"\"\"\n\n    TOC = \"table_of_contents\"\n    OVERVIEW = \"overview\"\n    FEATURES = \"features\"\n    STRUCTURE = \"project_structure\"\n    GETTING_STARTED = \"getting_started\"\n    ROADMAP = \"roadmap\"\n    CONTRIBUTING = \"contributing\"\n    LICENSE = \"license\"\n    ACKNOWLEDGMENTS = \"acknowledgments\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "class",
    "name": "SectionType",
    "loc": 13,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:class:HeaderData:chunk1",
    "text": "class HeaderData(TypedDict):\n    \"\"\"\n    Type definition for header style template data.\n    \"\"\"\n\n    align: str\n    logo: str\n    logo_size: str\n    repo_name: str\n    tagline: str\n    shields_icons: str\n    tech_stack_description: str\n    tech_stack_icons: str",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "class",
    "name": "HeaderData",
    "loc": 54,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:class:HeaderTemplate:chunk1",
    "text": "class HeaderTemplate(BaseModel):\n    \"\"\"\n    Define the header templates for the README file.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n        use_enum_values=True,\n    )\n\n    file_handler: FileHandler = FileHandler()\n    style: str = HeaderStyles.CLASSIC\n    header_styles: HeaderStyles = file_handler.read(\n        \"readmeai/config/settings/templates/headers.toml\"\n    )\n    templates: Dict[HeaderStyles, str] = {\n        HeaderStyles(k): v[\"template\"]\n        for k, v in header_styles[\"header_styles\"].items()\n    }\n\n    @computed_field\n    def get_template(self) -> str:\n        \"\"\"Fetch the selected header style template.\"\"\"\n        return self.templates.get(\n            HeaderStyles(self.style), self.templates[HeaderStyles.CLASSIC]\n        )\n\n    def render(self, header_data: HeaderData) -> str:\n        \"\"\"Render the header template with provided data.\"\"\"\n        template = self.templates.get(\n            HeaderStyles(self.style), self.templates[HeaderStyles.CLASSIC]\n        )\n        return template.format(**header_data)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "class",
    "name": "HeaderTemplate",
    "loc": 69,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:class:HeaderConfig:chunk1",
    "text": "class HeaderConfig(BaseModel):\n    \"\"\"\n    Configuration for header styling and theming.\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n        populate_by_name=True,\n        use_enum_values=True,\n    )\n\n    section: SectionType = Field(default=SectionType.OVERVIEW)\n    variants: List[str] = []\n    themed_title: Optional[str] = None\n    plain_title: str\n    level: int = Field(default=2, ge=1, le=6, description=\"Markdown header level\")\n\n    @property\n    def markdown_prefix(self) -> str:\n        \"\"\"Generate markdown header prefix based on level\"\"\"\n        return \"#\" * self.level",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "class",
    "name": "HeaderConfig",
    "loc": 105,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\headers.py:class:HeaderRegistry:chunk1",
    "text": "class HeaderRegistry:\n    \"\"\"\n    Centralized registry for managing header templates and configurations.\n    \"\"\"\n\n    def __init__(self, emoji_theme: str, theme_data: dict) -> None:\n        self.emoji_theme = emoji_theme\n        self.theme_mapping = theme_data.get(\"mapping\", {})\n        self.section_aliases = SECTION_ALIASES\n        self.headers: Dict[str, HeaderConfig] = {}\n        self.theme_data = theme_data.get(\"mapping\", {})\n\n    def prepare_section_data(self, sections: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Prepare section data with proper hierarchical structure for navigation.\"\"\"\n        formatted_sections = []\n        for section in sections:\n            formatted_section = {\n                \"title\": section[\"title\"],\n                \"level\": 0,\n                \"subsections\": [],\n            }\n\n            if \"subsections\" in section:\n                for subsection in section[\"subsections\"]:\n                    formatted_subsection = {\"title\": subsection[\"title\"], \"level\": 1}\n                    formatted_section[\"subsections\"].append(formatted_subsection)\n\n            formatted_sections.append(formatted_section)\n\n        return {\"sections\": formatted_sections}\n\n    def get_themed_title(self, section: str) -> str:\n        \"\"\"Get themed version of section title with proper formatting.\"\"\"\n        normalized_section = normalize_section_name(section)\n        standard_section = self.section_aliases.get(normalized_section, section)\n        themed_title = self.theme_mapping.get(standard_section, section)\n\n        # Ensure proper spacing after emoji if present\n        emoji = self._extract_emoji(themed_title)\n        if emoji:\n            clean_title = self._strip_emoji(themed_title)\n            return f\"{emoji} {clean_title}\"\n        return themed_title\n\n    def _extract_emoji(self, text: str) -> str:\n        \"\"\"Extract emoji from beginning of text if present.\"\"\"\n        emoji_pattern = re.compile(\n            r\"^(?:\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\"\n        )\n        match = emoji_pattern.search(text)\n        return match.group(0) if match else \"\"\n\n    def _strip_emoji(self, text: str) -> str:\n        \"\"\"Remove emoji from beginning of text.\"\"\"\n        emoji_pattern = re.compile(\n            r\"^(?:\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\"\n        )\n        return emoji_pattern.sub(\"\", text).strip()\n\n    @staticmethod\n    def _remove_emoji(text: str) -> str:\n        \"\"\"\n        Remove emoji characters from text while preserving the rest of the string.\n        Uses regex pattern matching to identify and remove emoji unicode ranges.\n        \"\"\"\n        emoji_pattern = re.compile(\n            r\"[\"\n            r\"\\U0001f600-\\U0001f64f\"  # emoticons\n            r\"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n            r\"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n            r\"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n            r\"\\U00002702-\\U000027b0\"  # dingbats\n            r\"\\U000024c2-\\U0001f251\"\n            r\"]+\",\n            flags=re.UNICODE,\n        )\n        return emoji_pattern.sub(\"\", text)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\headers.py",
    "type": "class",
    "name": "HeaderRegistry",
    "loc": 129,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\navigation.py:class:NavigationTemplate:chunk1",
    "text": "class NavigationTemplate:\n    \"\"\"\n    Render a table of contents or navigation menu for the README file.\n    \"\"\"\n\n    def __init__(self, style: str, header_registry: HeaderRegistry) -> None:\n        try:\n            self.style = NavigationStyles(style.lower())\n        except ValueError:\n            logger.warning(f\"Invalid navigation style: {style}. Defaulting to bullet.\")\n            self.style = NavigationStyles.BULLET\n\n        self.header_registry = header_registry\n        self.using_emojis = header_registry.emoji_theme.lower()\n        logger.debug(f\"Initialized navigation style: {self.style}\")\n        logger.debug(f\"Using emojis: {self.using_emojis}\")\n\n    def render(self, data: Dict[str, Any]) -> str:\n        \"\"\"Render complete table of contents with proper markdown formatting.\"\"\"\n        toc_lines = []\n\n        sections = data.get(\"sections\", [])\n        for i, section in enumerate(sections, 1):\n            title = self.header_registry.get_themed_title(section[\"title\"])\n            toc_lines.append(self._format_link(title, level=0, index=i))\n\n            if section.get(\"subsections\"):\n                for j, subsection in enumerate(section[\"subsections\"], 1):\n                    sub_title = self.header_registry.get_themed_title(\n                        subsection[\"title\"]\n                    )\n                    toc_lines.append(\n                        self._format_link(\n                            sub_title,\n                            level=1,\n                            index=i,\n                            sub_index=j,\n                        )\n                    )\n\n        content = \"\".join(toc_lines)\n\n        if self.style == NavigationStyles.ACCORDION:\n            summary_title = self.header_registry.get_themed_title(\"Table of Contents\")\n            return (\n                \"<details>\\n\"\n                f\"<summary>{summary_title}</summary>\\n\\n\"\n                f\"{content}\"\n                \"\\n</details>\\n\"\n            )\n\n        return content\n\n    def _format_link(\n        self,\n        title: str,\n        level: int = 0,\n        index: Optional[int] = None,\n        sub_index: Optional[int] = None,\n    ) -> str:\n        \"\"\"Format a single navigation link with proper markdown formatting.\"\"\"\n        display_title = title.strip()\n        clean_anchor = self._generate_anchor(title)\n        indent = \"    \" * level if level > 0 else \"\"\n\n        # Only add dash prefix to anchor if emojis are explicitly enabled\n        anchor = f\"-{clean_anchor}\" if self.using_emojis else clean_anchor\n        if self.using_emojis == \"default\":\n            anchor = anchor.lstrip(\"-\")\n\n        if self.style == NavigationStyles.NUMBER:\n            if sub_index is not None:\n                return f\"{indent}{index}.{sub_index}. [{display_title}](#{anchor})\\n\"\n            return f\"{indent}{index}. [{display_title}](#{anchor})\\n\"\n\n        elif self.style == NavigationStyles.ROMAN:\n            if sub_index is not None:\n                # Explicitly set 4 spaces for proper rendering\n                indent_ = \"&nbsp;&nbsp;&nbsp;&nbsp;\"\n                # 97 is 'a' in ASCII\n                subsection_letter = chr(96 + sub_index)\n                prefix = f\"{indent_}{self._to_roman(index)}.{subsection_letter}.\"\n                # Reset indent since included in prefix\n                indent = \"\"\n            else:\n                prefix = f\"{self._to_roman(index)}.\"\n            return f\"{indent}{prefix} [{display_title}](#{anchor})<br>\\n\"\n        else:\n            prefix = \"-\"\n            return f\"{indent}{prefix} [{display_title}](#{anchor})\\n\"\n\n    def _generate_anchor(self, title: str) -> str:\n        \"\"\"\n        Generate GitHub-compatible anchor link.\n\n        This follows GitHub's heading ID generation rules:\n        1. Remove special characters (except alphanumeric, spaces, hyphens)\n        2. Replace spaces with hyphens\n        3. Convert to lowercase\n        4. Remove extra hyphens\n        \"\"\"\n        clean_title = self.header_registry._strip_emoji(title).strip()\n        anchor = re.sub(r\"[^\\w\\s-]\", \"\", clean_title)\n        anchor = re.sub(r\"\\s+\", \" \", anchor).strip()\n        anchor = anchor.lower()\n        anchor = re.sub(r\"\\s\", \"-\", anchor)\n        anchor = re.sub(r\"-+\", \"-\", anchor)\n        return anchor\n\n    def _to_roman(self, num: int) -> str:\n        \"\"\"Convert integer to Roman numeral.\"\"\"\n        val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n        syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n        roman_num = \"\"\n        i = 0\n        while num > 0:\n            for _ in range(num // val[i]):\n                roman_num += syb[i]\n                num -= val[i]\n            i += 1\n        return roman_num",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\navigation.py",
    "type": "class",
    "name": "NavigationTemplate",
    "loc": 13,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\quickstart.py:class:QuickStartBuilder:chunk1",
    "text": "class QuickStartBuilder:\n    \"\"\"\n    Dynamically construct the install, ssage, and testing Qucickstart guides.\n    \"\"\"\n\n    def __init__(\n        self, config_loader: ConfigLoader, repo_context: RepositoryContext\n    ) -> None:\n        self.config_loader = config_loader\n        self.config = config_loader.config\n        self.repo_context = repo_context\n        self.git = self.config.git\n        self.md = self.config.md\n        self.user_guides = self._load_user_guides()\n\n    def _load_user_guides(self) -> dict:\n        \"\"\"Load the user guide templates from the config file.\"\"\"\n        config_path = (\n            Path(__file__).parent.parent\n            / \"config\"\n            / \"settings\"\n            / self.config.files.quickstart_guides\n        )\n        with open(config_path, \"rb\") as f:\n            return tomllib.load(f)\n\n    def build(self) -> str:\n        \"\"\"Create the Installation, Usage, and Testing instructions.\"\"\"\n        repo_url = (\n            f\"../{self.git.name}\"\n            if self.git.host_domain.lower() == \"local\"\n            else self.git.repository\n        )\n        usage_guides = QuickStartGenerator(self.config_loader).generate(\n            self.repo_context.language_counts, self.repo_context.metadata\n        )\n\n        main_template = Template(self.user_guides[\"templates\"][\"main\"])\n        return main_template.safe_substitute(\n            prerequisites_section=self._format_prerequisites(usage_guides),\n            installation_section=self._format_installation(usage_guides, repo_url),\n            usage_section=self._format_usage(usage_guides),\n            testing_section=self._format_testing(usage_guides),\n        )\n\n    def _format_prerequisites(self, usage_guides: QuickStart) -> str:\n        template = Template(self.user_guides[\"templates\"][\"prerequisites\"])\n        return template.safe_substitute(\n            repo_name=self.git.name,\n            system_requirements=self._format_system_requirements(usage_guides),\n        )\n\n    def _format_system_requirements(self, usage_guides: QuickStart) -> str:\n        formatting = self.user_guides[\"formatting\"]\n        requirements = [\n            formatting[\"system_requirements_prefix\"].replace(\n                \"$key\", \"Programming Language\"\n            )\n            + usage_guides.primary_language\n        ]\n        if usage_guides.package_managers:\n            pkg_managers = \", \".join(\n                tool.capitalize() for tool in usage_guides.package_managers\n            )\n            requirements.append(\n                formatting[\"system_requirements_prefix\"].replace(\n                    \"$key\", formatting[\"package_managers_label\"]\n                )\n                + pkg_managers\n            )\n        if usage_guides.containers:\n            containers = \", \".join(\n                tool.capitalize() for tool in usage_guides.containers\n            )\n            requirements.append(\n                formatting[\"system_requirements_prefix\"].replace(\n                    \"$key\", formatting[\"containers_label\"]\n                )\n                + containers\n            )\n        return \"\\n\".join(requirements)\n\n    def _format_installation(self, usage_guide: QuickStart, repo_url: str) -> str:\n        template = Template(self.user_guides[\"templates\"][\"installation\"])\n        install_steps_template = Template(self.user_guides[\"install_steps\"][\"template\"])\n\n        install_steps = install_steps_template.safe_substitute(\n            repo_name=self.git.name,\n            repo_url=repo_url,\n            install_commands=usage_guide.install_commands.strip()\n            or self.config_loader.dev_setup[\"default\"][\"install\"],\n        )\n        return template.safe_substitute(\n            repo_name=self.git.name,\n            header=\"Installation\",\n            install_steps=install_steps,\n        )\n\n    def _format_usage(self, usage_guides: QuickStart) -> str:\n        template = Template(self.user_guides[\"templates\"][\"usage\"])\n        usage_commands = (\n            usage_guides.usage_commands\n            or self.config_loader.dev_setup[\"default\"][\"usage\"]\n        )\n        usage_commands = usage_commands.strip()\n        return template.safe_substitute(\n            usage_instructions=f\"\"\"Run the project with:\\n\\n{usage_commands}\"\"\"\n        )\n\n    def _format_testing(self, usage_guides: QuickStart) -> str:\n        template = Template(self.user_guides[\"templates\"][\"testing\"])\n        test_commands = (\n            usage_guides.test_commands\n            or self.config_loader.dev_setup[\"default\"][\"test\"]\n        )\n        test_commands = test_commands.strip()\n        return template.safe_substitute(\n            test_instructions=(\n                f\"\"\"{self.git.name.capitalize()} uses the {{__test_framework__}} \"\"\"\n                f\"\"\"test framework. Run the test suite with:\\n\\n{test_commands}\"\"\"\n            )\n        )",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\quickstart.py",
    "type": "class",
    "name": "QuickStartBuilder",
    "loc": 19,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\quickstart.py:class:QuickStartGenerator:chunk1",
    "text": "class QuickStartGenerator:\n    \"\"\"\n    Generate Quickstart instructions for a repository.\n    \"\"\"\n\n    config: ConfigLoader\n    default_cmds: dict = field(init=False)\n    language_names: dict = field(init=False)\n    tool_names: dict = field(init=False)\n\n    def __post_init__(self):\n        self.default_cmds = self.config.dev_setup[\"default\"]\n        self.language_names = self.config.language_map.get(\"languages\", {})\n        self.tool_names = self.config.dev_setup\n\n    def generate(\n        self,\n        language_counts: dict[str, int],\n        metadata: dict[str, dict[str, str]],\n    ) -> QuickStart:\n        \"\"\"Generate all user setup guides for the project.\"\"\"\n        try:\n            primary_language = self._get_primary_language(language_counts)\n            if not primary_language:\n                primary_language = (\n                    f\"Error detecting primary_language: {language_counts}\"\n                )\n\n            quickstart = QuickStart(\n                primary_language=primary_language,\n                language_counts=language_counts,\n                package_managers=metadata.get(\"package_managers\", {}),\n                containers=metadata.get(\"containers\", {}),\n            )\n            self._generate_commands(quickstart, primary_language)\n\n        except Exception as e:\n            _logger.warning(f\"Error generating quickstart guide: {e!r}\")\n            quickstart = QuickStart()\n\n        return quickstart\n\n    def _get_primary_language(self, counts: dict[str, int]) -> str:\n        \"\"\"\n        Determine the primary language of the project based on file extensions.\n\n        Example\n        -------\n            Input counts = {'py': 10, 'txt': 5, 'json': 3, 'csv': 2}\n            Output: 'Python' (csv, json, and txt are assumed to be data files)\n        \"\"\"\n        default_language = self.language_names[\"unknown\"]\n\n        try:\n            asset_files = {\n                \"png\",\n                \"jpg\",\n                \"jpeg\",\n                \"gif\",\n                \"svg\",\n                \"ico\",\n                \"ttf\",\n                \"woff\",\n                \"woff2\",\n                \"eot\",\n            }\n            config_files = {\n                \"yaml\",\n                \"yml\",\n                \"json\",\n                \"toml\",\n                \"ini\",\n                \"cfg\",\n                \"conf\",\n                \"config\",\n                \"properties\",\n            }\n            data_files = {\n                \"txt\",\n                \"csv\",\n                \"tsv\",\n                \"log\",\n                \"md\",\n                \"rst\",\n                \"doc\",\n                \"docx\",\n                \"pdf\",\n                \"xls\",\n                \"xlsx\",\n            }\n            excluded_extensions = config_files | data_files | asset_files\n\n            valid_counts = {\n                k: v\n                for k, v in counts.items()\n                if k.lower() not in excluded_extensions and v > 0\n            }\n            if not valid_counts:\n                _logger.info(\"Failed to detect primary language of project.\")\n                top_lang = default_language\n            else:\n                top_lang = max(valid_counts, key=lambda k: valid_counts[k])\n\n            _logger.info(\n                f\"Detected primary language: {top_lang} \"\n                f\"from valid languages: {valid_counts}\"\n            )\n            return self.language_names.get(top_lang, default_language)\n\n        except Exception as e:\n            _logger.error(f\"Error detecting primary language of project: {e!r}\")\n            return default_language\n\n    def _generate_commands(self, quickstart: QuickStart, top_language: str) -> None:\n        \"\"\"Generate install, usage, and test commands.\"\"\"\n        command_types = [\"install\", \"usage\", \"test\"]\n        tool_types = [\"containers\", \"package_managers\", \"documentation\"]\n\n        for cmd_type in command_types:\n            commands: list[str] = []\n            for tool_type in tool_types:\n                tool_names = getattr(quickstart, tool_type, {})\n                if not tool_names:\n                    continue\n\n                commands.extend(\n                    filter(\n                        None,\n                        [\n                            self._format_command(\n                                tool_name=tool_name,\n                                primary_language=top_language,\n                                file_path=file_path,\n                                cmd_type=cmd_type,\n                                tool_type=tool_type,\n                            )\n                            for tool_name, file_path in tool_names.items()\n                        ],\n                    )\n                )\n            setattr(quickstart, f\"{cmd_type}_commands\", \"\\n\".join(commands))\n\n    def _format_command(\n        self,\n        tool_name: str,\n        primary_language: str,\n        file_path: str,\n        cmd_type: str,\n        tool_type: str,\n    ) -> Optional[str]:\n        \"\"\"Format a command type for a specific tool/framework.\"\"\"\n        try:\n            if not primary_language or not tool_name:\n                return None\n\n            lang_key = primary_language.lower()\n\n            conf = (\n                self.tool_names.get(lang_key, {}).get(tool_type, {}).get(tool_name, {})\n            ) or self.tool_names.get(tool_type, {}).get(tool_name, {})\n\n            shields = conf.get(\"shield\", conf.get(\"default\"))\n            website = conf.get(\"website\", conf.get(\"default\"))\n\n            cmd = conf.get(cmd_type, self.default_cmds.get(cmd_type))\n            if not cmd:\n                return None\n\n            if cmd_type == \"install\" and tool_type == \"containers\":\n                cmd = cmd.replace(\n                    \"{image_name}\", self.config.config.git.full_name or \"\"\n                )\n\n            if cmd_type in {\"install\", \"test\"} and tool_type != \"containers\":\n                cmd = cmd.replace(\"{file}\", file_path or \"\")\n\n            if cmd_type in {\"usage\", \"test\"}:\n                cmd = cmd.replace(\"{executable}\", self.config.config.git.name or \"\")\n                return f\"\"\"**Using [{tool_name}]({website}):**\\n```sh\\n{cmd}\\n```\"\"\"\n\n            return f\"\"\"<!-- SHIELDS BADGE CURRENTLY DISABLED -->\n    <!-- [![{tool_name}][{tool_name}-shield]][{tool_name}-link] -->\n    <!-- REFERENCE LINKS -->\n    <!-- [{tool_name}-shield]: {shields} -->\n    <!-- [{tool_name}-link]: {website} -->\n\n    **Using [{tool_name}]({website}):**\n\n    ```sh\n    ❯ {cmd}\n    ```\n    \"\"\".strip().replace(\"    \", \"\\t\")\n\n        except Exception as e:\n            _logger.error(f\"Error building {cmd_type} command: {e!r}\")\n            return conf.get(\"default\")",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\quickstart.py",
    "type": "class",
    "name": "QuickStartGenerator",
    "loc": 144,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:_generate_nested_module_content:chunk1",
    "text": "def _generate_nested_module_content(\n    module_data: dict[str, dict | list[tuple[str, str]]] | list[tuple[str, str]],\n    repo_path: str | Path,\n    is_local_repo: bool,\n    repo_url: str,\n    indent: str = \"\",\n) -> list[str]:\n    \"\"\"Generates nested content for modules and submodules using HTML tables.\"\"\"\n    content = []\n\n    if isinstance(module_data, list):\n        # module_data is a list of files\n        content.append(f\"{indent}<table>\")\n        content.extend(\n            _generate_table_rows(\n                module_data,\n                repo_path,\n                is_local_repo,\n                repo_url,\n                indent=indent,\n            )\n        )\n        content.append(f\"{indent}</table>\")\n    elif isinstance(module_data, dict):\n        # Check if there are files at this module level\n        files = module_data.get(\"\", [])\n        if files:\n            content.append(f\"{indent}<table>\")\n            content.extend(\n                _generate_table_rows(\n                    files,\n                    repo_path,\n                    is_local_repo,\n                    repo_url,\n                    indent=indent,\n                )\n            )\n            content.append(f\"{indent}</table>\")\n        # Process submodules\n        for submodule_name, submodule_data in module_data.items():\n            if submodule_name == \"\":\n                continue\n            content.extend((\n                f\"{indent}<details>\",\n                f\"{indent}\\t<summary><b>{submodule_name}</b></summary>\",\n                f\"{indent}\\t<blockquote>\",\n            ))\n            # Recurse into submodule\n            content.extend(\n                _generate_nested_module_content(\n                    submodule_data,\n                    repo_path,\n                    is_local_repo,\n                    repo_url,\n                    indent=indent + \"\\t\\t\",\n                )\n            )\n            content.extend((f\"{indent}\\t</blockquote>\", f\"{indent}</details>\"))\n    else:\n        _logger.warning(f\"Unexpected data type in module data: {type(module_data)}\")\n    return content",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "_generate_nested_module_content",
    "loc": 9,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:generate_nested_module_tables:chunk1",
    "text": "def generate_nested_module_tables(\n    summaries: list[tuple[str, str]],\n    project_path: str | Path,\n    repository_url: str,\n) -> str:\n    \"\"\"Create structured Markdown tables with nested submodules.\"\"\"\n    summaries_by_module = group_summaries_by_nested_module(summaries)\n    return build_submodule_disclosure_widget(\n        summaries_by_module, project_path, repository_url\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "generate_nested_module_tables",
    "loc": 72,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:build_submodule_disclosure_widget:chunk1",
    "text": "def build_submodule_disclosure_widget(\n    data: dict[str, dict | list[tuple[str, str]]],\n    repo_path: str | Path,\n    repo_url: str,\n) -> str:\n    \"\"\"\n    Builds expandable sections via <details> HTML element,\n    for each module with nested submodules using HTML tables.\n    Includes breadcrumb-style path navigation and column headers.\n    \"\"\"\n    if not data:\n        return \"\"\n\n    is_local_repo = Path(repo_path).exists()\n    project_name = (\n        Path(repo_path).name if is_local_repo else repo_url.rstrip(\"/\").split(\"/\")[-1]\n    )\n\n    sections = [\n        \"<details open>\",\n        f\"\\t<summary><b><code>{project_name.upper()}/</code></b></summary>\",\n    ]\n\n    def add_path_header(current_path: list[str], indent: str) -> list[str]:\n        \"\"\"Creates a header showing the current directory path.\"\"\"\n        if not current_path:\n            return []\n        path_parts = \".\".join(current_path)\n        return [\n            f\"{indent}<div class='directory-path' style='padding: 8px 0; color: #666;'>\",\n            f\"{indent}\\t<code><b>⦿ {path_parts}</b></code>\",\n        ]\n\n    def create_table_header(indent: str) -> list[str]:\n        \"\"\"Creates a standardized table header.\"\"\"\n        return [\n            f\"{indent}<table style='width: 100%; border-collapse: collapse;'>\",\n            f\"{indent}<thead>\",\n            f\"{indent}\\t<tr style='background-color: #f8f9fa;'>\",\n            f\"{indent}\\t\\t<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>\",\n            f\"{indent}\\t\\t<th style='text-align: left; padding: 8px;'>Summary</th>\",\n            f\"{indent}\\t</tr>\",\n            f\"{indent}</thead>\",\n        ]\n\n    def process_module(\n        module_name: str, module_data: dict | list, current_path: list[str], indent: str\n    ) -> list[str]:\n        \"\"\"Processes a module and its contents recursively.\"\"\"\n        section = []\n\n        # Add module header with current path\n        section.extend([\n            f\"{indent}<!-- {module_name} Submodule -->\",\n            f\"{indent}<details>\",\n            f\"{indent}\\t<summary><b>{module_name}</b></summary>\",\n            f\"{indent}\\t<blockquote>\",\n        ])\n\n        # Add path navigation\n        section.extend(add_path_header(current_path + [module_name], indent + \"\\t\\t\"))\n\n        if isinstance(module_data, list):\n            # Handle files at this level\n            section.extend(create_table_header(indent + \"\\t\\t\"))\n            section.extend(\n                _generate_table_rows(\n                    module_data,\n                    repo_path,\n                    is_local_repo,\n                    repo_url,\n                    indent=indent + \"\\t\\t\\t\",\n                )\n            )\n            section.append(f\"{indent}\\t\\t</table>\")\n        elif isinstance(module_data, dict):\n            # Handle nested structure\n            files = module_data.get(\"\", [])\n            if files:\n                section.extend(create_table_header(indent + \"\\t\\t\"))\n                section.extend(\n                    _generate_table_rows(\n                        files,\n                        repo_path,\n                        is_local_repo,\n                        repo_url,\n                        indent=indent + \"\\t\\t\\t\",\n                    )\n                )\n                section.append(f\"{indent}\\t\\t</table>\")\n\n            # Process submodules\n            for submodule_name, submodule_data in module_data.items():\n                if submodule_name == \"\":\n                    continue\n                section.extend(\n                    process_module(\n                        submodule_name,\n                        submodule_data,\n                        current_path + [module_name],\n                        indent + \"\\t\\t\",\n                    )\n                )\n\n        section.extend([f\"{indent}\\t</blockquote>\", f\"{indent}</details>\"])\n        return section\n\n    # Process each top-level module\n    for module_name, module_data in data.items():\n        sections.extend(process_module(module_name, module_data, [], \"\\t\"))\n\n    sections.append(\"</details>\")\n    return \"\\n\".join(sections)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "build_submodule_disclosure_widget",
    "loc": 84,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:add_path_header:chunk1",
    "text": "def add_path_header(current_path: list[str], indent: str) -> list[str]:\n        \"\"\"Creates a header showing the current directory path.\"\"\"\n        if not current_path:\n            return []\n        path_parts = \".\".join(current_path)\n        return [\n            f\"{indent}<div class='directory-path' style='padding: 8px 0; color: #666;'>\",\n            f\"{indent}\\t<code><b>⦿ {path_parts}</b></code>\",\n        ]",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "add_path_header",
    "loc": 107,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:create_table_header:chunk1",
    "text": "def create_table_header(indent: str) -> list[str]:\n        \"\"\"Creates a standardized table header.\"\"\"\n        return [\n            f\"{indent}<table style='width: 100%; border-collapse: collapse;'>\",\n            f\"{indent}<thead>\",\n            f\"{indent}\\t<tr style='background-color: #f8f9fa;'>\",\n            f\"{indent}\\t\\t<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>\",\n            f\"{indent}\\t\\t<th style='text-align: left; padding: 8px;'>Summary</th>\",\n            f\"{indent}\\t</tr>\",\n            f\"{indent}</thead>\",\n        ]",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "create_table_header",
    "loc": 117,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:process_module:chunk1",
    "text": "def process_module(\n        module_name: str, module_data: dict | list, current_path: list[str], indent: str\n    ) -> list[str]:\n        \"\"\"Processes a module and its contents recursively.\"\"\"\n        section = []\n\n        # Add module header with current path\n        section.extend([\n            f\"{indent}<!-- {module_name} Submodule -->\",\n            f\"{indent}<details>\",\n            f\"{indent}\\t<summary><b>{module_name}</b></summary>\",\n            f\"{indent}\\t<blockquote>\",\n        ])\n\n        # Add path navigation\n        section.extend(add_path_header(current_path + [module_name], indent + \"\\t\\t\"))\n\n        if isinstance(module_data, list):\n            # Handle files at this level\n            section.extend(create_table_header(indent + \"\\t\\t\"))\n            section.extend(\n                _generate_table_rows(\n                    module_data,\n                    repo_path,\n                    is_local_repo,\n                    repo_url,\n                    indent=indent + \"\\t\\t\\t\",\n                )\n            )\n            section.append(f\"{indent}\\t\\t</table>\")\n        elif isinstance(module_data, dict):\n            # Handle nested structure\n            files = module_data.get(\"\", [])\n            if files:\n                section.extend(create_table_header(indent + \"\\t\\t\"))\n                section.extend(\n                    _generate_table_rows(\n                        files,\n                        repo_path,\n                        is_local_repo,\n                        repo_url,\n                        indent=indent + \"\\t\\t\\t\",\n                    )\n                )\n                section.append(f\"{indent}\\t\\t</table>\")\n\n            # Process submodules\n            for submodule_name, submodule_data in module_data.items():\n                if submodule_name == \"\":\n                    continue\n                section.extend(\n                    process_module(\n                        submodule_name,\n                        submodule_data,\n                        current_path + [module_name],\n                        indent + \"\\t\\t\",\n                    )\n                )\n\n        section.extend([f\"{indent}\\t</blockquote>\", f\"{indent}</details>\"])\n        return section",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "process_module",
    "loc": 129,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:_generate_table_rows:chunk1",
    "text": "def _generate_table_rows(\n    files: list[tuple[str, str]],\n    repo_path: str | Path,\n    is_local_repo: bool,\n    repo_url: str,\n    indent: str = \"\",\n) -> list[str]:\n    \"\"\"Generates table rows for files with improved styling.\"\"\"\n    content = []\n    for file, summary in files:\n        file_name = Path(file).name\n        file_link = _get_file_hyperlink(file, repo_path, is_local_repo, repo_url)\n        formatted_summary = format_summary(summary)\n        content.append(\n            f\"{indent}<tr style='border-bottom: 1px solid #eee;'>\"\n            f\"\\n{indent}\\t<td style='padding: 8px;'><b><a href='{file_link}'>{file_name}</a></b></td>\"\n            f\"\\n{indent}\\t<td style='padding: 8px;'>{formatted_summary}</td>\"\n            f\"\\n{indent}</tr>\"\n        )\n    return content",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "_generate_table_rows",
    "loc": 199,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:_get_file_hyperlink:chunk1",
    "text": "def _get_file_hyperlink(\n    file_path_str: str,\n    repo_path: str | Path,\n    is_local: bool,\n    repo_url: str,\n) -> str:\n    \"\"\"Generates a hyperlink to the file in the remote git repository.\"\"\"\n    if not is_local:\n        return f\"{repo_url.rstrip('/')}/blob/master/{file_path_str}\"\n    file_path = Path(repo_path) / file_path_str\n    return f\"{file_path.as_posix()}\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "_get_file_hyperlink",
    "loc": 221,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:group_summaries_by_nested_module:chunk1",
    "text": "def group_summaries_by_nested_module(\n    summaries: list[tuple[str, str]],\n) -> dict[str, dict | list[tuple[str, str]]]:\n    \"\"\"Group code summaries by their nested module structure.\"\"\"\n    module_map: dict[str, dict | list[tuple[str, str]]] = {\"__root__\": []}\n\n    for module, summary in summaries:\n        if not isinstance(module, str) or not isinstance(summary, str):\n            _logger.error(f\"Invalid entry: ({module}, {summary}). Skipping...\")\n            continue\n\n        parts = Path(module).parts\n\n        if len(parts) == 1:\n            module_map[\"__root__\"].append((module, summary))\n        else:\n            current = module_map\n            for part in parts[:-1]:\n                current = current.setdefault(part, {})\n            current.setdefault(\"\", []).append((module, summary))\n\n    return module_map",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "group_summaries_by_nested_module",
    "loc": 234,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:format_code_summaries:chunk1",
    "text": "def format_code_summaries(\n    placeholder: str,\n    code_summaries: list[tuple[str, str]],\n) -> list:\n    \"\"\"Converts the given code summaries into a formatted list.\"\"\"\n    return [\n        (module, summary_text)\n        if isinstance(summary, tuple) and len(summary) == 2\n        else (summary, placeholder)\n        for summary in code_summaries\n        for module, summary_text in (\n            [summary]\n            if isinstance(summary, tuple) and len(summary) == 2\n            else [(summary, placeholder)]\n        )\n    ]",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "format_code_summaries",
    "loc": 258,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tables.py:function:format_summary:chunk1",
    "text": "def format_summary(summary: str) -> str:\n    \"\"\"\n    Formats the summary with multi-line support if needed.\n    \"\"\"\n    lines = summary.strip().split(\". \")\n    if len(lines) > 1:\n        return \"<br>\".join(f\"- {line.strip()}\" for line in lines)\n    return summary.strip()",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tables.py",
    "type": "function",
    "name": "format_summary",
    "loc": 276,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\tree.py:class:TreeGenerator:chunk1",
    "text": "class TreeGenerator:\n    \"\"\"\n    Generates a directory tree structure from a given root directory.\n    \"\"\"\n\n    def __init__(\n        self,\n        repo_name: str,\n        root_dir: Path,\n        repo_url: str,\n        max_depth: int,\n    ):\n        self.repo_name = repo_name\n        self.root_dir = root_dir\n        self.repo_url = repo_url\n        self.max_depth = max_depth\n\n    def generate(\n        self,\n        directory: Path,\n        prefix: str = \"\",\n        is_last: bool = True,\n        depth: int = 0,\n    ) -> str:\n        \"\"\"Build string representation of a directory tree.\"\"\"\n        if depth > self.max_depth:\n            return \"\"\n\n        children = sorted(directory.iterdir()) if directory.is_dir() else []\n        children = list(children)\n\n        if not children and directory.is_dir():\n            return \"\"\n\n        parts = [f\"{prefix}{'└── ' if is_last else '├── '}{directory.name}\"]\n\n        for index, child in enumerate(children):\n            child_prefix = prefix + (\"    \" if is_last else \"│   \")\n            if child_tree := self.generate(\n                child,\n                child_prefix,\n                index == len(children) - 1,\n                depth + 1,\n            ):\n                parts.append(child_tree)\n\n        return self._format_tree(parts)\n\n    def _format_tree(self, parts: list[str]) -> str:\n        \"\"\"Format tree structure and replace root directory name.\"\"\"\n        tree = \"\\n\".join(parts)\n        return tree.replace(self.root_dir.name, f\"{self.repo_name}/\")",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\tree.py",
    "type": "class",
    "name": "TreeGenerator",
    "loc": 4,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:generate_banner:chunk1",
    "text": "def generate_banner(title: str) -> str:\n    \"\"\"Generate an ASCII banner for the project name.\"\"\"\n    lines = [\"\"] * 5\n    for char in title:\n        letter = _create_letter(char)\n        for i in range(5):\n            lines[i] += f\"{letter[i]} \"\n\n    return _wrap_with_pre_tag(\"\\n\".join(lines))",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "generate_banner",
    "loc": 1,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:generate_box_banner:chunk1",
    "text": "def generate_box_banner(title: str, tagline: str = \"\") -> str:\n    \"\"\"Generate an ASCII banner for the project name and tagline.\"\"\"\n    lines = [\"\"] * 5\n    for char in title:\n        letter = _create_letter(char)\n        for i in range(5):\n            lines[i] += f\"{letter[i]} \"\n\n    max_width = max(len(line) for line in lines)\n    box_width = max(max_width, len(tagline)) + 4\n\n    banner = [\n        \"╔\" + \"═\" * box_width + \"╗\",\n        \"║\" + \" \" * box_width + \"║\",\n    ]\n    banner.extend(f\"║  {line.ljust(box_width - 2)}║\" for line in lines)\n    banner.extend([\n        \"║\" + \" \" * box_width + \"║\",\n        # f\"║  {tagline.center(box_width - 4)}  ║\",\n        \"║\" + \" \" * box_width + \"║\",\n        \"╚\" + \"═\" * box_width + \"╝\",\n    ])\n    return _wrap_with_pre_tag(\"\\n\".join(banner))",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "generate_box_banner",
    "loc": 12,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:generate_console_banner:chunk1",
    "text": "def generate_console_banner(title: str) -> str:\n    \"\"\"Generate an ASCII banner for the project name and tagline.\"\"\"\n    lines = [\"\"] * 5\n    for char in title:\n        letter = _create_letter(char)\n        for i in range(5):\n            lines[i] += f\"{letter[i]} \"\n    return _wrap_with_console_tag(\"\\n\".join(lines))",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "generate_console_banner",
    "loc": 37,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:_create_letter:chunk1",
    "text": "def _create_letter(char) -> list[str]:\n    \"\"\"Create an ASCII letter from a character.\"\"\"\n    letters = {\n        \"A\": [\"  ██  \", \" ████ \", \"██  ██\", \"██████\", \"██  ██\"],\n        \"B\": [\"██████\", \"██   █\", \"██████\", \"██   █\", \"██████\"],\n        \"C\": [\" ████ \", \"██    \", \"██    \", \"██    \", \" ████ \"],\n        \"D\": [\"████  \", \"██  ██\", \"██  ██\", \"██  ██\", \"████  \"],\n        \"E\": [\"██████\", \"██    \", \"████  \", \"██    \", \"██████\"],\n        \"F\": [\"██████\", \"██    \", \"████  \", \"██    \", \"██    \"],\n        \"G\": [\" ████ \", \"██    \", \"██ ███\", \"██  ██\", \" ████ \"],\n        \"H\": [\"██  ██\", \"██  ██\", \"██████\", \"██  ██\", \"██  ██\"],\n        \"I\": [\"██████\", \"  ██  \", \"  ██  \", \"  ██  \", \"██████\"],\n        \"J\": [\"██████\", \"   ██ \", \"   ██ \", \"██ ██ \", \" ███  \"],\n        \"K\": [\"██  ██\", \"██ ██ \", \"████  \", \"██ ██ \", \"██  ██\"],\n        \"L\": [\"██    \", \"██    \", \"██    \", \"██    \", \"██████\"],\n        \"M\": [\"██   ██\", \"███ ███\", \"██ █ ██\", \"██   ██\", \"██   ██\"],\n        \"N\": [\"██   ██\", \"███  ██\", \"██ █ ██\", \"██  ███\", \"██   ██\"],\n        \"O\": [\" ████ \", \"██  ██\", \"██  ██\", \"██  ██\", \" ████ \"],\n        \"P\": [\"██████\", \"██  ██\", \"██████\", \"██    \", \"██    \"],\n        \"Q\": [\" ████ \", \"██  ██\", \"██  ██\", \"██ ███\", \" ██ ██\"],\n        \"R\": [\"██████\", \"██  ██\", \"██████\", \"██ ██ \", \"██  ██\"],\n        \"S\": [\" ████ \", \"██    \", \" ████ \", \"    ██\", \"█████ \"],\n        \"T\": [\"██████\", \"  ██  \", \"  ██  \", \"  ██  \", \"  ██  \"],\n        \"U\": [\"██  ██\", \"██  ██\", \"██  ██\", \"██  ██\", \" ████ \"],\n        \"V\": [\"██  ██\", \"██  ██\", \"██  ██\", \" ████ \", \"  ██  \"],\n        \"W\": [\"██   ██\", \"██   ██\", \"██ █ ██\", \"███ ███\", \"██   ██\"],\n        \"X\": [\"██  ██\", \" ████ \", \"  ██  \", \" ████ \", \"██  ██\"],\n        \"Y\": [\"██  ██\", \" ████ \", \"  ██  \", \"  ██  \", \"  ██  \"],\n        \"Z\": [\"██████\", \"   ██ \", \"  ██  \", \" ██   \", \"██████\"],\n        \"-\": [\"      \", \"      \", \"██████\", \"      \", \"      \"],\n        \".\": [\"      \", \"      \", \"  ██  \", \"  ██  \", \"      \"],\n        \" \": [\"    \", \"    \", \"    \", \"    \", \"    \"],\n    }\n    return letters.get(char.upper(), [\"?????\"] * 5)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "_create_letter",
    "loc": 47,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:_wrap_with_pre_tag:chunk1",
    "text": "def _wrap_with_pre_tag(text: str) -> str:\n    \"\"\"Wrap the text content with a <pre> tag.\"\"\"\n    return f\"<pre>\\n{text}\\n</pre>\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "_wrap_with_pre_tag",
    "loc": 83,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\ascii.py:function:_wrap_with_console_tag:chunk1",
    "text": "def _wrap_with_console_tag(text: str) -> str:\n    \"\"\"Wrap the text content with a console code block.\"\"\"\n    return f\"\"\"```console\\n{text}\"\"\"",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\ascii.py",
    "type": "function",
    "name": "_wrap_with_console_tag",
    "loc": 88,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\svg.py:class:SVGBannerSettings:chunk1",
    "text": "class SVGBannerSettings(BaseModel):\n    \"\"\"\n    Pydantic model for SVG configuration settings.\n    \"\"\"\n\n    border_radius: float = Field(\n        ..., ge=0, description=\"Border radius for the SVG container.\"\n    )\n    font_color: Annotated[\n        str,\n        StringConstraints(\n            pattern=r\"^#[0-9A-Fa-f]{6}$\",\n        ),\n    ]\n    font_family: Annotated[\n        str,\n        StringConstraints(\n            strip_whitespace=True,\n            min_length=1,\n        ),\n    ]\n    font_size: int = Field(..., gt=0, description=\"Font size for the title.\")\n    height: int = Field(..., gt=0, description=\"Height of the SVG banner.\")\n    pattern_opacity: float = Field(\n        ..., ge=0, le=1, description=\"Opacity of the pattern (0-1).\"\n    )\n    pattern_size: float = Field(\n        ..., gt=0, description=\"Pattern size for background elements.\"\n    )\n    shadow_blur: float = Field(..., description=\"Blur radius for the shadow.\")\n    shadow_dx: float = Field(..., description=\"Horizontal shadow offset.\")\n    shadow_dy: float = Field(..., description=\"Vertical shadow offset.\")\n    shadow_opacity: float = Field(\n        ..., ge=0, le=1, description=\"Opacity of the shadow (0-1).\"\n    )\n    shape_opacity: float = Field(\n        ..., ge=0, le=1, description=\"Opacity for additional shapes.\"\n    )\n    subtitle_size: int = Field(..., gt=0, description=\"Font size for the subtitle.\")\n    width: int = Field(..., gt=0, description=\"Width of the SVG banner.\")\n\n    model_config = ConfigDict(strict=True, extra=\"forbid\", validate_assignment=True)\n\n    @model_validator(mode=\"after\")\n    def validate_sizes(self) -> \"SVGBannerSettings\":\n        \"\"\"\n        Ensures that the width is greater than or equal to the height.\n        \"\"\"\n        if self.width < self.height:\n            raise ValueError(\"Width must be greater than or equal to height.\")\n        return self",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\svg.py",
    "type": "class",
    "name": "SVGBannerSettings",
    "loc": 9,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\svg.py:class:SVGBannerConfig:chunk1",
    "text": "class SVGBannerConfig(BaseModel):\n    \"\"\"\n    Pydantic model for the overall SVG configuration.\n    \"\"\"\n\n    banners: dict[str, dict[str, str]] = Field(\n        ..., description=\"Banners configuration.\"\n    )\n    settings: SVGBannerSettings\n\n    model_config = ConfigDict(strict=True, extra=\"forbid\")",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\svg.py",
    "type": "class",
    "name": "SVGBannerConfig",
    "loc": 62,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\banners\\svg.py:class:SVGBannerGenerator:chunk1",
    "text": "class SVGBannerGenerator:\n    \"\"\"\n    Generate SVG banners for the README.md file.\n    \"\"\"\n\n    def __init__(self, config_file: str) -> None:\n        \"\"\"Validate the configuration and load the SVG template.\"\"\"\n        file_handler = FileHandler()\n        cwd = Path(__file__).parent.parent.parent\n        raw_config = file_handler.read(cwd / config_file)\n        self.config = SVGBannerConfig.model_validate(raw_config)\n        self.svg_template = self.config.banners[\"svg\"][\"template\"]\n\n    def generate_svg(self, title: str) -> str:\n        \"\"\"Generate an SVG banner for the project name and tagline.\"\"\"\n        c = self.config.settings\n        colors = generate_gradient_colors()\n        svg_content = self.svg_template.format(\n            width=c.width,\n            height=c.height,\n            color1=colors[0],\n            color2=colors[1],\n            color3=colors[2],\n            shadow_dx=c.shadow_dx,\n            shadow_dy=c.shadow_dy,\n            shadow_blur=c.shadow_blur,\n            shadow_opacity=c.shadow_opacity,\n            pattern_size=c.pattern_size,\n            pattern_opacity=c.pattern_opacity,\n            border_radius=c.border_radius,\n            width_08=c.width * 0.08,\n            height_025=c.height * 0.25,\n            height_015=c.height * 0.15,\n            width_92=c.width * 0.92,\n            height_075=c.height * 0.75,\n            height_02=c.height * 0.2,\n            width_2=c.width / 2,\n            height_0125=c.height * 0.125,\n            width_2_plus_height_025=c.width / 2 + c.height * 0.25,\n            width_2_minus_height_025=c.width / 2 - c.height * 0.25,\n            height_0375=c.height * 0.375,\n            font_family=c.font_family,\n            font_size=c.font_size,\n            font_color=c.font_color,\n            subtitle_size=c.subtitle_size,\n            shape_opacity=c.shape_opacity,\n            height_2=c.height / 2,\n            height_0625=c.height * 0.75,\n            title=title,\n        )\n        return svg_content",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\banners\\svg.py",
    "type": "class",
    "name": "SVGBannerGenerator",
    "loc": 75,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\colors\\converters.py:function:hex_to_hls:chunk1",
    "text": "def hex_to_hls(hex_color: str) -> tuple[float, float, float]:\n    \"\"\"Converts a hex color to HLS.\"\"\"\n    hex_color = hex_color.lstrip(\"#\")\n    rgb = tuple(int(hex_color[i : i + 2], 16) for i in (0, 2, 4))\n    return colorsys.rgb_to_hls(rgb[0] / 255, rgb[1] / 255, rgb[2] / 255)",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\colors\\converters.py",
    "type": "function",
    "name": "hex_to_hls",
    "loc": 4,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\colors\\gradients.py:function:generate_base_color:chunk1",
    "text": "def generate_base_color() -> tuple:\n    \"\"\"Generate a random base color in HSV.\"\"\"\n    # Random hue value between 0 and 1\n    hue = randint(0, 360) / 360.0\n    # Keep saturation constant for related colors\n    saturation = 0.8\n    # Keep brightness constant\n    value = 0.9\n    return hue, saturation, value",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\colors\\gradients.py",
    "type": "function",
    "name": "generate_base_color",
    "loc": 11,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\colors\\gradients.py:function:generate_gradient_colors:chunk1",
    "text": "def generate_gradient_colors() -> list:\n    \"\"\"Generate a list of related colors to form a gradient.\"\"\"\n    base_hue, saturation, value = generate_base_color()\n    # Generate 3 related colors\n    gradient_colors = [\n        generate_related_color(base_hue, shift / 10) for shift in range(3)\n    ]\n    _logger.info(f\"Generated gradient colors: {gradient_colors}\")\n    return gradient_colors",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\colors\\gradients.py",
    "type": "function",
    "name": "generate_gradient_colors",
    "loc": 22,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\colors\\gradients.py:function:generate_random_color:chunk1",
    "text": "def generate_random_color() -> str:\n    \"\"\"Generate a random hex color.\"\"\"\n    color = f\"#{randint(0, 255):02x}{randint(0, 255):02x}{randint(0, 255):02x}\"\n    _logger.info(f\"Generated random color: {color}\")\n    return color",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\colors\\gradients.py",
    "type": "function",
    "name": "generate_random_color",
    "loc": 33,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\generators\\colors\\gradients.py:function:generate_related_color:chunk1",
    "text": "def generate_related_color(base_hue: float, shift: float) -> str:\n    \"\"\"Generate a related color by shifting the hue of the base color.\"\"\"\n    # Wrap around if the hue exceeds 1.0\n    hue = (base_hue + shift) % 1.0\n    r, g, b = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n    color = f\"#{int(r * 255):02x}{int(g * 255):02x}{int(b * 255):02x}\"\n    _logger.info(f\"Generated related color: {color}\")\n    return color",
    "repo": "readme-ai",
    "path": "readmeai\\generators\\colors\\gradients.py",
    "type": "function",
    "name": "generate_related_color",
    "loc": 40,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\anthropic.py:class:AnthropicHandler:chunk1",
    "text": "class AnthropicHandler(BaseModelHandler):\n    \"\"\"\n    Anthropic LLM API service implementation.\n    \"\"\"\n\n    def __init__(self, config_loader: ConfigLoader, context: RepositoryContext) -> None:\n        super().__init__(config_loader, context)\n        self._model_settings()\n\n    def _model_settings(self):\n        self.client = anthropic.AsyncAnthropic()\n        self.model = AnthropicModels.CLAUDE35_SONNET.value\n\n    async def _build_payload(self, prompt: str) -> dict[str, Any]:\n        \"\"\"Build request body for making text generation requests.\"\"\"\n\n        return {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"system\": self.system_message,\n        }\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((\n            *ANTHROPIC_EXCEPTIONS,\n            aiohttp.ClientError,\n            aiohttp.ClientResponseError,\n            aiohttp.ClientConnectorError,\n        )),\n    )\n    async def _make_request(\n        self,\n        index: str | None,\n        prompt: str | None,\n        tokens: int | None,\n        repo_files: list[tuple[str, str]] | None,\n    ) -> Any:\n        \"\"\"Processes Anthropic API responses and returns generated text.\"\"\"\n        try:\n            prompt = await token_handler(\n                config=self.config,\n                index=index,\n                prompt=prompt,\n                tokens=tokens,\n            )\n            parameters = await self._build_payload(prompt)\n\n            async with self.rate_limit_semaphore:\n                response = await self.client.messages.create(**parameters)\n                content = (\n                    response.content[0].text\n                    if hasattr(response, \"content\")\n                    else str(response)\n                )\n                self._logger.info(f\"Response from Anthropic for '{index}': {content}\")\n                return index, content\n\n        except (\n            *ANTHROPIC_EXCEPTIONS,\n            aiohttp.ClientError,\n            aiohttp.ClientResponseError,\n            aiohttp.ClientConnectorError,\n        ) as e:\n            self._logger.error(f\"Anthropic API error for '{index}': {e!r}\")\n            raise  # Re-raise for retry decorator\n\n        except Exception as e:\n            self._logger.error(f\"Unexpected error for '{index}': {e!r}\")\n            return index, self.placeholder",
    "repo": "readme-ai",
    "path": "readmeai\\models\\anthropic.py",
    "type": "class",
    "name": "AnthropicHandler",
    "loc": 34,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\base.py:class:BaseModelHandler:chunk1",
    "text": "class BaseModelHandler(ABC):\n    \"\"\"\n    Interface for LLM API handler implementations.\n    \"\"\"\n\n    def __init__(self, config_loader: ConfigLoader, context: RepositoryContext) -> None:\n        self._logger = get_logger(__name__)\n        self._session: aiohttp.ClientSession | None = None\n        self.config = config_loader.config\n        self.prompts = config_loader.prompts\n        self.placeholder = self.config.md.placeholder\n        self.max_tokens = self.config.llm.tokens\n        self.rate_limit = self.config.llm.rate_limit\n        self.rate_limit_semaphore = asyncio.Semaphore(self.rate_limit)\n        self.system_message = self.config.llm.system_message\n        self.temperature = self.config.llm.temperature\n        self.top_p = self.config.llm.top_p\n        self.repo_context = context\n        self.dependencies = context.dependencies\n        self.documents = [\n            (file.path, file.content)\n            for file in context.files\n            if \".lock\" not in file.name\n        ]\n\n    @asynccontextmanager\n    async def use_api(self) -> AsyncGenerator[Any, None]:\n        \"\"\"Async context manager for managing HTTP client lifecycle.\"\"\"\n        async with aiohttp.ClientSession(\n            timeout=aiohttp.ClientTimeout(total=None),\n        ) as session:\n            self._session = session\n            try:\n                yield self\n            finally:\n                await self.close()\n\n    async def close(self) -> None:\n        \"\"\"Closes the HTTP client session.\"\"\"\n        if self._session:\n            await self._session.close()\n            self._logger.debug(\"HTTP client closed.\")\n        else:\n            self._logger.debug(\"HTTP client is already closed.\")\n\n    @abstractmethod\n    async def _model_settings(self) -> None:\n        \"\"\"Initializes LLM API settings for a given service.\"\"\"\n        ...\n\n    @abstractmethod\n    async def _build_payload(self, prompt: str) -> dict[str, Any]:\n        \"\"\"Build request body for making text generation requests.\"\"\"\n        ...\n\n    @abstractmethod\n    async def _make_request(\n        self,\n        index: str | None,\n        prompt: str | None,\n        tokens: int | None,\n        repo_files: list[tuple[str, str]] | None,\n    ) -> Any:\n        \"\"\"Makes a POST request to the LLM API and returns the response.\"\"\"\n        ...\n\n    async def batch_request(\n        self,\n    ) -> Tuple[List[Any], str, str, str]:\n        \"\"\"Generates a batch of prompts and processes the responses.\"\"\"\n        if self.config.llm.api == LLMProviders.OFFLINE.value:\n            file_summaries = await self._make_request(\n                index=None,\n                prompt=None,\n                tokens=None,\n                repo_files=self.documents,\n            )\n            return (\n                file_summaries,\n                self.placeholder,\n                self.placeholder,\n                self.placeholder,\n            )\n        summaries_prompts = set_summary_context(\n            self.config,\n            self.documents,\n        )\n        summaries_responses = await self._batch_prompts(summaries_prompts)\n        additional_prompts = set_additional_contexts(\n            self.config,\n            self.repo_context,\n            summaries_responses,\n        )\n        additional_responses = await self._batch_prompts(additional_prompts)\n\n        return summaries_responses + additional_responses\n\n    async def _batch_prompts(\n        self,\n        prompts: Any,\n        batch_size: int = 10,\n    ) -> list[tuple[str, str]]:\n        \"\"\"Processes a batch of prompts and returns the generated text.\"\"\"\n        responses = []\n\n        for batch in self._generate_batches(prompts, batch_size):\n            batch_responses = await asyncio.gather(\n                *[self._process_batch(prompt) for prompt in batch],\n                return_exceptions=False,\n            )\n            responses.extend(batch_responses)\n\n        return responses\n\n    def _generate_batches(\n        self,\n        items: list[Any],\n        batch_size: int,\n    ) -> Generator[list[Any], None, None]:\n        \"\"\"Generates batches of items to be processed.\"\"\"\n        for i in range(0, len(items), batch_size):\n            yield items[i : i + batch_size]\n\n    async def _process_batch(self, prompt: dict[str, Any]) -> Any:\n        \"\"\"Processes a single prompt and returns the generated text.\"\"\"\n        if prompt[\"type\"] == \"file_summary\":\n            return await self._make_request_code_summary(\n                prompt[\"context\"],\n            )\n        else:\n            formatted_prompt = get_prompt_context(\n                self.prompts,\n                prompt[\"type\"],\n                prompt[\"context\"],\n            )\n            tokens = update_max_tokens(\n                self.config.llm.tokens,\n                formatted_prompt,\n            )\n            _, summary = await self._make_request(\n                prompt[\"type\"],\n                formatted_prompt,\n                tokens,\n                None,\n            )\n            return summary\n\n    async def _make_request_code_summary(\n        self,\n        file_context: dict[str, list[tuple[str, str]]],\n    ) -> Any:\n        \"\"\"Generates code summaries for each file in the project.\"\"\"\n        if not file_context or \"repo_files\" not in file_context:\n            return await self._make_request(\n                index=None,\n                prompt=None,\n                tokens=None,\n                repo_files=file_context[\"repo_files\"],\n            )\n        else:\n            file_summaries = []\n            for file_path, file_content in file_context[\"repo_files\"]:\n                prompt = self.prompts[\"prompts\"][\"file_summary\"].format(\n                    self.config.md.directory_structure,\n                    file_path,\n                    file_content,\n                )\n                tokens = update_max_tokens(self.config.llm.tokens, prompt)\n                _, summary_or_error = await self._make_request(\n                    file_path,\n                    prompt,\n                    tokens,\n                    None,\n                )\n                file_summaries.append((file_path, summary_or_error))\n\n            return file_summaries",
    "repo": "readme-ai",
    "path": "readmeai\\models\\base.py",
    "type": "class",
    "name": "BaseModelHandler",
    "loc": 24,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\dalle.py:class:DalleHandler:chunk1",
    "text": "class DalleHandler:\n    \"\"\"\n    Generate and download an image using OpenAI's DALL-E model.\n    \"\"\"\n\n    def __init__(self, config: ConfigLoader) -> None:\n        self.config = config\n        self.default_image = DefaultLogos.PURPLE.value\n        self.filename = f\"{config.config.git.name}.png\"\n        self._logger = get_logger(__name__)\n        self._model_settings()\n\n    def _model_settings(self) -> None:\n        self.client = openai.AsyncOpenAI()\n        self.model = \"dall-e-3\"\n        self.size = \"1792x1024\"\n        self.quality = \"standard\"\n        self.n = 1\n\n    async def __aenter__(self):\n        self.session = aiohttp.ClientSession()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.session.close()\n        self._logger.debug(f\"Closed {self.model.upper()} API session...\")\n\n    def _build_payload(self) -> dict[str, Any]:\n        \"\"\"Formats the prompt string using configuration data.\"\"\"\n        return {\n            \"prompt\": self.config.prompts[\"prompts\"][\"logo\"].format(\n                project_name=self.config.config.git.name,\n                project_tagline=self.config.config.md.tagline,\n                project_overview=self.config.config.md.overview,\n            ),\n            \"model\": self.model,\n            \"size\": self.size,\n            \"quality\": self.quality,\n            \"n\": self.n,\n        }\n\n    async def _make_request(self) -> str:\n        \"\"\"Generate an image using the DALL-E model, and return its URL.\"\"\"\n        try:\n            payload = self._build_payload()\n            response = await self.client.images.generate(**payload)\n            if response and response.data and response.data[0].url:\n                return response.data[0].url\n            else:\n                self._logger.error(f\"Failed to generate project logo image: {response}\")\n                return self.default_image\n\n        except (Exception, openai.OpenAIError) as e:\n            self._logger.error(f\"Error generating project logo image: {e!r}\")\n            return self.default_image\n\n    async def download(self, image_url) -> str:\n        \"\"\"Download the generated image from the provided URL.\"\"\"\n        try:\n            async with self.session.get(image_url) as response:\n                if response.status == 200:\n                    content = await response.read()\n                    with open(self.filename, \"wb\") as f:\n                        f.write(content)\n\n                    self._logger.info(\n                        f\"{self.model.upper()} image successfully downloaded.\"\n                    )\n                    return self.filename\n                else:\n                    self._logger.error(f\"Failed to download image: {response.status}\")\n\n        except Exception as e:\n            self._logger.error(f\"Error downloading image: {e!r}\")\n\n        return self.default_image\n\n    async def generate_and_download(self) -> str:\n        \"\"\"Generate and download the image.\"\"\"\n        image_url = await self._make_request()\n        return await self.download(image_url)",
    "repo": "readme-ai",
    "path": "readmeai\\models\\dalle.py",
    "type": "class",
    "name": "DalleHandler",
    "loc": 18,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:LLMAuthKeys:chunk1",
    "text": "class LLMAuthKeys(str, Enum):\n    \"\"\"\n    LLM API service environment variable keys.\n    \"\"\"\n\n    ANTHROPIC_API_KEY = \"ANTHROPIC_API_KEY\"\n    GOOGLE_API_KEY = \"GOOGLE_API_KEY\"\n    OLLAMA_HOST = \"OLLAMA_HOST\"\n    OPENAI_API_KEY = \"OPENAI_API_KEY\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "LLMAuthKeys",
    "loc": 4,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:LLMProviders:chunk1",
    "text": "class LLMProviders(str, Enum):\n    \"\"\"\n    LLM API services supported by readme-ai.\n    \"\"\"\n\n    ANTHROPIC = \"anthropic\"\n    GEMINI = \"gemini\"\n    OLLAMA = \"ollama\"\n    OPENAI = \"openai\"\n    OFFLINE = \"offline\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "LLMProviders",
    "loc": 15,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:AnthropicModels:chunk1",
    "text": "class AnthropicModels(str, Enum):\n    \"\"\"\n    Enumerated list of supported Anthropic models.\n    \"\"\"\n\n    CLAUDE3_HAIKU = \"claude-3-haiku-20240307\"\n    CLAUDE3_OPUS = \"claude-3-opus-20240229\"\n    CLAUDE3_SONNET = \"claude-3-sonnet-20240229\"\n    CLAUDE35_SONNET = \"claude-3-5-sonnet-20240620\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "AnthropicModels",
    "loc": 27,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:GeminiModels:chunk1",
    "text": "class GeminiModels(str, Enum):\n    \"\"\"\n    Enumerated list of supported Gemini models.\n    \"\"\"\n\n    GEMINI_FLASH_2 = \"gemini-2.0-flash-exp\"\n    GEMINI_FLASH = \"gemini-1.5-flash\"\n    GEMINI_PRO = \"gemini-1.5-pro\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "GeminiModels",
    "loc": 38,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:OllamaModels:chunk1",
    "text": "class OllamaModels(str, Enum):\n    \"\"\"\n    Enumerated list of supported Ollama models.\n    \"\"\"\n\n    LLAMA31 = \"llama3.1\"\n    LLAMA32 = \"llama3.2\"\n    GEMMA2 = \"gemma2\"\n    MISTRAL = \"mistral\"\n    QWEN = \"qwen2.5-coder\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "OllamaModels",
    "loc": 48,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:OpenAIModels:chunk1",
    "text": "class OpenAIModels(str, Enum):\n    \"\"\"\n    Enumerated list of supported OpenAI models.\n    \"\"\"\n\n    GPT35_TURBO = \"gpt-3.5-turbo\"\n    GPT4_TURBO = \"gpt-4-turbo\"\n    GPT4O_MINI = \"gpt-4o-mini\"\n    GPT4O = \"gpt-4o\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "OpenAIModels",
    "loc": 60,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\enums.py:class:BaseURLs:chunk1",
    "text": "class BaseURLs(str, Enum):\n    \"\"\"\n    Enumerated list of supported API base URLs.\n    \"\"\"\n\n    ANTHROPIC = \"https://api.anthropic.com/\"\n    GEMINI = \"https://generativelanguage.googleapis.com/\"\n    OLLAMA = \"http://localhost:11434/\"\n    OPENAI = \"https://api.openai.com/\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\enums.py",
    "type": "class",
    "name": "BaseURLs",
    "loc": 71,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\factory.py:class:ModelFactory:chunk1",
    "text": "class ModelFactory:\n    \"\"\"\n    Factory class for creating LLM API handler instances.\n    \"\"\"\n\n    _model_map: ClassVar[dict] = {\n        LLMProviders.ANTHROPIC: AnthropicHandler,\n        LLMProviders.GEMINI.value: GeminiHandler,\n        LLMProviders.OLLAMA.value: OpenAIHandler,\n        LLMProviders.OPENAI.value: OpenAIHandler,\n        LLMProviders.OFFLINE.value: OfflineHandler,\n    }\n\n    @staticmethod\n    def get_backend(\n        config: ConfigLoader, context: RepositoryContext\n    ) -> BaseModelHandler:\n        \"\"\"Retrieves configured LLM API handler instance.\"\"\"\n        llm_service = ModelFactory._model_map.get(config.config.llm.api)\n\n        if llm_service is None:\n            raise UnsupportedServiceError(\n                f\"Unsupported LLM provider: {config.config.llm.api}\"\n            )\n\n        return llm_service(config, context)",
    "repo": "readme-ai",
    "path": "readmeai\\models\\factory.py",
    "type": "class",
    "name": "ModelFactory",
    "loc": 14,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\gemini.py:class:GeminiHandler:chunk1",
    "text": "class GeminiHandler(BaseModelHandler):\n    \"\"\"\n    Google Gemini LLM API service implementation.\n    \"\"\"\n\n    def __init__(self, config_loader: ConfigLoader, context: RepositoryContext) -> None:\n        super().__init__(config_loader, context)\n        self._model_settings()\n\n    def _model_settings(self):\n        if is_available(\"google.generativeai\"):\n            import google.generativeai as genai\n\n            self.model_name = GeminiModels.GEMINI_FLASH.value\n            genai.configure()\n            self.model = genai.GenerativeModel(self.model_name)\n            self.generation_config = genai.types.GenerationConfig(\n                max_output_tokens=self.max_tokens,\n                temperature=self.temperature,\n                top_p=self.top_p,\n            )\n        else:\n            self._logger.error(\n                \"Google Generative AI library is not installed in current environment.\"\n            )\n            raise RuntimeError(\n                \"\"\"Install the optional dependencies for Gemini:\n                pip install 'readmeai[google-generativeai]'\"\"\"\n            )\n\n    async def _build_payload(self, prompt: str) -> Any:\n        \"\"\"Build request body for making text generation requests.\"\"\"\n        return self.generation_config\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((\n            aiohttp.ClientError,\n            aiohttp.ClientResponseError,\n            aiohttp.ClientConnectorError,\n        )),\n    )\n    async def _make_request(\n        self,\n        index: str | None,\n        prompt: str | None,\n        tokens: int | None,\n        repo_files: Any,\n    ) -> Any:\n        \"\"\"Processes Gemini API responses and returns generated text.\"\"\"\n        try:\n            prompt = await token_handler(\n                config=self.config,\n                index=index,\n                prompt=prompt,\n                tokens=tokens,\n            )\n\n            parameters = await self._build_payload(prompt)\n\n            async with self.rate_limit_semaphore:\n                response = await self.model.generate_content_async(\n                    prompt,\n                    generation_config=parameters,\n                )\n                content = response.text if hasattr(response, \"text\") else str(response)\n                self._logger.info(f\"Response from Gemini for '{index}': {content}\")\n                return index, content\n\n        except (\n            aiohttp.ClientError,\n            aiohttp.ClientResponseError,\n            aiohttp.ClientConnectorError,\n        ) as e:\n            self._logger.error(f\"Gemini API error for '{index}': {e!r}\")\n            raise  # Re-raise for retry decorator\n\n        except Exception as e:\n            self._logger.error(f\"Unexpected error for '{index}': {e!r}\")\n            return index, self.placeholder",
    "repo": "readme-ai",
    "path": "readmeai\\models\\gemini.py",
    "type": "class",
    "name": "GeminiHandler",
    "loc": 20,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\offline.py:class:OfflineHandler:chunk1",
    "text": "class OfflineHandler(BaseModelHandler):\n    \"\"\"\n    OfflineMode model handler implementation.\n    \"\"\"\n\n    def __init__(self, config_loader: ConfigLoader, context: RepositoryContext) -> None:\n        super().__init__(config_loader, context)\n\n    async def _model_settings(self) -> None: ...\n\n    async def _build_payload(self, prompt: str) -> dict[str, Any]:\n        \"\"\"Builds the payload for the POST request to the LLM API.\"\"\"\n        return {}\n\n    async def _make_request(\n        self,\n        index: str | None,\n        prompt: str | None,\n        tokens: int | None,\n        repo_files: Any,\n    ) -> Any:\n        \"\"\"Returns placeholder content in a consistent format.\"\"\"\n        if repo_files is None or len(repo_files) == 0:\n            files = [(self.placeholder, self.placeholder)]\n        else:\n            files = [(file_path, self.placeholder) for file_path, _ in repo_files]\n        return files",
    "repo": "readme-ai",
    "path": "readmeai\\models\\offline.py",
    "type": "class",
    "name": "OfflineHandler",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\openai.py:class:OpenAIHandler:chunk1",
    "text": "class OpenAIHandler(BaseModelHandler):\n    \"\"\"\n    OpenAI API model handler implementation, with Ollama support.\n    \"\"\"\n\n    def __init__(self, config_loader: ConfigLoader, context: RepositoryContext) -> None:\n        super().__init__(config_loader, context)\n        self._model_settings()\n\n    def _model_settings(self):\n        \"\"\"Handles both OpenAI API and Ollama local deployments.\"\"\"\n        self.host_name = BaseURLs[\"OPENAI\"].value\n        self.localhost = BaseURLs[\"OLLAMA\"].value\n        self.max_tokens = self.config.llm.tokens\n        self.model = self.config.llm.model\n        self.resource = self.config.llm.resource\n        self.top_p = self.config.llm.top_p\n\n        if self.config.llm.api == LLMProviders.OPENAI.value:\n            self.url = f\"{self.host_name}{self.resource}\"\n            if os.getenv(\"OPENAI_API_KEY\") is None:\n                raise ValueError(\"OpenAI API key not set in environment.\")\n            self.client = openai.OpenAI()\n\n        elif self.config.llm.api == LLMProviders.OLLAMA.value:\n            self.url = f\"{self.localhost}{self.resource}\"\n            self.client = openai.OpenAI(\n                base_url=f\"{self.localhost}v1\",\n                api_key=LLMProviders.OLLAMA.name,\n            )\n\n        self.headers = {\"Authorization\": f\"Bearer {self.client.api_key}\"}\n\n    async def _build_payload(self, prompt: str) -> dict[str, Any]:\n        \"\"\"Build request body for making text generation requests.\"\"\"\n        return {\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": self.system_message,\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n        }\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type(\n            (\n                aiohttp.ClientError,\n                aiohttp.ClientResponseError,\n                aiohttp.ClientConnectorError,\n                openai.OpenAIError,\n            ),\n        ),\n    )\n    async def _make_request(\n        self,\n        index: str | None,\n        prompt: str | None,\n        tokens: int | None,\n        repo_files: Any,\n    ):\n        \"\"\"Process requests to OpenAI API, with retries and error handling.\"\"\"\n        try:\n            if prompt is None:\n                raise ValueError(\"Prompt cannot be None\")\n\n            prompt = await token_handler(\n                config=self.config,\n                index=index,\n                prompt=prompt,\n                tokens=tokens,\n            )\n            if not prompt:\n                raise ValueError(\"Token handler returned empty prompt\")\n\n            if index == \"file_summary\":\n                self.max_tokens = 100\n\n            parameters = await self._build_payload(prompt)\n\n            async with self._session.post(\n                self.url,\n                headers=self.headers,\n                json=parameters,\n            ) as response:\n                response.raise_for_status()\n                response = await response.json()\n                content = response[\"choices\"][0][\"message\"][\"content\"]\n\n                if not content:\n                    raise ValueError(\"Empty response from API\")\n\n                self._logger.info(\n                    f\"Response from {self.config.llm.api.capitalize()} for '{index}': {content}\",\n                )\n                return index, content\n\n        except (\n            aiohttp.ClientError,\n            aiohttp.ClientResponseError,\n            aiohttp.ClientConnectorError,\n            openai.OpenAIError,\n        ) as e:\n            self._logger.error(f\"Error processing request for '{index}': {e!r}\")\n            raise  # Re-raise for retry decorator\n\n        except Exception as e:\n            self._logger.error(f\"Unexpected error for '{index}': {e!r}\")\n            return index, self.placeholder",
    "repo": "readme-ai",
    "path": "readmeai\\models\\openai.py",
    "type": "class",
    "name": "OpenAIHandler",
    "loc": 21,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\prompts.py:function:get_prompt_context:chunk1",
    "text": "def get_prompt_context(prompts: dict, prompt_type: str, context: dict) -> str:\n    \"\"\"Generates a prompt for the LLM API.\"\"\"\n    prompt_template = get_prompt_template(prompts, prompt_type)\n\n    if not prompt_template:\n        _logger.error(f\"Prompt type '{prompt_type}' not found.\")\n        return \"\"\n\n    return inject_prompt_context(prompt_template, context)",
    "repo": "readme-ai",
    "path": "readmeai\\models\\prompts.py",
    "type": "function",
    "name": "get_prompt_context",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\prompts.py:function:get_prompt_template:chunk1",
    "text": "def get_prompt_template(prompts: dict, prompt_type: str) -> str:\n    \"\"\"Retrieves the template for the given prompt type.\"\"\"\n    prompt_templates = {\n        \"features_table\": prompts[\"prompts\"][\"features_table\"],\n        \"overview\": prompts[\"prompts\"][\"overview\"],\n        \"tagline\": prompts[\"prompts\"][\"tagline\"],\n    }\n    return prompt_templates.get(prompt_type, \"\")",
    "repo": "readme-ai",
    "path": "readmeai\\models\\prompts.py",
    "type": "function",
    "name": "get_prompt_template",
    "loc": 21,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\prompts.py:function:inject_prompt_context:chunk1",
    "text": "def inject_prompt_context(template: str, context: dict) -> str:\n    \"\"\"Format the template with the provided context.\"\"\"\n    try:\n        return template.format(*[context[key] for key in context])\n    except KeyError as e:\n        _logger.error(f\"Missing context for prompt key: {e!r}\")\n        return \"\"\n    except Exception as e:\n        _logger.error(f\"Failed to format prompt template: {e!r}\")\n        return \"\"",
    "repo": "readme-ai",
    "path": "readmeai\\models\\prompts.py",
    "type": "function",
    "name": "inject_prompt_context",
    "loc": 31,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\prompts.py:function:set_additional_contexts:chunk1",
    "text": "def set_additional_contexts(\n    config: Settings,\n    repo_context: RepositoryContext,\n    file_summaries: list[tuple[str, str]],\n) -> list[dict]:\n    \"\"\"Build additional prompts for README content generation.\"\"\"\n    return [\n        {\"type\": prompt_type, \"context\": context}\n        for prompt_type, context in [\n            (\n                \"features_table\",\n                {\n                    \"project_name\": config.git.name,\n                    \"repository\": config.git.repository,\n                    \"languages\": repo_context.languages,\n                    \"dependencies\": repo_context.dependencies,\n                    \"cicd\": repo_context.metadata.get(\"cicd\", []),\n                    \"containers\": repo_context.metadata.get(\"containers\", []),\n                    \"documentation\": repo_context.metadata.get(\"documentation\", []),\n                    \"package_managers\": repo_context.metadata.get(\n                        \"package_managers\", []\n                    ),\n                    \"file_summaries\": file_summaries,\n                },\n            ),\n            (\n                \"overview\",\n                {\n                    \"project_name\": config.git.name,\n                    \"file_summary\": file_summaries,\n                },\n            ),\n            (\n                \"tagline\",\n                {\n                    \"name\": config.git.name,\n                    \"repo\": config.git.repository,\n                    \"file_summary\": file_summaries,\n                },\n            ),\n        ]\n    ]",
    "repo": "readme-ai",
    "path": "readmeai\\models\\prompts.py",
    "type": "function",
    "name": "set_additional_contexts",
    "loc": 43,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\prompts.py:function:set_summary_context:chunk1",
    "text": "def set_summary_context(\n    config: Settings,\n    repo_files: list[tuple[str, str]],\n) -> list[dict]:\n    \"\"\"Generates the summary prompts to be used by the LLM API.\"\"\"\n    return [\n        {\"type\": prompt_type, \"context\": context}\n        for prompt_type, context in [\n            (\n                \"file_summary\",\n                {\n                    \"tree\": config.md.directory_structure,\n                    \"repo_files\": repo_files,\n                },\n            ),\n        ]\n    ]",
    "repo": "readme-ai",
    "path": "readmeai\\models\\prompts.py",
    "type": "function",
    "name": "set_summary_context",
    "loc": 87,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\tokens.py:function:_set_encoding_cache:chunk1",
    "text": "def _set_encoding_cache(encoding_name: str) -> Encoding:\n    \"\"\"Set the encoding cache for a specific encoding.\"\"\"\n    if encoding_name not in _encoding_cache:\n        _encoding_cache[encoding_name] = get_encoding(encoding_name)\n    return _encoding_cache[encoding_name]",
    "repo": "readme-ai",
    "path": "readmeai\\models\\tokens.py",
    "type": "function",
    "name": "_set_encoding_cache",
    "loc": 13,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\tokens.py:function:count_tokens:chunk1",
    "text": "def count_tokens(text: str, encoder: str) -> int:\n    \"\"\"Return the number of tokens in a text string.\"\"\"\n    try:\n        encoding = _set_encoding_cache(encoder)\n        token_count = len(encoding.encode(text, disallowed_special=()))\n\n    except (UnicodeEncodeError, ValueError) as exc:\n        _logger.error(\n            f\"Error counting tokens for '{text}' with {encoder}: {exc}\",\n        )\n        token_count = 0\n\n    return token_count",
    "repo": "readme-ai",
    "path": "readmeai\\models\\tokens.py",
    "type": "function",
    "name": "count_tokens",
    "loc": 46,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\tokens.py:function:truncate_tokens:chunk1",
    "text": "def truncate_tokens(encoding: str, text: str, max_count: int) -> str:\n    \"\"\"Truncate a text string to a maximum number of tokens.\"\"\"\n    if not text:\n        return text\n    try:\n        encoder = _set_encoding_cache(encoding)\n        token_count = len(encoder.encode(text))\n        if token_count <= max_count:\n            return text\n        char_total = len(text)\n        chars_per_token = char_total / token_count\n        truncated_total = int(chars_per_token * max_count)\n        return text[:truncated_total]\n\n    except Exception as exc:\n        _logger.error(f\"Error truncating tokens for '{text}': {exc}\")\n        return text",
    "repo": "readme-ai",
    "path": "readmeai\\models\\tokens.py",
    "type": "function",
    "name": "truncate_tokens",
    "loc": 61,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\models\\tokens.py:function:update_max_tokens:chunk1",
    "text": "def update_max_tokens(\n    max_tokens: int,\n    prompt: str,\n    target: str = \"Hello!\",\n) -> int:\n    \"\"\"Adjust the maximum number of tokens based on the specific prompt.\"\"\"\n    is_valid_prompt = prompt.strip().startswith(target.strip())\n    return max_tokens if is_valid_prompt else max_tokens // 2",
    "repo": "readme-ai",
    "path": "readmeai\\models\\tokens.py",
    "type": "function",
    "name": "update_max_tokens",
    "loc": 80,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\base.py:class:BaseFileParser:chunk1",
    "text": "class BaseFileParser(ABC):\n    \"\"\"\n    Abstract base class for dependency file parsers.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._logger = get_logger(__name__)\n\n    @abstractmethod\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Parses content of file and return list of dependencies.\"\"\"\n        ...\n\n    def log_error(self, message: str):\n        \"\"\"Logs error message when parsing fails.\"\"\"\n        self._logger.error(f\"Error parsing dependency file {message}\")\n\n    def handle_parsing_error(self, error: Exception) -> list[str]:\n        \"\"\"Standardized error handling for parsing exceptions.\"\"\"\n        self.log_error(repr(error))\n        return []",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\base.py",
    "type": "class",
    "name": "BaseFileParser",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\base.py:class:DefaultParser:chunk1",
    "text": "class DefaultParser(BaseFileParser):\n    \"\"\"\n    Default parser for unknown file types.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Returns an empty list for unknown file types.\"\"\"\n        return []",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\base.py",
    "type": "class",
    "name": "DefaultParser",
    "loc": 29,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\cpp.py:class:CMakeParser:chunk1",
    "text": "class CMakeParser(BaseFileParser):\n    \"\"\"\n    Parser for CMake dependency files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts dependencies, libs, and software from a CMakeLists.txt.\"\"\"\n        try:\n            extracted_dependencies = []\n            for line in content.splitlines():\n                line = line.strip()\n                if (\n                    line.startswith(\"find_package\")\n                    or \"target_link_libraries\" in line\n                ):\n                    dependencies = re.findall(\n                        r\"(?:find_package\\(|target_link_libraries\\()\\s*(\\w+)\",\n                        line,\n                    )\n                    extracted_dependencies.extend(dependencies)\n\n                if line.startswith(\"target_link_libraries\") or line.startswith(\n                    \"find_package\",\n                ):\n                    libs = re.findall(\n                        r\"target_link_libraries\\([^)]+\\s+([^)]+)\\)\",\n                        line,\n                    )\n                    extracted_dependencies.extend(libs)\n\n            return list(set(extracted_dependencies))\n\n        except re.error as exc:\n            return self.handle_parsing_error(f\"CMakeLists.txt: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\cpp.py",
    "type": "class",
    "name": "CMakeParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\cpp.py:class:ConfigureAcParser:chunk1",
    "text": "class ConfigureAcParser(BaseFileParser):\n    \"\"\"\n    Parser for configure.ac dependency files.\n    \"\"\"\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from a configure.ac file.\"\"\"\n        try:\n            regex = re.compile(r\"AC_CHECK_LIB\\([^)]+\\s+([^)]+)\\)\")\n            return regex.findall(content)\n        except re.error as exc:\n            return self.handle_parsing_error(f\"configure.ac: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\cpp.py",
    "type": "class",
    "name": "ConfigureAcParser",
    "loc": 50,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\cpp.py:class:MakefileAmParser:chunk1",
    "text": "class MakefileAmParser(BaseFileParser):\n    \"\"\"\n    Parser for Makefile dependency files.\n    \"\"\"\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts dependencies from Makefile.am files.\"\"\"\n        try:\n            extracted_packages = []\n            bin_programs_regex = r\"bin_PROGRAMS\\s*=\\s*(.*)\"\n            match = re.search(bin_programs_regex, content)\n\n            if match:\n                programs = match.group(1).split()\n                extracted_packages.extend(programs)\n\n            lib_sources_regex = r\"libfoo_la_SOURCES\\s*=\\s*(.*)\"\n            match = re.search(lib_sources_regex, content)\n\n            if match:\n                sources = match.group(1).split()\n                extracted_packages.extend(sources)\n\n            check_programs_regex = r\"check_PROGRAMS\\s*=\\s*(.*)\"\n            match = re.search(check_programs_regex, content)\n\n            if match:\n                programs = match.group(1).split()\n                extracted_packages.extend(programs)\n\n            check_libs_regex = r\"check_LTLIBRARIES\\s*=\\s*(.*)\"\n            match = re.search(check_libs_regex, content)\n\n            return list(set(extracted_packages))\n\n        except re.error as exc:\n            return self.handle_parsing_error(f\"Makefile.am: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\cpp.py",
    "type": "class",
    "name": "MakefileAmParser",
    "loc": 64,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\docker.py:class:DockerfileParser:chunk1",
    "text": "class DockerfileParser(BaseFileParser):\n    \"\"\"\n    Parser for Dockerfile dependency files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from a Dockerfile.\"\"\"\n        try:\n            dependencies = []\n            lines = content.split(\"\\n\")\n            for line in lines:\n                if match := re.search(\n                    r\"^FROM\\s+(?:--platform=[^\\s]+\\s+)?([^\\s:]+):?([^\\s]*)\",\n                    line,\n                ):\n                    base_image, version = match.groups()\n                    if not version:\n                        version = \"latest\"\n                    dependencies.append((base_image, version))\n\n            return [f\"{dep[0]}: {dep[1]}\" for dep in dependencies]\n\n        except re.error as e:\n            return self.handle_parsing_error(e)",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\docker.py",
    "type": "class",
    "name": "DockerfileParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\docker.py:class:DockerComposeParser:chunk1",
    "text": "class DockerComposeParser(BaseFileParser):\n    \"\"\"\n    Parser for docker-compose.yaml files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n        self.compose_data: dict[str, Any] = {}\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Parses docker-compose YAML content and returns dependency list.\"\"\"\n        try:\n            self.compose_data = yaml.safe_load(content)\n            return self.get_services()\n        except yaml.YAMLError as e:\n            return self.handle_parsing_error(e)\n\n    def get_services(self) -> list[str]:\n        \"\"\"Get a list of all service names from the docker-compose.yaml file.\"\"\"\n        if self.compose_data is None:\n            return []\n        return list(self.compose_data.get(\"services\", {}).keys())\n\n    def get_service_info(self, service_name: str) -> dict[str, Any]:\n        \"\"\"Get detailed information for a specific service.\"\"\"\n        return self.compose_data.get(\"services\", {}).get(service_name, {})\n\n    def get_service_environment(self, service_name: str) -> list[str]:\n        \"\"\"Get the environment variables for a specific service.\"\"\"\n        service_info = self.get_service_info(service_name)\n        return service_info.get(\"environment\", [])\n\n    def get_service_ports(self, service_name: str) -> list[str]:\n        \"\"\"Get the ports for a specific service.\"\"\"\n        service_info = self.get_service_info(service_name)\n        return service_info.get(\"ports\", [])\n\n    def get_service_command(self, service_name: str) -> str:\n        \"\"\"Get the command for a specific service.\"\"\"\n        service_info = self.get_service_info(service_name)\n        return service_info.get(\"command\", \"\")\n\n    def get_service_networks(self, service_name: str) -> list[str]:\n        \"\"\"Get the networks for a specific service.\"\"\"\n        service_info = self.get_service_info(service_name)\n        return service_info.get(\"networks\", [])\n\n    def get_service_image(self, service_name: str) -> str:\n        \"\"\"Get the image used by a specific service.\"\"\"\n        service_info = self.get_service_info(service_name)\n        return service_info.get(\"image\", \"\")\n\n    def get_all_service_details(self) -> list[dict[str, dict[str, Any]]]:\n        \"\"\"Get all metadata for all services.\"\"\"\n        return [\n            {\n                service_name: {\n                    \"image\": self.get_service_image(service_name),\n                    \"environment\": self.get_service_environment(service_name),\n                    \"ports\": self.get_service_ports(service_name),\n                    \"command\": self.get_service_command(service_name),\n                    \"networks\": self.get_service_networks(service_name),\n                    \"details\": self.get_service_info(service_name),\n                }\n                for service_name in self.get_services()\n            }\n        ]",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\docker.py",
    "type": "class",
    "name": "DockerComposeParser",
    "loc": 39,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\factory.py:class:ParserFactory:chunk1",
    "text": "class ParserFactory:\n    \"\"\"\n    Factory for creating dependency file parser callable objects.\n    \"\"\"\n\n    _parsers: ClassVar[dict[str, type[BaseFileParser]]] = {\n        # Python\n        \"Pipfile\": TomlParser,\n        \"pyproject.toml\": TomlParser,\n        \"requirements.in\": RequirementsParser,\n        \"requirements.txt\": RequirementsParser,\n        \"requirements-dev.txt\": RequirementsParser,\n        \"requirements-test.txt\": RequirementsParser,\n        \"requirements-prod.txt\": RequirementsParser,\n        \"dev-requirements.txt\": RequirementsParser,\n        \"environment.yml\": YamlParser,\n        \"environment.yaml\": YamlParser,\n        \"poetry.lock\": DefaultParser,\n        \"pdm.lock\": DefaultParser,\n        # C/C++\n        \"CMakeLists.txt\": CMakeParser,\n        \"configure.ac\": ConfigureAcParser,\n        \"Makefile.am\": MakefileAmParser,\n        # JavaScript/Node.js\n        \"package.json\": PackageJsonParser,\n        # Kotlin/Kotlin DSL\n        \"build.gradle\": BuildGradleParser,\n        \"build.gradle.kts\": BuildGradleKtsParser,\n        # Go\n        \"go.mod\": GoModParser,\n        # Java\n        \"pom.xml\": MavenParser,\n        # Rust\n        \"Cargo.toml\": CargoTomlParser,\n        # Swift\n        \"Package.swift\": SwiftPackageParser,\n        # Docker\n        \"Dockerfile\": DockerfileParser,\n        \"docker-compose.yaml\": DockerComposeParser,\n        \"docker-compose.yml\": DockerComposeParser,\n        # Properties\n        \".properties\": PropertiesParser,\n    }\n\n    @classmethod\n    def register_parser(cls, file_name: str, parser_class: type[BaseFileParser]):\n        \"\"\"Register a parser for the given file name.\"\"\"\n        cls._parsers[file_name] = parser_class\n\n    @classmethod\n    def create_parser(cls, file_name: str) -> BaseFileParser:\n        \"\"\"Create a parser for the given file name.\"\"\"\n        if parser_class := cls._parsers.get(file_name):\n            return parser_class()\n        else:\n            return DefaultParser()",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\factory.py",
    "type": "class",
    "name": "ParserFactory",
    "loc": 18,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\go.py:class:GoModParser:chunk1",
    "text": "class GoModParser(BaseFileParser):\n    \"\"\"\n    Parser for go.mod files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Parse the content of a go.mod file.\"\"\"\n        try:\n            lines = content.split(\"\\n\")\n            pattern = r\"^\\s*([\\w\\.\\-_/]+)\\s+v[\\w\\.\\-_/]+\"\n            regex = re.compile(pattern)\n            package_names = (\n                regex.match(line.strip()).group(1).split(\"/\")[-1]\n                for line in lines\n                if regex.match(line.strip())\n            )\n            return list(package_names)\n\n        except Exception as exc:\n            return self.handle_parsing_error(f\"go.mod: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\go.py",
    "type": "class",
    "name": "GoModParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\gradle.py:class:BuildGradleParser:chunk1",
    "text": "class BuildGradleParser(BaseFileParser):\n    \"\"\"\n    Parser for build.gradle dependency files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from a build.gradle file.\"\"\"\n        try:\n            pattern = r\"(implementation|classpath|api|testImplementation|androidTestImplementation|kapt)\\s+['\\\"]([^'\\\"]+)['\\\"]\"\n            matches = re.findall(pattern, content)\n            package_names = set()\n            for _, dependency in matches:\n                parts = dependency.split(\":\")\n                for part in parts:\n                    part = part.split(\".\")\n                    for p in part:\n                        if p.isalpha():\n                            package_names.add(p)\n\n            return list(package_names)\n\n        except re.error as exc:\n            return self.handle_parsing_error(f\"build.gradle: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\gradle.py",
    "type": "class",
    "name": "BuildGradleParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\gradle.py:class:BuildGradleKtsParser:chunk1",
    "text": "class BuildGradleKtsParser(BaseFileParser):\n    \"\"\"\n    Parser for build.gradle.kts dependency files.\n    \"\"\"\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from a build.gradle.kts file.\"\"\"\n        try:\n            pattern = r\"(\\bimplementation|testImplementation)\\s*\\((['\\\"])([^'\\\"]+)\\2\\)\"\n            matches = re.findall(pattern, content)\n            package_names = set()\n            for _, _, dependency in matches:\n                parts = dependency.split(\":\")\n                for part in parts:\n                    part = part.split(\".\")\n                    for p in part:\n                        if p.isalpha():\n                            package_names.add(p)\n            return list(package_names)\n\n        except re.error as error:\n            return self.handle_parsing_error(f\"build.gradle.kts: {error!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\gradle.py",
    "type": "class",
    "name": "BuildGradleKtsParser",
    "loc": 39,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\maven.py:class:MavenParser:chunk1",
    "text": "class MavenParser(BaseFileParser):\n    \"\"\"\n    Parser for Maven dependency files in pom.xml format.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extract packages names from Maven pom.xml files.\"\"\"\n        try:\n            regex = re.compile(\n                r\"<dependency>\\s*<groupId>([^<]+)</groupId>\\s*<artifactId>([^<]+)</artifactId>\\s*<version>([^<]+)</version>\",\n            )\n            matches = regex.findall(content)\n            dependencies = [\n                artifact_id for group_id, artifact_id, version in matches\n            ]\n            if any(\"spring\" in dependency for dependency in dependencies):\n                dependencies.append(\"spring\")\n            return set(dependencies)\n\n        except re.error as exc:\n            return self.handle_parsing_error(f\"pom.xml: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\maven.py",
    "type": "class",
    "name": "MavenParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\npm.py:class:PackageJsonParser:chunk1",
    "text": "class PackageJsonParser(BaseFileParser):\n    \"\"\"\n    Parser for package.json dependency files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Returns a list of dependencies parsed from a json file.\"\"\"\n        try:\n            data = json.loads(content)\n            package_names = []\n            for section in [\n                \"dependencies\",\n                \"devDependencies\",\n                \"peerDependencies\",\n            ]:\n                if section in data:\n                    package_names.extend(data[section].keys())\n            return package_names\n\n        except json.JSONDecodeError as exc:\n            return self.handle_parsing_error(exc)",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\npm.py",
    "type": "class",
    "name": "PackageJsonParser",
    "loc": 8,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\properties.py:class:PropertiesParser:chunk1",
    "text": "class PropertiesParser(BaseFileParser):\n    def __init__(self):\n        super().__init__()\n        super().__init__()\n        self.common_tech_keywords = {\n            \"spring\",\n            \"gradle\",\n            \"maven\",\n            \"java\",\n            \"kotlin\",\n            \"python\",\n            \"node\",\n            \"react\",\n            \"angular\",\n            \"vue\",\n            \"docker\",\n            \"kubernetes\",\n            \"aws\",\n            \"azure\",\n            \"gcp\",\n            \"android\",\n            \"ios\",\n            \"swift\",\n            \"flutter\",\n            \"django\",\n            \"flask\",\n            \"fastapi\",\n            \"express\",\n            \"nest\",\n            \"tensorflow\",\n            \"pytorch\",\n            \"pandas\",\n            \"numpy\",\n            \"scikit\",\n            \"hadoop\",\n            \"spark\",\n            \"kafka\",\n            \"redis\",\n            \"mongodb\",\n            \"postgresql\",\n            \"mysql\",\n            \"oracle\",\n            \"elasticsearch\",\n            \"graphql\",\n            \"rest\",\n            \"grpc\",\n            \"protobuf\",\n            \"openapi\",\n            \"swagger\",\n            \"selenium\",\n            \"terraform\",\n            \"ansible\",\n            \"helm\",\n            \"git\",\n            \"svn\",\n            \"cicd\",\n            \"jenkins\",\n            \"circleci\",\n            \"gitlab\",\n            \"github\",\n            \"bitbucket\",\n            \"travis\",\n            \"webpack\",\n            \"babel\",\n            \"typescript\",\n            \"ruby\",\n            \"rails\",\n            \"scala\",\n            \"clojure\",\n            \"erlang\",\n            \"elixir\",\n            \"phoenix\",\n            \"laravel\",\n            \"svelte\",\n            \"webassembly\",\n            \"rust\",\n            \"go\",\n            \"julia\",\n            \"csharp\",\n            \".net\",\n            \"asp.net\",\n            \"qml\",\n            \"unity\",\n            \"unreal\",\n            \"cryengine\",\n            \"vulkan\",\n            \"opengl\",\n            \"threejs\",\n            \"blender\",\n            \"matlab\",\n            \"octave\",\n            \"fortran\",\n            \"r\",\n            \"tableau\",\n            \"powerbi\",\n            \"qlik\",\n            \"sap\",\n            \"salesforce\",\n            \"dynamics\",\n            \"stripe\",\n            \"paypal\",\n            \"braintree\",\n            \"rabbitmq\",\n            \"zeromq\",\n            \"socket.io\",\n            \"websockets\",\n            \"webrtc\",\n            \"oauth\",\n            \"jwt\",\n            \"saml\",\n            \"openid\",\n            \"hashicorp\",\n            \"cassandra\",\n            \"neo4j\",\n            \"arangodb\",\n            \"couchbase\",\n            \"dynamodb\",\n            \"memcached\",\n            \"sphinx\",\n            \"solr\",\n            \"lucene\",\n            \"next.js\",\n            \"nuxt.js\",\n            \"remix\",\n            \"gatsby\",\n            \"emotion\",\n            \"styled-components\",\n            \"tailwindcss\",\n            \"bootstrap\",\n            \"material-ui\",\n            \"ant-design\",\n            \"chakra-ui\",\n            \"quasar\",\n            \"primefaces\",\n            \"vuetify\",\n            \"fivem\",\n            \"qt\",\n            \"wxwidgets\",\n            \"tkinter\",\n            \"xamarin\",\n            \"react-native\",\n            \"electron\",\n            \"cordova\",\n            \"ionic\",\n            \"nativescript\",\n            \"appium\",\n            \"xstate\",\n            \"rxjs\",\n            \"mobx\",\n            \"redux\",\n            \"vuex\",\n            \"recoil\",\n            \"pinia\",\n            \"akka\",\n            \"vert.x\",\n            \"play\",\n            \"quarkus\",\n            \"micronaut\",\n            \"spring-boot\",\n            \"netty\",\n            \"kotlinx.coroutines\",\n            \"jetpack\",\n            \"room\",\n            \"workmanager\",\n            \"compose\",\n            \"pulumi\",\n            \"boto3\",\n            \"sdkman\",\n            \"jvm\",\n            \"graalvm\",\n            \"openjdk\",\n            \"v8\",\n            \"spidermonkey\",\n            \"webgl\",\n            \"d3.js\",\n            \"highcharts\",\n            \"amcharts\",\n            \"echarts\",\n            \"plotly\",\n            \"vega\",\n            \"kendo-ui\",\n        }\n\n    def parse(self, content: str) -> list[str]:\n        lines = content.split(\"\\n\")\n        dependencies = set()\n\n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n\n            dependencies.update(self._extract_from_line(line))\n\n        return sorted(list(dependencies))\n\n    def _clean_word(self, word: str) -> str:\n        word = word.lower()\n        word = re.sub(\n            r\"^(lib|org|com|net|io|api|sdk|cli|ui|app|service|utils?|helpers?|core|base|impl|test|dev|prod)\\.?\",\n            \"\",\n            word,\n        )\n        word = re.sub(\n            r\"(version|tool|module|plugin|dependency|lib|framework|platform)s?$\",\n            \"\",\n            word,\n        )\n        return word.strip()\n\n    def _extract_from_line(self, line: str) -> set[str]:\n        extracted = set()\n\n        if \"=\" in line:\n            key, value = line.split(\"=\", 1)\n            extracted.update(self._extract_words(key))\n            extracted.update(self._extract_words(value))\n\n            # Handle version-specific dependencies\n            if \"version\" in key.lower():\n                dep_name = re.sub(\n                    r\"version.*$\", \"\", key, flags=re.IGNORECASE\n                ).strip()\n                extracted.add(f\"{dep_name}: {value}\")\n        else:\n            extracted.update(self._extract_words(line))\n\n        return extracted\n\n    def _extract_words(self, text: str) -> set[str]:\n        words = {text.lower()}\n        parts = re.split(r\"[._-]\", text)\n        words.update(part.lower() for part in parts)\n\n        for i in range(len(parts)):\n            for j in range(i + 1, len(parts) + 1):\n                words.add(\".\".join(parts[i:j]).lower())\n\n        # Split camelCase\n        for part in parts:\n            words.update(self._split_camel_case(part))\n\n        return words\n\n    def _filter_technologies(self, words: set[str]) -> set[str]:\n        return {\n            word\n            for word in words\n            if word in self.common_tech_keywords or len(word) > 1\n        }\n\n    def _split_camel_case(self, text: str) -> set[str]:\n        words = re.findall(\n            r\"[A-Z]?[a-z]+|[A-Z]{2,}(?=[A-Z][a-z]|\\d|\\W|$)|\\d+\", text\n        )\n        return {word.lower() for word in words}",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\properties.py",
    "type": "class",
    "name": "PropertiesParser",
    "loc": 8,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\python.py:class:RequirementsParser:chunk1",
    "text": "class RequirementsParser(BaseFileParser):\n    \"\"\"\n    Parser for requirements.txt files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"\n        Extracts package names from requirements.txt file.\n        Excludes the version specifiers and comments.\n        \"\"\"\n        try:\n            lines = content.splitlines()\n            package_names = []\n            for line in lines:\n                line = line.split(\"#\", 1)[0].strip()\n                if not line:\n                    continue\n                if match := re.match(r\"([a-zA-Z0-9\\-_]+)\", line):\n                    package_names.append(match[1])\n            return package_names\n\n        except re.error as exc:\n            return self.handle_parsing_error(f\"requirements.txt: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\python.py",
    "type": "class",
    "name": "RequirementsParser",
    "loc": 16,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\python.py:class:TomlParser:chunk1",
    "text": "class TomlParser(BaseFileParser):\n    \"\"\"\n    Parser for Python TOML dependency files.\n\n    - Handles build systems:\n        - Pipenv, Poetry, Flit, Hatch, etc.\n    \"\"\"\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts all package dependencies from a TOML file.\"\"\"\n        try:\n            data = toml.loads(content)\n            dependencies = []\n\n            # Look for dependencies in different possible sections\n            # Pipfile-style\n            if \"packages\" in data:\n                dependencies.extend(data.get(\"packages\", {}).keys())\n            if \"dev-packages\" in data:\n                dependencies.extend(data.get(\"dev-packages\", {}).keys())\n\n            # pyproject.toml-style (Poetry, Hatch, Flit)\n            if \"project\" in data:\n                dependencies.extend(\n                    self.extract_package_names(\n                        data[\"project\"].get(\"dependencies\", [])\n                    )\n                )\n                if \"optional-dependencies\" in data[\"project\"]:\n                    for group_deps in data[\"project\"][\n                        \"optional-dependencies\"\n                    ].values():\n                        dependencies.extend(\n                            self.extract_package_names(group_deps)\n                        )\n\n            # For build-system specific tools like Poetry\n            if \"tool\" in data:\n                if \"poetry\" in data[\"tool\"]:\n                    poetry_data = data[\"tool\"][\"poetry\"]\n                    dependencies.extend(\n                        poetry_data.get(\"dependencies\", {}).keys()\n                    )\n\n                    # Add dev-dependencies and any group dependencies\n                    for group in [\"dev-dependencies\", \"group\"]:\n                        if group in poetry_data:\n                            group_data = poetry_data[group]\n                            if isinstance(group_data, dict):\n                                for section in group_data.values():\n                                    dependencies.extend(\n                                        section.get(\"dependencies\", {}).keys()\n                                    )\n\n                if \"hatch\" in data[\"tool\"]:\n                    hatch_data = data[\"tool\"][\"hatch\"].get(\"envs\", {})\n                    for env, env_data in hatch_data.items():\n                        dependencies.extend(env_data.get(\"dependencies\", []))\n                        if \"extra-dependencies\" in env_data:\n                            dependencies.extend(env_data[\"extra-dependencies\"])\n\n            # For Cargo.toml (Rust) style\n            if \"dependencies\" in data:\n                dependencies.extend(data.get(\"dependencies\", {}).keys())\n            if \"dev-dependencies\" in data:\n                dependencies.extend(data.get(\"dev-dependencies\", {}).keys())\n\n            return list(set(dependencies))\n\n        except Exception as exc:\n            return self.handle_parsing_error(exc)\n\n    def extract_package_names(self, dependencies: list[str]) -> list[str]:\n        \"\"\"Helper method to extract package names from a list of dependencies.\"\"\"\n        package_names = []\n        for dep in dependencies:\n            # Handle cases where dependencies might be in the format 'package_name>=1.0.0'\n            if isinstance(dep, str):\n                package_name = (\n                    dep.split(\">=\")[0].split(\"<=\")[0].split(\"==\")[0].strip()\n                )\n                package_names.append(package_name)\n            elif isinstance(dep, dict):\n                package_names.extend(dep.keys())\n        return package_names\n\n    def extract_package_names(self, dependencies: list[str]) -> list[str]:\n        \"\"\"Helper method to extract package names from dependency strings.\"\"\"\n        package_names = []\n        for dep in dependencies:\n            match = re.match(r\"([a-zA-Z0-9\\-_]+)\", dep.split(\";\")[0].strip())\n            if match:\n                package_names.append(match.group(1))\n        return package_names",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\python.py",
    "type": "class",
    "name": "TomlParser",
    "loc": 44,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\python.py:class:YamlParser:chunk1",
    "text": "class YamlParser(BaseFileParser):\n    \"\"\"\n    Parser for Python YAML based dependency files i.e. environment.yml\n    \"\"\"\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from environment.yml file.\"\"\"\n        try:\n            data = yaml.safe_load(content)\n\n            if isinstance(data, dict) and \"dependencies\" in data:\n                dependencies = []\n\n                for package in data[\"dependencies\"]:\n                    if isinstance(package, str):\n                        dependencies.append(\n                            package.split(\"=\")[0].split(\">\")[0].split(\"<\")[0],\n                        )\n                    elif isinstance(package, dict):\n                        for pip_package in package.values():\n                            if isinstance(pip_package, list):\n                                for pip_dep in pip_package:\n                                    dependencies.append(pip_dep.split(\"==\")[0])\n\n            return dependencies\n\n        except yaml.YAMLError as exc:\n            return self.handle_parsing_error(exc)",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\python.py",
    "type": "class",
    "name": "YamlParser",
    "loc": 140,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\rust.py:class:CargoTomlParser:chunk1",
    "text": "class CargoTomlParser(BaseFileParser):\n    \"\"\"\n    Parser for Rust cargo.toml dependency files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extract packages names from Rust TOML files.\"\"\"\n        try:\n            data = toml.loads(content)\n            dependencies = []\n\n            if \"dependencies\" in data:\n                dependencies.extend(data[\"dependencies\"].keys())\n            if \"dev-dependencies\" in data:\n                dependencies.extend(data[\"dev-dependencies\"].keys())\n\n            for key in data:\n                if key.startswith(\"dependencies.\") and isinstance(\n                    data[key],\n                    dict,\n                ):\n                    dependencies.extend(data[key].keys())\n\n            return dependencies\n\n        except Exception as exc:\n            return self.handle_parsing_error(f\"cargo.toml: {exc!s}\")",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\rust.py",
    "type": "class",
    "name": "CargoTomlParser",
    "loc": 15,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\parsers\\swift.py:class:SwiftPackageParser:chunk1",
    "text": "class SwiftPackageParser(BaseFileParser):\n    \"\"\"\n    Parser for Swift Package.swift files.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initializes the handler with given configuration.\"\"\"\n        super().__init__()\n\n    def parse(self, content: str) -> list[str]:\n        \"\"\"Extracts package names from a Package.swift file.\"\"\"\n        try:\n            package_names = set()\n            lines = content.splitlines()\n\n            for line in lines:\n                line = line.strip()\n\n                if \".package(url:\" in line or \".package(name:\" in line:\n                    url_match = re.search(r'url:\\s*\"([^\"]+)\"', line)\n                    name_match = re.search(r'name:\\s*\"([^\"]+)\"', line)\n\n                    if url_match:\n                        url = url_match.group(1)\n                        package_name = self.extract_package_name_from_url(url)\n                        package_names.add(package_name)\n                    elif name_match:\n                        package_name = name_match.group(1)\n                        package_names.add(package_name)\n\n                elif \"dependencies:\" in line:\n                    dep_match = re.findall(r\"\\\"([^\\\"]+)\\\"\", line)\n                    package_names.update(dep_match)\n\n            return list(package_names)\n\n        except Exception as exc:\n            return self.handle_parsing_error(f\"Package.swift: {exc!s}\")\n\n    @staticmethod\n    def extract_package_name_from_url(url: str) -> str:\n        \"\"\"Extracts the package name from a GitHub URL.\"\"\"\n        package_name = url.split(\"/\")[-1]\n        if package_name.endswith(\".git\"):\n            package_name = package_name[:-4]\n        return package_name",
    "repo": "readme-ai",
    "path": "readmeai\\parsers\\swift.py",
    "type": "class",
    "name": "SwiftPackageParser",
    "loc": 10,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\markdown_to_html.py:function:convert:chunk1",
    "text": "def convert(text: str) -> str:\n    \"\"\"Convert any markdown syntax in the given text to HTML.\"\"\"\n\n    def process_inline(content: str) -> str:\n        \"\"\"Process inline markdown syntax.\"\"\"\n\n        def nested_format(match) -> str | Any:\n            \"\"\"Handle nested formatting within inline elements.\"\"\"\n            inner_content = process_inline(match.group(1))\n            if match.re == BOLD_RE:\n                return f\"<strong>{inner_content}</strong>\"\n            elif match.re == ITALIC_RE:\n                return f\"<em>{inner_content}</em>\"\n            return match.group(0)\n\n        content = INLINE_CODE_RE.sub(r\"<code>\\1</code>\", content)\n        content = BOLD_RE.sub(nested_format, content)\n        content = ITALIC_RE.sub(nested_format, content)\n        content = LINK_RE.sub(r'<a href=\"\\2\">\\1</a>', content)\n        return content\n\n    for i, pattern in enumerate(HEADER_RE, start=1):\n        text = pattern.sub(\n            lambda m: f\"<h{7 - i}>{process_inline(m.group(1))}</h{7 - i}>\", text\n        )\n\n    lines = text.split(\"\\n\")\n    in_list = False\n    list_type = None\n    list_indent = \"\"\n    result = []\n\n    for line in lines:\n        unordered_match = UNORDERED_LIST_RE.match(line)\n        ordered_match = ORDERED_LIST_RE.match(line)\n\n        if unordered_match or ordered_match:\n            match = unordered_match or ordered_match\n            indent, content = match.groups()\n\n            if not in_list:\n                list_indent = indent\n                list_type = \"ul\" if unordered_match else \"ol\"\n                result.append(f\"{indent}<{list_type}>\")\n                in_list = True\n            elif (\n                list_type != (\"ul\" if unordered_match else \"ol\")\n                or indent != list_indent\n            ):\n                result.append(f\"{list_indent}</{list_type}>\")\n                list_indent = indent\n                list_type = \"ul\" if unordered_match else \"ol\"\n                result.append(f\"{indent}<{list_type}>\")\n\n            result.append(f\"{indent}<li>{process_inline(content)}</li>\")\n        else:\n            if in_list:\n                result.append(f\"{list_indent}</{list_type}>\")\n                in_list = False\n                list_type = None\n            result.append(process_inline(line))\n\n    if in_list:\n        result.append(f\"{list_indent}</{list_type}>\")\n\n    return \"\\n\".join(result)",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\markdown_to_html.py",
    "type": "function",
    "name": "convert",
    "loc": 35,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\markdown_to_html.py:function:process_inline:chunk1",
    "text": "def process_inline(content: str) -> str:\n        \"\"\"Process inline markdown syntax.\"\"\"\n\n        def nested_format(match) -> str | Any:\n            \"\"\"Handle nested formatting within inline elements.\"\"\"\n            inner_content = process_inline(match.group(1))\n            if match.re == BOLD_RE:\n                return f\"<strong>{inner_content}</strong>\"\n            elif match.re == ITALIC_RE:\n                return f\"<em>{inner_content}</em>\"\n            return match.group(0)\n\n        content = INLINE_CODE_RE.sub(r\"<code>\\1</code>\", content)\n        content = BOLD_RE.sub(nested_format, content)\n        content = ITALIC_RE.sub(nested_format, content)\n        content = LINK_RE.sub(r'<a href=\"\\2\">\\1</a>', content)\n        return content",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\markdown_to_html.py",
    "type": "function",
    "name": "process_inline",
    "loc": 38,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\markdown_to_html.py:function:nested_format:chunk1",
    "text": "def nested_format(match) -> str | Any:\n            \"\"\"Handle nested formatting within inline elements.\"\"\"\n            inner_content = process_inline(match.group(1))\n            if match.re == BOLD_RE:\n                return f\"<strong>{inner_content}</strong>\"\n            elif match.re == ITALIC_RE:\n                return f\"<em>{inner_content}</em>\"\n            return match.group(0)",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\markdown_to_html.py",
    "type": "function",
    "name": "nested_format",
    "loc": 41,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:fix_markdown_table_rows:chunk1",
    "text": "def fix_markdown_table_rows(md_table: str) -> str:\n    \"\"\"Format a Markdown table with feature and description columns.\"\"\"\n    lines = md_table.split(\"||\")\n\n    formatted_md_table = \"| Feature | Description |\\n|---------|-------------|\\n\"\n\n    for line in lines[2:]:\n        clean_line = line.strip(\"|\")\n        parts = clean_line.split(\"|\")\n\n        if len(parts) >= 3:\n            feature = parts[1].strip()\n            description = parts[2].strip()\n            formatted_row = f\"| {feature} | {description} |\\n\"\n            formatted_md_table += formatted_row\n\n    return formatted_md_table",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "fix_markdown_table_rows",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:format_markdown_table:chunk1",
    "text": "def format_markdown_table(text: str) -> str:\n    \"\"\"\n    Pattern to match a Markdown table. Looks for a\n    header row with at least two columns, followed by\n    a separator row, and then one or more data rows.\n    This version is designed to be more robust in removing\n    text around the markdown table.\n    \"\"\"\n    if \"REPLACE-ME</code>\" in text:\n        return text\n\n    pattern = r\"(?:.*\\n)*(\\|.*\\|.*\\n\\|[-: ]+\\|[-: ]+\\|.*\\n(?:\\|.*\\|.*\\n)*)(?:.*\\n)*\"\n    match = re.search(pattern, text, re.DOTALL)\n    return match[1].strip() if match else \"\"",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "format_markdown_table",
    "loc": 25,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:process_markdown:chunk1",
    "text": "def process_markdown(text):\n    \"\"\"Remove uneven Markdown syntax while preserving valid formatting.\"\"\"\n    # Remove extra asterisks at the end of lines\n    text = re.sub(r\"\\*+$\", \"\", text, flags=re.MULTILINE)\n\n    # Remove unmatched bullets or hyphens at the beginning of lines\n    text = re.sub(r\"^[\\s]*[-*]\\s+\", \"\", text, flags=re.MULTILINE)\n\n    # Preserve valid bold and italic formatting\n    # This regex handles nested bold and italic formatting\n    text = re.sub(\n        r\"\\*{1,2}(?P<content>[^*\\n]+(?:\\*{1,2}[^*\\n]+\\*{1,2}[^*\\n]+)*)\\*{1,2}\",\n        lambda m: m.group(0) if m.group(0).count(\"*\") % 2 == 0 else m.group(0)[1:-1],\n        text,\n    )\n\n    # Remove standalone asterisks or underscores\n    text = re.sub(r\"(?<!\\*)\\*(?!\\*)|(?<!_)_(?!_)\", \"\", text)\n\n    return text.strip()",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "process_markdown",
    "loc": 41,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:process_text:chunk1",
    "text": "def process_text(text: str) -> str:\n    \"\"\"Format and clean generated text from the LLM.\"\"\"\n    # Dynamically remove all text before and including the first colon if any exist\n    text = re.sub(r\"^[^:]*:\\s*\", \"\", text)\n\n    # Remove any text before and including \"**:\"\n    text = re.sub(r\"\\*\\*:\\s*\", \"\", text, flags=re.DOTALL)\n\n    # Remove single and double quotes that are missing their closing counterpart\n    text = re.sub(r\"['\\\"](.*?)$\", r\"\\1\", text)\n    text = re.sub(r\"^(.*?)['\\\"]\", r\"\\1\", text)\n\n    # Remove specific pattern and rephrase\n    text = re.sub(\n        r\"\\*\\*Code Summary:\\*\\*\\s*(.*?)\\s*provides functions to\",\n        r\"Provides functions to\",\n        text,\n        flags=re.DOTALL,\n    )\n    # Remove single and double quotes around any text\n    text = re.sub(r\"(?<!\\w)['\\\"](.*?)['\\\"](?!\\w)\", r\"\\1\", text)\n\n    # Remove newlines and tabs\n    text = text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n\n    # Remove non-letter characters from the beginning of the string\n    text = re.sub(r\"^[^a-zA-Z]*\", \"\", text)\n\n    # Remove extra white space around punctuation except for '('\n    text = re.sub(r\"\\s*([)'.!,?;:])(?!\\.\\s*\\w)\", r\"\\1\", text)\n\n    # Remove extra white space before opening parentheses\n    text = re.sub(r\"(\\()\\s*\", r\"\\1\", text)\n\n    # Replace multiple consecutive spaces with a single space\n    text = re.sub(r\" +\", \" \", text)\n\n    # Remove extra white space around hyphens\n    text = re.sub(r\"\\s*-\\s*\", \"-\", text)\n\n    # Specifically target and remove trailing special characters like asterisks\n    text = re.sub(r\"\\*+$\", \"\", text)\n\n    text = text.strip()\n\n    # Ensure the first letter is capitalized if it's alphabetic\n    if text and not text[0].isupper() and text[0].isalpha():\n        text = text[0].upper() + text[1:]\n\n    return text",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "process_text",
    "loc": 63,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:extract_text_between_tags:chunk1",
    "text": "def extract_text_between_tags(input_string: str, start_tag: str, end_tag: str) -> str:\n    \"\"\"Extract text between <overview/intro> tags in a string.\"\"\"\n    match = re.search(\n        rf\"{start_tag}(.*){end_tag}\",\n        input_string,\n        re.DOTALL | re.IGNORECASE,\n    )\n    return match.group(1).strip() if match else \"\"",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "extract_text_between_tags",
    "loc": 115,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\postprocessor\\response_cleaner.py:function:remove_quotes:chunk1",
    "text": "def remove_quotes(text: str) -> str:\n    \"\"\"Remove quotes from a string if they exist.\"\"\"\n    if not text or len(text) < 2:\n        return text\n    quote_chars = (\"'\", '\"', \"`\")\n    return text[1:-1] if text[0] == text[-1] and text[0] in quote_chars else text",
    "repo": "readme-ai",
    "path": "readmeai\\postprocessor\\response_cleaner.py",
    "type": "function",
    "name": "remove_quotes",
    "loc": 125,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\preprocessor\\document_cleaner.py:class:DocumentCleaner:chunk1",
    "text": "class DocumentCleaner:\n    \"\"\"\n    Document cleaner to preprocess repository content.\n    \"\"\"\n\n    def __init__(\n        self,\n        remove_empty_lines: bool = True,\n        remove_extra_whitespaces: bool = True,\n        remove_trailing_whitespaces: bool = True,\n        normalize_indentation: bool = True,\n        dedent: bool = False,\n    ):\n        self.remove_empty_lines = remove_empty_lines\n        self.remove_extra_whitespaces = remove_extra_whitespaces\n        self.remove_trailing_whitespaces = remove_trailing_whitespaces\n        self.normalize_indentation = normalize_indentation\n        self.dedent = dedent\n\n    def clean(self, code: str) -> str:\n        \"\"\"Clean the given document string.\"\"\"\n        lines = code.splitlines()\n\n        if self.remove_empty_lines:\n            lines = [line for line in lines if line.strip()]\n\n        if self.remove_trailing_whitespaces:\n            lines = [line.rstrip() for line in lines]\n\n        if self.normalize_indentation:\n            lines = self._normalize_indentation(\"\\n\".join(lines)).splitlines()\n\n        result = \"\\n\".join(lines)\n\n        if self.dedent:\n            result = textwrap.dedent(result)\n\n        if self.remove_extra_whitespaces:\n            # Only remove extra spaces within each line, preserving leading spaces\n            lines = result.splitlines()\n            lines = [\n                self._preserve_indent_remove_extra_spaces(line)\n                for line in lines\n            ]\n            result = \"\\n\".join(lines)\n\n        return result.rstrip()\n\n    def _preserve_indent_remove_extra_spaces(self, line: str) -> str:\n        \"\"\"Remove extra whitespaces while preserving leading indentation.\"\"\"\n        if not line.strip():\n            return \"\"\n        indent = len(line) - len(line.lstrip())\n        return \" \" * indent + re.sub(r\"\\s+\", \" \", line.lstrip())\n\n    def _normalize_indentation(self, code: str) -> str:\n        \"\"\"Normalize indentation to spaces.\"\"\"\n        if not code:\n            return code\n\n        lines = code.splitlines()\n        normalized_lines = []\n\n        for line in lines:\n            if not line.strip():\n                normalized_lines.append(\"\")\n                continue\n\n            # Calculate leading whitespace count, handling tabs\n            leading_space_count = 0\n            for char in line:\n                if char == \" \":\n                    leading_space_count += 1\n                elif char == \"\\t\":\n                    # Round up to the next multiple of 4\n                    leading_space_count = (leading_space_count + 4) & ~3\n                else:\n                    break\n\n            # Preserve the original indentation level\n            normalized_line = \" \" * leading_space_count + line.lstrip()\n            normalized_lines.append(normalized_line)\n\n        return \"\\n\".join(normalized_lines)\n\n    def _remove_empty_lines(self, code: str) -> str:\n        \"\"\"Remove empty lines and lines with only whitespace.\"\"\"\n        return \"\\n\".join(line for line in code.splitlines() if line.strip())\n\n    def _remove_extra_whitespaces(self, code: str) -> str:\n        \"\"\"Remove extra whitespaces within lines while preserving newlines.\"\"\"\n        lines = code.splitlines()\n        return \"\\n\".join(\n            self._preserve_indent_remove_extra_spaces(line) for line in lines\n        )\n\n    def _remove_trailing_whitespaces(self, code: str) -> str:\n        \"\"\"Remove trailing whitespaces from each line.\"\"\"\n        return \"\\n\".join(line.rstrip() for line in code.splitlines())",
    "repo": "readme-ai",
    "path": "readmeai\\preprocessor\\document_cleaner.py",
    "type": "class",
    "name": "DocumentCleaner",
    "loc": 5,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\preprocessor\\file_filter.py:function:is_excluded:chunk1",
    "text": "def is_excluded(ignore_list: dict, file_path: Path, repo_path: Path) -> bool:\n    \"\"\"Check if the file should be ignored based on the ignore list.\"\"\"\n    relative_path = file_path.relative_to(repo_path)\n\n    for ignored_dir in ignore_list.get(\"directories\", []):\n        if ignored_dir in relative_path.parts:\n            return True\n\n    if file_path.suffix.lstrip(\".\") in ignore_list.get(\"extensions\", []):\n        return True\n\n    return file_path.name in ignore_list.get(\"files\", [])",
    "repo": "readme-ai",
    "path": "readmeai\\preprocessor\\file_filter.py",
    "type": "function",
    "name": "is_excluded",
    "loc": 6,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\retrievers\\git\\metadata.py:function:_parse_repository_metadata:chunk1",
    "text": "def _parse_repository_metadata(repo_data: dict) -> RepositoryMetadata:\n    \"\"\"Converts raw repository data from GitHub API into dataclass.\"\"\"\n    languages = repo_data.get(\"languages\", {})\n    license_info = repo_data.get(\"license\", {}) or {}\n    owner_info = repo_data.get(\"owner\", {}) or {}\n\n    return RepositoryMetadata(\n        name=repo_data.get(\"name\", \"\"),\n        full_name=repo_data.get(\"full_name\", \"\"),\n        owner=owner_info.get(\"login\", \"\"),\n        owner_url=owner_info.get(\"html_url\", \"\"),\n        description=repo_data.get(\"description\", \"\"),\n        stars_count=repo_data.get(\"stargazers_count\", 0),\n        forks_count=repo_data.get(\"forks_count\", 0),\n        watchers_count=repo_data.get(\"watchers_count\", 0),\n        open_issues_count=repo_data.get(\"open_issues_count\", 0),\n        default_branch=repo_data.get(\"default_branch\", \"\"),\n        created_at=repo_data.get(\"created_at\", \"\"),\n        updated_at=repo_data.get(\"updated_at\", \"\"),\n        pushed_at=repo_data.get(\"pushed_at\", \"\"),\n        size_kb=repo_data.get(\"size\", 0),\n        clone_url_http=repo_data.get(\"clone_url\", \"\"),\n        clone_url_ssh=repo_data.get(\"ssh_url\", \"\"),\n        contributors_url=repo_data.get(\"contributors_url\"),\n        languages_url=repo_data.get(\"languages_url\", \"\"),\n        issues_url=repo_data.get(\"issues_url\"),\n        language=repo_data.get(\"language\", \"\"),\n        languages=list(languages.keys()) if languages else [],\n        topics=repo_data.get(\"topics\", []),\n        has_wiki=repo_data.get(\"has_wiki\", False),\n        has_issues=repo_data.get(\"has_issues\", False),\n        has_projects=repo_data.get(\"has_projects\", False),\n        is_private=repo_data.get(\"private\", False),\n        homepage_url=repo_data.get(\"homepage\", \"\"),\n        license_name=license_info.get(\"name\", \"\"),\n        license_url=license_info.get(\"url\", \"\"),\n    )",
    "repo": "readme-ai",
    "path": "readmeai\\retrievers\\git\\metadata.py",
    "type": "function",
    "name": "_parse_repository_metadata",
    "loc": 62,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\retrievers\\git\\metadata.py:class:RepositoryMetadata:chunk1",
    "text": "class RepositoryMetadata:\n    \"\"\"\n    Dataclass to store GitHub repository metadata.\n    \"\"\"\n\n    name: str\n    full_name: str\n    owner: str\n    owner_url: str | None\n    description: str | None\n\n    # Repository statistics\n    stars_count: int\n    forks_count: int\n    watchers_count: int\n    open_issues_count: int\n\n    # Repository details\n    default_branch: str\n    created_at: str\n    updated_at: str\n    pushed_at: str\n    size_kb: int\n\n    # Repository URLs\n    clone_url_http: str\n    clone_url_ssh: str\n    contributors_url: str | None\n    languages_url: str\n    issues_url: str | None\n\n    # Programming languages and topics\n    language: str | None\n    languages: list[str]\n    topics: list[str]\n\n    # Additional repository settings\n    has_wiki: bool\n    has_issues: bool\n    has_projects: bool\n    is_private: bool\n    homepage_url: str | None\n\n    # License information\n    license_name: str | None\n    license_url: str | None",
    "repo": "readme-ai",
    "path": "readmeai\\retrievers\\git\\metadata.py",
    "type": "class",
    "name": "RepositoryMetadata",
    "loc": 14,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\retrievers\\git\\providers.py:function:parse_git_url:chunk1",
    "text": "def parse_git_url(url: Union[str, Path]) -> tuple[str, str, str, str]:\n    \"\"\"Parse repository URL and return host, full name, and project name.\"\"\"\n    if isinstance(url, Path) or (isinstance(url, str) and Path(url).is_dir()):\n        host_domain = host = GitHost.LOCAL.name\n        name = Path(url).name\n        full_name = f\"{Path(url).parent.name}/{name}\"\n    else:\n        try:\n            parsed_url = HttpUrl(url)\n            if parsed_url.scheme not in [\"http\", \"https\"]:\n                raise GitURLError(\n                    url,\n                    f\"Uknown scheme provided: {parsed_url.scheme}\",\n                )\n        except ValueError as e:\n            raise GitURLError(url) from e\n\n        assert parsed_url.host and parsed_url.path, (\n            f\"Invalid Git repository URL: {parsed_url}\"\n        )\n\n        path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n\n        full_name = \"/\".join(path_parts[:2])\n\n        host = parsed_url.host.split(\".\")[0].lower()\n\n        host_domain = parsed_url.host\n\n        name = path_parts[-1]\n\n    return host_domain, host, name, full_name",
    "repo": "readme-ai",
    "path": "readmeai\\retrievers\\git\\providers.py",
    "type": "function",
    "name": "parse_git_url",
    "loc": 115,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\retrievers\\git\\providers.py:class:GitHost:chunk1",
    "text": "class GitHost(enum.Enum):\n    \"\"\"\n    Git repository hosting providers.\n    \"\"\"\n\n    GITHUB = (\n        \"github.com\",\n        \"https://api.github.com/repos/\",\n        \"https://github.com/{full_name}/blob/main/{file_path}\",\n    )\n    GITLAB = (\n        \"gitlab.com\",\n        \"https://gitlab.com/api/v4/projects/\",\n        \"https://gitlab.com/{full_name}/-/blob/main/{file_path}\",\n    )\n    BITBUCKET = (\n        \"bitbucket.org\",\n        \"https://api.bitbucket.org/2.0/repositories/\",\n        \"https://bitbucket.org/{full_name}/src/master/{file_path}\",\n    )\n    LOCAL = (\"local\", \"\", \"{file_path}\")\n\n    def __init__(self, name: str, api_url: str, file_url_template: str):\n        self.domain = name\n        self.api_url = api_url\n        self.file_url_template = file_url_template",
    "repo": "readme-ai",
    "path": "readmeai\\retrievers\\git\\providers.py",
    "type": "class",
    "name": "GitHost",
    "loc": 16,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\retrievers\\git\\providers.py:class:GitURL:chunk1",
    "text": "class GitURL(BaseModel):\n    \"\"\"\n    Git repository URL model with validation and parsing methods.\n    \"\"\"\n\n    url: HttpUrl\n    host: Optional[GitHost] = Field(default=None)\n    host_domain: str = Field(default=\"\")\n    name: str = Field(default=\"\")\n    full_name: str = Field(default=\"\")\n\n    model_config = {\n        \"frozen\": True,\n        \"use_enum_values\": True,\n        \"extra\": \"forbid\",\n        \"arbitrary_types_allowed\": True,\n    }\n\n    @classmethod\n    @functools.lru_cache(maxsize=100)\n    def create(cls, url: HttpUrl) -> \"GitURL\":\n        \"\"\"Create a GitURL object from a string URL.\"\"\"\n        return cls(url=url)\n\n    @field_validator(\"url\")\n    @classmethod\n    def validate_url(cls, v: HttpUrl) -> HttpUrl:\n        \"\"\"Validates the Git repository URL.\"\"\"\n        try:\n            parse_git_url(str(v))\n        except ValueError as e:\n            raise ValueError(f\"Invalid Git repository URL: {v}\") from e\n        return v\n\n    @model_validator(mode=\"after\")\n    def set_attributes(self) -> \"GitURL\":\n        \"\"\"Sets the Git URL attributes based on the URL.\"\"\"\n        try:\n            host_domain, host, name, full_name = parse_git_url(str(self.url))\n            object.__setattr__(self, \"host_domain\", host_domain)\n            object.__setattr__(self, \"name\", name)\n            object.__setattr__(self, \"full_name\", full_name)\n            for git_host in GitHost:\n                if git_host.name.lower() == host:\n                    object.__setattr__(self, \"host\", git_host)\n                    break\n        except ValueError as e:\n            raise ValueError(f\"Failed to parse Git URL: {self.url}\") from e\n        return self\n\n    def get_api_url(self) -> str:\n        \"\"\"Return the REST API endpoint URL for a git repository.\"\"\"\n        if self.host is None or self.host == GitHost.LOCAL:\n            raise ValueError(\n                f\"Unsupported Git host or local repository: {self.url}\",\n            )\n        if self.full_name:\n            return f\"{self.host.api_url}/{self.full_name}\"\n        else:\n            raise ValueError(\"Repository full name is required.\")\n\n    def get_file_url(self, file_path: str) -> str:\n        \"\"\"Return the URL of the file in the remote repository.\"\"\"\n        if self.host:\n            return self.host.file_url_template.format(\n                full_name=self.full_name,\n                file_path=file_path,\n            )\n        raise ValueError(f\"Unsupported Git host: {self.url}\")",
    "repo": "readme-ai",
    "path": "readmeai\\retrievers\\git\\providers.py",
    "type": "class",
    "name": "GitURL",
    "loc": 44,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\fetch_badges.py:function:extract_logo_name:chunk1",
    "text": "def extract_logo_name(url: str) -> str:\n    \"\"\"Extract the logo name from the badge URL.\"\"\"\n    parsed = urlparse(url)\n    query_params = parse_qs(parsed.query)\n    if \"logo\" in query_params:\n        return query_params[\"logo\"][0].lower()\n    return \"\"",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\fetch_badges.py",
    "type": "function",
    "name": "extract_logo_name",
    "loc": 21,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\fetch_badges.py:function:normalize_name:chunk1",
    "text": "def normalize_name(name: str) -> str:\n    \"\"\"Normalize badge key names to lowercase ASCII format.\"\"\"\n    try:\n        name = bytes(name, \"utf-8\").decode(\"unicode-escape\")\n    except UnicodeError as e:\n        _logger.warning(f\"Error decoding Unicode escape sequences: {e}\")\n    name = normalize_unicode(name)\n    name = name.lower()\n    name = name.replace(\" \", \"_\")\n    # name = re.sub(r\"[.\\s\\-]+\", \"_\", name)\n    # name = re.sub(r\"[^a-z0-9_]\", \"\", name)\n    # name = re.sub(r\"_+\", \"_\", name)\n    # name = name.strip(\"_\")\n    return name",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\fetch_badges.py",
    "type": "function",
    "name": "normalize_name",
    "loc": 30,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\fetch_badges.py:function:normalize_unicode:chunk1",
    "text": "def normalize_unicode(\n    text: str, normalization: Literal[\"NFC\", \"NFKC\", \"NFD\", \"NFKD\"] = \"NFKD\"\n) -> str:\n    \"\"\"Decompose unicode characters and remove non-ASCII characters.\"\"\"\n    normalized = unicodedata.normalize(normalization, text)\n    ascii_text = normalized.encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n    return ascii_text",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\fetch_badges.py",
    "type": "function",
    "name": "normalize_unicode",
    "loc": 46,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\fetch_badges.py:function:transform_badge_data:chunk1",
    "text": "def transform_badge_data(data: Dict[str, Any]) -> Dict[str, list]:\n    \"\"\"Transform badge data from original format to new format.\"\"\"\n    transformed = {}\n\n    if \"icons\" in data and isinstance(data[\"icons\"], list):\n        icons = data[\"icons\"]\n    elif \"icons\" in data and isinstance(data[\"icons\"][\"icons\"], list):\n        icons = data[\"icons\"][\"icons\"]\n    else:\n        icons = []\n\n    for icon in icons:\n        badge_url = icon[\"src\"].replace(\"style=for-the-badge\", \"style={0}\")\n        badge_data = [badge_url, icon[\"hex\"]]\n        name = normalize_name(icon[\"name\"])\n        transformed[name] = badge_data\n\n        logo_name = extract_logo_name(badge_url)\n        if logo_name and logo_name != name:\n            transformed[logo_name] = badge_data\n            _logger.debug(\n                f\"Added additional entry for logo name: {logo_name} (original: {name})\"\n            )\n\n        unicode_name = normalize_unicode(icon[\"name\"])\n        if unicode_name != name:\n            transformed[unicode_name] = badge_data\n            _logger.debug(\n                f\"Added additional entry for Unicode name: {unicode_name} (original: {name})\"\n            )\n\n    return transformed",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\fetch_badges.py",
    "type": "function",
    "name": "transform_badge_data",
    "loc": 55,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\fetch_badges.py:function:merge_badge_data:chunk1",
    "text": "def merge_badge_data(\n    existing_data: Dict[str, list], new_data: Dict[str, list]\n) -> Dict[str, list]:\n    \"\"\"\n    Merge existing and new badge data, ensuring:\n    - All keys are lowercase\n    - No duplicate keys exist\n    - Preserving unique entries\n    \"\"\"\n    normalized_existing = {}\n    for key, value in existing_data.items():\n        if isinstance(value, list) and len(value) == 2:\n            norm_key = normalize_name(key)\n            value[0] = value[0].replace(\"style=for-the-badge\", \"style={0}\")\n            normalized_existing[norm_key] = value\n            logo_name = extract_logo_name(value[0])\n            if logo_name and logo_name != norm_key:\n                logo_norm_key = normalize_name(logo_name)\n                normalized_existing[logo_norm_key] = value\n\n    normalized_new = {}\n    for key, value in new_data.items():\n        if isinstance(value, list) and len(value) == 2:\n            norm_key = normalize_name(key)\n            value[0] = value[0].replace(\"style=for-the-badge\", \"style={0}\")\n            normalized_new[norm_key] = value\n            logo_name = extract_logo_name(value[0])\n            if logo_name and logo_name != norm_key:\n                logo_norm_key = normalize_name(logo_name)\n                normalized_new[logo_norm_key] = value\n\n    merged = {}\n    for k, v in {**normalized_existing, **normalized_new}.items():\n        if k not in merged:\n            merged[k] = v\n\n    return dict(sorted(merged.items()))",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\fetch_badges.py",
    "type": "function",
    "name": "merge_badge_data",
    "loc": 89,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\file_handler.py:class:FileHandler:chunk1",
    "text": "class FileHandler:\n    \"\"\"\n    File I/O support for md, json, toml, txt, and yaml formats.\n    \"\"\"\n\n    def __init__(self):\n        self.file_actions = {\n            \"html\": {\"read\": self.read_text, \"write\": self.write_text},\n            \"json\": {\"read\": self.read_json, \"write\": self.write_json},\n            \"md\": {\"read\": self.read_markdown, \"write\": self.write_markdown},\n            \"toml\": {\"read\": self.read_toml, \"write\": self.write_toml},\n            \"txt\": {\"read\": self.read_text, \"write\": self.write_text},\n            \"yaml\": {\"read\": self.read_yaml, \"write\": self.write_yaml},\n        }\n        self.cache = {}\n        self.read_json = functools.lru_cache(maxsize=100)(self.read_json)\n        self.read_toml = functools.lru_cache(maxsize=100)(self.read_toml)\n\n    def read(self, file_path: Union[str, Path]) -> Any:\n        \"\"\"Read the content of a file.\"\"\"\n        if file_path in self.cache:\n            return self.cache[file_path]\n\n        try:\n            file_extension = str(file_path).rsplit(\".\", maxsplit=1)[-1]\n            reader = self.get_action(file_extension, \"read\")\n            content = reader(file_path)\n            self.cache[file_path] = content\n            return content\n        except Exception as exc:\n            raise FileReadError(exc, file_path) from exc\n\n    def write(self, file_path: Union[str, Path], content: Any) -> None:\n        \"\"\"Write the content to a file.\"\"\"\n        try:\n            file_extension = str(file_path).rsplit(\".\", maxsplit=1)[-1]\n            writer = self.get_action(file_extension, \"write\")\n            writer(file_path, content)\n        except Exception as exc:\n            raise FileWriteError(exc, file_path) from exc\n\n    def get_action(self, file_extension: str, action_type: str) -> Callable[[str], Any]:\n        \"\"\"Get the method for the passed file extension and I/O operation.\"\"\"\n        file_actions = self.file_actions.get(file_extension)\n        if not file_actions:\n            raise ValueError(f\"Unsupported file type: {file_extension}\")\n\n        action = file_actions.get(action_type)\n        if not action:\n            raise ValueError(f\"Unsupported action type: {action_type}\")\n\n        return action\n\n    @staticmethod\n    def read_html(file_path: Union[str, Path]) -> str:\n        \"\"\"Read the content of an HTML file.\"\"\"\n        with open(file_path, encoding=\"utf-8\") as file:\n            return file.read()\n\n    @staticmethod\n    @functools.lru_cache(maxsize=100)\n    def read_json(file_path: Union[str, Path]) -> dict[str, Any]:\n        \"\"\"Read the content of a JSON file.\"\"\"\n        with open(file_path, encoding=\"utf-8\") as file:\n            return json.load(file)\n\n    @staticmethod\n    def read_markdown(file_path: Union[str, Path]) -> str:\n        \"\"\"Read the content of a Markdown file.\"\"\"\n        with open(file_path, encoding=\"utf-8\") as file:\n            return file.read()\n\n    @staticmethod\n    @functools.lru_cache(maxsize=100)\n    def read_toml(file_path: Union[str, Path]) -> dict[str, Any]:\n        \"\"\"Read the content of a TOML file.\"\"\"\n        if is_available(\"tomllib\"):  # pragma: no cover\n            import tomllib\n\n            with open(file_path, \"rb\") as file:\n                content = tomllib.load(file)\n\n        elif is_available(\"tomli\"):  # pragma: no cover\n            import tomli  # type: ignore\n\n            with open(file_path, \"rb\") as file:\n                content = tomli.load(file)\n\n        elif is_available(\"tomlkit\"):  # pragma: no cover\n            import tomlkit  # type: ignore\n\n            with open(file_path) as file:\n                content = tomlkit.load(file)\n\n        else:  # pragma: no cover\n            raise ImportError(\n                \"No TOML provider found in the current environment. \"\n                \"Please install tomli using 'pip install tomli'. \"\n                \"Alternatively, run readme-ai using Python 3.11+.\"\n            )\n        return {key.lower(): val for key, val in content.items()}\n\n    @staticmethod\n    def read_text(file_path: Union[str, Path]) -> str:\n        \"\"\"Read the content of a TXT file.\"\"\"\n        with open(file_path, encoding=\"utf-8\") as file:\n            return file.read()\n\n    @staticmethod\n    def read_yaml(file_path: Union[str, Path]) -> dict[str, Any]:\n        \"\"\"Read the content of a YAML file.\"\"\"\n        with open(file_path, encoding=\"utf-8\") as file:\n            return yaml.safe_load(file)\n\n    @staticmethod\n    def write_html(file_path: Union[str, Path], content: str) -> None:\n        \"\"\"Write the content to an HTML file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    @staticmethod\n    def write_json(file_path: Union[str, Path], content: dict[str, Any]) -> None:\n        \"\"\"Write the content to a JSON file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            json.dump(content, file, indent=4)\n\n    @staticmethod\n    def write_markdown(file_path: Union[str, Path], content: str) -> None:\n        \"\"\"Write the content to a Markdown file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    @staticmethod\n    def write_toml(file_path: Union[str, Path], content: dict[str, Any]) -> None:\n        \"\"\"Write the content to a TOML file.\"\"\"\n        raise NotImplementedError(\"Writing to TOML files is not supported.\")\n\n    @staticmethod\n    def write_text(file_path: Union[str, Path], content: str) -> None:\n        \"\"\"Write the content to a TXT file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    @staticmethod\n    def write_yaml(file_path: Union[str, Path], content: dict[str, Any]) -> None:\n        \"\"\"Write the content to a YAML file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            yaml.safe_dump(content, file)",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\file_handler.py",
    "type": "class",
    "name": "FileHandler",
    "loc": 12,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\importer.py:function:is_available:chunk1",
    "text": "def is_available(name: str) -> bool:\n    \"\"\"Check if a module is available for import.\"\"\"\n    try:\n        importlib.import_module(name)\n        return True\n    except ModuleNotFoundError:  # pragma: no cover\n        return False",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\importer.py",
    "type": "function",
    "name": "is_available",
    "loc": 4,
    "role": "src"
  },
  {
    "id": "readme-ai:readmeai\\utilities\\resource_manager.py:function:build_resource_path:chunk1",
    "text": "def build_resource_path(\n    file_path: str,\n    module: str = \"readmeai.config\",\n    submodule: str = \"settings\",\n) -> Path:\n    \"\"\"Retrieves the path to a resource file within the package.\n\n    Tries importlib.resources first, falls back to pkg_resources.\n    \"\"\"\n    try:\n        return resources.files(module).joinpath(submodule, file_path)\n\n    except (TypeError, FileNotFoundError) as e:\n        _logger.error(f\"Error loading resource file via importlib.resources: {e}\")\n\n    try:\n        import pkg_resources\n\n        submodule = submodule.replace(\".\", \"/\")\n        return Path(\n            pkg_resources.resource_filename(\n                module,\n                f\"{submodule}/{file_path}\",\n            ),\n        ).resolve()\n\n    except Exception as e:\n        raise FileReadError(\n            f\"Failed to load resource file: {file_path} from {module}.{submodule}\",\n        ) from e",
    "repo": "readme-ai",
    "path": "readmeai\\utilities\\resource_manager.py",
    "type": "function",
    "name": "build_resource_path",
    "loc": 12,
    "role": "src"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:capture_stderr:chunk1",
    "text": "def capture_stderr() -> Generator[StringIO, None, None]:\n    \"\"\"Fixture to capture stderr.\"\"\"\n    old_stderr = sys.stderr\n    sys.stderr = captured_output = StringIO()\n    try:\n        yield captured_output\n    finally:\n        sys.stderr = old_stderr",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "capture_stderr",
    "loc": 25,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:fixture_log_output:chunk1",
    "text": "def fixture_log_output() -> LogCapture:\n    return LogCapture()",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "fixture_log_output",
    "loc": 36,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:fixture_configure_structlog:chunk1",
    "text": "def fixture_configure_structlog(log_output: LogCapture) -> None:\n    structlog.configure(processors=[log_output])",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "fixture_configure_structlog",
    "loc": 41,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:output_file_path:chunk1",
    "text": "def output_file_path(tmp_path: Path) -> str:\n    return str(tmp_path / \"test_readme_ai.md\")",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "output_file_path",
    "loc": 49,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:temp_dir:chunk1",
    "text": "def temp_dir(tmpdir: LocalPath) -> LocalPath:\n    return tmpdir",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "temp_dir",
    "loc": 54,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:mock_config:chunk1",
    "text": "def mock_config() -> Settings:\n    return ConfigLoader().config",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "mock_config",
    "loc": 62,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:mock_config_loader:chunk1",
    "text": "def mock_config_loader() -> ConfigLoader:\n    return ConfigLoader()",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "mock_config_loader",
    "loc": 67,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:file_handler:chunk1",
    "text": "def file_handler():\n    return FileHandler()",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "file_handler",
    "loc": 75,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:json_file_path_fixture:chunk1",
    "text": "def json_file_path_fixture():\n    return Path(\"mock/path/file.json\")",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "json_file_path_fixture",
    "loc": 80,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:json_data_fixture:chunk1",
    "text": "def json_data_fixture():\n    return json.dumps({\"key\": \"value\"})",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "json_data_fixture",
    "loc": 85,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:toml_file_path_fixture:chunk1",
    "text": "def toml_file_path_fixture():\n    return Path(\"mock/path/file.toml\")",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "toml_file_path_fixture",
    "loc": 90,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:dependencies_fixture:chunk1",
    "text": "def dependencies_fixture():\n    \"\"\"\n    Project dependencies fixture.\n    \"\"\"\n    return [\"python\", \"pytest\", \"pytest-cov\", \"black\", \"flake8\", \"mypy\"]",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "dependencies_fixture",
    "loc": 98,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:mock_summaries:chunk1",
    "text": "def mock_summaries() -> list[tuple[str, str]]:\n    \"\"\"\n    LLM generated file summaries.\n    \"\"\"\n    return [\n        (\"/path/to/file1.py\", \"This is summary for file1.py\"),\n        (\"/path/to/file2.py\", \"This is summary for file2.py\"),\n        (\".github/workflows/ci.yml\", \"This is summary for ci.yml\"),\n    ]",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "mock_summaries",
    "loc": 106,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\conftest.py:function:mock_repository_context:chunk1",
    "text": "def mock_repository_context() -> RepositoryContext:\n    \"\"\"\n    Pytest fixture for the RepositoryContext model.\n    \"\"\"\n    return RepositoryContext(\n        files=[\n            FileContext(\n                path=\"requirements.txt\",\n                name=\"requirements.txt\",\n                ext=\"txt\",\n                content=\"pandas asyncio aiohttp aioresponses apache-flink apache-kafka pandas pyflink\",\n                language=\"requirements.txt\",\n                dependencies=[\n                    \"pandas\",\n                    \"asyncio\",\n                    \"aiohttp\",\n                    \"aioresponses\",\n                    \"apache-flink\",\n                    \"apache-kafka\",\n                    \"pandas\",\n                    \"pyflink\",\n                ],\n            ),\n            FileContext(\n                path=\"setup.py\",\n                name=\"setup.py\",\n                ext=\"py\",\n                content='''\"\"\" setup.py \"\"\"\nfrom pathlib import Path\nfrom setuptools import find_namespace_packages, setup\n\nBASE_DIR = Path(__file__).parent\n\nwith open(Path(BASE_DIR, \"requirements.txt\"), \"r\") as file:\n    required_packages = [ln.strip() for ln in file.readlines()]\n\ndocs_packages = [\"mkdocs==1.3.0\", \"mkdocstrings==0.18.1\"]\nstyle_packages = [\"black==22.3.0\", \"flake8==3.9.2\", \"isort==5.10.1\"]\ntest_packages = [\"pytest==7.1.2\", \"pytest-cov==2.10.1\", \"great-expectations==0.15.15\"]\n\nsetup(\n    name=\"STREAM-ON\",\n    version=0.1,\n    description=\"\",\n    author=\"\",\n    author_email=\"\",\n    url=\"\",\n    python_requires=\">=3.7\",\n    packages=find_namespace_packages(),\n    install_requires=[required_packages],\n    extras_require={\n        \"dev\": docs_packages + style_packages + test_packages + [\"pre-commit==2.19.0\"],\n        \"test\": test_packages,\n    },\n)\n                ''',\n                language=\"python\",\n                dependencies=[],\n            ),\n        ],\n        dependencies=[\n            \"pip\",\n            \"python\",\n            \"conf.toml\",\n            \"requirements.txt\",\n            \"shell\",\n            \"flink-config.yaml\",\n            \"aiohttp\",\n            \"pyflink\",\n            \"apache-kafka\",\n            \"asyncio\",\n            \"pandas\",\n            \"apache-flink\",\n            \"aioresponses\",\n        ],\n        languages=[\n            \"python\",\n            \"conf.toml\",\n            \"requirements.txt\",\n            \"shell\",\n            \"flink-config.yaml\",\n        ],\n        language_counts={\"txt\": 1, \"py\": 4, \"sh\": 3, \"yaml\": 1, \"toml\": 1},\n        metadata={\n            \"cicd\": {},\n            \"containers\": {},\n            \"documentation\": {},\n            \"package_managers\": {\"pip\": \"requirements.txt\"},\n        },\n        quickstart=QuickStart(\n            primary_language=\"Python\",\n            language_counts={\"txt\": 1, \"py\": 4, \"sh\": 3, \"yaml\": 1, \"toml\": 1},\n            package_managers={\"pip\": \"requirements.txt\"},\n            containers={},\n            install_commands=\"\"\"\n                **Using `pip`** &nbsp;\n                [<img align=\"center\" src=\"https://img.shields.io/badge/Pip-3776AB.svg?style={badge_style}&logo=pypi&logoColor=white\" />](https://pypi.org/project/pip/)\n\n                ```sh\n                ⨯ pip install -r requirements.txt\n                ```\n            \"\"\",\n            usage_commands=\"\"\"\n                **Using `pip`** &nbsp;\n                [<img align=\"center\" src=\"https://img.shields.io/badge/Pip-3776AB.svg?style={badge_style}&logo=pypi&logoColor=white\" />](https://pypi.org/project/pip/)\n\n                ```sh\n                ⨯ python {entrypoint}\n                ```\n            \"\"\",\n            test_commands=\"\"\"\n                **Using `pip`** &nbsp;\n                [<img align=\"center\" src=\"https://img.shields.io/badge/Pip-3776AB.svg?style={badge_style}&logo=pypi&logoColor=white\" />](https://pypi.org/project/pip/)\n\n                ```sh\n                ⨯ pytest\n                ```\n            \"\"\",\n        ),\n    )",
    "repo": "readme-ai",
    "path": "tests\\conftest.py",
    "type": "function",
    "name": "mock_repository_context",
    "loc": 118,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:cli_runner:chunk1",
    "text": "def cli_runner():\n    return CliRunner()",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "cli_runner",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:mock_config:chunk1",
    "text": "def mock_config():\n    return MagicMock(spec=ConfigLoader)",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "mock_config",
    "loc": 16,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:mock_readme_agent:chunk1",
    "text": "def mock_readme_agent():\n    with patch(\"readmeai.core.pipeline.readme_agent\") as mock:\n        yield mock",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "mock_readme_agent",
    "loc": 21,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_main_command_basic:chunk1",
    "text": "def test_main_command_basic(\n    cli_runner: CliRunner,\n    mock_config_loader: ConfigLoader,\n    output_file_path: str,\n):\n    with patch(\n        \"readmeai.config.settings.ConfigLoader\",\n        return_value=mock_config_loader,\n    ):\n        result = cli_runner.invoke(\n            main,\n            [\n                \"-r\",\n                \"https://github.com/eli64s/readme-ai-streamlit\",\n                \"-o\",\n                output_file_path,\n            ],\n        )\n\n    assert result.exit_code == 0",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_main_command_basic",
    "loc": 26,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_main_command_all_options:chunk1",
    "text": "def test_main_command_all_options(\n    cli_runner: CliRunner,\n    mock_config_loader: ConfigLoader,\n    output_file_path: str,\n):\n    mock_config = mock_config_loader\n    mock_config.config.git.repository = \"https://github.com/eli64s/readme-ai-streamlit\"\n\n    with patch(\"readmeai.config.settings.ConfigLoader\", return_value=mock_config):\n        result = cli_runner.invoke(\n            main,\n            [\n                \"--repository\",\n                \"https://github.com/eli64s/readme-ai-streamlit\",\n                \"--align\",\n                \"center\",\n                \"--api\",\n                \"offline\",\n                \"--badge-color\",\n                \"blue\",\n                \"--badge-style\",\n                \"flat\",\n                \"--context-window\",\n                \"4000\",\n                \"--header-style\",\n                \"modern\",\n                \"--logo\",\n                \"llm\",\n                \"--output\",\n                output_file_path,\n                \"--rate-limit\",\n                \"5\",\n                \"--temperature\",\n                \"0.7\",\n                \"--navigation-style\",\n                \"number\",\n                \"--top-p\",\n                \"0.9\",\n                \"--tree-max-depth\",\n                \"3\",\n            ],\n        )\n\n    assert result.exit_code == 0",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_main_command_all_options",
    "loc": 48,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_main_command_missing_repository:chunk1",
    "text": "def test_main_command_missing_repository(cli_runner: CliRunner):\n    result = cli_runner.invoke(main)\n    assert result.exit_code != 0\n    assert \"Missing option '-r' / '--repository'\" in result.output",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_main_command_missing_repository",
    "loc": 94,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_version_option:chunk1",
    "text": "def test_version_option(cli_runner: CliRunner):\n    result = cli_runner.invoke(main, [\"--version\"])\n    assert result.exit_code == 0\n    assert \"version\" in result.output.lower()",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_version_option",
    "loc": 100,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_invalid_option_values:chunk1",
    "text": "def test_invalid_option_values(\n    temp_dir: LocalPath, cli_runner: CliRunner, option, value, expected\n):\n    result = cli_runner.invoke(main, [\"--repository\", str(temp_dir), option, value])\n    assert result.exit_code != 0\n    assert expected in result.output",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_invalid_option_values",
    "loc": 164,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_main.py:function:test_main_command_exception_handling:chunk1",
    "text": "def test_main_command_exception_handling(cli_runner: CliRunner, mock_config: MagicMock):\n    with (\n        patch(\"readmeai.config.settings.ConfigLoader\", return_value=mock_config),\n        patch(\n            \"readmeai.core.pipeline.readme_agent\",\n            side_effect=Exception(\"Test error\"),\n        ),\n    ):\n        result = cli_runner.invoke(\n            main, [\"--repository\", \"https://github.com/user/repo\"]\n        )\n\n    assert result.exit_code != 0",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_main.py",
    "type": "function",
    "name": "test_main_command_exception_handling",
    "loc": 172,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_options.py:function:test_badge_options:chunk1",
    "text": "def test_badge_options():\n    \"\"\"Test the CLI options for badge icons.\"\"\"\n    assert BadgeStyles.FLAT == \"flat\"\n    assert BadgeStyles.FLAT_SQUARE == \"flat-square\"\n    assert BadgeStyles.FOR_THE_BADGE == \"for-the-badge\"\n    assert BadgeStyles.PLASTIC == \"plastic\"\n    assert BadgeStyles.SKILLS == \"skills\"\n    assert BadgeStyles.SKILLS_LIGHT == \"skills-light\"\n    assert BadgeStyles.SOCIAL == \"social\"",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_options.py",
    "type": "function",
    "name": "test_badge_options",
    "loc": 4,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\cli\\test_options.py:function:test_image_options:chunk1",
    "text": "def test_image_options():\n    \"\"\"Test the CLI options for header images.\"\"\"\n    assert CustomLogos.CUSTOM.value == \"CUSTOM\"\n    assert CustomLogos.LLM.value == \"LLM\"\n    assert isinstance(DefaultLogos.BLUE, str)\n    assert isinstance(DefaultLogos.GREEN, str)\n    assert isinstance(DefaultLogos.ORANGE, str)\n    assert isinstance(DefaultLogos.PURPLE, str)",
    "repo": "readme-ai",
    "path": "tests\\cli\\test_options.py",
    "type": "function",
    "name": "test_image_options",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_errors.py:function:test_readme_ai_exception:chunk1",
    "text": "def test_readme_ai_exception():\n    \"\"\"Test the ReadmeAIError class.\"\"\"\n    ex = ReadmeAIError(\"General error\")\n    assert isinstance(ex, Exception)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_errors.py",
    "type": "function",
    "name": "test_readme_ai_exception",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_errors.py:function:test_read_file_exception:chunk1",
    "text": "def test_read_file_exception():\n    \"\"\"Test the ReadFileException class.\"\"\"\n    ex = FileReadError(\"/path/to/file\", FileNotFoundError())\n    assert isinstance(ex, FileSystemError)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_errors.py",
    "type": "function",
    "name": "test_read_file_exception",
    "loc": 17,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_errors.py:function:test_write_file_exception:chunk1",
    "text": "def test_write_file_exception():\n    \"\"\"Test the WriteFileException class.\"\"\"\n    ex = FileWriteError(\"/path/to/file\", FileNotFoundError())\n    assert isinstance(ex, FileSystemError)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_errors.py",
    "type": "function",
    "name": "test_write_file_exception",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_errors.py:function:test_readme_generation_exception:chunk1",
    "text": "def test_readme_generation_exception():\n    \"\"\"Test the ReadmeGenerationException class.\"\"\"\n    ex = ReadmeGeneratorError(\"Error\", \"Traceback\")\n    assert isinstance(ex, Exception)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_errors.py",
    "type": "function",
    "name": "test_readme_generation_exception",
    "loc": 29,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_errors.py:function:test_unsupported_file_type_exception:chunk1",
    "text": "def test_unsupported_file_type_exception():\n    \"\"\"Test the UnsupportedFileTypeException class.\"\"\"\n    ex = UnsupportedServiceError(\"openai_sam_altman\")\n    assert isinstance(ex, ReadmeAIError)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_errors.py",
    "type": "function",
    "name": "test_unsupported_file_type_exception",
    "loc": 35,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_logger_log_methods:chunk1",
    "text": "def test_logger_log_methods(\n    log_method: Literal[\"info\"]\n    | Literal[\"debug\"]\n    | Literal[\"warning\"]\n    | Literal[\"error\"]\n    | Literal[\"critical\"],\n    log_level: Literal[20] | Literal[10] | Literal[30] | Literal[40] | Literal[50],\n    capture_stderr: StringIO,\n):\n    \"\"\"Test that different log methods work correctly.\"\"\"\n    logger = logging.getLogger(f\"test_logger_{log_method}\")\n    logger.setLevel(logging.DEBUG)\n    handler = logging.StreamHandler(capture_stderr)\n    handler.setLevel(log_level)\n    formatter = logging.Formatter(\"%(message)s\")\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    try:\n        log_message = f\"Test {log_method.upper()} message\"\n        log_method_func = getattr(logger, log_method)\n        log_method_func(log_message)\n        output = capture_stderr.getvalue().strip()\n        assert log_message in output\n    finally:\n        logger.removeHandler(handler)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_logger_log_methods",
    "loc": 27,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_custom_formatter:chunk1",
    "text": "def test_custom_formatter():\n    \"\"\"Test CustomFormatter for color and emoji formatting.\"\"\"\n    formatter = CustomFormatter(log_prefix=\"TEST\")\n    record = logging.LogRecord(\n        name=\"test_logger\",\n        level=logging.INFO,\n        pathname=\"test.py\",\n        lineno=10,\n        msg=\"Test message\",\n        args=(),\n        exc_info=None,\n    )\n    formatted_log = formatter.format(record)\n    assert LOG_LEVEL_COLORS[\"INFO\"] in formatted_log\n    assert LOG_LEVEL_EMOJIS[\"INFO\"] in formatted_log\n    assert \"TEST\" in formatted_log",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_custom_formatter",
    "loc": 55,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_logging_config_defaults:chunk1",
    "text": "def test_logging_config_defaults():\n    \"\"\"Test default logging configuration.\"\"\"\n    config = LoggingConfig()\n    assert config.log_level in [\"DEBUG\", \"INFO\"]\n    assert config.indent == 4\n    assert config.pad_event == 20\n    assert config.use_json is False\n    assert config.log_prefix == \"\"",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_logging_config_defaults",
    "loc": 73,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_logging_config_env_override:chunk1",
    "text": "def test_logging_config_env_override():\n    \"\"\"Test environment variable overrides for logging config.\"\"\"\n    config = LoggingConfig()\n    assert config.use_json is True",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_logging_config_env_override",
    "loc": 84,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_get_logger_multiple_calls:chunk1",
    "text": "def test_get_logger_multiple_calls():\n    \"\"\"Test multiple calls to get_logger with different names.\"\"\"\n    logger1 = get_logger(\"module1\")\n    logger2 = get_logger(\"module2\")\n    assert logger1 is not None\n    assert logger2 is not None\n    assert logger1.name == \"module1\"\n    assert logger2.name == \"module2\"",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_get_logger_multiple_calls",
    "loc": 90,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_logger_json_output:chunk1",
    "text": "def test_logger_json_output(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"Test JSON logging output.\"\"\"\n    from io import StringIO\n\n    json_output = StringIO()\n\n    class CustomJSONHandler(logging.StreamHandler):\n        def __init__(self, stream):\n            super().__init__(stream)\n            self.formatter = logging.Formatter(\n                '{\"timestamp\":\"%(asctime)s\", \"level\":\"%(levelname)s\", \"message\":\"%(message)s\"}'\n            )\n\n        def format(self, record):\n            return self.formatter.format(record)\n\n    logger = logging.getLogger(\"json_test\")\n    logger.setLevel(logging.INFO)\n    json_handler = CustomJSONHandler(json_output)\n    logger.addHandler(json_handler)\n    try:\n        logger.info(\"JSON log test\")\n        output = json_output.getvalue().strip()\n        try:\n            json_log = json.loads(output)\n            assert \"timestamp\" in json_log\n            assert \"level\" in json_log\n            assert \"message\" in json_log\n            assert json_log[\"message\"] == \"JSON log test\"\n            assert json_log[\"level\"] == \"INFO\"\n        except json.JSONDecodeError:\n            pytest.fail(f\"Failed to parse log output as JSON. Output was: {output}\")\n    finally:\n        logger.removeHandler(json_handler)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_logger_json_output",
    "loc": 100,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:function:test_logger_propagation:chunk1",
    "text": "def test_logger_propagation():\n    \"\"\"Test that logger does not propagate logs to parent loggers.\"\"\"\n    logger = logging.getLogger(\"test_propagation\")\n    # By default, propagation is True for Python's standard logging\n    # So we'll explicitly check and set it\n    assert logger.propagate is True\n    logger.propagate = False\n    assert logger.propagate is False",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "function",
    "name": "test_logger_propagation",
    "loc": 136,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\core\\test_logger.py:class:CustomJSONHandler:chunk1",
    "text": "class CustomJSONHandler(logging.StreamHandler):\n        def __init__(self, stream):\n            super().__init__(stream)\n            self.formatter = logging.Formatter(\n                '{\"timestamp\":\"%(asctime)s\", \"level\":\"%(levelname)s\", \"message\":\"%(message)s\"}'\n            )\n\n        def format(self, record):\n            return self.formatter.format(record)",
    "repo": "readme-ai",
    "path": "tests\\core\\test_logger.py",
    "type": "class",
    "name": "CustomJSONHandler",
    "loc": 106,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\extractors\\test_models.py:function:test_repository_context:chunk1",
    "text": "def test_repository_context(mock_repository_context: RepositoryContext):\n    context = mock_repository_context\n    assert len(context.files) > 0\n    assert \"python\" in context.languages\n    assert context.quickstart.primary_language == \"Python\"",
    "repo": "readme-ai",
    "path": "tests\\extractors\\test_models.py",
    "type": "function",
    "name": "test_repository_context",
    "loc": 4,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\conftest.py:function:quickstart_fixture:chunk1",
    "text": "def quickstart_fixture():\n    return QuickStart(\n        primary_language=\"Python\",\n        language_counts={\"py\": 10, \"sh\": 2},\n        package_managers={\"pip\": \"requirements.txt\"},\n        containers={},\n        install_commands=\"\",\n        usage_commands=\"\",\n        test_commands=\"\",\n    )",
    "repo": "readme-ai",
    "path": "tests\\generators\\conftest.py",
    "type": "function",
    "name": "quickstart_fixture",
    "loc": 12,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\conftest.py:function:quickstart_generator:chunk1",
    "text": "def quickstart_generator(mock_config_loader: ConfigLoader):\n    return QuickStartGenerator(mock_config_loader)",
    "repo": "readme-ai",
    "path": "tests\\generators\\conftest.py",
    "type": "function",
    "name": "quickstart_generator",
    "loc": 25,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_badges.py:function:testformat_badges:chunk1",
    "text": "def testformat_badges(badges: list[str], expected: Literal[\"\"] | LiteralString):\n    \"\"\"Tests the format_html method.\"\"\"\n    assert format_badges(badges) == expected",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_badges.py",
    "type": "function",
    "name": "testformat_badges",
    "loc": 26,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_badges.py:function:test_build_tech_stack:chunk1",
    "text": "def test_build_tech_stack(\n    dependencies: list[str],\n    svg_icons: dict[str, list[str]],\n    style: Literal[\"skills\"] | Literal[\"flat\"],\n    expected: Literal[\"\"] | LiteralString,\n):\n    \"\"\"Tests the generate_html method.\"\"\"\n    assert build_tech_stack(dependencies, svg_icons, style) == expected",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_badges.py",
    "type": "function",
    "name": "test_build_tech_stack",
    "loc": 48,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_badges.py:function:test_build_code_metrics_success:chunk1",
    "text": "def test_build_code_metrics_success(mock_config: Settings):\n    \"\"\"Tests build_code_metrics with valid inputs.\"\"\"\n    mock_config = mock_config\n    mock_config.git.host_domain = \"github.com\"\n    mock_config.md.badge_style = \"flat\"\n    badges = build_code_metrics(mock_config, \"github.com\", \"user/repo\")\n    assert isinstance(badges, str)\n    assert \"license\" in badges",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_badges.py",
    "type": "function",
    "name": "test_build_code_metrics_success",
    "loc": 58,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_badges.py:function:test_shieldsio_success:chunk1",
    "text": "def test_shieldsio_success(mock_config: Settings, dependencies_fixture: list[str]):\n    \"\"\"Tests shieldsio with valid inputs.\"\"\"\n    mock_config = mock_config\n    mock_config.md.badge_style = \"flat\"\n    badges = shieldsio(\n        mock_config,\n        dependencies_fixture,\n        \"github.com\",\n        \"user/repo\",\n    )\n    assert isinstance(badges, tuple), \"Badges should be returned as a tuple.\"\n    assert len(badges) > 0, \"Badges tuple should not be empty.\"\n    assert any(\"style=flat\" in badge for badge in badges), (\n        \"Expected 'style=flat' in one of the badges.\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_badges.py",
    "type": "function",
    "name": "test_shieldsio_success",
    "loc": 68,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_badges.py:function:test_skillicons_success:chunk1",
    "text": "def test_skillicons_success(mock_config: Settings, dependencies_fixture: list[str]):\n    \"\"\"Tests skillicons with valid inputs.\"\"\"\n    mock_config = mock_config\n    mock_config.md.badge_style = \"skills-light\"\n    badges = skillicons(mock_config, dependencies_fixture)\n    assert isinstance(badges, str)\n    assert \"theme=light\" in badges\n    assert \"md\" in badges",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_badges.py",
    "type": "function",
    "name": "test_skillicons_success",
    "loc": 85,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:mock_theme_manager:chunk1",
    "text": "def mock_theme_manager() -> Generator[Any, None, None]:\n    \"\"\"Mock theme manager with proper parent/subsection handling.\"\"\"\n    with patch(\"readmeai.generators.emojis.ThemeManager\") as mock:\n        theme_manager = mock.return_value\n        # Return subsection title if provided, otherwise return section title\n        theme_manager.apply_theme_to_section.side_effect = (\n            lambda section, subsection=None: subsection if subsection else section\n        )\n        theme_manager.get_themed_toc.return_value = \"Themed TOC\"\n        yield theme_manager",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "mock_theme_manager",
    "loc": 12,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:mock_header_template:chunk1",
    "text": "def mock_header_template() -> Generator[Any, None, None]:\n    \"\"\"Mock header template.\"\"\"\n    with patch(\"readmeai.generators.headers.HeaderTemplate\") as mock:\n        header_template = mock.return_value\n        header_template.render.return_value = \"Rendered Header\"\n        yield header_template",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "mock_header_template",
    "loc": 25,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:mock_toc_template:chunk1",
    "text": "def mock_toc_template() -> Generator[Any, None, None]:\n    \"\"\"Mock navigation template.\"\"\"\n    with patch(\"readmeai.generators.navigation.NavigationTemplate\") as mock:\n        toc_template = mock.return_value\n        toc_template.render.return_value = \"Rendered TOC\"\n        yield toc_template",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "mock_toc_template",
    "loc": 34,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:mock_tree_generator:chunk1",
    "text": "def mock_tree_generator() -> Generator[Any, None, None]:\n    \"\"\"Mock tree generator.\"\"\"\n    with patch(\"readmeai.generators.tree.TreeGenerator\") as mock:\n        tree_gen = mock.return_value\n        tree_gen.generate.return_value = \"Generated Tree\"\n        yield tree_gen",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "mock_tree_generator",
    "loc": 43,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:mock_quickstart_builder:chunk1",
    "text": "def mock_quickstart_builder() -> Generator[Any, None, None]:\n    \"\"\"Mock quickstart builder.\"\"\"\n    with patch(\"readmeai.generators.quickstart.QuickStartBuilder\") as mock:\n        quickstart = mock.return_value\n        quickstart.build.return_value = \"Quickstart Guide\"\n        yield quickstart",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "mock_quickstart_builder",
    "loc": 52,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:function:markdown_builder:chunk1",
    "text": "def markdown_builder(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n    tmp_path: Path,\n    mock_theme_manager: Any,\n    mock_header_template: Any,\n    mock_toc_template: Any,\n):\n    \"\"\"Create markdown builder with mocked dependencies.\"\"\"\n    builder = MarkdownBuilder(\n        config_loader=mock_config_loader,\n        repo_context=mock_repository_context,\n        file_summaries=[(\"file.py\", \"A test file\")],\n        temp_dir=str(tmp_path),\n    )\n    return builder",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "function",
    "name": "markdown_builder",
    "loc": 61,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_builder.py:class:TestMarkdownBuilder:chunk1",
    "text": "class TestMarkdownBuilder:\n    \"\"\"Test suite for MarkdownBuilder.\"\"\"\n\n    @pytest.fixture\n    def test_builder(\n        self,\n        mock_config_loader: ConfigLoader,\n        mock_repository_context: RepositoryContext,\n        tmp_path: Path,\n        mock_theme_manager: Any,\n        mock_header_template: Any,\n        mock_toc_template: Any,\n    ):\n        \"\"\"Create a test builder with properly injected mocks.\"\"\"\n        with patch(\n            \"readmeai.generators.builder.ThemeManager\", return_value=mock_theme_manager\n        ):\n            builder = MarkdownBuilder(\n                config_loader=mock_config_loader,\n                repo_context=mock_repository_context,\n                file_summaries=[(\"file.py\", \"A test file\")],\n                temp_dir=str(tmp_path),\n            )\n            return builder\n\n    def test_initialization(self, test_builder: MarkdownBuilder):\n        \"\"\"Test builder initialization.\"\"\"\n        assert test_builder.config is not None\n        assert test_builder.theme_manager is not None\n        assert test_builder.header_template is not None\n        assert test_builder.toc_template is not None\n\n    @patch(\"readmeai.generators.badges.shieldsio\")\n    def test_build_badges(self, mock_shieldsio, test_builder: MarkdownBuilder):\n        \"\"\"Test badge generation.\"\"\"\n        mock_shieldsio.return_value = (\"code metrics\", \"tech stack\")\n        code_metrics, tech_stack = test_builder._build_badges()\n\n        assert code_metrics == \"code metrics\"\n        assert tech_stack == \"tech stack\"\n        mock_shieldsio.assert_called_once()",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_builder.py",
    "type": "class",
    "name": "TestMarkdownBuilder",
    "loc": 79,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_enums.py:function:test_badge_options:chunk1",
    "text": "def test_badge_options():\n    assert BadgeStyles.DEFAULT == \"default\"\n    assert BadgeStyles.FLAT == \"flat\"\n    assert BadgeStyles.FLAT_SQUARE == \"flat-square\"\n    assert BadgeStyles.FOR_THE_BADGE == \"for-the-badge\"\n    assert BadgeStyles.PLASTIC == \"plastic\"\n    assert BadgeStyles.SKILLS == \"skills\"\n    assert BadgeStyles.SKILLS_LIGHT == \"skills-light\"\n    assert BadgeStyles.SOCIAL == \"social\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_enums.py",
    "type": "function",
    "name": "test_badge_options",
    "loc": 7,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_enums.py:function:test_image_options:chunk1",
    "text": "def test_image_options():\n    assert CustomLogos.CUSTOM.name == \"CUSTOM\"\n    assert CustomLogos.LLM.name == \"LLM\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_enums.py",
    "type": "function",
    "name": "test_image_options",
    "loc": 18,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:mock_header_registry:chunk1",
    "text": "def mock_header_registry():\n    \"\"\"Create a mock HeaderRegistry for testing.\"\"\"\n\n    class MockHeaderRegistry:\n        def get_themed_title(self, title):\n            return title\n\n        def _strip_emoji(self, title):\n            return title\n\n        emoji_theme = \"default\"\n\n    return MockHeaderRegistry()",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "mock_header_registry",
    "loc": 8,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:test_navigation_template_initialization:chunk1",
    "text": "def test_navigation_template_initialization(mock_header_registry: HeaderRegistry):\n    \"\"\"Test initialization of NavigationTemplate with different styles.\"\"\"\n    # Test default bullet style\n    nav_template = NavigationTemplate(\"bullet\", mock_header_registry)\n    assert nav_template.style == NavigationStyles.BULLET\n    assert nav_template.using_emojis == \"default\"\n\n    # Test other valid styles\n    styles = [\"number\", \"roman\", \"accordion\"]\n    for style in styles:\n        nav_template = NavigationTemplate(style, mock_header_registry)\n        assert nav_template.style == NavigationStyles[style.upper()]\n\n    # Test invalid style (should default to bullet)\n    nav_template = NavigationTemplate(\"invalid_style\", mock_header_registry)\n    assert nav_template.style == NavigationStyles.BULLET",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "test_navigation_template_initialization",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:test_generate_anchor:chunk1",
    "text": "def test_generate_anchor():\n    \"\"\"Test GitHub-compatible anchor link generation.\"\"\"\n    header_registry = HeaderRegistry(emoji_theme=\"default\", theme_data={\"mapping\": {}})\n    nav_template = NavigationTemplate(\"bullet\", header_registry)\n\n    # Test various title transformations\n    test_cases = [\n        (\"Hello World\", \"hello-world\"),\n        (\"Hello   World\", \"hello-world\"),\n        (\"Hello-World!\", \"hello-world\"),\n        (\"🚀 Deployment Guide\", \"deployment-guide\"),\n        (\"Advanced Setup & Configuration\", \"advanced-setup-configuration\"),\n        (\"Multiple---Hyphens\", \"multiple-hyphens\"),\n    ]\n\n    for input_title, expected_anchor in test_cases:\n        assert nav_template._generate_anchor(input_title) == expected_anchor",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "test_generate_anchor",
    "loc": 41,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:test_to_roman:chunk1",
    "text": "def test_to_roman(mock_header_registry: HeaderRegistry):\n    \"\"\"Test Roman numeral conversion.\"\"\"\n    nav_template = NavigationTemplate(\"roman\", mock_header_registry)\n    test_cases = [\n        (1, \"I\"),\n        (4, \"IV\"),\n        (9, \"IX\"),\n        (14, \"XIV\"),\n        (49, \"XLIX\"),\n        (99, \"XCIX\"),\n        (500, \"D\"),\n        (999, \"CMXCIX\"),\n    ]\n\n    for num, expected in test_cases:\n        assert nav_template._to_roman(num) == expected",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "test_to_roman",
    "loc": 60,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:test_format_link:chunk1",
    "text": "def test_format_link(mock_header_registry: HeaderRegistry):\n    \"\"\"Test link formatting for different navigation styles.\"\"\"\n    # Bullet style\n    bullet_nav = NavigationTemplate(\"bullet\", mock_header_registry)\n    assert (\n        bullet_nav._format_link(\"Hello World\", index=1)\n        == \"- [Hello World](#hello-world)\\n\"\n    )\n    assert (\n        bullet_nav._format_link(\"Nested Section\", level=1, index=1, sub_index=2)\n        == \"    - [Nested Section](#nested-section)\\n\"\n    )\n\n    # Numbered style\n    number_nav = NavigationTemplate(\"number\", mock_header_registry)\n    assert (\n        number_nav._format_link(\"Hello World\", index=1)\n        == \"1. [Hello World](#hello-world)\\n\"\n    )\n    assert (\n        number_nav._format_link(\"Nested Section\", level=1, index=1, sub_index=2)\n        == \"    1.2. [Nested Section](#nested-section)\\n\"\n    )\n\n    # Roman style\n    roman_nav = NavigationTemplate(\"roman\", mock_header_registry)\n    assert (\n        roman_nav._format_link(\"Hello World\", index=1)\n        == \"I. [Hello World](#hello-world)<br>\\n\"\n    )\n    assert (\n        roman_nav._format_link(\"Nested Section\", index=1, sub_index=2)\n        == \"&nbsp;&nbsp;&nbsp;&nbsp;I.b. [Nested Section](#nested-section)<br>\\n\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "test_format_link",
    "loc": 78,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:function:test_render_navigation:chunk1",
    "text": "def test_render_navigation(mock_header_registry: HeaderRegistry):\n    \"\"\"Test rendering of navigation with different styles.\"\"\"\n    # Prepare test data\n    test_data = {\n        \"sections\": [\n            {\n                \"title\": \"Introduction\",\n                \"subsections\": [\n                    {\"title\": \"Getting Started\"},\n                    {\"title\": \"Installation\"},\n                ],\n            },\n            {\"title\": \"Advanced Usage\"},\n        ]\n    }\n\n    # Test bullet style\n    bullet_nav = NavigationTemplate(\"bullet\", mock_header_registry)\n    bullet_result = bullet_nav.render(test_data)\n    assert \"- [Introduction](#introduction)\\n\" in bullet_result\n    assert \"    - [Getting Started](#getting-started)\\n\" in bullet_result\n\n    # Test accordion style\n    accordion_nav = NavigationTemplate(\"accordion\", mock_header_registry)\n    accordion_result = accordion_nav.render(test_data)\n    assert accordion_result.startswith(\"<details>\\n\")\n    assert \"<summary>Table of Contents</summary>\" in accordion_result\n    assert \"</details>\" in accordion_result\n\n    # Test roman style\n    roman_nav = NavigationTemplate(\"roman\", mock_header_registry)\n    roman_result = roman_nav.render(test_data)\n    assert \"I. [Introduction](#introduction)<br>\\n\" in roman_result\n    assert (\n        \"&nbsp;&nbsp;&nbsp;&nbsp;I.a. [Getting Started](#getting-started)<br>\\n\"\n        in roman_result\n    )",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "function",
    "name": "test_render_navigation",
    "loc": 114,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_navigation.py:class:MockHeaderRegistry:chunk1",
    "text": "class MockHeaderRegistry:\n        def get_themed_title(self, title):\n            return title\n\n        def _strip_emoji(self, title):\n            return title\n\n        emoji_theme = \"default\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_navigation.py",
    "type": "class",
    "name": "MockHeaderRegistry",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_quickstart_generator_init:chunk1",
    "text": "def test_quickstart_generator_init(\n    mock_config_loader: ConfigLoader,\n    quickstart_generator: QuickStartGenerator,\n):\n    \"\"\"Test QuickStartGenerator initialization\"\"\"\n    assert quickstart_generator.config == mock_config_loader\n    assert any(\n        language_name in quickstart_generator.language_names\n        for language_name in [\"python\", \"sql\", \"shell\", \"cpp\", \"java\"]\n    )\n    assert quickstart_generator.default_cmds == {\n        \"install\": \"echo 'INSERT-INSTALL-COMMAND-HERE'\",\n        \"usage\": \"echo 'INSERT-RUN-COMMAND-HERE'\",\n        \"test\": \"echo 'INSERT-TEST-COMMAND-HERE'\",\n        \"shield\": \"https://img.shields.io/badge/Replace%20Me-999999?style=flat&logo=github&logoColor=white\",\n        \"website\": \"https://img.shields.io/\",\n    }",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_quickstart_generator_init",
    "loc": 6,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_get_primary_language:chunk1",
    "text": "def test_get_primary_language(quickstart_generator: QuickStartGenerator):\n    \"\"\"Test primary language detection\"\"\"\n    # When multiple languages are present\n    counts = {\"py\": 10, \"sh\": 2}\n    primary_language = quickstart_generator._get_primary_language(counts)\n    assert primary_language == \"Python\"\n\n    # When no languages are present\n    counts = {}\n    primary_language = quickstart_generator._get_primary_language(counts)\n    assert primary_language == \"unknown\"  # Returns 'unknown' instead of None",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_get_primary_language",
    "loc": 25,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_format_command:chunk1",
    "text": "def test_format_command(quickstart_generator: QuickStartGenerator):\n    \"\"\"Test command formatting with current template style\"\"\"\n    command = quickstart_generator._format_command(\n        tool_name=\"pip\",\n        primary_language=\"Python\",\n        file_path=\"requirements.txt\",\n        cmd_type=\"install\",\n        tool_type=\"package_managers\",\n    )\n    expected_command = \"\"\"<!-- SHIELDS BADGE CURRENTLY DISABLED -->\n\\t<!-- [![pip][pip-shield]][pip-link] -->\n\\t<!-- REFERENCE LINKS -->\n\\t<!-- [pip-shield]: https://img.shields.io/badge/Pip-3776AB.svg?style={badge_style}&logo=pypi&logoColor=white -->\n\\t<!-- [pip-link]: https://pypi.org/project/pip/ -->\n\n\\t**Using [pip](https://pypi.org/project/pip/):**\n\n\\t```sh\n\\t❯ pip install -r requirements.txt\n\\t```\"\"\"\n    assert command.strip() == expected_command.strip()",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_format_command",
    "loc": 38,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_generate_commands:chunk1",
    "text": "def test_generate_commands(\n    quickstart_fixture: QuickStart,\n    quickstart_generator: QuickStartGenerator,\n):\n    \"\"\"Test command generation for Python project\"\"\"\n    quickstart_generator._generate_commands(quickstart_fixture, \"Python\")\n    # Update assertions to match current template format\n    assert (\n        \"<!-- SHIELDS BADGE CURRENTLY DISABLED -->\"\n        in quickstart_fixture.install_commands\n    )\n    assert (\n        \"**Using [pip](https://pypi.org/project/pip/):**\"\n        in quickstart_fixture.install_commands\n    )\n    assert \"pip install -r requirements.txt\" in quickstart_fixture.install_commands",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_generate_commands",
    "loc": 61,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_generate_quickstart:chunk1",
    "text": "def test_generate_quickstart(quickstart_generator: QuickStartGenerator):\n    \"\"\"Test complete quickstart generation\"\"\"\n    language_counts = {\"py\": 10, \"sh\": 2}\n    metadata = {\n        \"package_managers\": {\"pip\": \"requirements.txt\"},\n        \"containers\": {},\n    }\n    quickstart = quickstart_generator.generate(language_counts, metadata)\n    assert quickstart.primary_language == \"Python\"\n    assert quickstart.package_managers == {\"pip\": \"requirements.txt\"}\n    assert \"pip install -r requirements.txt\" in quickstart.install_commands\n    assert \"python\" in quickstart.usage_commands.lower()\n    assert \"pytest\" in quickstart.test_commands.lower()",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_generate_quickstart",
    "loc": 79,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_generate_quickstart_empty_args:chunk1",
    "text": "def test_generate_quickstart_empty_args(\n    quickstart_generator: QuickStartGenerator,\n):\n    \"\"\"Test quickstart generation with empty inputs\"\"\"\n    quickstart = quickstart_generator.generate({}, {})\n    # Should return 'unknown' for primary language with empty language counts\n    assert quickstart.primary_language == \"unknown\"\n    assert quickstart.install_commands == \"\"\n    assert quickstart.usage_commands == \"\"\n    assert quickstart.test_commands == \"\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_generate_quickstart_empty_args",
    "loc": 94,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_format_command_invalid_inputs:chunk1",
    "text": "def test_format_command_invalid_inputs(quickstart_generator: QuickStartGenerator):\n    \"\"\"Test command formatting with invalid inputs\"\"\"\n    command = quickstart_generator._format_command(\n        tool_name=\"\",\n        primary_language=\"\",\n        file_path=\"\",\n        cmd_type=\"install\",\n        tool_type=\"package_managers\",\n    )\n    assert command is None",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_format_command_invalid_inputs",
    "loc": 106,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_quickstart.py:function:test_format_command_containers:chunk1",
    "text": "def test_format_command_containers(quickstart_generator: QuickStartGenerator):\n    \"\"\"Test command formatting for containers\"\"\"\n    command = quickstart_generator._format_command(\n        tool_name=\"docker\",\n        primary_language=\"Python\",\n        file_path=\"Dockerfile\",\n        cmd_type=\"install\",\n        tool_type=\"containers\",\n    )\n    assert \"docker\" in command.lower()\n    assert \"<!-- SHIELDS BADGE CURRENTLY DISABLED -->\" in command",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_quickstart.py",
    "type": "function",
    "name": "test_format_command_containers",
    "loc": 118,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:sample_summaries:chunk1",
    "text": "def sample_summaries() -> list[tuple[str, str]]:\n    return [\n        (\"Makefile\", \"Facilitates project maintenance and development tasks.\"),\n        (\"pyproject.toml\", \"Contains project metadata and dependencies.\"),\n        (\"scripts/clean.sh\", \"Script to clean build artifacts.\"),\n        (\"src/cli.py\", \"Defines the command-line interface.\"),\n        (\"src/app.py\", \"Main application logic.\"),\n        (\"src/utils/helpers.py\", \"Utility functions for the app.\"),\n    ]",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "sample_summaries",
    "loc": 16,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:project_path:chunk1",
    "text": "def project_path() -> Path:\n    return Path(\"/path/to/readme-ai-streamlit\")",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "project_path",
    "loc": 28,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:repository_url:chunk1",
    "text": "def repository_url() -> str:\n    return \"https://github.com/eli64s/readme-ai-streamlit\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "repository_url",
    "loc": 33,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_format_summary:chunk1",
    "text": "def test_format_summary():\n    input_summary = \"This is a test. It has multiple sentences.\"\n    expected_output = \"- This is a test<br>- It has multiple sentences.\"\n    assert format_summary(input_summary) == expected_output\n\n    input_summary = \"Single sentence summary\"\n    expected_output = \"Single sentence summary\"\n    assert format_summary(input_summary) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_format_summary",
    "loc": 37,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_group_summaries_by_nested_module:chunk1",
    "text": "def test_group_summaries_by_nested_module(sample_summaries):\n    grouped = group_summaries_by_nested_module(sample_summaries)\n    expected = {\n        \"__root__\": [\n            (\n                \"Makefile\",\n                \"Facilitates project maintenance and development tasks.\",\n            ),\n            (\"pyproject.toml\", \"Contains project metadata and dependencies.\"),\n        ],\n        \"scripts\": {\n            \"\": [\n                (\"scripts/clean.sh\", \"Script to clean build artifacts.\"),\n            ]\n        },\n        \"src\": {\n            \"\": [],\n            \"utils\": {\n                \"\": [\n                    (\"src/utils/helpers.py\", \"Utility functions for the app.\"),\n                ]\n            },\n            \"cli.py\": {},\n            \"app.py\": {},\n        },\n    }\n    assert grouped[\"__root__\"] == expected[\"__root__\"]\n    assert grouped[\"scripts\"] == expected[\"scripts\"]\n    assert grouped[\"src\"][\"utils\"] == expected[\"src\"][\"utils\"]",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_group_summaries_by_nested_module",
    "loc": 47,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_generate_nested_module_tables:chunk1",
    "text": "def test_generate_nested_module_tables(sample_summaries, project_path, repository_url):\n    output = generate_nested_module_tables(\n        sample_summaries, project_path, repository_url\n    )\n    assert \"<details open>\" in output\n    assert (\n        \"<details open>\\n\\t<summary><b><code>README-AI-STREAMLIT/\"\n        \"</code></b></summary>\\n\\t<details>\" in output\n    )\n    assert \"Makefile\" in output\n    assert \"cli.py\" in output\n    assert \"helpers.py\" in output",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_generate_nested_module_tables",
    "loc": 78,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_get_file_hyperlink:chunk1",
    "text": "def test_get_file_hyperlink(\n    file_path_str, is_local, expected_link, project_path, repository_url\n):\n    link = _get_file_hyperlink(file_path_str, project_path, is_local, repository_url)\n    assert link == expected_link",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_get_file_hyperlink",
    "loc": 112,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_invalid_summary_handling:chunk1",
    "text": "def test_invalid_summary_handling():\n    invalid_summaries = [\n        (None, \"Valid summary\"),\n        (\"src/app.py\", None),\n        (123, \"Summary with non-string module\"),\n    ]\n    grouped = group_summaries_by_nested_module(invalid_summaries)\n    assert grouped == {\"__root__\": []}",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_invalid_summary_handling",
    "loc": 119,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_empty_summaries:chunk1",
    "text": "def test_empty_summaries():\n    \"\"\"Test generating tables with empty summaries\"\"\"\n    empty_summaries = []\n    output = generate_nested_module_tables(empty_summaries, \"/path\", \"https://repo.url\")\n    # Updated to match actual empty table structure with styling\n    expected_structure = \"\"\"<div class='directory-path' style='padding: 8px 0; color: #666;'>\n\\t\\t\\t\\t<code><b>⦿ __root__</b></code>\n\\t\\t\\t<table style='width: 100%; border-collapse: collapse;'>\"\"\"\n    assert expected_structure in output",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_empty_summaries",
    "loc": 129,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_generate_nested_module_tables:chunk1",
    "text": "def test_generate_nested_module_tables(sample_summaries, project_path, repository_url):\n    \"\"\"Test generating nested module tables\"\"\"\n    output = generate_nested_module_tables(\n        sample_summaries, project_path, repository_url\n    )\n    # Updated to include directory structure and styling\n    expected_header = \"\"\"<details open>\n\\t<summary><b><code>README-AI-STREAMLIT/</code></b></summary>\n\\t<!-- __root__ Submodule -->\n\\t<details>\"\"\"\n    assert expected_header in output\n    assert \"Makefile\" in output\n    assert \"cli.py\" in output\n    assert \"helpers.py\" in output",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_generate_nested_module_tables",
    "loc": 140,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_generate_table_rows:chunk1",
    "text": "def test_generate_table_rows():\n    \"\"\"Test generating table rows with styling\"\"\"\n    files = [\n        (\"src/app.py\", \"Main application logic.\"),\n        (\"src/cli.py\", \"Defines the command-line interface.\"),\n    ]\n    repo_path = \"/path/to/readme-ai-streamlit\"\n    repo_url = \"https://github.com/eli64s/readme-ai-streamlit\"\n    is_local_repo = False\n    rows = _generate_table_rows(files, repo_path, is_local_repo, repo_url, indent=\"\")\n    expected_row = \"\"\"<tr style='border-bottom: 1px solid #eee;'>\n\\t<td style='padding: 8px;'><b><a href='\"\"\"\n    assert expected_row in rows[0]\n    assert len(rows) == 2",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_generate_table_rows",
    "loc": 156,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_generate_nested_module_content:chunk1",
    "text": "def test_generate_nested_module_content():\n    \"\"\"Test generating nested module content\"\"\"\n    src_module = {\n        \"\": [\n            (\"src/cli.py\", \"Defines the command-line interface.\"),\n            (\"src/app.py\", \"Main application logic.\"),\n        ]\n    }\n    content = _generate_nested_module_content(\n        src_module, \"/path/to/readme-ai-streamlit\", True, \"\"\n    )\n    expected_row = \"\"\"<tr style='border-bottom: 1px solid #eee;'>\n\\t<td style='padding: 8px;'><b><a href='/path/to/readme-ai-streamlit/src/cli.py'>\"\"\"\n    assert expected_row in \"\".join(content)",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_generate_nested_module_content",
    "loc": 172,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tables.py:function:test_build_submodule_disclosure_widget:chunk1",
    "text": "def test_build_submodule_disclosure_widget(\n    sample_summaries, project_path, repository_url\n):\n    \"\"\"Test building the submodule disclosure widget\"\"\"\n    grouped = group_summaries_by_nested_module(sample_summaries)\n    output = build_submodule_disclosure_widget(grouped, project_path, repository_url)\n\n    # Check basic structure\n    assert \"<details open>\" in output\n    assert \"<summary><b><code>README-AI-STREAMLIT/</code></b></summary>\" in output\n\n    # Check table styling and structure\n    assert \"<table style='width: 100%; border-collapse: collapse;'>\" in output\n    assert \"<div class='directory-path' style='padding: 8px 0; color: #666;'>\" in output\n\n    # Check table headers\n    assert (\n        \"<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>\"\n        in output\n    )\n    assert \"<th style='text-align: left; padding: 8px;'>Summary</th>\" in output\n\n    # Check table rows\n    assert \"<tr style='border-bottom: 1px solid #eee;'>\" in output\n    assert \"<td style='padding: 8px;'>\" in output\n\n    # Check content\n    assert \"Makefile\" in output\n    assert \"pyproject.toml\" in output\n    assert \"clean.sh\" in output\n    assert \"helpers.py\" in output",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tables.py",
    "type": "function",
    "name": "test_build_submodule_disclosure_widget",
    "loc": 188,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:tree_gen:chunk1",
    "text": "def tree_gen(tmp_path: Path):\n    \"\"\"Fixture to create a TreeGenerator instance.\"\"\"\n    dir1 = tmp_path / \"dir1\"\n    dir1.mkdir()\n    (dir1 / \"file1.txt\").touch()\n    (tmp_path / \"dir2\").mkdir()\n    return TreeGenerator(\n        repo_name=tmp_path.name,\n        root_dir=tmp_path,\n        repo_url=Path(tmp_path).as_uri(),\n        max_depth=3,\n    )",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "tree_gen",
    "loc": 9,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_initialization:chunk1",
    "text": "def test_initialization(tmp_path: Path, tree_gen: TreeGenerator):\n    assert tree_gen.root_dir.is_dir()\n    assert tree_gen.repo_name == tmp_path.name\n    assert tree_gen.max_depth == 3",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_initialization",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_generate:chunk1",
    "text": "def test_generate(tmp_path: Path, tree_gen: TreeGenerator):\n    tree = tree_gen.generate(tmp_path)\n    expected_tree = f\"└── {tmp_path.name}/\\n    ├── dir1\\n    │   └── file1.txt\"\n    assert tree == expected_tree",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_generate",
    "loc": 29,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_max_depth_param:chunk1",
    "text": "def test_max_depth_param(\n    tree_gen: TreeGenerator,\n    depth: Literal[0] | Literal[1],\n    expected_suffix: Literal[\"\"] | Literal[\"\\n    ├── dir1\"],\n    tmp_path: Path,\n):\n    tree_gen.max_depth = depth\n    tree = tree_gen.generate(tmp_path)\n    expected_tree = f\"└── {tmp_path.name}/{expected_suffix}\"\n    assert tree == expected_tree",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_max_depth_param",
    "loc": 42,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_tree_method:chunk1",
    "text": "def test_tree_method(tmp_path: Path, tree_gen: TreeGenerator):\n    expected_tree = tree_gen.generate(directory=tree_gen.root_dir)\n    assert tmp_path.name in expected_tree\n    assert \"dir1\" in expected_tree\n    assert \"file1.txt\" in expected_tree",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_tree_method",
    "loc": 54,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_format_tree_simple:chunk1",
    "text": "def test_format_tree_simple():\n    tree_gen = TreeGenerator(\"repo\", Path(\"/root/dir\"), \"https://repo.url\", 3)\n    parts = [\"repo\", \"├── file1.py\", \"└── file2.py\"]\n    result = tree_gen._format_tree(parts)\n    assert result == \"repo\\n├── file1.py\\n└── file2.py\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_format_tree_simple",
    "loc": 61,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_format_tree_nested:chunk1",
    "text": "def test_format_tree_nested(tmp_path: Path, tree_gen: TreeGenerator):\n    parts = [\n        tmp_path.name,\n        \"├── dir1\",\n        \"│   └── file1.txt\",\n        \"└── dir2\",\n    ]\n    result = tree_gen._format_tree(parts)\n    assert result == (f\"{tmp_path.name}/\\n├── dir1\\n│   └── file1.txt\\n└── dir2\")",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_format_tree_nested",
    "loc": 68,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\test_tree.py:function:test_format_tree_empty:chunk1",
    "text": "def test_format_tree_empty():\n    tree_gen = TreeGenerator(\"repo\", Path(\"/root/dir\"), \"https://repo.url\", 3)\n    parts = []\n    result = tree_gen._format_tree(parts)\n    assert result == \"\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\test_tree.py",
    "type": "function",
    "name": "test_format_tree_empty",
    "loc": 79,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:temp_svg_file:chunk1",
    "text": "def temp_svg_file(tmp_path: Path) -> Generator[Path, None, None]:\n    \"\"\"Provide a temporary SVG file path.\"\"\"\n    svg_path = tmp_path / \"test_banner.svg\"\n    yield svg_path\n    if svg_path.exists():\n        svg_path.unlink()",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "temp_svg_file",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:mock_logger:chunk1",
    "text": "def mock_logger() -> Generator[Mock, None, None]:\n    \"\"\"Mock the logger for testing.\"\"\"\n    with patch(\"readmeai.generators.banner._logger\") as mock_log:\n        yield mock_log",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "mock_logger",
    "loc": 24,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_banner_basic:chunk1",
    "text": "def test_generate_banner_basic() -> None:\n    \"\"\"Test basic ASCII banner generation.\"\"\"\n    result = generate_banner(\"ABC\")\n    assert \"<pre>\" in result\n    assert \"</pre>\" in result\n    assert \"██████\" in result  # Common pattern in A, B, C\n    # Verify banner structure (5 lines of content)\n    content_lines = result.strip().split(\"\\n\")[1:-1]  # Remove pre tags\n    assert len(content_lines) == 5",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_banner_basic",
    "loc": 30,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_banner_empty:chunk1",
    "text": "def test_generate_banner_empty() -> None:\n    \"\"\"Test ASCII banner generation with empty string.\"\"\"\n    result = generate_banner(\"\")\n    assert result == \"<pre>\\n\\n\\n\\n\\n\\n</pre>\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_banner_empty",
    "loc": 41,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_banner_special_chars:chunk1",
    "text": "def test_generate_banner_special_chars() -> None:\n    \"\"\"Test ASCII banner generation with special characters.\"\"\"\n    result = generate_banner(\"A-B\")\n    assert \"<pre>\" in result\n    assert \"██████\" in result  # Pattern for horizontal line in '-'",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_banner_special_chars",
    "loc": 47,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_box_banner_basic:chunk1",
    "text": "def test_generate_box_banner_basic() -> None:\n    \"\"\"Test basic ASCII box banner generation.\"\"\"\n    result = generate_box_banner(\"TEST\")\n    assert \"╔\" in result  # Top left corner\n    assert \"╗\" in result  # Top right corner\n    assert \"╚\" in result  # Bottom left corner\n    assert \"╝\" in result  # Bottom right corner\n    assert \"═\" in result  # Horizontal border\n    assert \"║\" in result  # Vertical border",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_box_banner_basic",
    "loc": 54,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_box_banner_empty:chunk1",
    "text": "def test_generate_box_banner_empty() -> None:\n    \"\"\"Test ASCII box banner generation with empty string.\"\"\"\n    result = generate_box_banner(\"\")\n    # Check for the exact pattern that should appear\n    assert \"╔════╗\" in result\n    assert \"╚════╝\" in result\n    # Verify the empty content area\n    assert \"║    ║\" in result",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_box_banner_empty",
    "loc": 65,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_generate_box_banner_with_tagline:chunk1",
    "text": "def test_generate_box_banner_with_tagline() -> None:\n    \"\"\"Test ASCII box banner generation with a tagline.\"\"\"\n    result = generate_box_banner(\"TEST\", \"A test tagline\")\n    assert \"║\" in result\n    # Verify box contains both title and tagline space\n    lines = result.split(\"\\n\")\n    assert len(lines) > 7  # Header + spacing + 5 lines of text + footer",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_generate_box_banner_with_tagline",
    "loc": 75,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_create_letter:chunk1",
    "text": "def test_create_letter(char: str, expected_lines: list[str]) -> None:\n    \"\"\"Test letter creation with various inputs.\"\"\"\n    result = _create_letter(char)\n    assert result == expected_lines",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_create_letter",
    "loc": 93,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_wrap_with_pre_tag:chunk1",
    "text": "def test_wrap_with_pre_tag() -> None:\n    \"\"\"Test pre tag wrapping.\"\"\"\n    text = \"test content\"\n    result = _wrap_with_pre_tag(text)\n    assert result == f\"<pre>\\n{text}\\n</pre>\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_wrap_with_pre_tag",
    "loc": 99,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_wrap_with_pre_tag_empty:chunk1",
    "text": "def test_wrap_with_pre_tag_empty() -> None:\n    \"\"\"Test pre tag wrapping with empty content.\"\"\"\n    result = _wrap_with_pre_tag(\"\")\n    assert result == \"<pre>\\n\\n</pre>\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_wrap_with_pre_tag_empty",
    "loc": 106,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_ascii.py:function:test_wrap_with_pre_tag_multiline:chunk1",
    "text": "def test_wrap_with_pre_tag_multiline() -> None:\n    \"\"\"Test pre tag wrapping with multiline content.\"\"\"\n    text = \"line1\\nline2\"\n    result = _wrap_with_pre_tag(text)\n    assert result == f\"<pre>\\n{text}\\n</pre>\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_ascii.py",
    "type": "function",
    "name": "test_wrap_with_pre_tag_multiline",
    "loc": 112,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_svg.py:function:mock_file_handler:chunk1",
    "text": "def mock_file_handler():\n    \"\"\"Mock the file handler to avoid actual file operations\"\"\"\n    with patch(\"readmeai.utilities.file_handler.FileHandler\") as mock:\n        handler = mock.return_value\n        handler.read.side_effect = FileReadError(\n            \"File not found: config/settings/templates/banners.toml\"\n        )\n        yield mock",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_svg.py",
    "type": "function",
    "name": "mock_file_handler",
    "loc": 12,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_svg.py:function:valid_settings_dict:chunk1",
    "text": "def valid_settings_dict():\n    \"\"\"Real settings from banners.toml\"\"\"\n    return {\n        \"border_radius\": 5.0,\n        \"font_color\": \"#FFFFFF\",\n        \"font_family\": \"Arial, sans-serif\",\n        \"font_size\": 24,\n        \"height\": 200,\n        \"pattern_opacity\": 0.2,\n        \"pattern_size\": 20.0,\n        \"shadow_blur\": 4.0,\n        \"shadow_dx\": 2.0,\n        \"shadow_dy\": 2.0,\n        \"shadow_opacity\": 0.5,\n        \"shape_opacity\": 0.8,\n        \"subtitle_size\": 18,\n        \"width\": 800,\n    }",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_svg.py",
    "type": "function",
    "name": "valid_settings_dict",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_svg.py:function:valid_config_dict:chunk1",
    "text": "def valid_config_dict(valid_settings_dict: Dict[str, Dict[str, float]]):\n    \"\"\"Real template from banners.toml\"\"\"\n    return {\n        \"banners\": {\n            \"svg\": {\n                \"template\": \"\"\"\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\">\n    <defs>\n        <linearGradient id=\"bg\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"100%\">\n            <stop offset=\"0%\" style=\"stop-color:{color1};stop-opacity:1\" />\n            <stop offset=\"50%\" style=\"stop-color:{color2};stop-opacity:1\" />\n            <stop offset=\"100%\" style=\"stop-color:{color3};stop-opacity:1\" />\n        </linearGradient>\n        <filter id=\"shadow\">\n            <feDropShadow dx=\"{shadow_dx}\" dy=\"{shadow_dy}\" stdDeviation=\"{shadow_blur}\" flood-opacity=\"{shadow_opacity}\" />\n        </filter>\n        <pattern id=\"dots\" width=\"{pattern_size}\" height=\"{pattern_size}\" patternUnits=\"userSpaceOnUse\">\n            <circle cx=\"3\" cy=\"3\" r=\"1.5\" fill=\"rgba(255,255,255,{pattern_opacity})\" />\n        </pattern>\n    </defs>\n    <rect width=\"100%\" height=\"100%\" fill=\"url(#bg)\" rx=\"{border_radius}\" />\n    <rect width=\"100%\" height=\"100%\" fill=\"url(#dots)\" />\n    <circle cx=\"{width_08}\" cy=\"{height_025}\" r=\"{height_015}\" fill=\"rgba(255,255,255,{shape_opacity})\" />\n    <circle cx=\"{width_92}\" cy=\"{height_075}\" r=\"{height_02}\" fill=\"rgba(255,255,255,{shape_opacity})\" />\n    <path d=\"M {width_2} {height_0125}\n             L {width_2_plus_height_025} {height_0375}\n             L {width_2_minus_height_025} {height_0375} Z\" fill=\"rgba(255,255,255,{shape_opacity})\" />\n    <text x=\"{width_2}\" y=\"{height_2}\" font-family=\"{font_family}\" font-size=\"{font_size}\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"{font_color}\" filter=\"url(#shadow)\">\n        {title}\n    </text>\n    <text x=\"{width_2}\" y=\"{height_0625}\" font-family=\"{font_family}\" font-size=\"{subtitle_size}\" text-anchor=\"middle\" fill=\"rgba(255,255,255,0.9)\">\n\"\"\"\n            }\n        },\n        \"settings\": valid_settings_dict,\n    }",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_svg.py",
    "type": "function",
    "name": "valid_config_dict",
    "loc": 44,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_svg.py:class:TestSVGBannerSettings:chunk1",
    "text": "class TestSVGBannerSettings:\n    \"\"\"Test cases for SVGBannerSettings validation\"\"\"\n\n    def test_valid_settings(self, valid_settings_dict: Dict[str, Any]):\n        \"\"\"Test actual settings from banners.toml\"\"\"\n        settings = SVGBannerSettings(**valid_settings_dict)\n        assert settings.border_radius == 5.0\n        assert settings.font_color == \"#FFFFFF\"\n        assert settings.font_family == \"Arial, sans-serif\"\n        assert settings.font_size == 24\n        assert settings.height == 200\n        assert settings.pattern_opacity == 0.2\n        assert settings.pattern_size == 20.0\n        assert settings.shadow_blur == 4.0\n        assert settings.shadow_dx == 2.0\n        assert settings.shadow_dy == 2.0\n        assert settings.shadow_opacity == 0.5\n        assert settings.shape_opacity == 0.8\n        assert settings.subtitle_size == 18\n        assert settings.width == 800\n\n    @pytest.mark.parametrize(\n        \"field,invalid_value,error_type\",\n        [\n            (\"pattern_opacity\", 1.5, \"less_than_equal\"),\n            (\"shadow_opacity\", -0.1, \"greater_than_equal\"),\n            (\"font_size\", 0, \"greater_than\"),\n            (\"border_radius\", -1.0, \"greater_than_equal\"),\n        ],\n    )\n    def test_invalid_field_values(\n        self,\n        valid_settings_dict: Dict[str, Any],\n        field: Literal[\"pattern_opacity\"]\n        | Literal[\"shadow_opacity\"]\n        | Literal[\"font_size\"]\n        | Literal[\"border_radius\"],\n        invalid_value,\n        error_type,\n    ):\n        valid_settings_dict[field] = invalid_value\n        with pytest.raises(ValidationError) as exc_info:\n            SVGBannerSettings(**valid_settings_dict)\n        assert error_type in str(exc_info.value)\n\n    def test_invalid_font_color(self, valid_settings_dict: Dict[str, Any]):\n        valid_settings_dict[\"font_color\"] = \"rgb(255,255,255)\"  # Invalid format\n        with pytest.raises(ValidationError, match=\"pattern\"):\n            SVGBannerSettings(**valid_settings_dict)\n\n    def test_invalid_width_height_ratio(self, valid_settings_dict: Dict[str, Any]):\n        valid_settings_dict[\"width\"] = 150\n        valid_settings_dict[\"height\"] = 200\n        with pytest.raises(\n            ValidationError, match=\"Width must be greater than or equal to height\"\n        ):\n            SVGBannerSettings(**valid_settings_dict)",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_svg.py",
    "type": "class",
    "name": "TestSVGBannerSettings",
    "loc": 82,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\banners\\test_svg.py:class:TestSVGBannerGenerator:chunk1",
    "text": "class TestSVGBannerGenerator:\n    \"\"\"Test cases for SVGBannerGenerator\"\"\"\n\n    @pytest.fixture\n    def mock_gradient_colors(self):\n        \"\"\"Mock gradient colors with a passthrough to allow any valid hex colors\"\"\"\n        with patch(\n            \"readmeai.generators.colors.gradients.generate_gradient_colors\"\n        ) as mock:\n            # Don't specify colors, let the actual implementation provide them\n            yield mock\n\n    def test_svg_generation_with_real_template(\n        self,\n        mock_file_handler: MagicMock | AsyncMock,\n        mock_gradient_colors: MagicMock | AsyncMock,\n        valid_config_dict: Dict[str, Any],\n    ):\n        \"\"\"Test SVG generation with the actual template from banners.toml\"\"\"\n        file_handler_instance = Mock()\n        file_handler_instance.read.return_value = valid_config_dict\n        mock_file_handler.return_value = file_handler_instance\n\n        generator = SVGBannerGenerator(\"config/settings/templates/banners.toml\")\n        svg = generator.generate_svg(\"readme-ai\")\n\n        # Verify SVG structure\n        assert 'viewBox=\"0 0 800 200\"' in svg\n        assert 'linearGradient id=\"bg\"' in svg\n        assert 'pattern id=\"dots\"' in svg\n        assert 'filter id=\"shadow\"' in svg\n\n        # Verify gradient color format without checking specific colors\n        hex_color_pattern = r\"stop-color:#[0-9A-Fa-f]{6};stop-opacity:1\"\n        gradient_colors = re.findall(hex_color_pattern, svg)\n        assert len(gradient_colors) == 3  # Should have exactly 3 gradient stops\n\n        # Verify settings application\n        assert 'font-family=\"Arial, sans-serif\"' in svg\n        assert 'font-size=\"24\"' in svg\n        assert 'fill=\"#FFFFFF\"' in svg\n        assert \"readme-ai\" in svg\n\n    @pytest.mark.parametrize(\n        \"title\",\n        [\n            \"readme-ai\",\n            \"Project with <special> chars\",\n            \"Very \" * 10 + \"long project name\",\n            \"Project with 数字 and ñ\",\n            \"\",  # Empty title\n        ],\n    )\n    def test_title_handling(\n        self,\n        mock_file_handler: MagicMock | AsyncMock,\n        mock_gradient_colors: MagicMock | AsyncMock,\n        valid_config_dict: Dict[str, Any],\n        title: str,\n    ):\n        file_handler_instance = Mock()\n        file_handler_instance.read.return_value = valid_config_dict\n        mock_file_handler.return_value = file_handler_instance\n\n        generator = SVGBannerGenerator(\"config/settings/templates/banners.toml\")\n        svg = generator.generate_svg(title)\n\n        assert title in svg\n        assert 'text-anchor=\"middle\"' in svg\n        assert 'font-weight=\"bold\"' in svg\n\n    def test_missing_config_file(self, mock_file_handler: MagicMock | AsyncMock):\n        \"\"\"Test handling of missing config file\"\"\"\n        with pytest.raises(FileReadError) as e:\n            SVGBannerGenerator(\"config/settings/templates/missing.toml\")\n        assert isinstance(e.value, FileReadError)\n\n    def test_calculated_dimensions(\n        self,\n        mock_file_handler: MagicMock | AsyncMock,\n        valid_config_dict: Dict[str, Any],\n    ):\n        \"\"\"Test the calculated dimension values used in the SVG\"\"\"\n        file_handler_instance = Mock()\n        file_handler_instance.read.return_value = valid_config_dict\n        mock_file_handler.return_value = file_handler_instance\n\n        generator = SVGBannerGenerator(\"config/settings/templates/banners.toml\")\n        svg = generator.generate_svg(\"Test\")\n\n        # Verify calculated values are present\n        width = valid_config_dict[\"settings\"][\"width\"]\n        height = valid_config_dict[\"settings\"][\"height\"]\n\n        assert f'cx=\"{width * 0.08}\"' in svg  # width_08\n        assert f'cy=\"{height * 0.25}\"' in svg  # height_025\n        assert f'r=\"{height * 0.15}\"' in svg  # height_015\n        assert f'cx=\"{width * 0.92}\"' in svg  # width_92\n        assert f'cy=\"{height * 0.75}\"' in svg  # height_075\n        assert f'r=\"{height * 0.20}\"' in svg  # height_02",
    "repo": "readme-ai",
    "path": "tests\\generators\\banners\\test_svg.py",
    "type": "class",
    "name": "TestSVGBannerGenerator",
    "loc": 141,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\colors\\test_converters.py:function:test_hex_to_hls:chunk1",
    "text": "def test_hex_to_hls(hex_color: str, expected_hls: tuple[float, float, float]):\n    \"\"\"Test hex to HLS color conversion.\"\"\"\n    assert hex_to_hls(hex_color) == pytest.approx(expected_hls, abs=0.001)",
    "repo": "readme-ai",
    "path": "tests\\generators\\colors\\test_converters.py",
    "type": "function",
    "name": "test_hex_to_hls",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\generators\\colors\\test_gradients.py:class:TestColorUtils:chunk1",
    "text": "class TestColorUtils:\n    \"\"\"Test suite for color utility functions.\"\"\"\n\n    @pytest.fixture\n    def hex_color_pattern(self):\n        \"\"\"Regex pattern for validating hex color codes.\"\"\"\n        return re.compile(r\"^#[0-9a-fA-F]{6}$\")\n\n    def test_generate_base_color(self):\n        \"\"\"Test base color generation.\"\"\"\n        # Test multiple generations to ensure consistency\n        for _ in range(10):\n            hue, saturation, value = generate_base_color()\n\n            # Check value ranges\n            assert 0 <= hue <= 1, \"Hue should be between 0 and 1\"\n            assert saturation == 0.8, \"Saturation should be 0.8\"\n            assert value == 0.9, \"Value should be 0.9\"\n\n            # Check types\n            assert isinstance(hue, float)\n            assert isinstance(saturation, float)\n            assert isinstance(value, float)\n\n    def test_generate_random_color_format(self, hex_color_pattern):\n        \"\"\"Test random color generation format.\"\"\"\n        # Test multiple generations\n        for _ in range(10):\n            color = generate_random_color()\n            assert hex_color_pattern.match(color), f\"Invalid hex color format: {color}\"\n            assert len(color) == 7, (\n                f\"Color should be 7 characters (including #): {color}\"\n            )\n\n    def test_generate_related_color_format(self, hex_color_pattern):\n        \"\"\"Test related color generation format.\"\"\"\n        base_hue = 0.5\n        shifts = [0.1, 0.2, 0.3]\n\n        for shift in shifts:\n            color = generate_related_color(base_hue, shift)\n            assert hex_color_pattern.match(color), f\"Invalid hex color format: {color}\"\n            assert len(color) == 7, (\n                f\"Color should be 7 characters (including #): {color}\"\n            )\n\n    def test_generate_related_color_hue_wrapping(self):\n        \"\"\"Test hue wrapping for related colors.\"\"\"\n        base_hue = 0.9\n        shift = 0.2\n        color = generate_related_color(base_hue, shift)\n\n        # The result should still be a valid hex color\n        assert color.startswith(\"#\")\n        assert len(color) == 7\n\n        # Test with hue > 1.0\n        base_hue = 0.9\n        shift = 0.5\n        color = generate_related_color(base_hue, shift)\n        assert color.startswith(\"#\")\n        assert len(color) == 7\n\n    def test_generate_gradient_colors_length(self):\n        \"\"\"Test gradient colors list length.\"\"\"\n        colors = generate_gradient_colors()\n        assert len(colors) == 3, \"Should generate exactly 3 colors\"\n\n    def test_generate_gradient_colors_format(self, hex_color_pattern):\n        \"\"\"Test format of all gradient colors.\"\"\"\n        colors = generate_gradient_colors()\n        for color in colors:\n            assert hex_color_pattern.match(color), f\"Invalid hex color format: {color}\"\n            assert len(color) == 7, (\n                f\"Color should be 7 characters (including #): {color}\"\n            )\n\n    @patch(\"readmeai.generators.colors.gradients.randint\")\n    def test_random_color_deterministic(self, mock_randint):\n        \"\"\"Test random color generation with fixed random values.\"\"\"\n        mock_randint.side_effect = [255, 128, 0]  # RGB values\n        color = generate_random_color()\n        assert color == \"#ff8000\", f\"Expected #ff8000, got {color}\"\n\n    def test_related_colors_consistency(self):\n        \"\"\"Test that related colors are consistently generated.\"\"\"\n        base_hue = 0.5\n        color1 = generate_related_color(base_hue, 0.1)\n        color2 = generate_related_color(base_hue, 0.1)\n        assert color1 == color2, \"Same inputs should generate same color\"\n\n    def test_gradient_colors_relationships(self):\n        \"\"\"Test relationships between gradient colors.\"\"\"\n        with patch(\n            \"readmeai.generators.colors.gradients.generate_base_color\"\n        ) as mock_base:\n            mock_base.return_value = (0.5, 0.8, 0.9)  # Fixed base color\n            colors = generate_gradient_colors()\n\n            # Should have 3 different colors\n            assert len(set(colors)) == 3, \"All gradient colors should be different\"\n\n            # Colors should be in sequential order based on hue shift\n            for i in range(len(colors) - 1):\n                assert colors[i] != colors[i + 1], \"Adjacent colors should be different\"\n\n    @pytest.mark.parametrize(\n        \"base_hue,shift\",\n        [\n            (0.0, 0.1),  # Start of spectrum\n            (0.5, 0.1),  # Middle of spectrum\n            (0.9, 0.1),  # End of spectrum\n            (1.0, 0.1),  # Edge case\n            (0.5, 0.0),  # No shift\n            (0.5, 1.0),  # Full shift\n        ],\n    )\n    def test_related_color_various_inputs(self, base_hue, shift, hex_color_pattern):\n        \"\"\"Test related color generation with various inputs.\"\"\"\n        color = generate_related_color(base_hue, shift)\n        assert hex_color_pattern.match(color), f\"Invalid hex color format: {color}\"\n\n    def test_color_values_in_valid_range(self):\n        \"\"\"Test that generated colors have valid RGB values.\"\"\"\n\n        def is_valid_rgb(color: str) -> bool:\n            \"\"\"Check if color has valid RGB values.\"\"\"\n            # Remove '#' and convert to RGB integers\n            rgb = tuple(int(color[i : i + 2], 16) for i in (1, 3, 5))\n            return all(0 <= v <= 255 for v in rgb)\n\n        # Test random colors\n        for _ in range(10):\n            color = generate_random_color()\n            assert is_valid_rgb(color), f\"Invalid RGB values in color: {color}\"\n\n        # Test gradient colors\n        colors = generate_gradient_colors()\n        for color in colors:\n            assert is_valid_rgb(color), f\"Invalid RGB values in color: {color}\"",
    "repo": "readme-ai",
    "path": "tests\\generators\\colors\\test_gradients.py",
    "type": "class",
    "name": "TestColorUtils",
    "loc": 13,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:mock_config_loader:chunk1",
    "text": "def mock_config_loader():\n    \"\"\"Fixture to provide a ConfigLoader instance.\"\"\"\n    return ConfigLoader()",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "mock_config_loader",
    "loc": 16,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:mock_config:chunk1",
    "text": "def mock_config(mock_config_loader: ConfigLoader):\n    \"\"\"Fixture to provide a Settings instance.\"\"\"\n    config = mock_config_loader.config\n    config.llm.tokens = 100\n    config.md.placeholder = \"<code>❯ REPLACE-ME</code>\"\n    return config",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "mock_config",
    "loc": 22,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:mock_aiohttp_session:chunk1",
    "text": "def mock_aiohttp_session():\n    \"\"\"Fixture to provide a mocked aiohttp.ClientSession.\"\"\"\n    mock_session = MagicMock(spec=aiohttp.ClientSession)\n    mock_response_cm = AsyncMock()\n    mock_response = AsyncMock(\n        json=AsyncMock(\n            return_value={\n                \"choices\": [{\"message\": {\"content\": \"test_response\"}}],\n            },\n        ),\n    )\n    mock_response.raise_for_status = MagicMock()\n    mock_response_cm.__aenter__.return_value = mock_response\n    mock_session.post.return_value = mock_response_cm\n    return mock_session",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "mock_aiohttp_session",
    "loc": 31,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:anthropic_handler:chunk1",
    "text": "def anthropic_handler(\n    mock_config_loader: ConfigLoader, mock_repository_context: RepositoryContext\n):\n    if not ANTHROPIC_AVAILABLE:\n        pytest.skip(\"Anthropic library is not available\")\n    context = mock_repository_context\n    return AnthropicHandler(\n        config_loader=mock_config_loader,\n        context=context,\n    )",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "anthropic_handler",
    "loc": 49,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:anthropic_handler_with_mock_session:chunk1",
    "text": "def anthropic_handler_with_mock_session(\n    anthropic_handler: AnthropicHandler, monkeypatch: pytest.MonkeyPatch\n):\n    monkeypatch.setenv(\"ANTHROPIC_API_KEY\", \"test_api_key\")\n    mock_create = AsyncMock(\n        return_value=MagicMock(content=[MagicMock(text=\"test_response\")])\n    )\n    anthropic_handler.client.messages.create = mock_create\n    return anthropic_handler",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "anthropic_handler_with_mock_session",
    "loc": 62,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:gemini_handler:chunk1",
    "text": "def gemini_handler(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n):\n    \"\"\"Fixture to provide a GeminiHandler instance.\"\"\"\n    mock_config_loader.config.llm.model = GeminiModels.GEMINI_FLASH.value\n    with (\n        patch(\"google.generativeai.configure\"),\n        patch.dict(\"os.environ\", {\"GOOGLE_API_KEY\": \"test_key\"}, clear=True),\n    ):\n        yield GeminiHandler(mock_config_loader, mock_repository_context)",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "gemini_handler",
    "loc": 74,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:openai_handler:chunk1",
    "text": "def openai_handler(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n    monkeypatch: pytest.MonkeyPatch,\n):\n    \"\"\"Fixture to provide an OpenAIHandler instance with a mocked API key.\"\"\"\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"test_api_key\")\n    return OpenAIHandler(\n        config_loader=mock_config_loader,\n        context=mock_repository_context,\n    )",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "openai_handler",
    "loc": 88,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:openai_handler_with_mock_session:chunk1",
    "text": "def openai_handler_with_mock_session(\n    openai_handler: OpenAIHandler, mock_aiohttp_session: MagicMock\n):\n    \"\"\"Fixture to provide an OpenAIHandler with a mocked session.\"\"\"\n    openai_handler._session = mock_aiohttp_session\n    return openai_handler",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "openai_handler_with_mock_session",
    "loc": 102,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\conftest.py:function:ollama_localhost:chunk1",
    "text": "def ollama_localhost():\n    \"\"\"Fixture to provide a localhost URL for Ollama.\"\"\"\n    return \"http://localhost:11434/\"",
    "repo": "readme-ai",
    "path": "tests\\models\\conftest.py",
    "type": "function",
    "name": "ollama_localhost",
    "loc": 111,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_anthropic.py:function:test_anthropic_handler_sets_attributes:chunk1",
    "text": "def test_anthropic_handler_sets_attributes(anthropic_handler: AnthropicHandler):\n    assert hasattr(anthropic_handler, \"model\")\n    assert hasattr(anthropic_handler, \"max_tokens\")\n    assert hasattr(anthropic_handler, \"top_p\")",
    "repo": "readme-ai",
    "path": "tests\\models\\test_anthropic.py",
    "type": "function",
    "name": "test_anthropic_handler_sets_attributes",
    "loc": 14,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_anthropic.py:function:test_anthropic_endpoint_configuration_for_anthropic:chunk1",
    "text": "def test_anthropic_endpoint_configuration_for_anthropic(\n    mock_config_loader: ConfigLoader,\n    anthropic_handler: AnthropicHandler,\n):\n    mock_config_loader.config.llm.api = LLMProviders.ANTHROPIC.value\n    assert anthropic_handler.model in [model.value for model in AnthropicModels]",
    "repo": "readme-ai",
    "path": "tests\\models\\test_anthropic.py",
    "type": "function",
    "name": "test_anthropic_endpoint_configuration_for_anthropic",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_dalle.py:function:cleanup_mock_png_files:chunk1",
    "text": "def cleanup_mock_png_files(directory=\".\"):\n    \"\"\"Cleanup mock png files.\"\"\"\n    pattern = re.compile(\n        r\"<MagicMock name='mock\\.config\\.git\\.name' id='\\d+'>\\.png\",\n    )\n    for filename in Path(directory).glob(\"*.png\"):\n        if pattern.match(filename.name) and filename.exists():\n            filename.unlink()",
    "repo": "readme-ai",
    "path": "tests\\models\\test_dalle.py",
    "type": "function",
    "name": "cleanup_mock_png_files",
    "loc": 10,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:function:test_get_backend_openai:chunk1",
    "text": "def test_get_backend_openai(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n    monkeypatch: pytest.MonkeyPatch,\n):\n    \"\"\"Test getting OpenAI backend with proper environment setup.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.OPENAI.value\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"test_key\")\n    handler = ModelFactory.get_backend(mock_config_loader, mock_repository_context)\n    assert handler is not None\n    assert handler.__class__.__name__ == \"OpenAIHandler\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "function",
    "name": "test_get_backend_openai",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:function:test_get_backend_anthropic:chunk1",
    "text": "def test_get_backend_anthropic(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n    monkeypatch: pytest.MonkeyPatch,\n):\n    \"\"\"Test getting Anthropic backend.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.ANTHROPIC.value\n    monkeypatch.setenv(\"ANTHROPIC_API_KEY\", \"test_key\")\n    handler = ModelFactory.get_backend(mock_config_loader, mock_repository_context)\n    assert handler is not None\n    assert handler.__class__.__name__ == \"AnthropicHandler\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "function",
    "name": "test_get_backend_anthropic",
    "loc": 24,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:function:test_get_backend_gemini:chunk1",
    "text": "def test_get_backend_gemini(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n):\n    \"\"\"Test getting Gemini backend.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.GEMINI.value\n    with patch.dict(\"os.environ\", {\"GOOGLE_API_KEY\": \"test_key\"}, clear=True):\n        handler = ModelFactory.get_backend(mock_config_loader, mock_repository_context)\n        assert handler is not None\n        assert handler.__class__.__name__ == \"GeminiHandler\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "function",
    "name": "test_get_backend_gemini",
    "loc": 37,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:function:test_get_backend_offline:chunk1",
    "text": "def test_get_backend_offline(\n    mock_config_loader: ConfigLoader,\n    mock_repository_context: RepositoryContext,\n):\n    \"\"\"Test getting Offline backend.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.OFFLINE.value\n    handler = ModelFactory.get_backend(mock_config_loader, mock_repository_context)\n    assert handler is not None\n    assert handler.__class__.__name__ == \"OfflineHandler\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "function",
    "name": "test_get_backend_offline",
    "loc": 49,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:function:test_get_backend_unsupported_service:chunk1",
    "text": "def test_get_backend_unsupported_service(\n    mock_config_loader: ConfigLoader, mock_repository_context: RepositoryContext\n):\n    \"\"\"Test getting a backend with an unsupported service.\"\"\"\n\n    class UnsupportedConfig(ConfigLoader):\n        \"\"\"\n        Mock config loader that returns a config object with an unsupported service.\n        \"\"\"\n\n        def __init__(self) -> None:\n            self.config = type(\n                \"MockConfig\",\n                (),\n                {\"llm\": type(\"MockLLMConfig\", (), {\"api\": \"unsupported\"})()},\n            )()\n\n    unsupported_config = UnsupportedConfig()\n\n    with pytest.raises(UnsupportedServiceError) as e:\n        ModelFactory.get_backend(unsupported_config, mock_repository_context)\n\n    assert isinstance(e.value, UnsupportedServiceError)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "function",
    "name": "test_get_backend_unsupported_service",
    "loc": 60,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_factory.py:class:UnsupportedConfig:chunk1",
    "text": "class UnsupportedConfig(ConfigLoader):\n        \"\"\"\n        Mock config loader that returns a config object with an unsupported service.\n        \"\"\"\n\n        def __init__(self) -> None:\n            self.config = type(\n                \"MockConfig\",\n                (),\n                {\"llm\": type(\"MockLLMConfig\", (), {\"api\": \"unsupported\"})()},\n            )()",
    "repo": "readme-ai",
    "path": "tests\\models\\test_factory.py",
    "type": "class",
    "name": "UnsupportedConfig",
    "loc": 65,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_gemini.py:function:test_gemini_handler_sets_attributes:chunk1",
    "text": "def test_gemini_handler_sets_attributes(gemini_handler: GeminiHandler):\n    \"\"\"Test that the Gemini handler sets the correct attributes.\"\"\"\n    assert gemini_handler.model_name == \"gemini-1.5-flash\"\n    assert hasattr(gemini_handler, \"model\")\n    assert hasattr(gemini_handler, \"generation_config\")\n    assert gemini_handler.top_p == 0.9",
    "repo": "readme-ai",
    "path": "tests\\models\\test_gemini.py",
    "type": "function",
    "name": "test_gemini_handler_sets_attributes",
    "loc": 9,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_offline.py:function:offline_handler:chunk1",
    "text": "def offline_handler(\n    mock_config_loader: ConfigLoader, mock_repository_context: RepositoryContext\n):\n    \"\"\"Fixture to provide an OfflineHandler instance.\"\"\"\n    return OfflineHandler(mock_config_loader, mock_repository_context)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_offline.py",
    "type": "function",
    "name": "offline_handler",
    "loc": 8,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_openai.py:function:test_openai_handler_sets_attributes:chunk1",
    "text": "def test_openai_handler_sets_attributes(openai_handler: OpenAIHandler):\n    \"\"\"Test that the OpenAI handler sets the correct attributes.\"\"\"\n    assert hasattr(openai_handler, \"model\")\n    assert hasattr(openai_handler, \"max_tokens\")\n    assert hasattr(openai_handler, \"top_p\")",
    "repo": "readme-ai",
    "path": "tests\\models\\test_openai.py",
    "type": "function",
    "name": "test_openai_handler_sets_attributes",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_openai.py:function:test_openai_endpoint_configuration_for_openai:chunk1",
    "text": "def test_openai_endpoint_configuration_for_openai(\n    mock_config_loader: ConfigLoader,\n    openai_handler: OpenAIHandler,\n):\n    \"\"\"Test that the correct endpoint is set for OpenAI API.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.OPENAI.value\n    assert openai_handler.url == f\"{openai_handler.host_name}{openai_handler.resource}\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_openai.py",
    "type": "function",
    "name": "test_openai_endpoint_configuration_for_openai",
    "loc": 18,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_openai.py:function:test_openai_endpoint_configuration_for_ollama:chunk1",
    "text": "def test_openai_endpoint_configuration_for_ollama(\n    mock_config_loader: ConfigLoader,\n    ollama_localhost: str,\n):\n    \"\"\"Test that the correct endpoint is set for OLLAMA.\"\"\"\n    mock_config_loader.config.llm.api = LLMProviders.OLLAMA.value\n    mock_config_loader.config.llm.localhost = ollama_localhost\n    assert (\n        \"v1/chat/completions\"\n        in f\"{mock_config_loader.config.llm.localhost}v1/chat/completions\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\models\\test_openai.py",
    "type": "function",
    "name": "test_openai_endpoint_configuration_for_ollama",
    "loc": 27,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_get_prompt_context_found:chunk1",
    "text": "def test_get_prompt_context_found(mock_config_loader: ConfigLoader):\n    \"\"\"Test the retrieval of a prompt context.\"\"\"\n    with (\n        patch(\n            \"readmeai.models.prompts.get_prompt_template\",\n            return_value=\"Hello, {name}!\",\n        ),\n        patch(\n            \"readmeai.models.prompts.inject_prompt_context\",\n            return_value=\"Hello, World!\",\n        ),\n    ):\n        result = get_prompt_context(\n            mock_config_loader.prompts,\n            \"greeting\",\n            {\"name\": \"World\"},\n        )\n        assert result == \"Hello, World!\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_get_prompt_context_found",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_get_prompt_context_not_found:chunk1",
    "text": "def test_get_prompt_context_not_found(mock_config_loader: ConfigLoader):\n    \"\"\"Test the retrieval of a prompt context.\"\"\"\n    with patch(\"readmeai.models.prompts.get_prompt_template\", return_value=\"\"):\n        result = get_prompt_context(mock_config_loader.prompts, \"unknown\", {})\n        assert result == \"\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_get_prompt_context_not_found",
    "loc": 35,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_get_prompt_template:chunk1",
    "text": "def test_get_prompt_template(mock_config_loader: ConfigLoader):\n    \"\"\"Test the retrieval of a prompt template.\"\"\"\n    assert \"Hello!\" in get_prompt_template(mock_config_loader.prompts, \"features_table\")",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_get_prompt_template",
    "loc": 42,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_inject_prompt_context_success:chunk1",
    "text": "def test_inject_prompt_context_success(\n    mock_config: Settings,\n    mock_config_loader: ConfigLoader,\n    caplog: pytest.LogCaptureFixture,\n):\n    \"\"\"Test successful injection of prompt context.\"\"\"\n    template = \"Project {0} is a {1} repository in {2}.\"\n    context = {0: \"ReadmeAI\", 1: \"documentation\", 2: \"Python\"}\n    result = inject_prompt_context(template, context)\n    assert len(caplog.records) == 0\n    assert result == \"Project ReadmeAI is a documentation repository in Python.\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_inject_prompt_context_success",
    "loc": 47,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_inject_prompt_context_missing_key:chunk1",
    "text": "def test_inject_prompt_context_missing_key(caplog: pytest.LogCaptureFixture):\n    template = \"This is {a} and {b}.\"\n    context = {\"a\": \"A\"}\n    assert inject_prompt_context(template, context) == \"\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_inject_prompt_context_missing_key",
    "loc": 60,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_set_summary_context:chunk1",
    "text": "def test_set_summary_context(\n    mock_config: Settings, mock_summaries: list[tuple[str, str]]\n):\n    \"\"\"Test the generation of summary prompts.\"\"\"\n    mock_config.md.directory_structure = \"mock_tree\"\n    result = set_summary_context(mock_config, mock_summaries)\n    assert len(result) == 1\n    assert result[0][\"type\"] == \"file_summary\"\n    assert result[0][\"context\"][\"tree\"] == \"mock_tree\"\n    assert result[0][\"context\"][\"repo_files\"] == mock_summaries\n    result_empty = set_summary_context(mock_config, [])\n    assert len(result_empty) == 1\n    assert result_empty[0][\"type\"] == \"file_summary\"\n    assert result_empty[0][\"context\"][\"repo_files\"] == []",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_set_summary_context",
    "loc": 66,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_prompts.py:function:test_set_additional_contexts:chunk1",
    "text": "def test_set_additional_contexts(\n    mock_config: Settings,\n    mock_repository_context: RepositoryContext,\n    mock_summaries: list[tuple[str, str]],\n):\n    \"\"\"Test the generation of additional prompts.\"\"\"\n    result = set_additional_contexts(\n        mock_config, mock_repository_context, mock_summaries\n    )\n    assert len(result) == 3\n    assert result[0][\"type\"] == \"features_table\"\n    assert result[1][\"type\"] == \"overview\"\n    assert result[2][\"type\"] == \"tagline\"\n    assert result[0][\"context\"][\"file_summaries\"] == mock_summaries",
    "repo": "readme-ai",
    "path": "tests\\models\\test_prompts.py",
    "type": "function",
    "name": "test_set_additional_contexts",
    "loc": 82,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:mock_get_encoding:chunk1",
    "text": "def mock_get_encoding(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"Mock the get_encoding function.\"\"\"\n\n    def mock_encoder(encoding_name):\n        \"\"\"Mock the get_encoding function.\"\"\"\n        return MockEncoder()\n\n    monkeypatch.setattr(\"tiktoken.get_encoding\", mock_encoder)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "mock_get_encoding",
    "loc": 21,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:mock_encoder:chunk1",
    "text": "def mock_encoder(encoding_name):\n        \"\"\"Mock the get_encoding function.\"\"\"\n        return MockEncoder()",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "mock_encoder",
    "loc": 24,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_count_tokens_edge_cases:chunk1",
    "text": "def test_count_tokens_edge_cases(text, expected, ENCODING_NAME=\"cl100k_base\"):\n    \"\"\"\n    Test count_tokens function with various edge cases. See below for details:\n    https://github.com/run-llama/llama_index/issues/1206\n    \"\"\"\n    assert count_tokens(text, ENCODING_NAME) == expected",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_count_tokens_edge_cases",
    "loc": 100,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_count_tokens_exception:chunk1",
    "text": "def test_count_tokens_exception():\n    with pytest.raises(TypeError) as exc:\n        count_tokens([1, 2, 4, 5], ENCODING_NAME)\n    assert isinstance(exc.value, TypeError)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_count_tokens_exception",
    "loc": 108,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_update_max_tokens_valid_prompt:chunk1",
    "text": "def test_update_max_tokens_valid_prompt():\n    assert update_max_tokens(100, \"Hello! This is a test\") == 100",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_update_max_tokens_valid_prompt",
    "loc": 114,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_update_max_tokens_invalid_prompt:chunk1",
    "text": "def test_update_max_tokens_invalid_prompt():\n    assert update_max_tokens(100, \"Invalid prompt\") == 50",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_update_max_tokens_invalid_prompt",
    "loc": 118,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_update_max_tokens_edge_cases:chunk1",
    "text": "def test_update_max_tokens_edge_cases():\n    assert update_max_tokens(0, \"\") == 0",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_update_max_tokens_edge_cases",
    "loc": 122,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_set_encoding_cache_new:chunk1",
    "text": "def test_set_encoding_cache_new():\n    encoder = _set_encoding_cache(ENCODING_NAME)\n    assert _encoding_cache[ENCODING_NAME] == encoder",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_set_encoding_cache_new",
    "loc": 126,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_set_encoding_cache_invalid:chunk1",
    "text": "def test_set_encoding_cache_invalid():\n    with pytest.raises(ValueError) as exc:\n        _set_encoding_cache(\"invalid-encoding\")\n    assert isinstance(exc.value, ValueError)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_set_encoding_cache_invalid",
    "loc": 131,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_less_tokens:chunk1",
    "text": "def test_truncate_tokens_less_tokens(mock_get_encoding: None):\n    assert (\n        truncate_tokens(mock_get_encoding, \"Hello world\", 10) == \"Hello world\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_less_tokens",
    "loc": 137,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_more_tokens:chunk1",
    "text": "def test_truncate_tokens_more_tokens(mock_get_encoding: None):\n    assert (\n        truncate_tokens(mock_get_encoding, \"Hello world\", 1) == \"Hello world\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_more_tokens",
    "loc": 143,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_empty_string:chunk1",
    "text": "def test_truncate_tokens_empty_string(mock_get_encoding: None):\n    assert truncate_tokens(mock_get_encoding, \"\", 10) == \"\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_empty_string",
    "loc": 149,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_exception:chunk1",
    "text": "def test_truncate_tokens_exception(\n    mock_get_encoding: None, caplog: pytest.LogCaptureFixture\n):\n    with pytest.raises(Exception) as e:\n        truncate_tokens(mock_get_encoding, None, 10)\n        assert isinstance(e, Exception)",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_exception",
    "loc": 153,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_different_encodings:chunk1",
    "text": "def test_truncate_tokens_different_encodings(mock_get_encoding: None):\n    assert truncate_tokens(mock_get_encoding, \"Test\", 1) == \"Test\"",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_different_encodings",
    "loc": 161,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:function:test_truncate_tokens_truncation_required:chunk1",
    "text": "def test_truncate_tokens_truncation_required():\n    \"\"\"Test where the number of tokens exceeds the maximum allowed.\"\"\"\n    max_tokens = 3\n    long_text = \"This is a longer text that needs to be truncated\"\n    truncated_text = truncate_tokens(ENCODING_NAME, long_text, max_tokens)\n    assert len(truncated_text) < len(long_text)\n    assert \"This is a\" in truncated_text",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "function",
    "name": "test_truncate_tokens_truncation_required",
    "loc": 165,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\models\\test_tokens.py:class:MockEncoder:chunk1",
    "text": "class MockEncoder:\n    def encode(self, text):\n        \"\"\"Mock the encode method.\"\"\"\n        return text.split()",
    "repo": "readme-ai",
    "path": "tests\\models\\test_tokens.py",
    "type": "class",
    "name": "MockEncoder",
    "loc": 14,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:dockerfile_content:chunk1",
    "text": "def dockerfile_content() -> str:\n    \"\"\"Fixture for sample Dockerfile content.\"\"\"\n    return \"\"\"\n# Use a base image with Python 3.10 installed (multi-platform)\nFROM --platform=${BUILDPLATFORM} python:3.10-slim-buster\n\n# Set working directory\nWORKDIR /app\n\n# Set environment variable for Git Python\nENV GIT_PYTHON_REFRESH=quiet\n\n# Install system dependencies and clean up apt cache\nRUN apt-get update && apt-get install -y git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a non-root user with a specific UID and GID (i.e. 1000 in this case)\nRUN groupadd -r tempuser -g 1000 && \\\n    useradd -r -u 1000 -g tempuser tempuser && \\\n    mkdir -p /home/tempuser && \\\n    chown -R tempuser:tempuser /home/tempuser\n\n# Set permissions for the working directory to the new user\nRUN chown tempuser:tempuser /app\n\n# Switch to the new user\nUSER tempuser\n\n# Add the directory where pip installs user scripts to the PATH\nENV PATH=/home/tempuser/.local/bin:$PATH\n\n# Install the readmeai package from PyPI with a pinned version\nRUN pip install --no-cache-dir --user --upgrade readmeai\n\n# Set the command to run the CLI\nENTRYPOINT [\"readmeai\"]\nCMD [\"--help\"]\n\"\"\"",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "dockerfile_content",
    "loc": 12,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:dockerfile_parser:chunk1",
    "text": "def dockerfile_parser() -> DockerfileParser:\n    \"\"\"Fixture for DockerfileParser.\"\"\"\n    return DockerfileParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "dockerfile_parser",
    "loc": 53,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:docker_compose_content:chunk1",
    "text": "def docker_compose_content() -> str:\n    \"\"\"Fixture for sample docker-compose.yaml content.\"\"\"\n    return \"\"\"\nversion: '3.4'\n\nservices:\n    broker:\n        container_name: celery-broker\n        image: rabbitmq:3.8.2-management-alpine\n        ports:\n            - \"8080:15672\"\n            - \"5672:5672\"\n        networks:\n            - network\n\n    backend:\n        container_name: celery-backend\n        image: redis:5.0.7\n        ports:\n            - \"6379:6379\"\n        networks:\n            - network\n        command: redis-server --requirepass password\n\n    audio:\n        build: ./src/workers\n        container_name: celery-wAudio\n        environment:\n            - REDIS_HOST=backend\n            - REDIS_PORT=6379\n            - REDIS_DB=0\n            - REDIS_PASS=password\n            - RABBITMQ_HOST=broker\n            - RABBITMQ_PORT=5672\n            - RABBITMQ_USER=guest\n            - RABBITMQ_PASS=guest\n        links:\n            - backend:backend\n            - broker:broker\n        networks:\n            - network\n        command: celery worker -A audio.worker.audio --loglevel=INFO --concurrency=2 --hostname=wAudio@%h --queues audio -E --config=audio.config\n\n    euro:\n        build: ./src/workers\n        container_name: celery-wEuro\n        environment:\n            - REDIS_HOST=backend\n            - REDIS_PORT=6379\n            - REDIS_DB=0\n            - REDIS_PASS=password\n            - RABBITMQ_HOST=broker\n            - RABBITMQ_PORT=5672\n            - RABBITMQ_USER=guest\n            - RABBITMQ_PASS=guest\n        links:\n            - backend:backend\n            - broker:broker\n        networks:\n            - network\n        command: celery worker -A euro.worker.euro --loglevel=INFO --concurrency=2 --hostname=wEuro@%h --queues euro -E --config=euro.config\n\n    api:\n        build: ./src/api\n        container_name: celery-api\n        environment:\n            - REDIS_HOST=backend\n            - RABBITMQ_HOST=broker\n        links:\n            - backend:backend\n            - broker:broker\n        ports:\n            - \"5000:5000\"\n        networks:\n            - network\n\n    client:\n        build: ./src/client\n        container_name: celery-client\n        environment:\n            - API_URL=http://api:5000\n        links:\n            - api:api\n        networks:\n            - network\n\nnetworks:\n    network: {}\n\"\"\"",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "docker_compose_content",
    "loc": 62,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:docker_compose_parser:chunk1",
    "text": "def docker_compose_parser() -> DockerComposeParser:\n    \"\"\"Fixture for DockerComposeParser.\"\"\"\n    return DockerComposeParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "docker_compose_parser",
    "loc": 154,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:properties_content:chunk1",
    "text": "def properties_content() -> str:\n    \"\"\"Fixture for sample .properties content.\"\"\"\n    return \"\"\"\nversion=3.4.0-SNAPSHOT\nlatestVersion=true\nspring.build-type=oss\norg.gradle.caching=true\norg.gradle.parallel=true\norg.gradle.jvmargs=-Xmx2g -Dfile.encoding=UTF-8\nassertjVersion=3.26.3\ncheckstyleToolVersion=10.12.4\ncommonsCodecVersion=1.17.1\ngraalVersion=22.3\nhamcrestVersion=2.2\njacksonVersion=2.17.2\njavaFormatVersion=0.0.43\njunitJupiterVersion=5.11.0\nkotlinVersion=1.9.25\nmavenVersion=3.9.4\nmockitoVersion=5.13.0\nnativeBuildToolsVersion=0.10.3\nsnakeYamlVersion=2.3\nspringFrameworkVersion=6.2.0-RC1\nspringFramework60xVersion=6.0.23\ntomcatVersion=10.1.30\nkotlin.stdlib.default.dependency=false\n\"\"\"",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "properties_content",
    "loc": 163,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\conftest.py:function:properties_parser:chunk1",
    "text": "def properties_parser() -> PropertiesParser:\n    \"\"\"Fixture for .properties parser.\"\"\"\n    return PropertiesParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\conftest.py",
    "type": "function",
    "name": "properties_parser",
    "loc": 193,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_cmake_parser:chunk1",
    "text": "def test_cmake_parser():\n    \"\"\"Test the CMake parser.\"\"\"\n    parser = CMakeParser()\n    expected = \"arrow\"\n    result = parser.parse(cmake_file)\n    assert expected in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_cmake_parser",
    "loc": 129,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:cmake_parser:chunk1",
    "text": "def cmake_parser():\n    return CMakeParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "cmake_parser",
    "loc": 138,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:makefile_am_parser:chunk1",
    "text": "def makefile_am_parser():\n    return MakefileAmParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "makefile_am_parser",
    "loc": 143,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:configureac_parser:chunk1",
    "text": "def configureac_parser():\n    return ConfigureAcParser()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "configureac_parser",
    "loc": 148,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:content_cmakelists:chunk1",
    "text": "def content_cmakelists(tmp_path):\n    cmakelists_file = tmp_path / \"CMakeLists.txt\"\n    cmakelists_file.write_text(SAMPLE_CONTENT_CMAKELISTSTXT)\n    return cmakelists_file",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "content_cmakelists",
    "loc": 153,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:content_configureac:chunk1",
    "text": "def content_configureac(tmp_path):\n    configureac_file = tmp_path / \"configure.ac\"\n    configureac_file.write_text(SAMPLE_CONTENT_CONFIGUREAC)\n    return configureac_file",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "content_configureac",
    "loc": 160,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:content_makefileam:chunk1",
    "text": "def content_makefileam(tmp_path):\n    makefileam_file = tmp_path / \"Makefile.am\"\n    makefileam_file.write_text(SAMPLE_CONTENT_MAKEFILEAM)\n    return makefileam_file",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "content_makefileam",
    "loc": 167,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_cmake_parser_valid:chunk1",
    "text": "def test_cmake_parser_valid(cmake_parser, content_cmakelists):\n    extracted_dependencies = cmake_parser.parse(content_cmakelists.read_text())\n    \"\"\"\n    expected_dependencies = [\n        \"Qt5Widgets\",\n        \"Boost::system\",\n        \"Boost::thread\",\n        \"Boost::filesystem\",\n    ]\n    \"\"\"\n    assert \"Qt5Widgets\" in extracted_dependencies",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_cmake_parser_valid",
    "loc": 173,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_cmake_parser_invalid:chunk1",
    "text": "def test_cmake_parser_invalid(cmake_parser):\n    extracted_dependencies = cmake_parser.parse(\"Invalid Content\")\n    assert extracted_dependencies == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_cmake_parser_invalid",
    "loc": 186,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_configureac_parser_valid:chunk1",
    "text": "def test_configureac_parser_valid(configureac_parser, content_configureac):\n    extracted_packages = configureac_parser.parse(\n        content_configureac.read_text(),\n    )\n    expected_packages = [\"mp\", \"clock_gettime\", \"rt\", \"dl\", \"pthread\"]\n    assert sorted(extracted_packages) == sorted(expected_packages)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_configureac_parser_valid",
    "loc": 192,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_configureac_parser_invalid:chunk1",
    "text": "def test_configureac_parser_invalid(configureac_parser):\n    extracted_packages = configureac_parser.parse(\"Invalid Content\")\n    assert extracted_packages == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_configureac_parser_invalid",
    "loc": 200,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_makefile_am_parser_valid:chunk1",
    "text": "def test_makefile_am_parser_valid(makefile_am_parser, content_makefileam):\n    extracted_packages = makefile_am_parser.parse(\n        content_makefileam.read_text(),\n    )\n    # expected_packages = [\"my_program\", \"libfoo\", \"check\"]\n    assert \"my_program\" in extracted_packages",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_makefile_am_parser_valid",
    "loc": 205,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_cpp.py:function:test_makefile_am_parser_invalid:chunk1",
    "text": "def test_makefile_am_parser_invalid(makefile_am_parser):\n    extracted_packages = makefile_am_parser.parse(\"Invalid Content\")\n    assert extracted_packages == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_cpp.py",
    "type": "function",
    "name": "test_makefile_am_parser_invalid",
    "loc": 213,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_docker.py:function:test_dockerfile_empty:chunk1",
    "text": "def test_dockerfile_empty(dockerfile_parser: DockerfileParser):\n    \"\"\"Test parsing empty Dockerfile.\"\"\"\n    result = dockerfile_parser.parse(\"\")\n    assert result == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_docker.py",
    "type": "function",
    "name": "test_dockerfile_empty",
    "loc": 4,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_docker.py:function:test_docker_compose_empty:chunk1",
    "text": "def test_docker_compose_empty(docker_compose_parser: DockerComposeParser):\n    \"\"\"Test parsing empty docker-compose.yaml.\"\"\"\n    result = docker_compose_parser.parse(\"\")\n    assert result == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_docker.py",
    "type": "function",
    "name": "test_docker_compose_empty",
    "loc": 10,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_docker.py:function:test_dockerfile_parser:chunk1",
    "text": "def test_dockerfile_parser(\n    dockerfile_parser: DockerfileParser, dockerfile_content: str\n):\n    \"\"\"Test parsing Dockerfile.\"\"\"\n    result = dockerfile_parser.parse(dockerfile_content)\n    assert result == [\"python: 3.10-slim-buster\"]",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_docker.py",
    "type": "function",
    "name": "test_dockerfile_parser",
    "loc": 16,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_docker.py:function:test_docker_compose_parser_services:chunk1",
    "text": "def test_docker_compose_parser_services(\n    docker_compose_parser: DockerComposeParser, docker_compose_content: str\n):\n    \"\"\"Test getting services from docker-compose.yaml.\"\"\"\n    docker_compose_parser.parse(docker_compose_content)\n    expected_services = [\"broker\", \"backend\", \"audio\", \"euro\", \"api\", \"client\"]\n    assert docker_compose_parser.get_services() == expected_services",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_docker.py",
    "type": "function",
    "name": "test_docker_compose_parser_services",
    "loc": 24,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_docker.py:function:test_docker_compose_parser_service_details:chunk1",
    "text": "def test_docker_compose_parser_service_details(\n    docker_compose_parser: DockerComposeParser, docker_compose_content: str\n):\n    \"\"\"Test getting detailed service information from docker-compose.yaml.\"\"\"\n    docker_compose_parser.parse(docker_compose_content)\n    service_details = docker_compose_parser.get_all_service_details()[0]\n    # Validate details for broker service\n    assert \"broker\" in service_details\n    assert service_details[\"broker\"][\"image\"] == \"rabbitmq:3.8.2-management-alpine\"\n    assert service_details[\"broker\"][\"ports\"] == [\"8080:15672\", \"5672:5672\"]\n    # Validate details for backend service\n    assert \"backend\" in service_details\n    assert service_details[\"backend\"][\"image\"] == \"redis:5.0.7\"\n    assert (\n        service_details[\"backend\"][\"command\"] == \"redis-server --requirepass password\"\n    )\n    # Validate details for audio service\n    assert \"audio\" in service_details\n    assert \"REDIS_HOST=backend\" in service_details[\"audio\"][\"environment\"]\n    assert service_details[\"audio\"][\"command\"] == (\n        \"celery worker -A audio.worker.audio --loglevel=INFO --concurrency=2 --hostname=wAudio@%h --queues audio -E --config=audio.config\"\n    )",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_docker.py",
    "type": "function",
    "name": "test_docker_compose_parser_service_details",
    "loc": 33,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_factory.py:function:mock_parser:chunk1",
    "text": "def mock_parser():\n    return MagicMock(spec=BaseFileParser)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_factory.py",
    "type": "function",
    "name": "mock_parser",
    "loc": 14,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_factory.py:function:test_default_parser:chunk1",
    "text": "def test_default_parser():\n    parser = ParserFactory.create_parser(\"unknown.extension\")\n    assert isinstance(parser, DefaultParser)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_factory.py",
    "type": "function",
    "name": "test_default_parser",
    "loc": 18,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_factory.py:function:test_registered_parsers:chunk1",
    "text": "def test_registered_parsers(\n    file_name: Literal[\"Pipfile\"]\n    | Literal[\"requirements.txt\"]\n    | Literal[\"CMakeLists.txt\"]\n    | Literal[\"package.json\"]\n    | Literal[\"build.gradle\"]\n    | Literal[\"go.mod\"]\n    | Literal[\"pom.xml\"]\n    | Literal[\"Cargo.toml\"]\n    | Literal[\"Package.swift\"]\n    | Literal[\"Dockerfile\"]\n    | Literal[\"docker-compose.yaml\"]\n    | Literal[\".properties\"],\n    parser_name: Literal[\"TomlParser\"]\n    | Literal[\"RequirementsParser\"]\n    | Literal[\"CMakeParser\"]\n    | Literal[\"PackageJsonParser\"]\n    | Literal[\"BuildGradleParser\"]\n    | Literal[\"GoModParser\"]\n    | Literal[\"MavenParser\"]\n    | Literal[\"CargoTomlParser\"]\n    | Literal[\"SwiftPackageParser\"]\n    | Literal[\"DockerfileParser\"]\n    | Literal[\"DockerComposeParser\"]\n    | Literal[\"PropertiesParser\"],\n):\n    parser = ParserFactory.create_parser(file_name)\n    assert parser.__class__.__name__ == parser_name",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_factory.py",
    "type": "function",
    "name": "test_registered_parsers",
    "loc": 40,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_factory.py:function:test_register_parser:chunk1",
    "text": "def test_register_parser(mock_parser: MagicMock):\n    ParserFactory.register_parser(\"newext\", mock_parser)\n    parser = ParserFactory.create_parser(\"file.newext\")\n    assert isinstance(parser, DefaultParser)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_factory.py",
    "type": "function",
    "name": "test_register_parser",
    "loc": 70,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_go.py:function:test_go_mod:chunk1",
    "text": "def test_go_mod():\n    \"\"\"Tests the go.mod parser.\"\"\"\n    parser = GoModParser()\n    expected_dependencies = [\"uuid\", \"goja_nodejs\", \"mysql\"]\n    assert parser.parse(content) == expected_dependencies",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_go.py",
    "type": "function",
    "name": "test_go_mod",
    "loc": 24,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_gradle.py:function:test_build_gradle:chunk1",
    "text": "def test_build_gradle():\n    \"\"\"Test parsing build.gradle files.\"\"\"\n    parser = BuildGradleParser()\n    dependencies = parser.parse(build_gradle)\n    assert sorted(dependencies) == sorted(\n        [\n            \"android\",\n            \"bintray\",\n            \"build\",\n            \"com\",\n            \"dcendents\",\n            \"github\",\n            \"gradle\",\n            \"jfrog\",\n            \"tools\",\n        ],\n    )",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_gradle.py",
    "type": "function",
    "name": "test_build_gradle",
    "loc": 102,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_gradle.py:function:test_build_gradle_kts:chunk1",
    "text": "def test_build_gradle_kts():\n    \"\"\"Test parsing build.gradle.kts files.\"\"\"\n    parser = BuildGradleKtsParser()\n    dependencies = parser.parse(build_gradle_kts)\n    assert sorted(dependencies) == sorted(\n        [\n            \"junit\",\n            \"test\",\n            \"uiautomator\",\n            \"espresso\",\n            \"ext\",\n            \"androidx\",\n            \"benchmark\",\n        ],\n    )",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_gradle.py",
    "type": "function",
    "name": "test_build_gradle_kts",
    "loc": 121,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_maven.py:function:test_maven_parser:chunk1",
    "text": "def test_maven_parser():\n    \"\"\"Tests the Maven parser for pom.xml files.\"\"\"\n    parser = MavenParser()\n    expected_dependencies = [\n        \"spring\",\n        \"junit\",\n        \"spring-test\",\n        \"mockito-core\",\n        \"spring-core\",\n        \"spring-web\",\n        \"spring-webmvc\",\n        \"spring-context\",\n        \"javax.servlet-api\",\n        \"jstl\",\n        \"logback-classic\",\n    ]\n    assert sorted(parser.parse(content)) == sorted(expected_dependencies)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_maven.py",
    "type": "function",
    "name": "test_maven_parser",
    "loc": 142,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_maven.py:function:test_maven_parser_exception_handling:chunk1",
    "text": "def test_maven_parser_exception_handling():\n    \"\"\"Test the Maven parser's exception handling.\"\"\"\n    parser = MavenParser()\n\n    with patch(\"re.compile\") as mock_compile:\n        mock_compile.side_effect = re.error(\"Test Exception\")\n        with patch.object(parser, \"handle_parsing_error\") as mock_handle:\n            parser.parse(content)\n            mock_handle.assert_called_once_with(\"pom.xml: Test Exception\")",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_maven.py",
    "type": "function",
    "name": "test_maven_parser_exception_handling",
    "loc": 161,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_npm.py:function:test_package_json_parser:chunk1",
    "text": "def test_package_json_parser():\n    \"\"\"Tests the JSON parser.\"\"\"\n    parser = PackageJsonParser()\n    dependencies = parser.parse(package_json_file)\n    assert dependencies == [\n        \"@react-navigation/native\",\n        \"@react-navigation/native-stack\",\n        \"expo\",\n        \"expo-clipboard\",\n        \"expo-status-bar\",\n        \"react\",\n        \"react-dom\",\n        \"react-native\",\n        \"react-native-safe-area-context\",\n        \"react-native-screens\",\n        \"react-native-web\",\n        \"react-uuid\",\n        \"@babel/core\",\n        \"@types/react\",\n        \"@types/react-native\",\n        \"typescript\",\n    ]",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_npm.py",
    "type": "function",
    "name": "test_package_json_parser",
    "loc": 41,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_npm.py:function:test_package_json_parser_success:chunk1",
    "text": "def test_package_json_parser_success():\n    parser = PackageJsonParser()\n    content = json.dumps(\n        {\n            \"dependencies\": {\"packageA\": \"1.0.0\"},\n            \"devDependencies\": {\"packageB\": \"2.0.0\"},\n            \"peerDependencies\": {\"packageC\": \"3.0.0\"},\n        },\n    )\n    result = parser.parse(content)\n    assert set(result) == {\"packageA\", \"packageB\", \"packageC\"}",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_npm.py",
    "type": "function",
    "name": "test_package_json_parser_success",
    "loc": 65,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_npm.py:function:test_package_json_parser_no_dependencies:chunk1",
    "text": "def test_package_json_parser_no_dependencies():\n    parser = PackageJsonParser()\n    content = json.dumps({})\n    result = parser.parse(content)\n    assert result == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_npm.py",
    "type": "function",
    "name": "test_package_json_parser_no_dependencies",
    "loc": 78,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_npm.py:function:test_package_json_parser_invalid_json:chunk1",
    "text": "def test_package_json_parser_invalid_json():\n    parser = PackageJsonParser()\n    content = '{\"dependencies\": {\"packageA\": \"1.0.0\", invalid json}'\n    data = parser.parse(content)\n    assert isinstance(data, list)\n    assert data == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_npm.py",
    "type": "function",
    "name": "test_package_json_parser_invalid_json",
    "loc": 85,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:parser:chunk1",
    "text": "def parser(properties_parser: PropertiesParser):\n    return properties_parser",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "parser",
    "loc": 7,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_parser_initialization:chunk1",
    "text": "def test_parser_initialization(parser: PropertiesParser):\n    assert isinstance(parser, PropertiesParser)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_parser_initialization",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_parse_properties_content:chunk1",
    "text": "def test_parse_properties_content(\n    parser: PropertiesParser, properties_content: str\n):\n    result = parser.parse(properties_content)\n    assert isinstance(result, list)\n    assert len(result) > 0",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_parse_properties_content",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_extracted_dependencies:chunk1",
    "text": "def test_extracted_dependencies(\n    parser: PropertiesParser, properties_content: str\n):\n    result = parser.parse(properties_content)\n    assert \"spring\" in result\n    assert \"kotlin\" in result\n    assert \"java\" in result\n    assert \"junit\" in result\n    assert \"kotlin\" in result\n    assert \"tomcat\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_extracted_dependencies",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_version_extraction:chunk1",
    "text": "def test_version_extraction(parser: PropertiesParser):\n    content = \"javaVersion=1.2.3\\notherToolVersion=4.5.6\"\n    result = parser.parse(content)\n    assert \"java\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_version_extraction",
    "loc": 35,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_camelcase_splitting:chunk1",
    "text": "def test_camelcase_splitting(parser: PropertiesParser):\n    content = \"camelCaseProperty=someValue\"\n    result = parser.parse(content)\n    assert \"camelcaseproperty\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_camelcase_splitting",
    "loc": 41,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_snake_case_splitting:chunk1",
    "text": "def test_snake_case_splitting(parser: PropertiesParser):\n    content = \"snake_case_property=some_value\"\n    result = parser.parse(content)\n    assert \"snake_case_property\" in result\n    assert \"some_value\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_snake_case_splitting",
    "loc": 47,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_common_prefix_removal:chunk1",
    "text": "def test_common_prefix_removal(parser: PropertiesParser):\n    content = \"org.myproject.core.MainClass\"\n    result = parser.parse(content)\n    assert \"mainclass\" in result\n    assert \"myproject\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_common_prefix_removal",
    "loc": 54,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_common_suffix_removal:chunk1",
    "text": "def test_common_suffix_removal(parser: PropertiesParser):\n    content = \"myAwesomeLibrary=1.0.0\\ncoolFrameworkVersion=2.0.0\"\n    result = parser.parse(content)\n    assert \"myawesomelibrary\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_common_suffix_removal",
    "loc": 61,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_short_word_filtering:chunk1",
    "text": "def test_short_word_filtering(parser: PropertiesParser):\n    content = \"a=1\\nbb=2\\nccc=3\"\n    result = parser.parse(content)\n    assert \"a\" in result\n    assert \"ccc\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_short_word_filtering",
    "loc": 67,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_common_tech_keywords:chunk1",
    "text": "def test_common_tech_keywords(parser: PropertiesParser):\n    content = \"using=java,python,react\"\n    result = parser.parse(content)\n    assert \"java\" in result\n    assert \"python\" in result\n    assert \"react\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_common_tech_keywords",
    "loc": 74,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_empty_content:chunk1",
    "text": "def test_empty_content(parser: PropertiesParser):\n    result = parser.parse(\"\")\n    assert result == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_empty_content",
    "loc": 82,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_comment_handling:chunk1",
    "text": "def test_comment_handling(parser: PropertiesParser):\n    content = \"# This is a comment\\nrealProperty=value\"\n    result = parser.parse(content)\n    assert \"realproperty\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_comment_handling",
    "loc": 87,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_multiple_occurrences:chunk1",
    "text": "def test_multiple_occurrences(parser: PropertiesParser):\n    content = \"spring.version=5.0\\nspring.boot.version=2.5\"\n    result = parser.parse(content)\n    assert result.count(\"spring\") == 1  # Should only appear once",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_multiple_occurrences",
    "loc": 93,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_properties.py:function:test_case_insensitivity:chunk1",
    "text": "def test_case_insensitivity(parser: PropertiesParser):\n    content = \"JavaVersion=11\\nkotlinVERSION=1.5\"\n    result = parser.parse(content)\n    assert \"java\" in result\n    assert \"kotlin\" in result",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_properties.py",
    "type": "function",
    "name": "test_case_insensitivity",
    "loc": 99,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_requirements_parser:chunk1",
    "text": "def test_requirements_parser():\n    \"\"\"Tests the requirements.txt parser.\"\"\"\n    content = \"\"\"\n    # This is a comment\n    aiohttp==3.8.6\n    aiosignal==1.3.1\n    gitpython==3.1.40 ; python_full_version >= \"3.8.1\" and python_full_version < \"4.0.0\"\n    \"\"\"\n    parser = RequirementsParser()\n    expected_dependencies = [\"aiohttp\", \"aiosignal\", \"gitpython\"]\n    assert parser.parse(content) == expected_dependencies",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_requirements_parser",
    "loc": 6,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_pipfile_parser:chunk1",
    "text": "def test_pipfile_parser():\n    \"\"\"Tests the Pipfile parser.\"\"\"\n    content = \"\"\"\n    [[source]]\n    url = \"https://pypi.org/simple\"\n    verify_ssl = true\n    name = \"pypi\"\n\n    [packages]\n    aiohttp = \"==3.8.6\"\n    aiosignal = \"==1.3.1\"\n\n    [dev-packages]\n    pytest = \"==6.2.5\"\n\n    [requires]\n    python_version = \"3.11\"\n    \"\"\"\n    parser = TomlParser()\n    expected_dependencies = [\"aiohttp\", \"aiosignal\", \"pytest\"]\n    assert parser.parse(content).sort() == expected_dependencies.sort()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_pipfile_parser",
    "loc": 19,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_pyproject_poetry_parser:chunk1",
    "text": "def test_pyproject_poetry_parser():\n    \"\"\"Tests the pyproject.toml parser.\"\"\"\n    content = \"\"\"\n    [build-system]\n    requires = [\"poetry-core\"]\n    build-backend = \"poetry.core.masonry.api\"\n\n    [tool.poetry.dependencies]\n    python = \"^3.8.1\"\n    click = \"^8.1.7\"\n\n    [tool.poetry.dev-dependencies]\n    black = \"*\"\n    flake8 = \"*\"\n    isort = \"*\"\n    pytest = \"*\"\n    pytest-cov = \"*\"\n    pre-commit = \"*\"\n    \"\"\"\n    parser = TomlParser()\n    expected_dependencies = [\n        \"python\",\n        \"click\",\n        \"black\",\n        \"flake8\",\n        \"isort\",\n        \"pytest\",\n        \"pytest-cov\",\n        \"pre-commit\",\n    ]\n    assert parser.parse(content).sort() == expected_dependencies.sort()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_pyproject_poetry_parser",
    "loc": 42,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_pyproject_flit_parser:chunk1",
    "text": "def test_pyproject_flit_parser():\n    \"\"\"Tests the pyproject.toml parser for Flit.\"\"\"\n    content = \"\"\"\n    [build-system]\n    requires = [\"flit_core >=3,<4\"]\n    build-backend = \"flit_core.buildapi\"\n\n    [project]\n    dependencies = [\n        \"requests >=2.6\",\n        \"configparser; python_version == '2.7'\",\n    ]\n\n    [project.optional-dependencies]\n    test = [\n        \"pytest >=2.7.3\",\n        \"pytest-cov\",\n    ]\n    doc = [\"sphinx\"]\n    \"\"\"\n    parser = TomlParser()\n    expected_dependencies = [\n        \"requests\",\n        \"configparser\",\n        \"pytest\",\n        \"pytest-cov\",\n        \"sphinx\",\n    ]\n    assert parser.parse(content).sort() == expected_dependencies.sort()",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_pyproject_flit_parser",
    "loc": 75,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_conda_env_yaml_parser:chunk1",
    "text": "def test_conda_env_yaml_parser():\n    \"\"\"Tests the conda environment.yml parser.\"\"\"\n    content = \"\"\"\n    name: readmeai\n    channels:\n    - conda-forge\n    - defaults\n    dependencies:\n    - python>=3.9\n    - pip\n    - pip:\n        - pandas==1.3.3\n        - snowflake-connector-python==2.4.6\n    \"\"\"\n    parser = YamlParser()\n    expected_dependencies = [\n        \"python\",\n        \"pip\",\n        \"pandas\",\n        \"snowflake-connector-python\",\n    ]\n    assert parser.parse(content) == expected_dependencies",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_conda_env_yaml_parser",
    "loc": 106,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_requirements_parser_success:chunk1",
    "text": "def test_requirements_parser_success():\n    content = \"package1\\npackage2==1.0\\n# Comment\"\n    parser = RequirementsParser()\n    assert parser.parse(content) == [\"package1\", \"package2\"]",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_requirements_parser_success",
    "loc": 130,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_requirements_parser_regex_error:chunk1",
    "text": "def test_requirements_parser_regex_error():\n    content = \"[Invalid Regex\"\n    parser = RequirementsParser()\n    data = parser.parse(content)\n    assert data == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_requirements_parser_regex_error",
    "loc": 136,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_toml_parser_toml_decode_error:chunk1",
    "text": "def test_toml_parser_toml_decode_error():\n    content = \"[Invalid TOML\"\n    parser = TomlParser()\n    data = parser.parse(content)\n    assert data == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_toml_parser_toml_decode_error",
    "loc": 143,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_yaml_parser_success:chunk1",
    "text": "def test_yaml_parser_success():\n    content = \"dependencies:\\n  - package1\\n  - package2=1.0\"\n    parser = YamlParser()\n    assert parser.parse(content) == [\"package1\", \"package2\"]",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_yaml_parser_success",
    "loc": 150,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_python.py:function:test_yaml_parser_yaml_error:chunk1",
    "text": "def test_yaml_parser_yaml_error():\n    content = \"{Invalid YAML\"\n    parser = YamlParser()\n    data = parser.parse(content)\n    assert data == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_python.py",
    "type": "function",
    "name": "test_yaml_parser_yaml_error",
    "loc": 156,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_rust.py:function:test_cargo_toml_parser:chunk1",
    "text": "def test_cargo_toml_parser():\n    \"\"\"Tests the Cargo.toml parser.\"\"\"\n    parser = CargoTomlParser()\n    expected_dependencies = [\n        \"bech32\",\n        \"lightning\",\n        \"secp256k1\",\n        \"num-traits\",\n        \"bitcoin_hashes\",\n        \"hashbrown\",\n        \"serde\",\n        \"bitcoin\",\n        \"lightning\",\n        \"hex\",\n        \"serde_json\",\n    ]\n    assert parser.parse(content) == expected_dependencies",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_rust.py",
    "type": "function",
    "name": "test_cargo_toml_parser",
    "loc": 35,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_rust.py:function:test_cargo_toml_parser_missing_section:chunk1",
    "text": "def test_cargo_toml_parser_missing_section():\n    parser = CargoTomlParser()\n    content = \"\"\"\n    [dependencies]\n    packageA = \"1.0.0\"\n    \"\"\"\n    result = parser.parse(content)\n    assert set(result) == {\"packageA\"}",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_rust.py",
    "type": "function",
    "name": "test_cargo_toml_parser_missing_section",
    "loc": 54,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_rust.py:function:test_cargo_toml_parser_extended_dependency_tables:chunk1",
    "text": "def test_cargo_toml_parser_extended_dependency_tables():\n    parser = CargoTomlParser()\n    content = \"\"\"\n    [dependencies]\n    packageA = \"1.0.0\"\n\n    [dev-dependencies]\n    packageC = \"3.0.0\"\n    \"\"\"\n    result = parser.parse(content)\n    assert set(result) == {\"packageA\", \"packageC\"}",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_rust.py",
    "type": "function",
    "name": "test_cargo_toml_parser_extended_dependency_tables",
    "loc": 64,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_rust.py:function:test_cargo_toml_parser_invalid_toml:chunk1",
    "text": "def test_cargo_toml_parser_invalid_toml():\n    parser = CargoTomlParser()\n    content = \"This is not a valid TOML\"\n    data = parser.parse(content)\n    assert isinstance(data, list)\n    assert data == []",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_rust.py",
    "type": "function",
    "name": "test_cargo_toml_parser_invalid_toml",
    "loc": 77,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\parsers\\test_swift.py:function:test_swift_parser:chunk1",
    "text": "def test_swift_parser(content, expected):\n    \"\"\"Test the Swift Package parser.\"\"\"\n    parser = SwiftPackageParser()\n    dependencies = parser.parse(content)\n    assert sorted(dependencies) == sorted(expected)\n    assert len(dependencies) == len(expected)",
    "repo": "readme-ai",
    "path": "tests\\parsers\\test_swift.py",
    "type": "function",
    "name": "test_swift_parser",
    "loc": 214,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:converter:chunk1",
    "text": "def converter() -> Callable[[str], str]:\n    return convert",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "converter",
    "loc": 8,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_basic_markdown_conversion:chunk1",
    "text": "def test_basic_markdown_conversion(\n    converter: Callable[[str], str], markdown: str, expected_html: str\n):\n    assert converter(markdown) == expected_html",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_basic_markdown_conversion",
    "loc": 30,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_unordered_list:chunk1",
    "text": "def test_unordered_list(converter: Callable[[str], str]):\n    markdown = \"\"\"\n    - Item 1\n    - Item 2\n    - Item 3\n    \"\"\"\n    expected_html = \"\"\"\n    <ul>\n    <li>Item 1</li>\n    <li>Item 2</li>\n    <li>Item 3</li>\n    </ul>\n    \"\"\"\n    assert converter(markdown).strip() == expected_html.strip()\n    assert converter(markdown).strip() == expected_html.strip()",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_unordered_list",
    "loc": 36,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_ordered_list:chunk1",
    "text": "def test_ordered_list(converter: Callable[[str], str]):\n    markdown = \"\"\"\n1. First item\n2. Second item\n3. Third item\n\"\"\"\n    expected_html = \"\"\"\n<ol>\n<li>First item</li>\n<li>Second item</li>\n<li>Third item</li>\n</ol>\n\"\"\"\n    assert converter(markdown).strip() == expected_html.strip()",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_ordered_list",
    "loc": 53,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_mixed_formatting:chunk1",
    "text": "def test_mixed_formatting(converter: Callable[[str], str]):\n    markdown = \"This is **bold** and *italic* text with a [link](https://example.com) and `inline code`.\"\n    expected_html = 'This is <strong>bold</strong> and <em>italic</em> text with a <a href=\"https://example.com\">link</a> and <code>inline code</code>.'\n    assert converter(markdown) == expected_html",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_mixed_formatting",
    "loc": 69,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_nested_formatting:chunk1",
    "text": "def test_nested_formatting(converter: Callable[[str], str]):\n    markdown = \"**Bold text with *italic* inside**\"\n    expected_html = \"<strong>Bold text with <em>italic</em> inside</strong>\"\n    assert converter(markdown) == expected_html",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_nested_formatting",
    "loc": 75,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_multiple_paragraphs:chunk1",
    "text": "def test_multiple_paragraphs(converter: Callable[[str], str]):\n    markdown = \"\"\"\nFirst paragraph with **bold** text.\n\nSecond paragraph with *italic* text.\n\n### Header in between\n\nLast paragraph with a [link](https://example.com).\n\"\"\"\n    expected_html = \"\"\"\nFirst paragraph with <strong>bold</strong> text.\n\nSecond paragraph with <em>italic</em> text.\n\n<h3>Header in between</h3>\n\nLast paragraph with a <a href=\"https://example.com\">link</a>.\n\"\"\"\n    assert converter(markdown).strip() == expected_html.strip()",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_multiple_paragraphs",
    "loc": 81,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_markdown_to_html.py:function:test_edge_cases:chunk1",
    "text": "def test_edge_cases(converter: Callable[[str], str]):\n    assert converter(\"\") == \"\"\n    assert converter(\"Plain text without markdown\") == \"Plain text without markdown\"\n    assert converter(\"**\") == \"**\"\n    assert converter(\"*\") == \"*\"\n    assert converter(\"[]()\") == \"[]()\"",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_markdown_to_html.py",
    "type": "function",
    "name": "test_edge_cases",
    "loc": 103,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_fix_markdown_table_rows:chunk1",
    "text": "def test_fix_markdown_table_rows():\n    \"\"\"Test that the markdown table is extracted from the input string.\"\"\"\n    text = (\n        \"| Feat | Summary ||---------|-------------|| Content | Content | \"\n        \"| Content | Content |\"\n    )\n    formatted_md_table = fix_markdown_table_rows(text)\n    assert isinstance(formatted_md_table, str)\n    assert len(formatted_md_table.split(\"\\n\")) == 4",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_fix_markdown_table_rows",
    "loc": 12,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_fix_markdown_table_rows_malformed:chunk1",
    "text": "def test_fix_markdown_table_rows_malformed():\n    \"\"\"Test malformed markdown table rows.\"\"\"\n    malformed_table = \"\"\"| Feature || Description ||---------|-------------| | Data || More Data |\"\"\"\n    result = fix_markdown_table_rows(malformed_table)\n    assert isinstance(result, str)\n    assert \"| Feature | Description |\" in result",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_fix_markdown_table_rows_malformed",
    "loc": 23,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_process_text:chunk1",
    "text": "def test_process_text(input_text, expected):\n    \"\"\"Test that the markdown text is extracted from the input string.\"\"\"\n    result = process_text(input_text)\n    assert result == expected\n    assert isinstance(result, str)",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_process_text",
    "loc": 57,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_format_markdown_table_parametrize:chunk1",
    "text": "def test_format_markdown_table_parametrize(input_text, expected_output):\n    \"\"\"Test format_markdown_table with realistic table input.\"\"\"\n    result = format_markdown_table(input_text)\n    assert result == expected_output",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_format_markdown_table_parametrize",
    "loc": 76,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_process_markdown:chunk1",
    "text": "def test_process_markdown(input_text, expected_output):\n    assert process_markdown(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_process_markdown",
    "loc": 112,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_process_text_unmatched_quotes:chunk1",
    "text": "def test_process_text_unmatched_quotes(input_text, expected):\n    \"\"\"Test process_text with unmatched quotes.\"\"\"\n    result = process_text(input_text)\n    assert result == expected",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_process_text_unmatched_quotes",
    "loc": 128,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\postprocessor\\test_response_cleaner.py:function:test_remove_quotes:chunk1",
    "text": "def test_remove_quotes(input_string, expected_output):\n    assert remove_quotes(input_string) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\postprocessor\\test_response_cleaner.py",
    "type": "function",
    "name": "test_remove_quotes",
    "loc": 159,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_document_cleaner.py:function:test_remove_empty_lines:chunk1",
    "text": "def test_remove_empty_lines(input_text, expected_output):\n    cleaner = DocumentCleaner(\n        remove_empty_lines=True,\n        remove_extra_whitespaces=False,\n        remove_trailing_whitespaces=False,\n        normalize_indentation=False,\n    )\n    assert cleaner.clean(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_document_cleaner.py",
    "type": "function",
    "name": "test_remove_empty_lines",
    "loc": 13,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_document_cleaner.py:function:test_remove_extra_whitespaces:chunk1",
    "text": "def test_remove_extra_whitespaces(input_text, expected_output):\n    cleaner = DocumentCleaner(\n        remove_empty_lines=False,\n        remove_extra_whitespaces=True,\n        remove_trailing_whitespaces=True,  # Changed to true to match expected output\n        normalize_indentation=False,\n    )\n    assert cleaner.clean(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_document_cleaner.py",
    "type": "function",
    "name": "test_remove_extra_whitespaces",
    "loc": 32,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_document_cleaner.py:function:test_remove_trailing_whitespaces:chunk1",
    "text": "def test_remove_trailing_whitespaces(input_text, expected_output):\n    cleaner = DocumentCleaner(\n        remove_empty_lines=False,\n        remove_extra_whitespaces=False,\n        remove_trailing_whitespaces=True,\n        normalize_indentation=False,\n    )\n    assert cleaner.clean(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_document_cleaner.py",
    "type": "function",
    "name": "test_remove_trailing_whitespaces",
    "loc": 49,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_document_cleaner.py:function:test_normalize_indentation:chunk1",
    "text": "def test_normalize_indentation(input_text, expected_output):\n    cleaner = DocumentCleaner(\n        remove_empty_lines=False,\n        remove_extra_whitespaces=False,\n        remove_trailing_whitespaces=False,\n        normalize_indentation=True,\n    )\n    assert cleaner.clean(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_document_cleaner.py",
    "type": "function",
    "name": "test_normalize_indentation",
    "loc": 69,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_document_cleaner.py:function:test_clean_all:chunk1",
    "text": "def test_clean_all(input_text, expected_output):\n    cleaner = DocumentCleaner(\n        remove_empty_lines=True,\n        remove_extra_whitespaces=True,\n        remove_trailing_whitespaces=True,\n        normalize_indentation=True,\n    )\n    assert cleaner.clean(input_text) == expected_output",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_document_cleaner.py",
    "type": "function",
    "name": "test_clean_all",
    "loc": 87,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:repo_path:chunk1",
    "text": "def repo_path() -> Path:\n    return Path(\"/home/user/project\")",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "repo_path",
    "loc": 10,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:ignore_list:chunk1",
    "text": "def ignore_list() -> dict[str, Any]:\n    return {\n        \"directories\": [\"node_modules\", \".git\"],\n        \"extensions\": [\"pyc\", \"tmp\"],\n        \"files\": [\".DS_Store\", \"Thumbs.db\"],\n    }",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "ignore_list",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:test_is_excluded:chunk1",
    "text": "def test_is_excluded(\n    file_path: Path,\n    expected: bool,\n    repo_path: Path,\n    ignore_list: dict[str, Any],\n) -> None:\n    assert is_excluded(ignore_list, file_path, repo_path) == expected",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "test_is_excluded",
    "loc": 36,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:test_is_excluded_empty_ignore_list:chunk1",
    "text": "def test_is_excluded_empty_ignore_list(repo_path: Path):\n    empty_ignore_list = {\"directories\": [], \"extensions\": [], \"files\": []}\n    file_path = Path(\"/home/user/project/src/main.py\")\n    assert not is_excluded(empty_ignore_list, file_path, repo_path)",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "test_is_excluded_empty_ignore_list",
    "loc": 45,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:test_is_excluded_no_match:chunk1",
    "text": "def test_is_excluded_no_match(repo_path: Path, ignore_list: dict[str, Any]):\n    file_path = Path(\"/home/user/project/src/app.js\")\n    assert not is_excluded(ignore_list, file_path, repo_path)",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "test_is_excluded_no_match",
    "loc": 51,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:test_is_excluded_case_sensitivity:chunk1",
    "text": "def test_is_excluded_case_sensitivity(\n    repo_path: Path, ignore_list: dict[str, Any]\n):\n    file_path = Path(\"/home/user/project/.GIT/config\")\n    assert not is_excluded(ignore_list, file_path, repo_path)",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "test_is_excluded_case_sensitivity",
    "loc": 56,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\preprocessor\\test_file_filter.py:function:test_is_excluded_nested_directory:chunk1",
    "text": "def test_is_excluded_nested_directory(\n    repo_path: Path, ignore_list: dict[str, Any]\n):\n    file_path = Path(\"/home/user/project/src/node_modules/package.json\")\n    assert is_excluded(ignore_list, file_path, repo_path)",
    "repo": "readme-ai",
    "path": "tests\\preprocessor\\test_file_filter.py",
    "type": "function",
    "name": "test_is_excluded_nested_directory",
    "loc": 63,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\retrievers\\git\\test_repository.py:function:mock_git_repo:chunk1",
    "text": "def mock_git_repo():\n    with patch(\"git.Repo\") as mock:\n        yield mock",
    "repo": "readme-ai",
    "path": "tests\\retrievers\\git\\test_repository.py",
    "type": "function",
    "name": "mock_git_repo",
    "loc": 17,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_json:chunk1",
    "text": "def test_read_json(file_handler: FileHandler, tmp_path: Path):\n    \"\"\"Test that a JSON file is read.\"\"\"\n    test_data = {\"key\": \"value\"}\n    file = tmp_path / \"data.json\"\n    file.write_text(json.dumps(test_data))\n    data = file_handler.read(file)\n    assert data == test_data",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_json",
    "loc": 10,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_json_cache:chunk1",
    "text": "def test_read_json_cache(file_handler: FileHandler, tmp_path: Path):\n    \"\"\"Test that a file read is cached.\"\"\"\n    test_data = {\"key\": \"value\"}\n    file = tmp_path / \"data.json\"\n    file.write_text(json.dumps(test_data))\n    data = file_handler.read(file)\n\n    if isinstance(data, dict):\n        assert file_handler.cache.get(file) == data\n\n    with patch.object(FileHandler, \"read_json\") as read_json_mock:\n        file_handler.read(file)\n        read_json_mock.assert_not_called()",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_json_cache",
    "loc": 19,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_write_json:chunk1",
    "text": "def test_write_json(file_handler: FileHandler, tmp_path: Path):\n    \"\"\"Test that a JSON file is written.\"\"\"\n    test_data = {\"key\": \"value\"}\n    file = tmp_path / \"out.json\"\n    file_handler.write(file, test_data)\n    data = json.loads(file.read_text())\n    assert data == test_data",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_write_json",
    "loc": 34,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_exception:chunk1",
    "text": "def test_read_exception(file_handler: FileHandler, json_file_path_fixture: Path):\n    \"\"\"Test that a read exception raises a FileReadError.\"\"\"\n    with (\n        patch(\n            \"readmeai.utilities.file_handler.FileHandler.get_action\",\n            side_effect=Exception(\"Read error\"),\n        ),\n        pytest.raises(FileReadError) as exc,\n    ):\n        file_handler.read(json_file_path_fixture)\n        assert isinstance(exc.value, FileReadError)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_exception",
    "loc": 43,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_write_exception:chunk1",
    "text": "def test_write_exception(\n    file_handler: FileHandler,\n    json_file_path_fixture: Path,\n    json_data_fixture: str,\n):\n    \"\"\"Test that a write exception raises a FileWriteError.\"\"\"\n    with (\n        patch(\n            \"readmeai.utilities.file_handler.FileHandler.get_action\",\n            side_effect=Exception(\"Write error\"),\n        ),\n        pytest.raises(FileWriteError) as exc,\n    ):\n        file_handler.write(json_file_path_fixture, json_data_fixture)\n        assert isinstance(exc.value, FileWriteError)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_write_exception",
    "loc": 56,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_caching:chunk1",
    "text": "def test_caching(\n    file_handler: FileHandler,\n    json_data_fixture: str,\n    json_file_path_fixture: Path,\n):\n    \"\"\"Test that file reads are cached.\"\"\"\n    with patch(\"builtins.open\", mock_open(read_data=json_data_fixture), create=True):\n        file_handler.read(json_file_path_fixture)\n\n    with patch.object(FileHandler, \"read_json\") as read_json_mock:\n        file_handler.read(json_file_path_fixture)\n        read_json_mock.assert_not_called()\n        assert isinstance(file_handler.cache, dict)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_caching",
    "loc": 73,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_unsupported_file_type_read:chunk1",
    "text": "def test_unsupported_file_type_read(file_handler: FileHandler):\n    \"\"\"Test that an unsupported file type raises a FileReadError.\"\"\"\n    with pytest.raises(FileReadError) as exc:\n        file_handler.read(\"unsupported_file.xyz\")\n    assert isinstance(exc.value, FileReadError)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_unsupported_file_type_read",
    "loc": 88,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_unsupported_file_type_write:chunk1",
    "text": "def test_unsupported_file_type_write(file_handler: FileHandler, json_data_fixture: str):\n    \"\"\"Test that an unsupported file type raises a FileWriteError.\"\"\"\n    with pytest.raises(FileWriteError) as exc:\n        file_handler.write(\"unsupported_file.xyz\", json_data_fixture)\n    assert isinstance(exc.value, FileWriteError)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_unsupported_file_type_write",
    "loc": 95,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_unsupported_action_type:chunk1",
    "text": "def test_unsupported_action_type(file_handler: FileHandler):\n    \"\"\"Test that an unsupported action type raises a ValueError.\"\"\"\n    with pytest.raises(ValueError):\n        file_handler.get_action(\"json\", \"unsupported_action\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_unsupported_action_type",
    "loc": 102,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_write_json:chunk1",
    "text": "def test_read_write_json(file_handler: FileHandler, json_file_path_fixture: Path):\n    \"\"\"Test that a JSON file is read and written.\"\"\"\n    test_data = {\"key\": \"value\"}\n    with patch(\n        \"builtins.open\",\n        mock_open(read_data=json.dumps(test_data)),\n        create=True,\n    ):\n        file_handler.write_json(json_file_path_fixture, test_data)\n        content = file_handler.read_json(json_file_path_fixture)\n        assert content == test_data",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_write_json",
    "loc": 108,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_write_markdown:chunk1",
    "text": "def test_read_write_markdown(file_handler: FileHandler, json_file_path_fixture: Path):\n    \"\"\"Test that a Markdown file is read and written.\"\"\"\n    test_content = \"# Markdown Content\"\n    with patch(\"builtins.open\", mock_open(read_data=test_content), create=True):\n        file_handler.write_markdown(json_file_path_fixture, test_content)\n        content = file_handler.read_markdown(json_file_path_fixture)\n        assert content == test_content",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_write_markdown",
    "loc": 121,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_file_handler.py:function:test_read_write_yaml:chunk1",
    "text": "def test_read_write_yaml(file_handler: FileHandler, json_file_path_fixture: Path):\n    \"\"\"Test that a YAML file is read and written.\"\"\"\n    test_data = {\"key\": \"value\"}\n    with patch(\n        \"builtins.open\",\n        mock_open(read_data=json.dumps(test_data)),\n        create=True,\n    ):\n        file_handler.write_yaml(json_file_path_fixture, test_data)\n        content = file_handler.read_yaml(json_file_path_fixture)\n        assert content == test_data",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_file_handler.py",
    "type": "function",
    "name": "test_read_write_yaml",
    "loc": 130,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:mock_import:chunk1",
    "text": "def mock_import():\n    \"\"\"Provides a mock for importlib.import_module.\"\"\"\n    with mock.patch(\"importlib.import_module\") as mock_imp:\n        yield mock_imp",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "mock_import",
    "loc": 9,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_existing_module:chunk1",
    "text": "def test_existing_module():\n    \"\"\"Verify stdlib modules are correctly identified as available.\"\"\"\n    assert is_available(\"os\") is True",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_existing_module",
    "loc": 15,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_nonexistent_module:chunk1",
    "text": "def test_nonexistent_module():\n    \"\"\"Verify non-existent modules are correctly identified as unavailable.\"\"\"\n    assert is_available(\"definitely_not_a_real_module_name_12345\") is False",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_nonexistent_module",
    "loc": 20,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_module_import_error:chunk1",
    "text": "def test_module_import_error(mock_import: mock.MagicMock | mock.AsyncMock):\n    \"\"\"Verify proper handling of ModuleNotFoundError.\"\"\"\n    mock_import.side_effect = ModuleNotFoundError()\n    assert is_available(\"mocked_module\") is False\n    mock_import.assert_called_once_with(\"mocked_module\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_module_import_error",
    "loc": 25,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_module_import_error_exception:chunk1",
    "text": "def test_module_import_error_exception(\n    mock_import: mock.MagicMock | mock.AsyncMock,\n    exception_class: type[ImportError] | type[PermissionError],\n):\n    \"\"\"Verify that non-ModuleNotFoundError exceptions are propagated.\"\"\"\n    mock_import.side_effect = exception_class()\n    with pytest.raises(exception_class):\n        is_available(\"mocked_module\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_module_import_error_exception",
    "loc": 39,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_empty_module_name:chunk1",
    "text": "def test_empty_module_name():\n    \"\"\"Verify that empty module names raise appropriate ValueError.\"\"\"\n    with pytest.raises(ValueError, match=\"Empty module name\"):\n        is_available(\"\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_empty_module_name",
    "loc": 49,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_standard_modules:chunk1",
    "text": "def test_standard_modules(\n    module_name: Literal[\"sys\"]\n    | Literal[\"datetime\"]\n    | Literal[\"collections\"]\n    | Literal[\"json\"],\n):\n    \"\"\"Verify availability check for various standard library modules.\"\"\"\n    assert is_available(module_name) is True",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_standard_modules",
    "loc": 64,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_module_side_effects:chunk1",
    "text": "def test_module_side_effects(mock_import: mock.MagicMock | mock.AsyncMock):\n    \"\"\"Verify that availability check doesn't pollute global namespace.\"\"\"\n    test_module = \"some_test_module\"\n    is_available(test_module)\n\n    mock_import.assert_called_once_with(test_module)\n    assert test_module not in globals()",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_module_side_effects",
    "loc": 74,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_importer.py:function:test_case_sensitivity:chunk1",
    "text": "def test_case_sensitivity(module_name, expected):\n    \"\"\"Verify case-sensitive module name handling.\"\"\"\n    assert is_available(module_name) is expected",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_importer.py",
    "type": "function",
    "name": "test_case_sensitivity",
    "loc": 90,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:test_build_resource_path_default_module:chunk1",
    "text": "def test_build_resource_path_default_module(tmp_path: Path):\n    \"\"\"Test build_resource_path with default module and submodule.\"\"\"\n    config_file = tmp_path / \"config.toml\"\n    config_file.write_text(\"# Test Config\")\n    # Monkey patch the resources.files method to return our temp directory\n\n    def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()\n\n    with pytest.MonkeyPatch.context() as m:\n        m.setattr(\"importlib.resources.files\", mock_files)\n        path = build_resource_path(\"config.toml\")\n        assert isinstance(path, Path)\n        assert path.name == \"config.toml\"\n        assert str(tmp_path) in str(path)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "test_build_resource_path_default_module",
    "loc": 11,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:mock_files:chunk1",
    "text": "def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "mock_files",
    "loc": 17,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:test_build_resource_path_custom_module:chunk1",
    "text": "def test_build_resource_path_custom_module(tmp_path: Path):\n    \"\"\"Test build_resource_path with a custom module and submodule.\"\"\"\n    config_file = tmp_path / \"test_config.toml\"\n    config_file.write_text(\"# Test Custom Config\")\n\n    # Monkey patch the resources.files method to return our temp directory\n    def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()\n\n    with pytest.MonkeyPatch.context() as m:\n        m.setattr(\"importlib.resources.files\", mock_files)\n        path = build_resource_path(\n            file_path=\"test_config.toml\", module=\"readmeai.tests\", submodule=\"resources\"\n        )\n        assert isinstance(path, Path)\n        assert path.name == \"test_config.toml\"\n        assert str(tmp_path) in str(path)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "test_build_resource_path_custom_module",
    "loc": 32,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:mock_files:chunk1",
    "text": "def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "mock_files",
    "loc": 38,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:test_build_resource_path_fallback:chunk1",
    "text": "def test_build_resource_path_fallback(tmp_path):\n    \"\"\"Test fallback mechanism when importlib.resources fails.\"\"\"\n\n    # Simulate importlib.resources failure\n    def mock_files(module):\n        raise TypeError(\"Simulated importlib.resources error\")\n\n    # Attempt to import pkg_resources, but use a fallback if not available\n    try:\n        import pkg_resources\n\n        # Create a mock config file\n        config_file = tmp_path / \"config.toml\"\n        config_file.write_text(\"# Test Fallback Config\")\n\n        # Monkey patch pkg_resources.resource_filename to return our temp path\n        with pytest.MonkeyPatch.context() as m:\n            m.setattr(\"importlib.resources.files\", mock_files)\n            m.setattr(\n                \"pkg_resources.resource_filename\",\n                lambda module, path: str(tmp_path / path),\n            )\n            path = build_resource_path(\"config.toml\")\n            assert isinstance(path, Path)\n            assert path.name == \"config.toml\"\n            assert str(tmp_path) in str(path)\n\n    except ImportError:\n        pytest.skip(\"pkg_resources not available\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "test_build_resource_path_fallback",
    "loc": 55,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:mock_files:chunk1",
    "text": "def mock_files(module):\n        raise TypeError(\"Simulated importlib.resources error\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "mock_files",
    "loc": 59,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:test_build_resource_path_failure:chunk1",
    "text": "def test_build_resource_path_failure(tmp_path: Path):\n    \"\"\"Test that FileReadError is raised when all methods fail.\"\"\"\n\n    # Simulate both importlib.resources and pkg_resources failures\n    def mock_files(module):\n        raise TypeError(\"Simulated importlib.resources error\")\n\n    with pytest.MonkeyPatch.context() as m:\n        m.setattr(\"importlib.resources.files\", mock_files)\n\n        # Remove pkg_resources if it exists\n        if \"pkg_resources\" in sys.modules:\n            del sys.modules[\"pkg_resources\"]\n\n        with pytest.raises(FileReadError, match=\"Failed to load resource file\"):\n            build_resource_path(\"nonexistent.toml\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "test_build_resource_path_failure",
    "loc": 86,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:mock_files:chunk1",
    "text": "def mock_files(module):\n        raise TypeError(\"Simulated importlib.resources error\")",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "mock_files",
    "loc": 90,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:test_build_resource_path_multiple_files:chunk1",
    "text": "def test_build_resource_path_multiple_files(\n    tmp_path: Path,\n    file_path: Literal[\"config.toml\"]\n    | Literal[\"settings.json\"]\n    | Literal[\"template.txt\"],\n):\n    \"\"\"Test building paths for multiple different files.\"\"\"\n    mock_file = tmp_path / file_path\n    mock_file.write_text(f\"# Test {file_path}\")\n\n    # Monkey patch the resources.files method to return our temp directory\n    def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()\n\n    with pytest.MonkeyPatch.context() as m:\n        m.setattr(\"importlib.resources.files\", mock_files)\n        path = build_resource_path(file_path)\n        assert isinstance(path, Path)\n        assert path.name == file_path\n        assert str(tmp_path) in str(path)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "test_build_resource_path_multiple_files",
    "loc": 105,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:function:mock_files:chunk1",
    "text": "def mock_files(module):\n        class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)\n\n        return MockPath()",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "function",
    "name": "mock_files",
    "loc": 116,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:class:MockPath:chunk1",
    "text": "class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "class",
    "name": "MockPath",
    "loc": 18,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:class:MockPath:chunk1",
    "text": "class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "class",
    "name": "MockPath",
    "loc": 39,
    "role": "tests"
  },
  {
    "id": "readme-ai:tests\\utilities\\test_resource_manager.py:class:MockPath:chunk1",
    "text": "class MockPath:\n            def joinpath(self, *args):\n                return tmp_path / os.path.join(*args)",
    "repo": "readme-ai",
    "path": "tests\\utilities\\test_resource_manager.py",
    "type": "class",
    "name": "MockPath",
    "loc": 117,
    "role": "tests"
  }
]