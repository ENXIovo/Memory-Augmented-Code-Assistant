[
  {
    "id": "setup.py",
    "summary": "No description | classes: build_ext, concat_license_files, wheel_concatenate, install, clean, sdist | functions: _get_package_path, report, get_submodule_folders, check_submodules, check_for_files, not_exists_or_empty | imports: platform, filecmp, glob, importlib | [setup.py]",
    "role": "src",
    "loc": 1040
  },
  {
    "id": "android\\pytorch_android\\generate_test_torchscripts.py",
    "summary": "No description | classes: Test | functions: scriptAndSave | imports: torch | [android pytorch_android generate_test_torchscripts.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "android\\test_app\\make_assets.py",
    "summary": "No description | imports: torchvision, torch | [android test_app make_assets.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "android\\test_app\\make_assets_custom.py",
    "summary": "This is a script for PyTorch Android custom selective build test. It prepares | imports: yaml, torchvision, torch | [android test_app make_assets_custom.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "aten\\src\\ATen\\native\\quantized\\cpu\\qnnpack\\configure.py",
    "summary": "No description | functions: main | imports: confu | [aten src ATen native quantized cpu qnnpack configure.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "aten\\src\\ATen\\native\\quantized\\cpu\\qnnpack\\generate-wrapper.py",
    "summary": "No description | [aten src ATen native quantized cpu qnnpack generate-wrapper.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "aten\\src\\ATen\\native\\quantized\\cpu\\qnnpack\\deps\\clog\\configure.py",
    "summary": "No description | functions: main | imports: confu | [aten src ATen native quantized cpu qnnpack deps clog configure.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "aten\\src\\ATen\\native\\transformers\\cuda\\mem_eff_attention\\kernels\\generate_kernels.py",
    "summary": "No description | classes: FwdKernel, BwdKernel | functions: write_decl_impl, main | imports: argparse, dataclasses | [aten src ATen native transformers cuda mem_eff_attention kernels generate_kernels.py]",
    "role": "src",
    "loc": 326
  },
  {
    "id": "aten\\src\\ATen\\nnapi\\codegen.py",
    "summary": "Code generator for NNAPI wrapper.  We can't link directly against | functions: main | imports: textwrap | [aten src ATen nnapi codegen.py]",
    "role": "src",
    "loc": 263
  },
  {
    "id": "benchmarks\\compare-fastrnn-results.py",
    "summary": "No description | functions: construct_name, get_times | imports: argparse, json | [benchmarks compare-fastrnn-results.py]",
    "role": "benchmarks",
    "loc": 60
  },
  {
    "id": "benchmarks\\upload_scribe.py",
    "summary": "Scribe Uploader for Pytorch Benchmark Data | classes: ScribeUploader, PytorchBenchmarkUploader | imports: argparse, json, subprocess, requests | [benchmarks upload_scribe.py]",
    "role": "benchmarks",
    "loc": 149
  },
  {
    "id": "benchmarks\\distributed\\ddp\\benchmark.py",
    "summary": "No description | classes: Benchmark, TorchvisionBenchmark | functions: allgather_object, allgather_run, allequal, benchmark_process_group, run_benchmark, sweep | imports: argparse, json, shlex, subprocess | [benchmarks distributed ddp benchmark.py]",
    "role": "benchmarks",
    "loc": 219
  },
  {
    "id": "benchmarks\\distributed\\ddp\\diff.py",
    "summary": "No description | functions: load, main | imports: argparse, json, numpy | [benchmarks distributed ddp diff.py]",
    "role": "benchmarks",
    "loc": 59
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\launcher.py",
    "summary": "A function that gets the name for the rank | functions: get_name, get_server_rank, get_cuda_server_rank, get_server_rref, run_trainer, call_trainers | imports: argparse, json, data, metrics | [benchmarks distributed rpc parameter_server launcher.py]",
    "role": "benchmarks",
    "loc": 493
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\utils.py",
    "summary": "A helper function creates a list containing the indices, values, and size | functions: sparse_tensor_to_rpc_format, sparse_rpc_format_to_tensor, process_bucket_with_remote_server, callback | imports: torch | [benchmarks distributed rpc parameter_server utils.py]",
    "role": "benchmarks",
    "loc": 50
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\data\\DummyData.py",
    "summary": "No description | classes: DummyData | imports: random, numpy, torch | [benchmarks distributed rpc parameter_server data DummyData.py]",
    "role": "benchmarks",
    "loc": 45
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\data\\__init__.py",
    "summary": "Package initializer | imports: DummyData | [benchmarks distributed rpc parameter_server data __init__.py]",
    "role": "benchmarks",
    "loc": 2
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\metrics\\CPUMetric.py",
    "summary": "No description | classes: CPUMetric | imports: MetricBase | [benchmarks distributed rpc parameter_server metrics CPUMetric.py]",
    "role": "benchmarks",
    "loc": 17
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\metrics\\CUDAMetric.py",
    "summary": "No description | classes: CUDAMetric | imports: torch, MetricBase | [benchmarks distributed rpc parameter_server metrics CUDAMetric.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\metrics\\MetricBase.py",
    "summary": "No description | classes: MetricBase | imports: abc | [benchmarks distributed rpc parameter_server metrics MetricBase.py]",
    "role": "benchmarks",
    "loc": 19
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\metrics\\MetricsLogger.py",
    "summary": "No description | classes: MetricsLogger | imports: CPUMetric, CUDAMetric | [benchmarks distributed rpc parameter_server metrics MetricsLogger.py]",
    "role": "benchmarks",
    "loc": 68
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\metrics\\ProcessedMetricsPrinter.py",
    "summary": "No description | classes: ProcessedMetricsPrinter | imports: statistics, pandas, tabulate | [benchmarks distributed rpc parameter_server metrics ProcessedMetricsPrinter.py]",
    "role": "benchmarks",
    "loc": 73
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\models\\DummyModel.py",
    "summary": "No description | classes: DummyModel | imports: torch | [benchmarks distributed rpc parameter_server models DummyModel.py]",
    "role": "benchmarks",
    "loc": 33
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\models\\__init__.py",
    "summary": "Package initializer | imports: DummyModel | [benchmarks distributed rpc parameter_server models __init__.py]",
    "role": "benchmarks",
    "loc": 2
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\server\\server.py",
    "summary": "No description | classes: ParameterServerBase, AverageParameterServer, AverageBatchParameterServer | imports: functools, threading, abc, metrics | [benchmarks distributed rpc parameter_server server server.py]",
    "role": "benchmarks",
    "loc": 297
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\server\\__init__.py",
    "summary": "Package initializer | imports: server | [benchmarks distributed rpc parameter_server server __init__.py]",
    "role": "benchmarks",
    "loc": 5
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\criterions.py",
    "summary": "A function that creates a CrossEntropyLoss | functions: cel | imports: torch | [benchmarks distributed rpc parameter_server trainer criterions.py]",
    "role": "benchmarks",
    "loc": 8
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\ddp_models.py",
    "summary": "A function that creates a ddp_model and hook_state objects. | functions: basic_ddp_model | imports: torch | [benchmarks distributed rpc parameter_server trainer ddp_models.py]",
    "role": "benchmarks",
    "loc": 19
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\hooks.py",
    "summary": "A ddp communication hook that uses the process_group allreduce implementation. | functions: allreduce_hook, callback, hybrid_hook, rpc_hook, sparse_rpc_hook | imports: utils, torch | [benchmarks distributed rpc parameter_server trainer hooks.py]",
    "role": "benchmarks",
    "loc": 84
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\hook_states.py",
    "summary": "No description | classes: BasicHookState | [benchmarks distributed rpc parameter_server trainer hook_states.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\iteration_steps.py",
    "summary": "A function that performs an iteration of training. | functions: basic_iteration_step | [benchmarks distributed rpc parameter_server trainer iteration_steps.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\preprocess_data.py",
    "summary": "A function that moves the data from CPU to GPU | functions: preprocess_dummy_data | [benchmarks distributed rpc parameter_server trainer preprocess_data.py]",
    "role": "benchmarks",
    "loc": 12
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\trainer.py",
    "summary": "No description | classes: TrainerBase, DdpTrainer | imports: functools, abc, metrics, torch | [benchmarks distributed rpc parameter_server trainer trainer.py]",
    "role": "benchmarks",
    "loc": 220
  },
  {
    "id": "benchmarks\\distributed\\rpc\\parameter_server\\trainer\\__init__.py",
    "summary": "Package initializer | imports: criterions, ddp_models, hook_states, hooks | [benchmarks distributed rpc parameter_server trainer __init__.py]",
    "role": "benchmarks",
    "loc": 19
  },
  {
    "id": "benchmarks\\distributed\\rpc\\rl\\agent.py",
    "summary": "No description | classes: Policy, AgentBase | imports: operator, threading, functools, torch | [benchmarks distributed rpc rl agent.py]",
    "role": "benchmarks",
    "loc": 142
  },
  {
    "id": "benchmarks\\distributed\\rpc\\rl\\coordinator.py",
    "summary": "No description | classes: CoordinatorBase | imports: numpy, agent, observer, torch | [benchmarks distributed rpc rl coordinator.py]",
    "role": "benchmarks",
    "loc": 112
  },
  {
    "id": "benchmarks\\distributed\\rpc\\rl\\launcher.py",
    "summary": "No description | functions: str2bool, run_worker, find_graph_variable, append_spaces, print_benchmark_results, main | imports: argparse, json, coordinator, torch | [benchmarks distributed rpc rl launcher.py]",
    "role": "benchmarks",
    "loc": 219
  },
  {
    "id": "benchmarks\\distributed\\rpc\\rl\\observer.py",
    "summary": "No description | classes: ObserverBase | imports: random, agent, torch | [benchmarks distributed rpc rl observer.py]",
    "role": "benchmarks",
    "loc": 65
  },
  {
    "id": "benchmarks\\dynamo\\benchmarks.py",
    "summary": "No description | functions: model_names, parse_args | imports: argparse, timm_models, huggingface, torchbench | [benchmarks dynamo benchmarks.py]",
    "role": "benchmarks",
    "loc": 74
  },
  {
    "id": "benchmarks\\dynamo\\cachebench.py",
    "summary": "No description | classes: RunResult | functions: get_compile_time, _run_torchbench_from_args, _run_torchbench_model, _write_results_to_json, parse_cmd_args, main | imports: argparse, dataclasses, json, subprocess | [benchmarks dynamo cachebench.py]",
    "role": "benchmarks",
    "loc": 214
  },
  {
    "id": "benchmarks\\dynamo\\check_accuracy.py",
    "summary": "No description | functions: get_field, check_accuracy, main | imports: argparse, textwrap, pandas | [benchmarks dynamo check_accuracy.py]",
    "role": "benchmarks",
    "loc": 122
  },
  {
    "id": "benchmarks\\dynamo\\check_csv.py",
    "summary": "Basic accuracy checking. | functions: check_csv | imports: argparse, textwrap, pandas | [benchmarks dynamo check_csv.py]",
    "role": "benchmarks",
    "loc": 31
  },
  {
    "id": "benchmarks\\dynamo\\check_graph_breaks.py",
    "summary": "No description | functions: get_field, check_graph_breaks, main | imports: argparse, textwrap, pandas | [benchmarks dynamo check_graph_breaks.py]",
    "role": "benchmarks",
    "loc": 113
  },
  {
    "id": "benchmarks\\dynamo\\check_memory_compression_ratio.py",
    "summary": "No description | functions: main | imports: argparse, textwrap, pandas | [benchmarks dynamo check_memory_compression_ratio.py]",
    "role": "benchmarks",
    "loc": 49
  },
  {
    "id": "benchmarks\\dynamo\\check_perf_csv.py",
    "summary": "Basic performance checking. | functions: check_perf_csv | imports: argparse, textwrap, pandas | [benchmarks dynamo check_perf_csv.py]",
    "role": "benchmarks",
    "loc": 41
  },
  {
    "id": "benchmarks\\dynamo\\combine_csv.py",
    "summary": "No description | imports: ast, csv | [benchmarks dynamo combine_csv.py]",
    "role": "benchmarks",
    "loc": 40
  },
  {
    "id": "benchmarks\\dynamo\\common.py",
    "summary": "No description | classes: CI, Stats, AOTInductorModelCache, TimeOutException, DummyGradScaler, BenchmarkRunner | functions: load_yaml_file, flatten, maybe_list_to_set, model_specified_by_path, load_model_from_path, write_outputs | imports: argparse, copy, csv, dataclasses | [benchmarks dynamo common",
    "role": "benchmarks",
    "loc": 3490
  },
  {
    "id": "benchmarks\\dynamo\\distributed.py",
    "summary": "No description | functions: torchviz_model, profile_model, run_model, move_tensor, print_compile | imports: argparse, functools, torch, common | [benchmarks dynamo distributed.py]",
    "role": "benchmarks",
    "loc": 145
  },
  {
    "id": "benchmarks\\dynamo\\dist_util.py",
    "summary": "No description | classes: CustomLinear, MyModule, ToyModel | functions: setup, cleanup, model_iter_fn, get_model, fsdp_checkpointing_base, check_fn | imports: argparse, functools, importlib, torch | [benchmarks dynamo dist_util.py]",
    "role": "benchmarks",
    "loc": 116
  },
  {
    "id": "benchmarks\\dynamo\\huggingface.py",
    "summary": "No description | classes: HuggingfaceRunner | functions: pip_install, process_hf_reformer_output, get_module_cls_by_model_name, get_sequence_length, generate_inputs_for_model, rand_int_tensor | imports: importlib, subprocess, common, torch | [benchmarks dynamo huggingface.py]",
    "role": "benchmarks",
    "loc": 506
  },
  {
    "id": "benchmarks\\dynamo\\join_results.py",
    "summary": "A tool to merge multiple csv files (generated by torchbench.py/etc) into a single csv file. | functions: longest_common_prefix, main | imports: argparse, functools, operator, pandas | [benchmarks dynamo join_results.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\dynamo\\parse_logs.py",
    "summary": "No description | functions: chunker, normalize_file | imports: csv | [benchmarks dynamo parse_logs.py]",
    "role": "benchmarks",
    "loc": 136
  },
  {
    "id": "benchmarks\\dynamo\\runner.py",
    "summary": "A wrapper over the benchmark infrastructure to generate commonly used commands, | classes: Parser, ParsePerformanceLogs, LogInfo, SummaryStatDiffer, RegressionDetector, RegressionTracker | functions: flag_speedup, flag_compilation_latency, flag_compression_ratio, flag_accuracy, percentage, parse_arg",
    "role": "benchmarks",
    "loc": 1334
  },
  {
    "id": "benchmarks\\dynamo\\summarize_perf.py",
    "summary": "No description | functions: gmean, find_csv_files, is_csv, main | imports: click, pandas, tabulate | [benchmarks dynamo summarize_perf.py]",
    "role": "benchmarks",
    "loc": 119
  },
  {
    "id": "benchmarks\\dynamo\\test.py",
    "summary": "No description | classes: TestDynamoBenchmark | functions: is_asan_or_tsan | imports: unittest, common, torchbench, aiplatform | [benchmarks dynamo test.py]",
    "role": "benchmarks",
    "loc": 36
  },
  {
    "id": "benchmarks\\dynamo\\timm_models.py",
    "summary": "No description | classes: TimmRunner | functions: pip_install, refresh_model_names, read_models_from_docs, get_family_name, populate_family, timm_main | imports: importlib, subprocess, common, torch | [benchmarks dynamo timm_models.py]",
    "role": "benchmarks",
    "loc": 346
  },
  {
    "id": "benchmarks\\dynamo\\torchao_backend.py",
    "summary": "No description | functions: setup_baseline, torchao_optimize_ctx, inner, _torchao_apply | imports: torch, torchao | [benchmarks dynamo torchao_backend.py]",
    "role": "benchmarks",
    "loc": 47
  },
  {
    "id": "benchmarks\\dynamo\\torchbench.py",
    "summary": "No description | classes: TorchBenchmarkRunner | functions: _reassign_parameters, state_dict_hook, setup_torchbench_cwd, process_hf_reformer_output, process_hf_whisper_output, torchbench_main | imports: gc, importlib, torch, common | [benchmarks dynamo torchbench.py]",
    "role": "benchmarks",
    "loc": 374
  },
  {
    "id": "benchmarks\\dynamo\\training_loss.py",
    "summary": "No description | functions: data_processing, tokenize_function, training_iter_fn, model_training_evaluation, check_loss, parse_args | imports: argparse, inspect, datetime, datasets | [benchmarks dynamo training_loss.py]",
    "role": "benchmarks",
    "loc": 171
  },
  {
    "id": "benchmarks\\dynamo\\__init__.py",
    "summary": "Package initializer | [benchmarks dynamo __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\dynamo\\ci_expected_accuracy\\update_expected.py",
    "summary": "Update commited CSV files used as reference points by dynamo/inductor CI. | functions: query_job_sha, parse_job_name, parse_test_str, get_artifacts_urls, normalize_suite_filename, download_artifacts_and_extract_csvs | imports: argparse, json, subprocess, urllib | [benchmarks dynamo ci_expected_accur",
    "role": "benchmarks",
    "loc": 185
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\analyze_templates.py",
    "summary": "This script uses linear programming to analyze outputs of triton mm config tuning. | functions: parse_log_file, optimize_templates, main | imports: json, click, pulp | [benchmarks dynamo microbenchmarks analyze_templates.py]",
    "role": "benchmarks",
    "loc": 163
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\benchmark_helper.py",
    "summary": "No description | functions: time_with_torch_timer | imports: torch | [benchmarks dynamo microbenchmarks benchmark_helper.py]",
    "role": "benchmarks",
    "loc": 8
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\bench_mm_fusion.py",
    "summary": "No description | classes: Func | functions: bench, tflops, fn | imports: triton, prettytable, torch | [benchmarks dynamo microbenchmarks bench_mm_fusion.py]",
    "role": "benchmarks",
    "loc": 87
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\cache_debug_microbenchmarks.py",
    "summary": "No description | functions: huge_graph, fn, main | imports: timeit, torch | [benchmarks dynamo microbenchmarks cache_debug_microbenchmarks.py]",
    "role": "benchmarks",
    "loc": 20
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\cache_hit_microbenchmarks.py",
    "summary": "No description | functions: huge_graph, main, setup, fn | imports: timeit, torch | [benchmarks dynamo microbenchmarks cache_hit_microbenchmarks.py]",
    "role": "benchmarks",
    "loc": 34
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\dynamo_guard_eval.py",
    "summary": "No description | functions: bench, run_fn, main | imports: timeit, numpy, torch | [benchmarks dynamo microbenchmarks dynamo_guard_eval.py]",
    "role": "benchmarks",
    "loc": 30
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\dynamo_microbenchmarks.py",
    "summary": "No description | functions: symbolic_convert_overhead_stress_test, main, fn, profile | imports: cProfile, pstats, timeit, torch | [benchmarks dynamo microbenchmarks dynamo_microbenchmarks.py]",
    "role": "benchmarks",
    "loc": 32
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\fx_microbenchmarks.py",
    "summary": "No description | functions: huge_graph, fn, main | imports: timeit, torch | [benchmarks dynamo microbenchmarks fx_microbenchmarks.py]",
    "role": "benchmarks",
    "loc": 19
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\inductor_bmm.py",
    "summary": "No description | functions: inductor_aten_bmm, inductor_triton_bmm, torch_bmm, test_total_time | imports: benchmark_helper, torch | [benchmarks dynamo microbenchmarks inductor_bmm.py]",
    "role": "benchmarks",
    "loc": 40
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\inductor_cpu_atomic.py",
    "summary": "No description | functions: inductor_scatter_add, torch_scatter_add, test_total_time | imports: benchmark_helper, torch | [benchmarks dynamo microbenchmarks inductor_cpu_atomic.py]",
    "role": "benchmarks",
    "loc": 62
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\inductor_mm.py",
    "summary": "No description | functions: inductor_aten_mm, inductor_triton_mm, torch_mm, triton_mm, test_total_time, test_GPU_time | imports: triton, benchmark_helper, torch | [benchmarks dynamo microbenchmarks inductor_mm.py]",
    "role": "benchmarks",
    "loc": 100
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\matmul_relu.py",
    "summary": "No description | functions: inductor_mm, torch_mm_relu, torch_mm | imports: benchmark_helper, torch | [benchmarks dynamo microbenchmarks matmul_relu.py]",
    "role": "benchmarks",
    "loc": 80
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\microbench.py",
    "summary": "No description | classes: MyModel1, MyModel2, MicroBenchmarks | functions: compute_speedups, microbenchmark, main | imports: argparse, inspect, numpy, tabulate | [benchmarks dynamo microbenchmarks microbench.py]",
    "role": "benchmarks",
    "loc": 145
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\model.py",
    "summary": "No description | [benchmarks dynamo microbenchmarks model.py]",
    "role": "benchmarks",
    "loc": 18
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\operatorbench.py",
    "summary": "No description | functions: maybe_record_function, compute_speedups, strip_overloads, convert_to_jit, to_channels_last, microbenchmark | imports: csv, click, numpy, operator_inp_utils | [benchmarks dynamo microbenchmarks operatorbench.py]",
    "role": "benchmarks",
    "loc": 314
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\operator_inp_utils.py",
    "summary": "No description | classes: FuncCallWrapper, OperatorInputsMode, OperatorInputsLoader | functions: truncate_inp, serialize_sparse_tensor, deserialize_sparse_tensor, deserialize_tensor, serialize_tensor, serialize_torch_args | imports: functools, torch | [benchmarks dynamo microbenchmarks operator_inp_",
    "role": "benchmarks",
    "loc": 263
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\overheads.py",
    "summary": "No description | functions: add1, bench, main | imports: timeit, numpy, torch | [benchmarks dynamo microbenchmarks overheads.py]",
    "role": "benchmarks",
    "loc": 30
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\tensor_layout_mini_benchmark.py",
    "summary": "No description | functions: to_channels_last, bench_conv, baseline_fn, test_fn, main | imports: torch | [benchmarks dynamo microbenchmarks tensor_layout_mini_benchmark.py]",
    "role": "benchmarks",
    "loc": 49
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\utils.py",
    "summary": "No description | functions: rounded_linspace, powspace | imports: torch | [benchmarks dynamo microbenchmarks utils.py]",
    "role": "benchmarks",
    "loc": 14
  },
  {
    "id": "benchmarks\\dynamo\\microbenchmarks\\__init__.py",
    "summary": "Package initializer | [benchmarks dynamo microbenchmarks __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\check_results.py",
    "summary": "Keeps the first three digits of an integer and replaces the rest with zeros. | classes: ExpectedFileEntry, ResultFileEntry | functions: replace_with_zeros, main, log | imports: copy, csv, json, dataclasses | [benchmarks dynamo pr_time_benchmarks check_results.py]",
    "role": "benchmarks",
    "loc": 175
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\log_benchmarking_time.py",
    "summary": "No description | functions: main | imports: json, torch | [benchmarks dynamo pr_time_benchmarks log_benchmarking_time.py]",
    "role": "benchmarks",
    "loc": 12
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\__init__.py",
    "summary": "Package initializer | [benchmarks dynamo pr_time_benchmarks __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\add_loop.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks add_loop.py]",
    "role": "benchmarks",
    "loc": 59
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\aotdispatcher.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks aotdispatcher.py]",
    "role": "benchmarks",
    "loc": 58
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\aotdispatcher_partitioner.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks aotdispatcher_partitioner.py]",
    "role": "benchmarks",
    "loc": 38
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\aotdispatcher_partitioner2.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks aotdispatcher_partitioner2.py]",
    "role": "benchmarks",
    "loc": 43
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\basic_modules_benchmarks.py",
    "summary": "No description | classes: ListOfLinears, Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks basic_modules_benchmarks.py]",
    "role": "benchmarks",
    "loc": 66
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\benchmark_base.py",
    "summary": "No description | classes: BenchmarkBase | imports: csv, gc, json, abc | [benchmarks dynamo pr_time_benchmarks benchmarks benchmark_base.py]",
    "role": "benchmarks",
    "loc": 190
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\float_args.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks float_args.py]",
    "role": "benchmarks",
    "loc": 33
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\sum_floordiv.py",
    "summary": "No description | classes: M, Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks sum_floordiv.py]",
    "role": "benchmarks",
    "loc": 30
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\symint_sum.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks symint_sum.py]",
    "role": "benchmarks",
    "loc": 46
  },
  {
    "id": "benchmarks\\dynamo\\pr_time_benchmarks\\benchmarks\\update_hint_benchmark.py",
    "summary": "No description | classes: Benchmark | functions: main | imports: benchmark_base, torch | [benchmarks dynamo pr_time_benchmarks benchmarks update_hint_benchmark.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\fastrnns\\bench.py",
    "summary": "No description | classes: Event | functions: fit_str, to_str, print_header, pretty_print, trainbench, train_batch | imports: argparse, copy, gc, json | [benchmarks fastrnns bench.py]",
    "role": "benchmarks",
    "loc": 288
  },
  {
    "id": "benchmarks\\fastrnns\\cells.py",
    "summary": "No description | functions: milstm_cell, lstm_cell, flat_lstm_cell, premul_lstm_cell, premul_lstm_cell_no_bias, gru_cell | imports: torch | [benchmarks fastrnns cells.py]",
    "role": "benchmarks",
    "loc": 98
  },
  {
    "id": "benchmarks\\fastrnns\\conftest.py",
    "summary": "No description | functions: pytest_generate_tests, pytest_addoption | imports: pytest | [benchmarks fastrnns conftest.py]",
    "role": "benchmarks",
    "loc": 27
  },
  {
    "id": "benchmarks\\fastrnns\\custom_lstms.py",
    "summary": "Returns a ScriptModule that mimics a PyTorch native LSTM. | classes: LSTMCell, LayerNorm, LayerNormLSTMCell, LSTMLayer, ReverseLSTMLayer, BidirLSTMLayer | functions: script_lstm, script_lnlstm, reverse, init_stacked_lstm, flatten_states, double_flatten_states | imports: numbers, torch | [benchmarks ",
    "role": "benchmarks",
    "loc": 393
  },
  {
    "id": "benchmarks\\fastrnns\\factory.py",
    "summary": "No description | functions: flatten_list, lstm_backward_setup, simple_backward_setup, simple_backward, pytorch_lstm_creator, lstm_creator | imports: torch, cells, custom_lstms | [benchmarks fastrnns factory.py]",
    "role": "benchmarks",
    "loc": 409
  },
  {
    "id": "benchmarks\\fastrnns\\fuser.py",
    "summary": "No description | functions: set_fuser | imports: torch | [benchmarks fastrnns fuser.py]",
    "role": "benchmarks",
    "loc": 32
  },
  {
    "id": "benchmarks\\fastrnns\\profile.py",
    "summary": "No description | functions: run_rnn, run_iter, profile, system, describe_sizes, nvprof_output_filename | imports: argparse, datetime, subprocess, torch | [benchmarks fastrnns profile.py]",
    "role": "benchmarks",
    "loc": 135
  },
  {
    "id": "benchmarks\\fastrnns\\runner.py",
    "summary": "No description | classes: DisableCuDNN, DummyContext, AssertNoJIT | functions: get_nn_runners | imports: functools, torchvision, torch, factory | [benchmarks fastrnns runner.py]",
    "role": "benchmarks",
    "loc": 90
  },
  {
    "id": "benchmarks\\fastrnns\\scratch.py",
    "summary": "No description | functions: fn, recurrent, recurrent_scaleshift | imports: torch | [benchmarks fastrnns scratch.py]",
    "role": "benchmarks",
    "loc": 35
  },
  {
    "id": "benchmarks\\fastrnns\\test.py",
    "summary": "No description | functions: barf, assertEqual, filter_requires_grad, test_rnns, test_vl_py | imports: argparse, torch, factory, runner | [benchmarks fastrnns test.py]",
    "role": "benchmarks",
    "loc": 141
  },
  {
    "id": "benchmarks\\fastrnns\\test_bench.py",
    "summary": "No description | classes: TestBenchNetwork | functions: modeldef, cuda_sync | imports: pytest, torch, fuser, runner | [benchmarks fastrnns test_bench.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\fastrnns\\__init__.py",
    "summary": "Package initializer | imports: cells, factory | [benchmarks fastrnns __init__.py]",
    "role": "benchmarks",
    "loc": 7
  },
  {
    "id": "benchmarks\\framework_overhead_benchmark\\framework_overhead_benchmark.py",
    "summary": "No description | functions: parse_op_args, print_results, benchmark_simple_fn, main | imports: argparse, pt_wrapper_module, SimpleAddModule, utils | [benchmarks framework_overhead_benchmark framework_overhead_benchmark.py]",
    "role": "benchmarks",
    "loc": 89
  },
  {
    "id": "benchmarks\\framework_overhead_benchmark\\pt_wrapper_module.py",
    "summary": "Wraps the instance of wrapped_type. | classes: WrapperModule | imports: torch | [benchmarks framework_overhead_benchmark pt_wrapper_module.py]",
    "role": "benchmarks",
    "loc": 43
  },
  {
    "id": "benchmarks\\framework_overhead_benchmark\\SimpleAddModule.py",
    "summary": "No description | classes: SimpleAddModule | functions: add_tensors_loop | imports: utils, torch | [benchmarks framework_overhead_benchmark SimpleAddModule.py]",
    "role": "benchmarks",
    "loc": 13
  },
  {
    "id": "benchmarks\\framework_overhead_benchmark\\utils.py",
    "summary": "No description | functions: ms_to_us, secs_to_us, secs_to_ms, benchmark_using_throughput_benchmark, benchmark_module | imports: torch | [benchmarks framework_overhead_benchmark utils.py]",
    "role": "benchmarks",
    "loc": 28
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\audio_text_models.py",
    "summary": "No description | functions: get_wav2letter, forward, get_deepspeech, get_transformer, get_multiheadattn | imports: torchaudio_models, utils, torch, functorch | [benchmarks functional_autograd_benchmark audio_text_models.py]",
    "role": "benchmarks",
    "loc": 114
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\compare.py",
    "summary": "No description | functions: main | imports: argparse, utils | [benchmarks functional_autograd_benchmark compare.py]",
    "role": "benchmarks",
    "loc": 65
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\functional_autograd_benchmark.py",
    "summary": "No description | classes: ModelDef | functions: get_task_func, hessian_fwdrev, hessian_revrev, jacfwd, jacrev, get_task_functorch | imports: argparse, torch, functorch, audio_text_models | [benchmarks functional_autograd_benchmark functional_autograd_benchmark.py]",
    "role": "benchmarks",
    "loc": 264
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\ppl_models.py",
    "summary": "No description | functions: get_simple_regression, forward, get_robust_regression | imports: utils, torch | [benchmarks functional_autograd_benchmark ppl_models.py]",
    "role": "benchmarks",
    "loc": 64
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\torchaudio_models.py",
    "summary": "Wav2Letter model architecture from the `\"Wav2Letter: an End-to-End ConvNet-based Speech Recognition System\" | classes: Wav2Letter, SequenceWise, MaskConv, InferenceBatchSoftmax, BatchRNN, Lookahead | imports: torch | [benchmarks functional_autograd_benchmark torchaudio_models.py]",
    "role": "benchmarks",
    "loc": 611
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\torchvision_models.py",
    "summary": "3x3 convolution with padding | classes: BasicBlock, Bottleneck, ResNet, IntermediateLayerGetter, _SimpleSegmentationModel, FCN | functions: conv3x3, conv1x1, _resnet, resnet18, resnet50, _segm_resnet | imports: torch, scipy | [benchmarks functional_autograd_benchmark torchvision_models.py]",
    "role": "benchmarks",
    "loc": 733
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\utils.py",
    "summary": "Deletes the attribute specified by the given list of names. | functions: _del_nested_attr, _set_nested_attr, extract_weights, load_weights, to_markdown_table, write_line | imports: torch, functorch | [benchmarks functional_autograd_benchmark utils.py]",
    "role": "benchmarks",
    "loc": 83
  },
  {
    "id": "benchmarks\\functional_autograd_benchmark\\vision_models.py",
    "summary": "No description | functions: get_resnet18, forward, get_fcn_resnet, get_detr | imports: torchvision_models, utils, torch, functorch | [benchmarks functional_autograd_benchmark vision_models.py]",
    "role": "benchmarks",
    "loc": 104
  },
  {
    "id": "benchmarks\\fuser\\plot_speedups.py",
    "summary": "No description | imports: pandas, matplotlib | [benchmarks fuser plot_speedups.py]",
    "role": "benchmarks",
    "loc": 18
  },
  {
    "id": "benchmarks\\fuser\\run_benchmarks.py",
    "summary": "No description | functions: rand, scalar, small, small_2d, small_broadcast, medium | imports: inspect, click, torch | [benchmarks fuser run_benchmarks.py]",
    "role": "benchmarks",
    "loc": 227
  },
  {
    "id": "benchmarks\\gpt_fast\\benchmark.py",
    "summary": "No description | classes: SimpleMLP | functions: run_mlp_layer_norm_gelu, run_layer_norm, run_gather_gemv, gather_gemv, run_gemv, gemv | imports: argparse, csv, dataclasses, json | [benchmarks gpt_fast benchmark.py]",
    "role": "benchmarks",
    "loc": 274
  },
  {
    "id": "benchmarks\\gpt_fast\\common.py",
    "summary": "No description | classes: Experiment | functions: register_experiment, decorator | imports: dataclasses | [benchmarks gpt_fast common.py]",
    "role": "benchmarks",
    "loc": 19
  },
  {
    "id": "benchmarks\\gpt_fast\\generate.py",
    "summary": "No description | classes: GPTModelConfig | functions: device_sync, get_arch_name, multinomial_sample_one_no_sync, logits_to_probs, sample, prefill | imports: dataclasses, platform, torchao, common | [benchmarks gpt_fast generate.py]",
    "role": "benchmarks",
    "loc": 553
  },
  {
    "id": "benchmarks\\gpt_fast\\mixtral_moe_model.py",
    "summary": "No description | classes: ModelArgs, KVCache, Transformer, TransformerBlock, Attention, ConditionalFeedForward | functions: find_multiple, precompute_freqs_cis, apply_rotary_emb | imports: dataclasses, torch | [benchmarks gpt_fast mixtral_moe_model.py]",
    "role": "benchmarks",
    "loc": 244
  },
  {
    "id": "benchmarks\\gpt_fast\\mixtral_moe_quantize.py",
    "summary": "No description | classes: WeightOnlyInt8QuantHandler, WeightOnlyInt8Linear, ConditionalFeedForwardInt8 | functions: dynamically_quantize_per_channel, replace_linear_weight_only_int8_per_channel | imports: mixtral_moe_model, torch | [benchmarks gpt_fast mixtral_moe_quantize.py]",
    "role": "benchmarks",
    "loc": 140
  },
  {
    "id": "benchmarks\\gpt_fast\\model.py",
    "summary": "No description | classes: ModelArgs, KVCache, Transformer, TransformerBlock, Attention, FeedForward | functions: find_multiple, precompute_freqs_cis, apply_rotary_emb | imports: dataclasses, torch | [benchmarks gpt_fast model.py]",
    "role": "benchmarks",
    "loc": 232
  },
  {
    "id": "benchmarks\\gpt_fast\\quantize.py",
    "summary": "No description | classes: WeightOnlyInt8QuantHandler, WeightOnlyInt8Linear | functions: dynamically_quantize_per_channel, replace_linear_weight_only_int8_per_channel | imports: torch | [benchmarks gpt_fast quantize.py]",
    "role": "benchmarks",
    "loc": 67
  },
  {
    "id": "benchmarks\\inference\\process_metrics.py",
    "summary": "This file will take the csv outputs from server.py, calculate the mean and | imports: argparse, pandas | [benchmarks inference process_metrics.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\inference\\server.py",
    "summary": "This worker will send requests to a backend process, and measure the | classes: FrontendWorker, BackendWorker | functions: trace_handler | imports: argparse, asyncio, subprocess, threading | [benchmarks inference server.py]",
    "role": "benchmarks",
    "loc": 330
  },
  {
    "id": "benchmarks\\instruction_counts\\main.py",
    "summary": "Basic runner for the instruction count microbenchmarks. | functions: main | imports: argparse, applications, core, definitions | [benchmarks instruction_counts main.py]",
    "role": "benchmarks",
    "loc": 35
  },
  {
    "id": "benchmarks\\instruction_counts\\applications\\ci.py",
    "summary": "Collect instruction counts for continuous integration. | functions: main | imports: argparse, hashlib, json, core | [benchmarks instruction_counts applications ci.py]",
    "role": "benchmarks",
    "loc": 61
  },
  {
    "id": "benchmarks\\instruction_counts\\applications\\__init__.py",
    "summary": "Package initializer | [benchmarks instruction_counts applications __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\instruction_counts\\core\\api.py",
    "summary": "Key enums and structs used to handle data flow within the benchmark. | classes: RuntimeMode, AutogradMode, AutoLabels, GroupedSetup, GroupedBenchmark | imports: dataclasses, textwrap, worker, torch | [benchmarks instruction_counts core api.py]",
    "role": "benchmarks",
    "loc": 341
  },
  {
    "id": "benchmarks\\instruction_counts\\core\\expand.py",
    "summary": "Logic for converting human-readable benchmarks into executable form. | functions: _generate_torchscript_file, _get_stmt, _get_setup, materialize | imports: importlib, textwrap, uuid, torch | [benchmarks instruction_counts core expand.py]",
    "role": "benchmarks",
    "loc": 185
  },
  {
    "id": "benchmarks\\instruction_counts\\core\\types.py",
    "summary": "Type annotations for various benchmark objects. | imports: core | [benchmarks instruction_counts core types.py]",
    "role": "benchmarks",
    "loc": 62
  },
  {
    "id": "benchmarks\\instruction_counts\\core\\utils.py",
    "summary": "No description | functions: get_temp_dir, _flatten, flatten, parse_stmts | imports: atexit, shutil, textwrap, core | [benchmarks instruction_counts core utils.py]",
    "role": "benchmarks",
    "loc": 79
  },
  {
    "id": "benchmarks\\instruction_counts\\core\\__init__.py",
    "summary": "Package initializer | [benchmarks instruction_counts core __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\instruction_counts\\definitions\\setup.py",
    "summary": "Define some common setup blocks which benchmarks can reuse. | classes: Setup | imports: core | [benchmarks instruction_counts definitions setup.py]",
    "role": "benchmarks",
    "loc": 33
  },
  {
    "id": "benchmarks\\instruction_counts\\definitions\\standard.py",
    "summary": "Default set of benchmarks. | imports: core, definitions | [benchmarks instruction_counts definitions standard.py]",
    "role": "benchmarks",
    "loc": 251
  },
  {
    "id": "benchmarks\\instruction_counts\\definitions\\__init__.py",
    "summary": "Package initializer | [benchmarks instruction_counts definitions __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\instruction_counts\\execution\\runner.py",
    "summary": "Run benchmarks while handling parallelism, isolation, and fault tolerance. | classes: WorkerFailed, CorePool, Runner | imports: multiprocessing, subprocess, textwrap, threading | [benchmarks instruction_counts execution runner.py]",
    "role": "benchmarks",
    "loc": 220
  },
  {
    "id": "benchmarks\\instruction_counts\\execution\\work.py",
    "summary": "Handle the details of subprocess calls and retries for a given benchmark run. | classes: WorkOrder, _BenchmarkProcess, InProgress | imports: dataclasses, json, pickle, signal | [benchmarks instruction_counts execution work.py]",
    "role": "benchmarks",
    "loc": 174
  },
  {
    "id": "benchmarks\\instruction_counts\\execution\\__init__.py",
    "summary": "Package initializer | [benchmarks instruction_counts execution __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\instruction_counts\\worker\\main.py",
    "summary": "File invoked through subprocess to actually carry out measurements. | classes: WorkerTimerArgs, WorkerOutput, WorkerFailure, WorkerUnpickler | functions: _run, main | imports: argparse, dataclasses, io, pickle | [benchmarks instruction_counts worker main.py]",
    "role": "benchmarks",
    "loc": 123
  },
  {
    "id": "benchmarks\\instruction_counts\\worker\\__init__.py",
    "summary": "Package initializer | [benchmarks instruction_counts worker __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\nested\\nested_bmm_bench.py",
    "summary": "No description | functions: bench, sweep_n | imports: argparse, random, torch | [benchmarks nested nested_bmm_bench.py]",
    "role": "benchmarks",
    "loc": 55
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_all_other_test.py",
    "summary": "No description | imports: pt, operator_benchmark | [benchmarks operator_benchmark benchmark_all_other_test.py]",
    "role": "benchmarks",
    "loc": 43
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_all_quantized_test.py",
    "summary": "No description | imports: pt, operator_benchmark | [benchmarks operator_benchmark benchmark_all_quantized_test.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_all_test.py",
    "summary": "No description | imports: benchmark_all_other_test, benchmark_all_quantized_test, pt, operator_benchmark | [benchmarks operator_benchmark benchmark_all_test.py]",
    "role": "benchmarks",
    "loc": 6
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_core.py",
    "summary": "BenchmarkRunner is responsible for benchmarking all the registered | classes: BenchmarkRunner | functions: _register_test, _create_test, _build_test | imports: ast, copy, functools, json | [benchmarks operator_benchmark benchmark_core.py]",
    "role": "benchmarks",
    "loc": 391
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_pytorch.py",
    "summary": "This is a base class used to create Pytorch operator benchmark. | classes: TorchBenchmarkBase, PyTorchOperatorTestCase | functions: create_pytorch_op_test_case | imports: json, torch | [benchmarks operator_benchmark benchmark_pytorch.py]",
    "role": "benchmarks",
    "loc": 157
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_runner.py",
    "summary": "No description | functions: parse_args, main | imports: argparse, benchmark_core, benchmark_utils, torch | [benchmarks operator_benchmark benchmark_runner.py]",
    "role": "benchmarks",
    "loc": 138
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_test_generator.py",
    "summary": "This function creates PyTorch op test based on the given operator | functions: generate_pt_test, generate_pt_gradient_test, generate_pt_tests_from_op_list, generate_pt_gradient_tests_from_op_list | imports: benchmark_core, benchmark_pytorch | [benchmarks operator_benchmark benchmark_test_generator.p",
    "role": "benchmarks",
    "loc": 34
  },
  {
    "id": "benchmarks\\operator_benchmark\\benchmark_utils.py",
    "summary": "No description | classes: RandomSample | functions: shape_to_string, str2bool, numpy_random, set_omp_threads, set_mkl_threads, cross_product | imports: argparse, bisect, random, numpy | [benchmarks operator_benchmark benchmark_utils.py]",
    "role": "benchmarks",
    "loc": 279
  },
  {
    "id": "benchmarks\\operator_benchmark\\operator_benchmark.py",
    "summary": "No description | imports: benchmark_runner, benchmark_pytorch, benchmark_test_generator, benchmark_utils | [benchmarks operator_benchmark operator_benchmark.py]",
    "role": "benchmarks",
    "loc": 4
  },
  {
    "id": "benchmarks\\operator_benchmark\\__init__.py",
    "summary": "Package initializer | [benchmarks operator_benchmark __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\repeat_benchmark.py",
    "summary": "No description | functions: generate_data_for_repeat, pt_repeat, pt_repeat_n_times | imports: numpy, torch | [benchmarks operator_benchmark common repeat_benchmark.py]",
    "role": "benchmarks",
    "loc": 46
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\__init__.py",
    "summary": "Package initializer | [benchmarks operator_benchmark common __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\add_ops_list_test.py",
    "summary": "No description | classes: UnaryOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests add_ops_list_test.py]",
    "role": "tests",
    "loc": 27
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\jit_forward_test.py",
    "summary": "No description | classes: TorchSumBenchmark | functions: torch_sumall | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests jit_forward_test.py]",
    "role": "tests",
    "loc": 25
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\pt_backward_test.py",
    "summary": "No description | classes: AddBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests pt_backward_test.py]",
    "role": "tests",
    "loc": 16
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\pt_configs_list_test.py",
    "summary": "No description | classes: AddBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests pt_configs_list_test.py]",
    "role": "tests",
    "loc": 28
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\pt_cpu_gpu_forward_backward_test.py",
    "summary": "No description | classes: AddBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests pt_cpu_gpu_forward_backward_test.py]",
    "role": "tests",
    "loc": 16
  },
  {
    "id": "benchmarks\\operator_benchmark\\common\\tests\\random_sample_test.py",
    "summary": "No description | classes: AddBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark common tests random_sample_test.py]",
    "role": "tests",
    "loc": 24
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\add_test.py",
    "summary": "No description | classes: AddBenchmark, AddmmBenchmark, AddrBenchmark, AddbmmBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt add_test.py]",
    "role": "benchmarks",
    "loc": 107
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\ao_sparsifier_test.py",
    "summary": "No description | classes: WeightNormSparsifierBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt ao_sparsifier_test.py]",
    "role": "benchmarks",
    "loc": 41
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\arange_test.py",
    "summary": "No description | classes: ArangeBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt arange_test.py]",
    "role": "benchmarks",
    "loc": 35
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\as_strided_test.py",
    "summary": "No description | classes: As_stridedBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt as_strided_test.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\batchnorm_test.py",
    "summary": "No description | classes: BatchNormBenchmark, BatchNorm1dBenchmark | functions: cudnn_benchmark_configs | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt batchnorm_test.py]",
    "role": "benchmarks",
    "loc": 107
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\binary_inplace_test.py",
    "summary": "No description | classes: InpBinaryOpBenchmark | functions: add_, sub_, div_, mul_, copy_ | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt binary_inplace_test.py]",
    "role": "benchmarks",
    "loc": 100
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\binary_test.py",
    "summary": "No description | classes: BinaryOpBcastBenchmark, BinaryOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt binary_test.py]",
    "role": "benchmarks",
    "loc": 155
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\bmm_test.py",
    "summary": "No description | classes: BatchedBinaryOpBenchmark, BatchedTernaryOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt bmm_test.py]",
    "role": "benchmarks",
    "loc": 65
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\cat_test.py",
    "summary": "No description | classes: CatBenchmark | imports: random, operator_benchmark, torch | [benchmarks operator_benchmark pt cat_test.py]",
    "role": "benchmarks",
    "loc": 139
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\channel_shuffle_test.py",
    "summary": "No description | classes: ChannelSHuffleBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt channel_shuffle_test.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\chunk_test.py",
    "summary": "No description | classes: ChunkBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt chunk_test.py]",
    "role": "benchmarks",
    "loc": 27
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\clip_ranges_test.py",
    "summary": "No description | classes: ClipRangesBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt clip_ranges_test.py]",
    "role": "benchmarks",
    "loc": 41
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\configs.py",
    "summary": "No description | functions: remove_cuda | imports: operator_benchmark | [benchmarks operator_benchmark pt configs.py]",
    "role": "benchmarks",
    "loc": 145
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\conv_test.py",
    "summary": "No description | classes: Conv1dBenchmark, ConvTranspose1dBenchmark, Conv2dBenchmark, ConvTranspose2dBenchmark, Conv2dPointwiseBenchmark, Conv3dBenchmark | imports: pt, operator_benchmark, torch | [benchmarks operator_benchmark pt conv_test.py]",
    "role": "benchmarks",
    "loc": 98
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\diag_test.py",
    "summary": "No description | classes: DiagBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt diag_test.py]",
    "role": "benchmarks",
    "loc": 37
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\embeddingbag_test.py",
    "summary": "No description | classes: EmbeddingBagBenchmark, EmbeddingBenchmark | imports: numpy, pt, operator_benchmark, torch | [benchmarks operator_benchmark pt embeddingbag_test.py]",
    "role": "benchmarks",
    "loc": 59
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\fill_test.py",
    "summary": "No description | classes: Fill_Benchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt fill_test.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\gather_test.py",
    "summary": "No description | classes: GatherBenchmark | imports: numpy, operator_benchmark, torch | [benchmarks operator_benchmark pt gather_test.py]",
    "role": "benchmarks",
    "loc": 35
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\gelu_test.py",
    "summary": "No description | classes: GeluBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt gelu_test.py]",
    "role": "benchmarks",
    "loc": 16
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\groupnorm_test.py",
    "summary": "No description | classes: GroupNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt groupnorm_test.py]",
    "role": "benchmarks",
    "loc": 27
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\hardsigmoid_test.py",
    "summary": "No description | classes: HardsigmoidBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt hardsigmoid_test.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\hardswish_test.py",
    "summary": "No description | classes: HardswishBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt hardswish_test.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\index_add__test.py",
    "summary": "No description | classes: IndexAddBenchmark | imports: numpy, operator_benchmark, torch | [benchmarks operator_benchmark pt index_add__test.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\index_select_test.py",
    "summary": "No description | classes: IndexSelectBenchmark | imports: numpy, operator_benchmark, torch | [benchmarks operator_benchmark pt index_select_test.py]",
    "role": "benchmarks",
    "loc": 47
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\instancenorm_test.py",
    "summary": "No description | classes: InstanceNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt instancenorm_test.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\interpolate_test.py",
    "summary": "No description | classes: InterpolateBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt interpolate_test.py]",
    "role": "benchmarks",
    "loc": 133
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\layernorm_test.py",
    "summary": "No description | classes: LayerNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt layernorm_test.py]",
    "role": "benchmarks",
    "loc": 27
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\linear_prepack_fp16_test.py",
    "summary": "No description | classes: LinearPrepackFP16Benchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt linear_prepack_fp16_test.py]",
    "role": "benchmarks",
    "loc": 34
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\linear_test.py",
    "summary": "No description | classes: LinearBenchmark | imports: pt, operator_benchmark, torch | [benchmarks operator_benchmark pt linear_test.py]",
    "role": "benchmarks",
    "loc": 17
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\linear_unpack_fp16_test.py",
    "summary": "No description | classes: LinearUnpackFP16Benchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt linear_unpack_fp16_test.py]",
    "role": "benchmarks",
    "loc": 36
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\matmul_test.py",
    "summary": "No description | classes: MatMulBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt matmul_test.py]",
    "role": "benchmarks",
    "loc": 40
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\matrix_mult_test.py",
    "summary": "No description | classes: BatchMatrixMultBenchmark, BatchElementWiseBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt matrix_mult_test.py]",
    "role": "benchmarks",
    "loc": 99
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\mm_test.py",
    "summary": "No description | classes: MmOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt mm_test.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\nan_to_num_test.py",
    "summary": "No description | classes: ReplaceNaNBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt nan_to_num_test.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\pool_test.py",
    "summary": "No description | classes: Pool1dBenchmark, Pool2dBenchmark, Pool3dBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt pool_test.py]",
    "role": "benchmarks",
    "loc": 132
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qactivation_test.py",
    "summary": "Base class for all the activations. | classes: QActivationBenchmarkBase, QActivationBenchmark, QActivationScaleZeroPointBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qactivation_test.py]",
    "role": "benchmarks",
    "loc": 88
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qarithmetic_test.py",
    "summary": "No description | classes: _QFunctionalBinaryArithmeticBenchmarkBase, QFunctionalBenchmark, QFunctionalScalarBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qarithmetic_test.py]",
    "role": "benchmarks",
    "loc": 65
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qatembedding_ops_test.py",
    "summary": "No description | classes: QATEmbeddingBagBenchmark, QATEmbeddingBenchmark | imports: numpy, pt, operator_benchmark, torch | [benchmarks operator_benchmark pt qatembedding_ops_test.py]",
    "role": "benchmarks",
    "loc": 78
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qbatchnorm_test.py",
    "summary": "No description | classes: QBatchNormBenchmark, QBatchNorm1dBenchmark, QBatchNorm2dBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qbatchnorm_test.py]",
    "role": "benchmarks",
    "loc": 79
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qcat_test.py",
    "summary": "No description | classes: QCatBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qcat_test.py]",
    "role": "benchmarks",
    "loc": 52
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qcomparators_test.py",
    "summary": "No description | classes: QComparatorBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qcomparators_test.py]",
    "role": "benchmarks",
    "loc": 58
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qconv_test.py",
    "summary": "No description | classes: QConv1dBenchmark, QConv2dBenchmark | imports: pt, operator_benchmark, torch | [benchmarks operator_benchmark pt qconv_test.py]",
    "role": "benchmarks",
    "loc": 59
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qembeddingbag_test.py",
    "summary": "No description | classes: QEmbeddingBagBenchmark | imports: numpy, pt, operator_benchmark, torch | [benchmarks operator_benchmark pt qembeddingbag_test.py]",
    "role": "benchmarks",
    "loc": 41
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qembedding_bag_lookups_test.py",
    "summary": "No description | classes: EmbedddingBag4BitRowwiseOffsetsTest, EmbedddingBagByteRowwiseOffsetsTest | functions: get_pruned_weights_and_mapping | imports: numpy, operator_benchmark, torch | [benchmarks operator_benchmark pt qembedding_bag_lookups_test.py]",
    "role": "benchmarks",
    "loc": 275
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qembedding_pack_test.py",
    "summary": "No description | classes: EmbeddingBagFloatToFusedBase, EmbeddingBagThreeDimFloatToFusedBase, EmbeddingBagFusedToFloatBase, EmbeddingBagThreeDimFusedToFloatBase | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qembedding_pack_test.py]",
    "role": "benchmarks",
    "loc": 89
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qgroupnorm_test.py",
    "summary": "No description | classes: QGroupNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qgroupnorm_test.py]",
    "role": "benchmarks",
    "loc": 51
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qinstancenorm_test.py",
    "summary": "No description | classes: QInstanceNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qinstancenorm_test.py]",
    "role": "benchmarks",
    "loc": 39
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qinterpolate_test.py",
    "summary": "No description | classes: QInterpolateBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qinterpolate_test.py]",
    "role": "benchmarks",
    "loc": 52
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qlayernorm_test.py",
    "summary": "No description | classes: QLayerNormBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qlayernorm_test.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qlinear_test.py",
    "summary": "No description | classes: _QLinearBenchmarkBase, QLinearBenchmark, QDynamicLinearBenchmark | imports: pt, operator_benchmark, torch | [benchmarks operator_benchmark pt qlinear_test.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qobserver_test.py",
    "summary": "No description | classes: QObserverBenchmark, QObserverBenchmarkCalculateQparams | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qobserver_test.py]",
    "role": "benchmarks",
    "loc": 118
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qpool_test.py",
    "summary": "No description | classes: _QPool2dBenchmarkBase, QMaxPool2dBenchmark, QAvgPool2dBenchmark, QAdaptiveAvgPool2dBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qpool_test.py]",
    "role": "benchmarks",
    "loc": 106
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qrnn_test.py",
    "summary": "No description | classes: LSTMBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qrnn_test.py]",
    "role": "benchmarks",
    "loc": 58
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qtensor_method_test.py",
    "summary": "No description | classes: _QMethodBenchmarkBase, QMethodTensorInputCopyBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qtensor_method_test.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\quantization_test.py",
    "summary": "Benchmarks both quantization and dequantization. | classes: QuantizePerTensorBenchmark, QuantizePerChannelBenchmark, FakeQuantizeBenchmark, FakeQuantizePerTensorBaseOpBenchmark, FakeQuantizePerChannelOpBenchmark | functions: fakeQuantizePerTensorLearnableKernel, fakeQuantizePerTensorOriginalKernel, ",
    "role": "benchmarks",
    "loc": 289
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\qunary_test.py",
    "summary": "No description | classes: QUnaryOpBenchmark, QTopkOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt qunary_test.py]",
    "role": "benchmarks",
    "loc": 84
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\remainder_test.py",
    "summary": "No description | classes: RemainderOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt remainder_test.py]",
    "role": "benchmarks",
    "loc": 48
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\softmax_test.py",
    "summary": "No description | classes: SoftmaxBenchmark, Softmax2DimsBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt softmax_test.py]",
    "role": "benchmarks",
    "loc": 83
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\split_test.py",
    "summary": "No description | classes: SplitBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt split_test.py]",
    "role": "benchmarks",
    "loc": 30
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\stack_test.py",
    "summary": "No description | classes: StackBenchmark | imports: random, operator_benchmark, torch | [benchmarks operator_benchmark pt stack_test.py]",
    "role": "benchmarks",
    "loc": 77
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\sum_test.py",
    "summary": "No description | classes: SumBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt sum_test.py]",
    "role": "benchmarks",
    "loc": 35
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\tensor_to_test.py",
    "summary": "No description | classes: FloatToHalfTensorConversionBenchmark, HalfToFloatTensorConversionBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt tensor_to_test.py]",
    "role": "benchmarks",
    "loc": 64
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\ternary_test.py",
    "summary": "No description | classes: TernaryOpBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt ternary_test.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\topk_test.py",
    "summary": "No description | classes: TopkBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt topk_test.py]",
    "role": "benchmarks",
    "loc": 33
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\unary_test.py",
    "summary": "No description | classes: UnaryOpBenchmark | functions: bernoulli_, cauchy_, digamma_, exponential_, normal_, random_ | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt unary_test.py]",
    "role": "benchmarks",
    "loc": 136
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\where_test.py",
    "summary": "No description | classes: WhereBenchmark | imports: operator_benchmark, torch | [benchmarks operator_benchmark pt where_test.py]",
    "role": "benchmarks",
    "loc": 36
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt\\__init__.py",
    "summary": "Package initializer | [benchmarks operator_benchmark pt __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt_extension\\cpp_extension_test.py",
    "summary": "No description | classes: TestConsumeOp | imports: unittest, benchmark_cpp_extension, torch | [benchmarks operator_benchmark pt_extension cpp_extension_test.py]",
    "role": "benchmarks",
    "loc": 32
  },
  {
    "id": "benchmarks\\operator_benchmark\\pt_extension\\setup.py",
    "summary": "No description | imports: setuptools, torch | [benchmarks operator_benchmark pt_extension setup.py]",
    "role": "benchmarks",
    "loc": 7
  },
  {
    "id": "benchmarks\\overrides_benchmark\\bench.py",
    "summary": "No description | functions: bench, main | imports: argparse, common, torch | [benchmarks overrides_benchmark bench.py]",
    "role": "benchmarks",
    "loc": 50
  },
  {
    "id": "benchmarks\\overrides_benchmark\\common.py",
    "summary": "No description | classes: SubTensor, WithTorchFunction, SubWithTorchFunction | imports: torch | [benchmarks overrides_benchmark common.py]",
    "role": "benchmarks",
    "loc": 22
  },
  {
    "id": "benchmarks\\overrides_benchmark\\pyspybench.py",
    "summary": "No description | imports: argparse, common, torch | [benchmarks overrides_benchmark pyspybench.py]",
    "role": "benchmarks",
    "loc": 22
  },
  {
    "id": "benchmarks\\profiler_benchmark\\profiler_bench.py",
    "summary": "No description | functions: loop_workload, parallel_workload, parallel_task, payload | imports: argparse, timeit, torch | [benchmarks profiler_benchmark profiler_bench.py]",
    "role": "benchmarks",
    "loc": 90
  },
  {
    "id": "benchmarks\\profiler_benchmark\\resnet_memory_profiler.py",
    "summary": "No description | imports: torchvision, torch | [benchmarks profiler_benchmark resnet_memory_profiler.py]",
    "role": "benchmarks",
    "loc": 22
  },
  {
    "id": "benchmarks\\record_function_benchmark\\record_function_bench.py",
    "summary": "No description | functions: prepare_lstm_jit, prepare_resnet50_jit, run_bench | imports: argparse, benchmarks, torchvision, torch | [benchmarks record_function_benchmark record_function_bench.py]",
    "role": "benchmarks",
    "loc": 95
  },
  {
    "id": "benchmarks\\serialization\\nested_annotation_str.py",
    "summary": "No description | functions: create_nested_dict_type | imports: torch | [benchmarks serialization nested_annotation_str.py]",
    "role": "benchmarks",
    "loc": 25
  },
  {
    "id": "benchmarks\\serialization\\simple_measurement.py",
    "summary": "No description | classes: Basic | imports: pyarkbench, torch | [benchmarks serialization simple_measurement.py]",
    "role": "benchmarks",
    "loc": 26
  },
  {
    "id": "benchmarks\\sparse\\spmm.py",
    "summary": "No description | functions: test_sparse_csr, test_sparse_coo, test_sparse_coo_and_csr | imports: argparse, utils, torch | [benchmarks sparse spmm.py]",
    "role": "benchmarks",
    "loc": 121
  },
  {
    "id": "benchmarks\\sparse\\spmv.py",
    "summary": "No description | functions: test_sparse_csr, test_sparse_coo, test_sparse_coo_and_csr | imports: argparse, torch, utils | [benchmarks sparse spmv.py]",
    "role": "benchmarks",
    "loc": 105
  },
  {
    "id": "benchmarks\\sparse\\triton_ops.py",
    "summary": "No description | functions: create_blocked_tensor, _test_worker, test_dense_dense_mm, test_func, test_torch_matmul, test_bsr_dense_mm | imports: torch, argparse, atexit, triton | [benchmarks sparse triton_ops.py]",
    "role": "benchmarks",
    "loc": 361
  },
  {
    "id": "benchmarks\\sparse\\utils.py",
    "summary": "No description | classes: Event | functions: gen_sparse_csr, gen_sparse_coo, gen_sparse_coo_and_csr | imports: functools, operator, random, numpy | [benchmarks sparse utils.py]",
    "role": "benchmarks",
    "loc": 42
  },
  {
    "id": "benchmarks\\sparse\\__init__.py",
    "summary": "Package initializer | [benchmarks sparse __init__.py]",
    "role": "benchmarks",
    "loc": 2
  },
  {
    "id": "benchmarks\\sparse\\dlmc\\matmul_bench.py",
    "summary": "No description | functions: scipy_matmul, matmul_backward, sparse_matmul_backward, parse_args, get_tasks, filter_ops | imports: argparse, scipy, torch, utils | [benchmarks sparse dlmc matmul_bench.py]",
    "role": "benchmarks",
    "loc": 139
  },
  {
    "id": "benchmarks\\sparse\\dlmc\\utils.py",
    "summary": "No description | functions: to_coo_scipy, sparse_grad_output, read_matrix_params, csr_to_coo, load_sparse_matrix, gen_vector | imports: scipy, torch | [benchmarks sparse dlmc utils.py]",
    "role": "benchmarks",
    "loc": 184
  },
  {
    "id": "benchmarks\\sparse\\dlmc\\__init__.py",
    "summary": "Package initializer | [benchmarks sparse dlmc __init__.py]",
    "role": "benchmarks",
    "loc": 2
  },
  {
    "id": "benchmarks\\tensorexpr\\attention.py",
    "summary": "No description | classes: BahdanauAttention | imports: torch | [benchmarks tensorexpr attention.py]",
    "role": "benchmarks",
    "loc": 69
  },
  {
    "id": "benchmarks\\tensorexpr\\benchmark.py",
    "summary": "No description | classes: Benchmark, DynamicShape | functions: cuda_pointwise_context, register_benchmark_class | imports: json, numpy, torch | [benchmarks tensorexpr benchmark.py]",
    "role": "benchmarks",
    "loc": 246
  },
  {
    "id": "benchmarks\\tensorexpr\\broadcast.py",
    "summary": "No description | classes: BroadcastMulBench, BroadcastRowBench, BroadcastMidBench, BroadcastColBench, BroadcastThreeArgs, BroadcastBench | functions: register_broadcast_ops | imports: operator, numpy, torch | [benchmarks tensorexpr broadcast.py]",
    "role": "benchmarks",
    "loc": 253
  },
  {
    "id": "benchmarks\\tensorexpr\\concat.py",
    "summary": "No description | classes: Concat2D2InputBench, ConcatGraphOptBench | imports: numpy, torch | [benchmarks tensorexpr concat.py]",
    "role": "benchmarks",
    "loc": 113
  },
  {
    "id": "benchmarks\\tensorexpr\\conv.py",
    "summary": "No description | classes: ConvImplBench, ConvBench, DepthwiseConvBench | [benchmarks tensorexpr conv.py]",
    "role": "benchmarks",
    "loc": 87
  },
  {
    "id": "benchmarks\\tensorexpr\\elementwise.py",
    "summary": "No description | classes: ElementBench, SimpleElementBench, DynamicSimpleElementBench | functions: register_element_ops | imports: operator, numpy, scipy, torch | [benchmarks tensorexpr elementwise.py]",
    "role": "benchmarks",
    "loc": 200
  },
  {
    "id": "benchmarks\\tensorexpr\\matmul.py",
    "summary": "No description | classes: MatMulBench | imports: numpy | [benchmarks tensorexpr matmul.py]",
    "role": "benchmarks",
    "loc": 53
  },
  {
    "id": "benchmarks\\tensorexpr\\microbenchmarks.py",
    "summary": "No description | classes: kernel_arena_scope | functions: gen_unary_nnc_fun, nnc_fun, compute, gen_unary_torch_fun, torch_fun, fun | imports: argparse, operator, matplotlib, numpy | [benchmarks tensorexpr microbenchmarks.py]",
    "role": "benchmarks",
    "loc": 256
  },
  {
    "id": "benchmarks\\tensorexpr\\normalization.py",
    "summary": "No description | classes: NormalizationBench, BatchNormBench, InstanceNormBench, LayerNormBench | [benchmarks tensorexpr normalization.py]",
    "role": "benchmarks",
    "loc": 62
  },
  {
    "id": "benchmarks\\tensorexpr\\pooling.py",
    "summary": "No description | classes: PoolingBench, MaxPoolBench, AvgPoolBench | [benchmarks tensorexpr pooling.py]",
    "role": "benchmarks",
    "loc": 50
  },
  {
    "id": "benchmarks\\tensorexpr\\pt_engine.py",
    "summary": "No description | classes: TorchTensorEngine | imports: torch | [benchmarks tensorexpr pt_engine.py]",
    "role": "benchmarks",
    "loc": 54
  },
  {
    "id": "benchmarks\\tensorexpr\\reduction.py",
    "summary": "No description | classes: ReduceBench, ReduceRowBench, ReduceMidBench, ReduceColBench, ReduceFullBench, Reduce2DBench | [benchmarks tensorexpr reduction.py]",
    "role": "benchmarks",
    "loc": 227
  },
  {
    "id": "benchmarks\\tensorexpr\\rnn_eltwise.py",
    "summary": "No description | classes: RNNEltwise, DynamicLSTM | imports: torch | [benchmarks tensorexpr rnn_eltwise.py]",
    "role": "benchmarks",
    "loc": 102
  },
  {
    "id": "benchmarks\\tensorexpr\\softmax.py",
    "summary": "No description | classes: SoftmaxBench | imports: scipy | [benchmarks tensorexpr softmax.py]",
    "role": "benchmarks",
    "loc": 44
  },
  {
    "id": "benchmarks\\tensorexpr\\swish.py",
    "summary": "No description | classes: SwishBench | imports: torch | [benchmarks tensorexpr swish.py]",
    "role": "benchmarks",
    "loc": 41
  },
  {
    "id": "benchmarks\\tensorexpr\\tensor_engine.py",
    "summary": "No description | functions: unsupported, wrapper, is_supported, set_engine_mode, get_engine | [benchmarks tensorexpr tensor_engine.py]",
    "role": "benchmarks",
    "loc": 34
  },
  {
    "id": "benchmarks\\tensorexpr\\__main__.py",
    "summary": "No description | functions: main, set_global_threads, run_default_configs, run_with_input_iter | imports: argparse, torch | [benchmarks tensorexpr __main__.py]",
    "role": "benchmarks",
    "loc": 301
  },
  {
    "id": "benchmarks\\transformer\\attention_bias_benchmarks.py",
    "summary": "No description | classes: ExperimentConfig, ExperimentResults, Experiment, CompositeMHA | functions: benchmark_torch_function_in_microseconds, generate_inputs, run_single_experiment, generate_experiment_configs, calculate_speedup, print_results | imports: dataclasses, functools, numpy, tabulate | [b",
    "role": "benchmarks",
    "loc": 195
  },
  {
    "id": "benchmarks\\transformer\\better_transformer_vs_mha_functional.py",
    "summary": "Tests the performance of torch.nn.MultiheadAttention's fast path (BetterTransformer) | functions: benchmark_torch_function, run, main | imports: argparse, json, random, pprint | [benchmarks transformer better_transformer_vs_mha_functional.py]",
    "role": "benchmarks",
    "loc": 186
  },
  {
    "id": "benchmarks\\transformer\\score_mod.py",
    "summary": "No description | classes: ExperimentConfig, Times, ExperimentResults, Experiment | functions: benchmark_torch_function_in_microseconds, generate_inputs, generate_jagged_inputs, offsets_to_lengths, query_key_value_clones, run_single_backend_sdpa | imports: argparse, csv, random, dataclasses | [benchm",
    "role": "benchmarks",
    "loc": 1024
  },
  {
    "id": "benchmarks\\transformer\\sdp.py",
    "summary": "No description | classes: ExperimentConfig, ExperimentResults, Experiment, CompositeMHA | functions: build_composite_mha_from_nn_mha, generate_rand_batch, benchmark_torch_function_in_microseconds, assert_close_tensors, run_single_experiment, generate_experiments | imports: argparse, random, dataclas",
    "role": "benchmarks",
    "loc": 295
  },
  {
    "id": "benchmarks\\transformer\\sdpa.py",
    "summary": "No description | classes: ExperimentConfig, ExperimentResults, Experiment | functions: benchmark_torch_function_in_microseconds, get_input, run_single_experiment, generate_experiment_configs, print_results, main | imports: dataclasses, tabulate, tqdm, torch | [benchmarks transformer sdpa.py]",
    "role": "benchmarks",
    "loc": 155
  },
  {
    "id": "caffe2\\perfkernels\\hp_emblookup_codegen.py",
    "summary": "No description | functions: unroll, compute, generic | imports: argparse | [caffe2 perfkernels hp_emblookup_codegen.py]",
    "role": "src",
    "loc": 510
  },
  {
    "id": "caffe2\\perfkernels\\sve_emblookup_codegen.py",
    "summary": "No description | functions: unroll, compute, generic, main | imports: argparse | [caffe2 perfkernels sve_emblookup_codegen.py]",
    "role": "src",
    "loc": 356
  },
  {
    "id": "docs\\cpp\\source\\conf.py",
    "summary": "No description | functions: setup | imports: textwrap | [docs cpp source conf.py]",
    "role": "docs",
    "loc": 95
  },
  {
    "id": "docs\\source\\conf.py",
    "summary": "No description | functions: linkcode_resolve, coverage_post_process, is_not_internal, process_docstring, setup, replace | imports: inspect, pkgutil, torch, torchvision | [docs source conf.py]",
    "role": "docs",
    "loc": 2979
  },
  {
    "id": "docs\\source\\scripts\\build_activation_images.py",
    "summary": "This script will generate input-out plots for all of the activation | functions: plot_function | imports: matplotlib, torch | [docs source scripts build_activation_images.py]",
    "role": "docs",
    "loc": 58
  },
  {
    "id": "docs\\source\\scripts\\build_opsets.py",
    "summary": "No description | functions: get_aten, get_prims, main | imports: torch, torchgen | [docs source scripts build_opsets.py]",
    "role": "docs",
    "loc": 51
  },
  {
    "id": "docs\\source\\scripts\\build_quantization_configs.py",
    "summary": "This script will generate default values of quantization configs. | functions: _sort_key_func | imports: torch | [docs source scripts build_quantization_configs.py]",
    "role": "docs",
    "loc": 38
  },
  {
    "id": "docs\\source\\scripts\\exportdb\\generate_example_rst.py",
    "summary": "Generates the .rst files for all the examples in db/examples/ | functions: generate_example_rst, generate_index_rst, generate_tag_rst, generate_rst | imports: inspect, torch | [docs source scripts exportdb generate_example_rst.py]",
    "role": "docs",
    "loc": 135
  },
  {
    "id": "docs\\source\\scripts\\onnx\\build_onnx_torchscript_supported_aten_op_csv_table.py",
    "summary": "This script generates a CSV table with all ATen operators | functions: _sort_key, _get_op_lists, main | imports: torch | [docs source scripts onnx build_onnx_torchscript_supported_aten_op_csv_table.py]",
    "role": "docs",
    "loc": 41
  },
  {
    "id": "functorch\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch __init__.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "functorch\\benchmarks\\chrome_trace_parser.py",
    "summary": "Get model name from a file in format {model_name}_chrome_trace_*.json | functions: get_model_name, get_total_length, main | imports: argparse, pandas, torch | [functorch benchmarks chrome_trace_parser.py]",
    "role": "benchmarks",
    "loc": 54
  },
  {
    "id": "functorch\\benchmarks\\cse.py",
    "summary": "No description | functions: profile_it, profile_function, f1, fsum, fconcat, fsum2 | imports: torch, functorch | [functorch benchmarks cse.py]",
    "role": "benchmarks",
    "loc": 77
  },
  {
    "id": "functorch\\benchmarks\\operator_authoring.py",
    "summary": "No description | functions: nnc_add, nnc_addnorm, eager_addnorm, inplace_addnorm, maybe_synced, _fn | imports: timeit, functools, numpy, pandas | [functorch benchmarks operator_authoring.py]",
    "role": "benchmarks",
    "loc": 220
  },
  {
    "id": "functorch\\benchmarks\\per_sample_grads.py",
    "summary": "No description | functions: compute_loss, functorch_per_sample_grad, opacus_per_sample_grad | imports: opacus, torchvision, torch, functorch | [functorch benchmarks per_sample_grads.py]",
    "role": "benchmarks",
    "loc": 67
  },
  {
    "id": "functorch\\benchmarks\\pointwise_scorecard.py",
    "summary": "No description | functions: rand, scalar, small, small_2d, small_broadcast, medium | imports: inspect, torch, functorch | [functorch benchmarks pointwise_scorecard.py]",
    "role": "benchmarks",
    "loc": 214
  },
  {
    "id": "functorch\\benchmarks\\process_scorecard.py",
    "summary": "No description | imports: matplotlib, pandas | [functorch benchmarks process_scorecard.py]",
    "role": "benchmarks",
    "loc": 18
  },
  {
    "id": "functorch\\compile\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch compile __init__.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "functorch\\dim\\batch_tensor.py",
    "summary": "No description | functions: _enable_layers | imports: torch | [functorch dim batch_tensor.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "functorch\\dim\\delayed_mul_tensor.py",
    "summary": "No description | classes: DelayedMulTensor | imports: torch, reference | [functorch dim delayed_mul_tensor.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "functorch\\dim\\dim.py",
    "summary": "No description | classes: LevelInfo, Dim | functions: extract_name, dims, _dim_set, convert | imports: dis, inspect, dataclasses | [functorch dim dim.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "functorch\\dim\\magic_trace.py",
    "summary": "No description | functions: magic_trace | imports: signal, subprocess | [functorch dim magic_trace.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "functorch\\dim\\op_properties.py",
    "summary": "No description | imports: torch | [functorch dim op_properties.py]",
    "role": "src",
    "loc": 301
  },
  {
    "id": "functorch\\dim\\reference.py",
    "summary": "No description | classes: isin, llist, ltuple, dim_tracker | functions: prod, _wrap_dim, _dims, _bind_dims_to_size, _tensor_levels, _match_levels | imports: torch, functorch, batch_tensor, tree_map | [functorch dim reference.py]",
    "role": "src",
    "loc": 501
  },
  {
    "id": "functorch\\dim\\tree_map.py",
    "summary": "No description | functions: tree_map | imports: functorch | [functorch dim tree_map.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "functorch\\dim\\wrap_type.py",
    "summary": "No description | functions: _py_wrap_method, impl, wrap_type, wrap_attr | imports: types, functorch | [functorch dim wrap_type.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "functorch\\dim\\__init__.py",
    "summary": "Package initializer | classes: DimensionMismatchError, DimensionBindError, _Tensor, Dim, Tensor | functions: cat, _def | imports: functorch, torch, tree_map, wrap_type | [functorch dim __init__.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "functorch\\docs\\source\\conf.py",
    "summary": "No description | functions: setup, patched_make_field, handle_item | imports: functorch, pytorch_sphinx_theme, sphinx, docutils | [functorch docs source conf.py]",
    "role": "docs",
    "loc": 146
  },
  {
    "id": "functorch\\einops\\rearrange.py",
    "summary": "Translate an `einops`-style pattern into a callable that performs the rearrange using first-class dimensions. | functions: _create_rearrange_callable, composition_to_dims, rearrange | imports: functools, torch, functorch, _parsing | [functorch einops rearrange.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "functorch\\einops\\_parsing.py",
    "summary": "Adapted from https://github.com/arogozhnikov/einops/blob/36c7bb16e57d6e57f8f3050f9e07abdf3f00469f/einops/parsing.py. | classes: AnonymousAxis, ParsedExpression | functions: parse_pattern, validate_rearrange_expressions, comma_separate | imports: keyword | [functorch einops _parsing.py]",
    "role": "src",
    "loc": 250
  },
  {
    "id": "functorch\\einops\\__init__.py",
    "summary": "Package initializer | imports: rearrange | [functorch einops __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "functorch\\examples\\compilation\\eager_fusion.py",
    "summary": "No description | functions: f, bench, bench_jax | imports: torch, functorch, jax | [functorch examples compilation eager_fusion.py]",
    "role": "examples",
    "loc": 37
  },
  {
    "id": "functorch\\examples\\compilation\\fuse_module.py",
    "summary": "No description | classes: Foo | functions: nop, run | imports: timeit, torch, functorch | [functorch examples compilation fuse_module.py]",
    "role": "examples",
    "loc": 40
  },
  {
    "id": "functorch\\examples\\compilation\\linear_train.py",
    "summary": "No description | classes: Foo | functions: bench, functional_step, jit_step, train | imports: torch, functorch | [functorch examples compilation linear_train.py]",
    "role": "examples",
    "loc": 63
  },
  {
    "id": "functorch\\examples\\compilation\\simple_function.py",
    "summary": "No description | functions: f, bench | imports: torch, functorch | [functorch examples compilation simple_function.py]",
    "role": "examples",
    "loc": 20
  },
  {
    "id": "functorch\\examples\\dp_cifar10\\cifar10_opacus.py",
    "summary": "Runs CIFAR10 training with differential privacy. | functions: save_checkpoint, accuracy, train, test, main, parse_args | imports: argparse, shutil, datetime, numpy | [functorch examples dp_cifar10 cifar10_opacus.py]",
    "role": "examples",
    "loc": 394
  },
  {
    "id": "functorch\\examples\\dp_cifar10\\cifar10_transforms.py",
    "summary": "Runs CIFAR10 training with differential privacy. | functions: save_checkpoint, accuracy, compute_norms, clip_and_accumulate_and_add_noise, train, compute_loss_and_output | imports: argparse, shutil, datetime, numpy | [functorch examples dp_cifar10 cifar10_transforms.py]",
    "role": "examples",
    "loc": 385
  },
  {
    "id": "functorch\\examples\\ensembling\\parallel_train.py",
    "summary": "No description | classes: MLPClassifier | functions: make_spirals, train_step_fn, compute_loss, step4, init_fn, step6 | imports: argparse, torch | [functorch examples ensembling parallel_train.py]",
    "role": "examples",
    "loc": 77
  },
  {
    "id": "functorch\\examples\\lennard_jones\\lennard_jones.py",
    "summary": "No description | functions: lennard_jones, lennard_jones_force, make_prediction, loss_fn | imports: torch | [functorch examples lennard_jones lennard_jones.py]",
    "role": "examples",
    "loc": 50
  },
  {
    "id": "functorch\\examples\\maml_omniglot\\maml-omniglot-higher.py",
    "summary": "This example shows how to use higher to do Model Agnostic Meta Learning (MAML) | classes: Flatten | functions: main, train, test, plot | imports: argparse, higher, matplotlib, numpy | [functorch examples maml_omniglot maml-omniglot-higher.py]",
    "role": "examples",
    "loc": 181
  },
  {
    "id": "functorch\\examples\\maml_omniglot\\maml-omniglot-ptonly.py",
    "summary": "This example shows how to use higher to do Model Agnostic Meta Learning (MAML) | classes: Flatten | functions: main, train, test, plot | imports: argparse, matplotlib, numpy, pandas | [functorch examples maml_omniglot maml-omniglot-ptonly.py]",
    "role": "examples",
    "loc": 177
  },
  {
    "id": "functorch\\examples\\maml_omniglot\\maml-omniglot-transforms.py",
    "summary": "This example shows how to use higher to do Model Agnostic Meta Learning (MAML) | functions: main, loss_for_task, compute_loss, train, test, plot | imports: argparse, functools, matplotlib, numpy | [functorch examples maml_omniglot maml-omniglot-transforms.py]",
    "role": "examples",
    "loc": 180
  },
  {
    "id": "functorch\\examples\\maml_omniglot\\support\\omniglot_loaders.py",
    "summary": "No description | classes: Omniglot, OmniglotNShot | functions: find_classes, index_classes | imports: errno, numpy, PIL, torchvision | [functorch examples maml_omniglot support omniglot_loaders.py]",
    "role": "examples",
    "loc": 246
  },
  {
    "id": "functorch\\examples\\maml_regression\\evjang.py",
    "summary": "No description | functions: net, sample_tasks, get_batch, get_loss_for_task | imports: matplotlib, numpy, torch | [functorch examples maml_regression evjang.py]",
    "role": "examples",
    "loc": 91
  },
  {
    "id": "functorch\\examples\\maml_regression\\evjang_transforms.py",
    "summary": "No description | functions: net, mse_loss, sample_tasks, get_batch, get_loss_for_task, inner_loss | imports: matplotlib, numpy, torch | [functorch examples maml_regression evjang_transforms.py]",
    "role": "examples",
    "loc": 93
  },
  {
    "id": "functorch\\examples\\maml_regression\\evjang_transforms_module.py",
    "summary": "No description | classes: ThreeLayerNet | functions: mse_loss, sample_tasks, get_batch, get_loss_for_task, inner_loss | imports: matplotlib, numpy, torch, functorch | [functorch examples maml_regression evjang_transforms_module.py]",
    "role": "examples",
    "loc": 91
  },
  {
    "id": "functorch\\experimental\\control_flow.py",
    "summary": "No description | imports: torch | [functorch experimental control_flow.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "functorch\\experimental\\ops.py",
    "summary": "No description | imports: torch | [functorch experimental ops.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "functorch\\experimental\\__init__.py",
    "summary": "Package initializer | imports: functorch, torch | [functorch experimental __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "functorch\\notebooks\\_src\\plot_ensembling.py",
    "summary": "Model ensembling | classes: SimpleCNN | imports: torch, functorch | [functorch notebooks _src plot_ensembling.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "functorch\\notebooks\\_src\\plot_jacobians_and_hessians.py",
    "summary": "Jacobians, hessians, and more | functions: predict, compute_jac, predict_with_output_summed | imports: functools, torch, functorch | [functorch notebooks _src plot_jacobians_and_hessians.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "functorch\\notebooks\\_src\\plot_per_sample_gradients.py",
    "summary": "Per-sample-gradients | classes: SimpleCNN | functions: loss_fn, compute_grad, compute_sample_grads, compute_loss | imports: torch, functorch | [functorch notebooks _src plot_per_sample_gradients.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "functorch\\op_analysis\\gen_data.py",
    "summary": "No description | functions: get_ops_for_key, gen_data, annotate_ops, name_check, full_name_check, remove_suffix | imports: csv, yaml, torch | [functorch op_analysis gen_data.py]",
    "role": "src",
    "loc": 153
  },
  {
    "id": "functorch\\_src\\__init__.py",
    "summary": "Package initializer | [functorch _src __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "functorch\\_src\\aot_autograd\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch _src aot_autograd __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "functorch\\_src\\eager_transforms\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch _src eager_transforms __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "functorch\\_src\\make_functional\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch _src make_functional __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "functorch\\_src\\vmap\\__init__.py",
    "summary": "Package initializer | imports: torch | [functorch _src vmap __init__.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "mypy_plugins\\check_mypy_version.py",
    "summary": "No description | functions: get_correct_mypy_version, plugin | imports: mypy | [mypy_plugins check_mypy_version.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "mypy_plugins\\sympy_mypy_plugin.py",
    "summary": "No description | classes: SympyPlugin | functions: add_assumptions, plugin | imports: mypy | [mypy_plugins sympy_mypy_plugin.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "scripts\\diagnose_protobuf.py",
    "summary": "Diagnoses the current protobuf situation. | imports: subprocess, google | [scripts diagnose_protobuf.py]",
    "role": "scripts",
    "loc": 74
  },
  {
    "id": "scripts\\get_python_cmake_flags.py",
    "summary": "No description | imports: sysconfig | [scripts get_python_cmake_flags.py]",
    "role": "scripts",
    "loc": 6
  },
  {
    "id": "scripts\\analysis\\format_test_csv.py",
    "summary": "This script takes a pytest CSV file produced by pytest --csv foo.csv | imports: argparse, csv, subprocess | [scripts analysis format_test_csv.py]",
    "role": "scripts",
    "loc": 41
  },
  {
    "id": "scripts\\compile_tests\\common.py",
    "summary": "No description | functions: open_test_results, get_testcases, find, skipped_test, condition, passed_test | imports: functools, lxml, xml | [scripts compile_tests common.py]",
    "role": "scripts",
    "loc": 99
  },
  {
    "id": "scripts\\compile_tests\\download_reports.py",
    "summary": "No description | functions: download_reports, subdir_path, download_report, parse_workflow_jobs | imports: json, pprint, subprocess, requests | [scripts compile_tests download_reports.py]",
    "role": "scripts",
    "loc": 94
  },
  {
    "id": "scripts\\compile_tests\\failures_histogram.py",
    "summary": "No description | functions: skip_reason, skip_reason_normalized, get_failures, repro, all_tests, failures_histogram | imports: argparse, common, passrate | [scripts compile_tests failures_histogram.py]",
    "role": "scripts",
    "loc": 129
  },
  {
    "id": "scripts\\compile_tests\\passrate.py",
    "summary": "No description | functions: testcases_by_time, should_exclude, compute_pass_rate | imports: argparse, common, download_reports | [scripts compile_tests passrate.py]",
    "role": "scripts",
    "loc": 72
  },
  {
    "id": "scripts\\compile_tests\\update_failures.py",
    "summary": "No description | functions: patch_file, format, remove_file, add_file, get_intersection_and_outside, build_dict | imports: argparse, subprocess, common, download_reports | [scripts compile_tests update_failures.py]",
    "role": "scripts",
    "loc": 190
  },
  {
    "id": "scripts\\export\\update_schema.py",
    "summary": "No description | imports: argparse, yaml, torch | [scripts export update_schema.py]",
    "role": "scripts",
    "loc": 75
  },
  {
    "id": "scripts\\jit\\log_extract.py",
    "summary": "No description | functions: test_runners, run | imports: argparse, functools, traceback, torch | [scripts jit log_extract.py]",
    "role": "scripts",
    "loc": 132
  },
  {
    "id": "scripts\\release_notes\\apply_categories.py",
    "summary": "No description | imports: csv, commitlist | [scripts release_notes apply_categories.py]",
    "role": "scripts",
    "loc": 19
  },
  {
    "id": "scripts\\release_notes\\categorize.py",
    "summary": "No description | classes: Categorizer | functions: main | imports: argparse, textwrap, common, classifier | [scripts release_notes categorize.py]",
    "role": "scripts",
    "loc": 179
  },
  {
    "id": "scripts\\release_notes\\classifier.py",
    "summary": "No description | classes: CommitClassifierInputs, CategoryConfig, CommitClassifier | functions: truncate_file, build_file_set, get_train_val_data, get_author_map, get_file_map, get_title_files_author_categories_zip_list | imports: argparse, pickle, random, dataclasses | [scripts release_notes classi",
    "role": "scripts",
    "loc": 339
  },
  {
    "id": "scripts\\release_notes\\commitlist.py",
    "summary": "No description | classes: Commit, CommitList | functions: create_new, update_existing, rerun_with_new_filters, get_hash_or_pr_url, to_markdown, cleanup_title | imports: argparse, csv, dataclasses, pprint | [scripts release_notes commitlist.py]",
    "role": "scripts",
    "loc": 490
  },
  {
    "id": "scripts\\release_notes\\common.py",
    "summary": "No description | classes: CategoryGroup, _CommitDataCache | functions: dict_to_features, features_to_dict, run, commit_body, commit_title, commit_files_changed | imports: json, locale, subprocess, dataclasses | [scripts release_notes common.py]",
    "role": "scripts",
    "loc": 274
  },
  {
    "id": "scripts\\release_notes\\namespace_check.py",
    "summary": "No description | functions: get_content, namespace_filter, run, main | imports: argparse, json, torch | [scripts release_notes namespace_check.py]",
    "role": "scripts",
    "loc": 103
  },
  {
    "id": "scripts\\release_notes\\test_release_notes.py",
    "summary": "No description | classes: TestCommitList | imports: tempfile, unittest, commitlist | [scripts release_notes test_release_notes.py]",
    "role": "scripts",
    "loc": 45
  },
  {
    "id": "test\\conftest.py",
    "summary": "No description | classes: _NodeReporterReruns, LogXMLReruns, StepcurrentPlugin | functions: pytest_addoption, pytest_configure, pytest_unconfigure, pytest_terminal_summary, pytest_pycollect_makemodule, pytest_report_teststatus | imports: copy, functools, json, xml | [test conftest.py]",
    "role": "src",
    "loc": 271
  },
  {
    "id": "test\\create_dummy_torchscript_model.py",
    "summary": "No description | classes: NeuralNetwork | imports: torch | [test create_dummy_torchscript_model.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\delete.py",
    "summary": "No description | [test delete.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\hi.py",
    "summary": "No description | [test hi.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\linear.py",
    "summary": "No description | classes: LinearMod | imports: torch | [test linear.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\load_torchscript_model.py",
    "summary": "No description | imports: torch | [test load_torchscript_model.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\mkldnn_verbose.py",
    "summary": "No description | classes: Module | functions: run_model | imports: argparse, torch | [test mkldnn_verbose.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "test\\mkl_verbose.py",
    "summary": "No description | functions: run_model | imports: argparse, torch | [test mkl_verbose.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "test\\pytest_shard_custom.py",
    "summary": "Custom pytest shard plugin | classes: PytestShardPlugin | functions: pytest_addoptions | imports: hashlib, _pytest | [test pytest_shard_custom.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "test\\run_test.py",
    "summary": "No description | classes: TestChoices, TestFailure, TestBatch | functions: maybe_set_hip_visible_devies, strtobool, print_to_stderr, get_executable_command, run_test, install_cpp_extensions | imports: argparse, copy, glob, json | [test run_test.py]",
    "role": "src",
    "loc": 1886
  },
  {
    "id": "test\\simulate_nccl_errors.py",
    "summary": "No description | imports: argparse, torch | [test simulate_nccl_errors.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "test\\test_accelerator.py",
    "summary": "No description | classes: TestAccelerator | imports: unittest, torch | [test test_accelerator.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "test\\test_ao_sparsity.py",
    "summary": "No description | imports: ao, torch | [test test_ao_sparsity.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "test\\test_autocast.py",
    "summary": "No description | classes: TestAutocastCPU, CustomLinear, WeightDTypeCastCounterMode, TestAutocastGPU, TestAutocastMPS, TestTorchAutocast | imports: unittest, torch | [test test_autocast.py]",
    "role": "src",
    "loc": 314
  },
  {
    "id": "test\\test_autograd.py",
    "summary": "No description | classes: Foo, Foo2, MyFunction, Func, SimulateBackwardError, MySquare | functions: graph_desc, index_perm_variable, bernoulli_scalar | imports: functools, gc, io, operator | [test test_autograd.py]",
    "role": "src",
    "loc": 10713
  },
  {
    "id": "test\\test_autograd_fallback.py",
    "summary": "No description | classes: NumpySin, NumpySin_, TestAutogradFallback | functions: autograd_fallback_mode | imports: numpy, torch | [test test_autograd_fallback.py]",
    "role": "src",
    "loc": 363
  },
  {
    "id": "test\\test_autoload.py",
    "summary": "No description | classes: TestDeviceBackendAutoload | imports: torch | [test test_autoload.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\test_binary_ufuncs.py",
    "summary": "No description | classes: TestBinaryUfuncs, UnknownType | functions: generate_not_implemented_tests, create_test_func, test | imports: operator, random, functools, numbers | [test test_binary_ufuncs.py]",
    "role": "src",
    "loc": 3713
  },
  {
    "id": "test\\test_bundled_images.py",
    "summary": "No description | classes: SingleTensorModel, TestBundledImages | functions: model_size, save_and_load, bundle_jpeg_image, get_tensor_from_raw_BGR | imports: io, cv2, torch | [test test_bundled_images.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "test\\test_bundled_inputs.py",
    "summary": "No description | classes: SingleTensorModel, StringAndIntModel, MultipleMethodModel, MyModel, TestBundledInputs | functions: model_size, save_and_load | imports: io, textwrap, torch | [test test_bundled_inputs.py]",
    "role": "src",
    "loc": 335
  },
  {
    "id": "test\\test_ci_sanity_check_fail.py",
    "summary": "No description | classes: TestCISanityCheck | imports: torch | [test test_ci_sanity_check_fail.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "test\\test_comparison_utils.py",
    "summary": "No description | classes: TestComparisonUtils | imports: unittest, torch | [test test_comparison_utils.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\test_compile_benchmark_util.py",
    "summary": "No description | classes: ToyModel, TestCompileBenchmarkUtil | imports: unittest, torch, tabulate | [test test_compile_benchmark_util.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "test\\test_complex.py",
    "summary": "No description | classes: TestComplexTensor | imports: torch | [test test_complex.py]",
    "role": "src",
    "loc": 388
  },
  {
    "id": "test\\test_content_store.py",
    "summary": "No description | classes: TestContentStore | imports: torch | [test test_content_store.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "test\\test_cpp_api_parity.py",
    "summary": "No description | classes: TestCppApiParity | imports: cpp_api_parity, torch | [test test_cpp_api_parity.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "test\\test_cpp_extensions_aot.py",
    "summary": "Tests ahead-of-time cpp extensions | classes: TestCppExtensionAOT, TestPybindTypeCasters, TestMAIATensor, TestRNGExtension, TestTorchLibrary | imports: subprocess, unittest, torch, pytest | [test test_cpp_extensions_aot.py]",
    "role": "src",
    "loc": 332
  },
  {
    "id": "test\\test_cpp_extensions_jit.py",
    "summary": "No description | classes: M, MyFn, TestCppExtensionJIT | imports: glob, locale, shutil, subprocess | [test test_cpp_extensions_jit.py]",
    "role": "src",
    "loc": 864
  },
  {
    "id": "test\\test_cpp_extensions_mtia_backend.py",
    "summary": "Tests MTIA backend with C++ extensions. | classes: TestCppExtensionMTIABackend | imports: tempfile, unittest, torch | [test test_cpp_extensions_mtia_backend.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "test\\test_cpp_extensions_open_device_registration.py",
    "summary": "No description | classes: _OpenRegMod, CustomFloatStorage, TestCppExtensionOpenRgistration | functions: generate_faked_module, generate_faked_module_methods, device_count, get_rng_state, set_rng_state, is_available | imports: _codecs, io, tempfile, unittest | [test test_cpp_extensions_open_device_re",
    "role": "src",
    "loc": 466
  },
  {
    "id": "test\\test_cpp_extensions_stream_and_event.py",
    "summary": "Tests Stream and Event with C++ extensions. | classes: TestCppExtensionStreamAndEvent | imports: tempfile, unittest, torch | [test test_cpp_extensions_stream_and_event.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "test\\test_cuda.py",
    "summary": "No description | classes: MultiplyInStream, StreamModel, MLP1, MLP2, ParameterlessModule, MyFunction | functions: get_cudagraph_segments, get_all_cudagraph_segments, cudagraphify, int8_cuda, live_blocks, tensor_metadata | imports: ctypes, gc, json, pickle | [test test_cuda.py]",
    "role": "src",
    "loc": 4460
  },
  {
    "id": "test\\test_cuda_expandable_segments.py",
    "summary": "No description | imports: test_cuda, torch, tools | [test test_cuda_expandable_segments.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "test\\test_cuda_multigpu.py",
    "summary": "No description | classes: TestCudaMultiGPU, TestNamedTupleInput_1, TestCudaComm | imports: ctypes, gc, io, queue | [test test_cuda_multigpu.py]",
    "role": "src",
    "loc": 1417
  },
  {
    "id": "test\\test_cuda_nvml_based_avail.py",
    "summary": "No description | classes: TestExtendedCUDAIsAvail, TestVisibleDeviceParses | imports: multiprocessing, unittest, torch | [test test_cuda_nvml_based_avail.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "test\\test_cuda_primary_ctx.py",
    "summary": "No description | classes: TestCudaPrimaryCtx | imports: unittest, torch | [test test_cuda_primary_ctx.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "test\\test_cuda_sanitizer.py",
    "summary": "No description | classes: TestArgumentHandler, TestEventHandler, MyT, TestMessages | functions: tensor_id, stream_id, event_id | imports: textwrap, traceback, torch | [test test_cuda_sanitizer.py]",
    "role": "src",
    "loc": 404
  },
  {
    "id": "test\\test_cuda_trace.py",
    "summary": "No description | classes: TestCudaTrace | imports: unittest, torch | [test test_cuda_trace.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "test\\test_custom_ops.py",
    "summary": "No description | classes: CustomOpTestCaseBase, Foo, TestCustomOpTesting, MySequence, TestCustomOp, Op | functions: requires_compile, op_with_incorrect_schema | imports: subprocess, unittest, numpy, torch | [test test_custom_ops.py]",
    "role": "src",
    "loc": 3308
  },
  {
    "id": "test\\test_dataloader.py",
    "summary": "No description | classes: CustomDataset, TestDatasetRandomSplit, CUDACountingDataset, CountingDataset, CountingIterableDataset, TestTensorDataset | functions: _clone_collate, set_faulthander_if_available, print_traces_of_all_threads, _test_timeout, _test_timeout_pin_memory, _test_large_sampler_indic",
    "role": "src",
    "loc": 2864
  },
  {
    "id": "test\\test_datapipe.py",
    "summary": "No description | classes: TestDataChunk, _FakeFD, TestStreamWrapper, TestIterableDataPipeBasic, TestCaptureDataFrame, TestDataFramesPipes | functions: create_temp_dir_and_files, reset_after_n_next_calls, odd_or_even, _fake_fn, _fake_add, _fake_filter_fn | imports: copy, importlib, pickle, pydoc | [t",
    "role": "src",
    "loc": 3251
  },
  {
    "id": "test\\test_decomp.py",
    "summary": "No description | classes: DecompCrossRefMode, TestDecomp, DecompOneOffTests, HasDecompTest | functions: overload_to_aten_name, diff_arg, is_differentiable_arg, _autograd_grad, _as_tuple, ref_vjp_no_create | imports: functools, unittest, torch | [test test_decomp.py]",
    "role": "src",
    "loc": 1032
  },
  {
    "id": "test\\test_deploy.py",
    "summary": "Tests the freeze.py script | classes: TestFreezer | imports: textwrap, types, torch | [test test_deploy.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\test_determination.py",
    "summary": "No description | classes: DummyOptions, DeterminationTest | imports: run_test, torch | [test test_determination.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "test\\test_dispatch.py",
    "summary": "No description | classes: TestDispatch, TestPythonDispatcher | functions: extract_dispatch_table_with_keys | imports: torch | [test test_dispatch.py]",
    "role": "src",
    "loc": 912
  },
  {
    "id": "test\\test_dlpack.py",
    "summary": "No description | classes: TensorDLPackWrapper, DLPackTensor, TestTorchDlPack | imports: torch | [test test_dlpack.py]",
    "role": "src",
    "loc": 221
  },
  {
    "id": "test\\test_dynamic_shapes.py",
    "summary": "No description | classes: FakeSymbolicTensor, CustomModule, TestSymInt, TestPySymInt, TestSymNumberMagicMethods, TestFloorDiv | functions: register_meta, decorator, add_func, binary_meta, cat_meta, narrow_copy_symint_meta | imports: copy, operator, unittest, numpy | [test test_dynamic_shapes.py]",
    "role": "src",
    "loc": 2436
  },
  {
    "id": "test\\test_expanded_weights.py",
    "summary": "No description | classes: TestContext, TestExpandedWeightHelperFunction, TestExpandedWeightFunctional, TestModule, RNNWrapper, CustomModule | functions: filter_supported_tests, run_op, make_expanded_weight, expanded_weight_or_clone, supported_inputs, filter_fn | imports: unittest, dataclasses, funct",
    "role": "src",
    "loc": 1016
  },
  {
    "id": "test\\test_extension_utils.py",
    "summary": "No description | classes: DummyPrivateUse1Module, TestExtensionUtils | imports: torch | [test test_extension_utils.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\test_fake_tensor.py",
    "summary": "No description | classes: ModuleNew, _TestPattern, MyNumpyModel, FakeTensorTest, FakeTensorConstHandling, FakeTensorOpInfoTest | functions: expectedFailurePropagateRealTensors, make_propagate_real_tensors_cls, contains_type | imports: copy, dataclasses, inspect, pickle | [test test_fake_tensor.py]",
    "role": "src",
    "loc": 1748
  },
  {
    "id": "test\\test_file_check.py",
    "summary": "No description | classes: TestFileCheck | imports: torch | [test test_file_check.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "test\\test_flop_counter.py",
    "summary": "No description | classes: _CustomOp, onlyConvs, Foo, Mod, Mod2, TestFlopCounter | functions: FlopCounterMode, get_total_flops, T | imports: functools, unittest, torch, torchvision | [test test_flop_counter.py]",
    "role": "src",
    "loc": 712
  },
  {
    "id": "test\\test_foreach.py",
    "summary": "No description | classes: RegularFuncWrapper, ForeachFuncWrapper, InplaceForeachVersionBumpCheck, Foo, TestForeach | functions: get_transform_func, transform, check_autodiff_sample | imports: random, unittest, weakref, numbers | [test test_foreach.py]",
    "role": "src",
    "loc": 1390
  },
  {
    "id": "test\\test_functionalization.py",
    "summary": "No description | classes: TestFunctionalization, TestCrossRefFunctionalization | functions: are_aliased, _functionalize, to_fun, wrapped | imports: unittest, torch | [test test_functionalization.py]",
    "role": "src",
    "loc": 1869
  },
  {
    "id": "test\\test_functionalization_of_rng_ops.py",
    "summary": "No description | classes: Custom, CustomOp1, CustomOp2, TestFunctionalizationRngOps, NegativeTest | functions: count_philox_rand | imports: functools, unittest, torch, functorch | [test test_functionalization_of_rng_ops.py]",
    "role": "src",
    "loc": 260
  },
  {
    "id": "test\\test_functional_autograd_benchmark.py",
    "summary": "No description | classes: TestFunctionalAutogradBenchmark | imports: subprocess, unittest, torch | [test test_functional_autograd_benchmark.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "test\\test_functional_optim.py",
    "summary": "No description | classes: MyModule, MyDummyFnOptimizer, TestFunctionalOptimParity | imports: unittest, torch | [test test_functional_optim.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "test\\test_function_schema.py",
    "summary": "No description | classes: TestFunctionSchema | imports: torch | [test test_function_schema.py]",
    "role": "src",
    "loc": 271
  },
  {
    "id": "test\\test_futures.py",
    "summary": "No description | classes: TestFuture | functions: add_one | imports: threading, torch, unittest | [test test_futures.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "test\\test_fx.py",
    "summary": "No description | classes: SimpleTest, Pair, Foo, Add, MySub, MyModule | functions: a_non_torch_leaf, fx_int, fx_int_x2, a_lifted_leaf, a_lifted_leaf2, wrapped_named_tup | imports: builtins, copy, functools, inspect | [test test_fx.py]",
    "role": "src",
    "loc": 3848
  },
  {
    "id": "test\\test_fx_experimental.py",
    "summary": "No description | classes: TestModule, MyRecommendationModule, MyModule, testModule, M, MetaTracerTestModule | functions: symbolic_trace_with_rewrite | imports: functools, numbers, operator, pickle | [test test_fx_experimental.py]",
    "role": "src",
    "loc": 1582
  },
  {
    "id": "test\\test_fx_passes.py",
    "summary": "No description | classes: TestModule, TestDeepModule, TestPartitionFunctions, MockOperatorSupport, TestFXGraphPasses, TestCase | imports: dataclasses, operator, torch | [test test_fx_passes.py]",
    "role": "src",
    "loc": 655
  },
  {
    "id": "test\\test_fx_reinplace_pass.py",
    "summary": "No description | classes: TestReinplacePass | imports: torch, functorch | [test test_fx_reinplace_pass.py]",
    "role": "src",
    "loc": 294
  },
  {
    "id": "test\\test_hop_infra.py",
    "summary": "No description | classes: TestHOPInfra | functions: do_imports | imports: importlib, pkgutil, torch | [test test_hop_infra.py]",
    "role": "src",
    "loc": 68
  },
  {
    "id": "test\\test_hub.py",
    "summary": "No description | classes: TestHub | functions: sum_of_state_dict | imports: tempfile, unittest, torch | [test test_hub.py]",
    "role": "src",
    "loc": 249
  },
  {
    "id": "test\\test_import_stats.py",
    "summary": "No description | classes: TestImportTime | imports: torch | [test test_import_stats.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "test\\test_indexing.py",
    "summary": "No description | classes: TestIndexing, NumpyTests | imports: operator, random, unittest, functools | [test test_indexing.py]",
    "role": "src",
    "loc": 1510
  },
  {
    "id": "test\\test_itt.py",
    "summary": "No description | classes: TestItt | imports: torch, unittest | [test test_itt.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "test\\test_jit.py",
    "summary": "No description | classes: FooToPickle, TestJitProfiler, M, MyModule, Foo, Madd_ | functions: canonical, LSTMCellF, doAutodiffCheck, LSTMCell, LSTMCellC, LSTMCellS | imports: torch, jit, copy, textwrap | [test test_jit.py]",
    "role": "src",
    "loc": 12459
  },
  {
    "id": "test\\test_jiterator.py",
    "summary": "No description | classes: TestPythonJiterator | functions: ref_fn | imports: torch | [test test_jiterator.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "test\\test_jit_autocast.py",
    "summary": "No description | classes: TestModule, Iface, Impl, Thing1, TestAutocast, convbn | imports: torch, unittest, test_jit, jit | [test test_jit_autocast.py]",
    "role": "src",
    "loc": 775
  },
  {
    "id": "test\\test_jit_disabled.py",
    "summary": "These tests are separate from the rest of the JIT tests because we need | classes: TestJitDisabled | functions: _jit_disabled | imports: subprocess, torch | [test test_jit_disabled.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "test\\test_jit_fuser.py",
    "summary": "No description | classes: ResLike, M, TestFuser | functions: strip_profiling_nodes, warmup_forward | imports: unittest, torch, textwrap, test_jit | [test test_jit_fuser.py]",
    "role": "src",
    "loc": 762
  },
  {
    "id": "test\\test_jit_fuser_legacy.py",
    "summary": "No description | imports: test_jit_fuser | [test test_jit_fuser_legacy.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\test_jit_fuser_te.py",
    "summary": "No description | classes: M, MyMod, TestTEFuser, TestTEFuserStatic, TestTEFuserDynamic, TestNNCOpInfoParent | functions: strip_profiling_nodes, warmup_forward, texpr_reductions_enabled, texpr_enable_strategy, inline_fusion_groups, get_name | imports: operator, unittest, torch, textwrap | [test test_",
    "role": "src",
    "loc": 2352
  },
  {
    "id": "test\\test_jit_legacy.py",
    "summary": "No description | imports: test_jit | [test test_jit_legacy.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\test_jit_llga_fuser.py",
    "summary": "No description | classes: JitLlgaTestCase, M, TestOp, TestFusionPattern, TestEnableDisableLlgaFuser, Seq | functions: separate_process, wrapper, is_avx512_supported, warmup_forward, get_eltwise_fn, _wrapper | imports: torch, unittest, functools, concurrent | [test test_jit_llga_fuser.py]",
    "role": "src",
    "loc": 691
  },
  {
    "id": "test\\test_jit_profiling.py",
    "summary": "No description | imports: test_jit | [test test_jit_profiling.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\test_jit_simple.py",
    "summary": "No description | imports: test_jit | [test test_jit_simple.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\test_jit_string.py",
    "summary": "No description | classes: TestScript | imports: test_jit, torch | [test test_jit_string.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "test\\test_kernel_launch_checks.py",
    "summary": "No description | classes: AlwaysCheckCudaLaunchTest | imports: torch | [test test_kernel_launch_checks.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "test\\test_legacy_vmap.py",
    "summary": "No description | classes: EnableVmapFallbackWarnings, TestVmapAPILegacy, TensorFactory, TestVmapBaseLegacy, Namespace, TestVmapOperatorsLegacy | functions: slice_inputs, reference_vmap, _vmap_test, should_allow_vmap_fallback_usage, allowVmapFallbackUsage, construct_v | imports: functools, types, tor",
    "role": "src",
    "loc": 2110
  },
  {
    "id": "test\\test_license.py",
    "summary": "No description | classes: TestLicense | imports: glob, io, unittest, torch | [test test_license.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "test\\test_linalg.py",
    "summary": "No description | classes: PrecisionContext, TestLinalg | functions: blaslt_supported_device, set_tunableop_defaults, tunableop_matmul, get_tunableop_validators | imports: torch, numpy, unittest, random | [test test_linalg.py]",
    "role": "src",
    "loc": 6797
  },
  {
    "id": "test\\test_logging.py",
    "summary": "No description | classes: LoggingTest | imports: torch | [test test_logging.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "test\\test_masked.py",
    "summary": "Tests for masked operations. | classes: mask_layouts, TestMasked | functions: apply_masked_reduction_along_dim, apply_masked_normalization_along_dim, _tensor_to_strided, to_strided, to_sparse_coo, to_sparse_csr | imports: torch, functools, unittest | [test test_masked.py]",
    "role": "src",
    "loc": 330
  },
  {
    "id": "test\\test_maskedtensor.py",
    "summary": "No description | classes: TestBasics, TestUnary, TestBinary, TestReductions, TestOperators | functions: _compare_mt_t, _compare_mts, _compare_forward_backward, _create_random_mask, _generate_sample_data, _fix_fn_name | imports: torch | [test test_maskedtensor.py]",
    "role": "src",
    "loc": 819
  },
  {
    "id": "test\\test_matmul_cuda.py",
    "summary": "Converts the amax value of a tensor to the fp8 scale. | classes: TestMatmulCuda, TestFP8MatmulCuda, TestMixedDtypesLinearCuda | functions: amax_to_scale, tensor_to_scale, mm_float8_emulated, addmm_float8_unwrapped, mm_float8, to_fp8_saturated | imports: json, tempfile, unittest, functools | [test te",
    "role": "src",
    "loc": 1035
  },
  {
    "id": "test\\test_meta.py",
    "summary": "No description | classes: TestMetaConverter, CheckStrides, Lit, MetaCrossRefFunctionMode, MetaCrossRefDispatchMode, TestMeta | functions: should_check_strides, assert_ref_meta_equal, test_assert, print_seen, fmt_dtypes, verbose_print | imports: torch, numpy, torchgen, copy | [test test_meta.py]",
    "role": "src",
    "loc": 1401
  },
  {
    "id": "test\\test_metal.py",
    "summary": "No description | classes: Conv2D, Conv2DRelu, Conv2DHardtanh, TestMetalRewritePass | imports: torch, io | [test test_metal.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "test\\test_mkldnn.py",
    "summary": "No description | classes: EnsureMkldnn, TestMkldnn | imports: copy, functools, unittest, torchvision | [test test_mkldnn.py]",
    "role": "src",
    "loc": 1357
  },
  {
    "id": "test\\test_mkldnn_fusion.py",
    "summary": "No description | classes: PointwisePostOp, M, TestMkldnnFusion | imports: unittest, torch, test_tensorexpr | [test test_mkldnn_fusion.py]",
    "role": "src",
    "loc": 351
  },
  {
    "id": "test\\test_mkldnn_verbose.py",
    "summary": "No description | classes: TestMKLDNNVerbose | imports: torch, subprocess | [test test_mkldnn_verbose.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\test_mkl_verbose.py",
    "summary": "No description | classes: TestMKLVerbose | imports: torch, subprocess | [test test_mkl_verbose.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\test_mobile_optimizer.py",
    "summary": "No description | classes: MyTestModule, BNTestModule, MyMobileOptimizedTagTest, MyPreserveMethodsTest, OptimizeNoForwardTest, BNTestNoForwardModule | imports: unittest, torch, torchvision | [test test_mobile_optimizer.py]",
    "role": "src",
    "loc": 479
  },
  {
    "id": "test\\test_model_exports_to_core_aten.py",
    "summary": "No description | classes: TestQuantizePT2EModels | functions: _get_ops_list | imports: copy, pytest, torch, torchvision | [test test_model_exports_to_core_aten.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\test_modules.py",
    "summary": "No description | classes: TestModule | imports: inspect, copy, tempfile, operator | [test test_modules.py]",
    "role": "src",
    "loc": 753
  },
  {
    "id": "test\\test_module_tracker.py",
    "summary": "No description | classes: Foo, Mod, MyMod, TestModuleTracker | imports: copy, torch | [test test_module_tracker.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "test\\test_monitor.py",
    "summary": "No description | classes: TestMonitor, TestMonitorTensorboard | imports: tempfile, unittest, datetime, torch | [test test_monitor.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "test\\test_mps.py",
    "summary": "No description | classes: MpsMemoryLeakCheck, TestAutocastMPS, TestCaseMPS, TestMemoryLeak, TestPixelShuffle, MPSReluTest | functions: xfailIf, wrapper, mps_ops_grad_modifier, addDecorator, mps_ops_modifier, mps_ops_error_inputs_modifier | imports: io, random, unittest, shutil | [test test_mps.py]",
    "role": "src",
    "loc": 9649
  },
  {
    "id": "test\\test_multiprocessing.py",
    "summary": "No description | classes: SubProcess, leak_checker, TestMultiprocessing | functions: _test_cuda_ipc_deadlock_actor, _test_cuda_ipc_deadlock_learner, simple_fill, simple_pool_fill, send_tensor, send_and_delete_tensors | imports: copy, gc, unittest, torch | [test test_multiprocessing.py]",
    "role": "src",
    "loc": 859
  },
  {
    "id": "test\\test_multiprocessing_spawn.py",
    "summary": "No description | classes: _TestMultiProcessing, SpawnTest, ForkTest, ParallelForkServerShouldWorkTest, ParallelForkServerPerfTest, Expensive | functions: _test_success_func, _test_success_single_arg_func, _test_exception_single_func, _test_exception_all_func, _test_terminate_signal_func, _test_termi",
    "role": "src",
    "loc": 232
  },
  {
    "id": "test\\test_namedtensor.py",
    "summary": "No description | classes: TestNamedTensor | functions: pass_name_to_python_arg_parser, flatten, parse_compressed_namedshape, parse_name, create, out_fn | imports: unittest, torch, functools, multiprocessing | [test test_namedtensor.py]",
    "role": "src",
    "loc": 1579
  },
  {
    "id": "test\\test_namedtuple_return_api.py",
    "summary": "No description | classes: TestNamedTupleAPI | imports: yaml, textwrap, torch | [test test_namedtuple_return_api.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "test\\test_native_functions.py",
    "summary": "No description | classes: FloatListWrapperModule, IntListWrapperModule, TestNativeFunctions | imports: torch | [test test_native_functions.py]",
    "role": "src",
    "loc": 160
  },
  {
    "id": "test\\test_native_mha.py",
    "summary": "No description | classes: NativeMHA, TestMHADeviceType | imports: copy, torch | [test test_native_mha.py]",
    "role": "src",
    "loc": 289
  },
  {
    "id": "test\\test_nestedtensor.py",
    "summary": "No description | classes: TestNestedTensor, TestNestedTensorDeviceType, TestNestedTensorAutograd, mha, MyFunction, CustomDispatchMode | functions: _iter_constructors, _recompiles_for_inputs, counter, random_nt_noncontiguous_pair, noncontiguous_to_padded_tensor, random_nt | imports: ast, io, random, ",
    "role": "src",
    "loc": 7308
  },
  {
    "id": "test\\test_nn.py",
    "summary": "No description | classes: MyMixin, MyModuleWithMixinBefore, MyModuleWithMixinAfter, Net, M1, M2 | functions: add_test, add, with_tf32_off, with_tf32_on, test_half, test_bfloat16 | imports: random, unittest, io, pickle | [test test_nn.py]",
    "role": "src",
    "loc": 10473
  },
  {
    "id": "test\\test_nnapi.py",
    "summary": "No description | classes: UnsqueezeModule, ReshapeModule, SliceModule, SliceModule2, CatModule, UnaryModule | functions: qpt, nhwc | imports: ctypes, unittest, torch | [test test_nnapi.py]",
    "role": "src",
    "loc": 593
  },
  {
    "id": "test\\test_numba_integration.py",
    "summary": "No description | classes: TestNumbaIntegration | imports: unittest, torch, numpy, numba | [test test_numba_integration.py]",
    "role": "src",
    "loc": 294
  },
  {
    "id": "test\\test_numpy_interop.py",
    "summary": "No description | classes: TestNumPyInterop | imports: numpy, torch | [test test_numpy_interop.py]",
    "role": "src",
    "loc": 497
  },
  {
    "id": "test\\test_openmp.py",
    "summary": "No description | classes: Network, TestOpenMP_ParallelFor | imports: unittest, torch, psutil | [test test_openmp.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "test\\test_ops.py",
    "summary": "No description | classes: TestCommon, TestCompositeCompliance, TestMathBits, _TestTagsMode, TestTags, TestSelfKwarg | functions: reduction_dtype_filter, check_inplace_view | imports: copy, inspect, unittest, functools | [test test_ops.py]",
    "role": "src",
    "loc": 2152
  },
  {
    "id": "test\\test_ops_fwd_gradients.py",
    "summary": "No description | classes: TestFwdGradients | imports: platform, functools, unittest, torch | [test test_ops_fwd_gradients.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\test_ops_gradients.py",
    "summary": "No description | classes: TestBwdGradients | imports: functools, torch | [test test_ops_gradients.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "test\\test_ops_jit.py",
    "summary": "No description | classes: TestJit | imports: functools, textwrap, torch | [test test_ops_jit.py]",
    "role": "src",
    "loc": 264
  },
  {
    "id": "test\\test_optim.py",
    "summary": "This test class validates the core optimizers and is structured as the correctness of: | classes: TestOptimRenewed | functions: rosenbrock, drosenbrock | imports: functools, tempfile, unittest, copy | [test test_optim.py]",
    "role": "src",
    "loc": 1777
  },
  {
    "id": "test\\test_out_dtype_op.py",
    "summary": "No description | classes: M, TestOutDtypeOp | imports: unittest, torch | [test test_out_dtype_op.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "test\\test_overrides.py",
    "summary": "A class with __torch_function__ and a specific diagonal representation | classes: DiagonalTensor, SubTensor, SubTensor2, SubSubTensor2, SubTensor3, SubDiagonalTensor | functions: foo, bar, baz, quux, implements_diagonal, decorator | imports: torch, numpy, inspect, functools | [test test_overrides.py",
    "role": "src",
    "loc": 1227
  },
  {
    "id": "test\\test_package.py",
    "summary": "No description | imports: package, torch | [test test_package.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\test_per_overload_api.py",
    "summary": "No description | classes: TestPerOverloadAPI | imports: copy, torch | [test test_per_overload_api.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "test\\test_prims.py",
    "summary": "No description | classes: TestPrims, TestPrimsBasic, TestRefs, TestDecomp | imports: functools, unittest, torch, scipy | [test test_prims.py]",
    "role": "src",
    "loc": 337
  },
  {
    "id": "test\\test_proxy_tensor.py",
    "summary": "No description | classes: UnwrapTensor, Foo, Emformer, TestGenericProxyTensor, TestGenericProxyTensorReal, TestGenericProxyTensorFake | functions: strip_end, show_guards, process_failures, process_failure_string, create_normalized_name, _create_new_input | imports: torch, unittest, operator, functoo",
    "role": "src",
    "loc": 1681
  },
  {
    "id": "test\\test_pruning_op.py",
    "summary": "No description | classes: PruningOpTest | imports: hypothesis, numpy, torch | [test test_pruning_op.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "test\\test_public_bindings.py",
    "summary": "No description | classes: TestPublicBindings | imports: importlib, inspect, json, pkgutil | [test test_public_bindings.py]",
    "role": "src",
    "loc": 528
  },
  {
    "id": "test\\test_python_dispatch.py",
    "summary": "No description | classes: TestDispatcherPythonBindings, TestPythonRegistration, A, ErrorA, ErrorB, B | functions: _identity | imports: tempfile, unittest, copy, torch | [test test_python_dispatch.py]",
    "role": "src",
    "loc": 1986
  },
  {
    "id": "test\\test_pytree.py",
    "summary": "No description | classes: GlobalDummyType, TestEnum, MyDict, TestGenericPytree, DummyType, Point2 | imports: inspect, subprocess, unittest, dataclasses | [test test_pytree.py]",
    "role": "src",
    "loc": 1244
  },
  {
    "id": "test\\test_quantization.py",
    "summary": "No description | imports: torch, quantization | [test test_quantization.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "test\\test_reductions.py",
    "summary": "No description | classes: TestReductions | functions: _generate_input, _rand_shape, _reduced_shape | imports: torch, numpy, random, functools | [test test_reductions.py]",
    "role": "src",
    "loc": 2922
  },
  {
    "id": "test\\test_scatter_gather_ops.py",
    "summary": "No description | classes: TestScatterGather | imports: random, torch | [test test_scatter_gather_ops.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "test\\test_schema_check.py",
    "summary": "No description | classes: IncorrectAliasTensor, SchemaInfoBindTestMode, TestSchemaCheck, TestSchemaCheckModeOpInfo | functions: secretly_aliasing, secretly_mutating, output_is_input | imports: torch, unittest | [test test_schema_check.py]",
    "role": "src",
    "loc": 396
  },
  {
    "id": "test\\test_segment_reductions.py",
    "summary": "No description | classes: TestSegmentReductions | functions: get_default_value | imports: functools, numpy, torch | [test test_segment_reductions.py]",
    "role": "src",
    "loc": 528
  },
  {
    "id": "test\\test_serialization.py",
    "summary": "No description | classes: FilelikeMock, Nested, ClassAMock, ClassBMock, UInt4Tensor, Int4Tensor | functions: up_size | imports: copy, functools, gc, gzip | [test test_serialization.py]",
    "role": "src",
    "loc": 3910
  },
  {
    "id": "test\\test_set_default_mobile_cpu_allocator.py",
    "summary": "No description | classes: TestSetDefaultMobileCPUAllocator | imports: torch | [test test_set_default_mobile_cpu_allocator.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "test\\test_shape_ops.py",
    "summary": "No description | classes: TestShapeOps | functions: _generate_input | imports: random, unittest, functools, numpy | [test test_shape_ops.py]",
    "role": "src",
    "loc": 664
  },
  {
    "id": "test\\test_show_pickle.py",
    "summary": "No description | classes: MyCoolModule, TestShowPickle | imports: io, tempfile, unittest, torch | [test test_show_pickle.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "test\\test_sort_and_select.py",
    "summary": "No description | classes: TestSortAndSelect | imports: random, numpy, torch | [test test_sort_and_select.py]",
    "role": "src",
    "loc": 1103
  },
  {
    "id": "test\\test_sparse.py",
    "summary": "No description | classes: CrossRefSparseFakeMode, TestSparseLegacyAndDeprecation, TestSparseBase, TestSparse, TestSparseOneOff, TestSparseUnaryUfuncs | functions: _op_supports_any_sparse, all_sparse_layouts, gradcheck_semantics, _sparse_to_dense | imports: torch, functools, operator, random | [test ",
    "role": "src",
    "loc": 4367
  },
  {
    "id": "test\\test_sparse_csr.py",
    "summary": "No description | classes: TestSparseCSRSampler, TestSparseCompressed, FakeBscMatrix, TestSparseCSR, skipped_cls, TestSparseCompressedTritonKernels | functions: _check_cusparse_triangular_solve_available, _check_cusparse_spgemm_available, _check_cusparse_sddmm_available, _test_addmm_addmv, convert_la",
    "role": "src",
    "loc": 3387
  },
  {
    "id": "test\\test_sparse_semi_structured.py",
    "summary": "No description | classes: Model, SparseSemiStructuredTensorCompileTest, TestSparseSemiStructured, TestSparseSemiStructuredTraining, TestSparseSemiStructuredCUTLASS, TestSparseSemiStructuredCUSPARSELT | functions: sparse24_largest_mask_2d, sparsify24_dense, rand_sparse_semi_structured_mask, rand_spar",
    "role": "src",
    "loc": 987
  },
  {
    "id": "test\\test_spectral_ops.py",
    "summary": "No description | classes: TestFFT, FFTDocTestFinder, TestFFTDocExamples | functions: _complex_stft, _hermitian_conj, _complex_istft, _stft_reference, skip_helper_for_fft, generate_doc_test | imports: torch, unittest, doctest, inspect | [test test_spectral_ops.py]",
    "role": "src",
    "loc": 1268
  },
  {
    "id": "test\\test_stateless.py",
    "summary": "No description | classes: MockModule, MockTiedModule, NonTensor, Foo, Bar, Module | imports: subprocess, unittest, torch | [test test_stateless.py]",
    "role": "src",
    "loc": 781
  },
  {
    "id": "test\\test_static_runtime.py",
    "summary": "No description | classes: MultiHeadAttentionLayer, SubModule, SubModule2, TestModule, Foo, Mod | functions: linear_shim, create_mlp, trivial_graph, elementwise_square_addition, fork_wait_graph1, fork_wait_graph2 | imports: unittest, numpy, torch | [test test_static_runtime.py]",
    "role": "src",
    "loc": 497
  },
  {
    "id": "test\\test_subclass.py",
    "summary": "No description | classes: UninitializedParam, MyModule, MyParametrization, MyLazyModule, NonRewrappingTensor, TestSubclass | imports: tempfile, copy, functools, unittest | [test test_subclass.py]",
    "role": "src",
    "loc": 211
  },
  {
    "id": "test\\test_sympy_utils.py",
    "summary": "No description | classes: TestNumbers, TestValueRanges, TestSympyInterp, TestSympySolve, TestSympyFunctions, TestSingletonInt | functions: valid_unary, valid_binary, generate_range, type_name_fn, parametrize_relational_types, wrapper | imports: functools, pickle, sympy, torch | [test test_sympy_util",
    "role": "src",
    "loc": 754
  },
  {
    "id": "test\\test_tensorboard.py",
    "summary": "Base class used for all TensorBoard tests | classes: BaseTestCase, TestTensorBoardPyTorchNumpy, TestTensorBoardUtils, TestTensorBoardWriter, TestTensorBoardSummaryWriter, TestTensorBoardEmbedding | functions: tensor_N, remove_whitespace, get_expected_file, read_expected_content, compare_image_proto,",
    "role": "src",
    "loc": 761
  },
  {
    "id": "test\\test_tensorexpr.py",
    "summary": "No description | classes: BaseTestClass, AliasModule, TestTensorExprFuser | functions: warmup_and_run_forward | imports: numpy, torch, unittest | [test test_tensorexpr.py]",
    "role": "src",
    "loc": 1373
  },
  {
    "id": "test\\test_tensorexpr_pybind.py",
    "summary": "No description | classes: TestModule, TestTensorExprPyBind, TestExprHandlePyBind | functions: construct_adder, compute | imports: torch, numpy, unittest | [test test_tensorexpr_pybind.py]",
    "role": "src",
    "loc": 362
  },
  {
    "id": "test\\test_tensor_creation_ops.py",
    "summary": "No description | classes: MockSequence, GoodMockSequence, TestTensorCreation, TestRandomTensorCreation, TestLikeTensorCreation, TestBufferProtocol | functions: _generate_input, _rand_shape, may_require_grad, get_dtype_size, get_another_device, identity | imports: torch, numpy, unittest, random | [te",
    "role": "src",
    "loc": 3288
  },
  {
    "id": "test\\test_testing.py",
    "summary": "Makes inputs for :func:`torch.testing.assert_close` functions based on two examples. | classes: TestTesting, TestFrameworkUtils, UnexpectedException, TestAssertClose, TestAssertCloseMultiDevice, TestAssertCloseErrorMessage | functions: make_assert_close_inputs, assert_close_with_inputs, _get_test_na",
    "role": "src",
    "loc": 1863
  },
  {
    "id": "test\\test_throughput_benchmark.py",
    "summary": "No description | classes: TwoLayerNet, TwoLayerNetModule, TestThroughputBenchmark | imports: torch | [test test_throughput_benchmark.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "test\\test_torch.py",
    "summary": "No description | classes: TestBasicVitalSigns, TestVitalSignsCuda, _PlaceHolderOptimizer, Optimizer1, Optimizer2, TestTorchDeviceType | functions: torch_vital_set, disable_gc, make_neg_dim_test, neg_dim_test, idx_tensor, add_neg_dim_tests | imports: torch, numpy, gc, io | [test test_torch.py]",
    "role": "src",
    "loc": 8427
  },
  {
    "id": "test\\test_transformers.py",
    "summary": "This context manager can be used to temporarily enable or disable deterministic algorithms. | classes: MyCustomLayer, FairseqDecoder, TestTransformers, TestSDPAFailureModes, TestSDPA, TestSDPACpuOnly | functions: use_deterministic_algorithims, _check_equal, check_out_and_grad, query_key_value_clones",
    "role": "src",
    "loc": 3301
  },
  {
    "id": "test\\test_transformers_privateuse1.py",
    "summary": "No description | classes: TestSDPAPrivateUse1Only | imports: unittest, functools, pytorch_openreg, torch | [test test_transformers_privateuse1.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "test\\test_type_hints.py",
    "summary": "Extracts all runnable python code from the examples | classes: TestTypeHints | functions: get_examples_from_docstring, get_all_examples | imports: doctest, inspect, tempfile, unittest | [test test_type_hints.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "test\\test_type_info.py",
    "summary": "No description | classes: TestDTypeInfo | imports: torch, unittest, numpy | [test test_type_info.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "test\\test_type_promotion.py",
    "summary": "No description | classes: TestTypePromotion | functions: float_double_default_dtype, wrapped_fn | imports: functools, unittest, torch, numpy | [test test_type_promotion.py]",
    "role": "src",
    "loc": 919
  },
  {
    "id": "test\\test_typing.py",
    "summary": "Split at the first occurance of the ``:`` character. | classes: TestTyping | functions: _key_func, _strip_filename, _run_mypy, get_test_cases, _test_fail, _construct_format_dict | imports: shutil, unittest, threading, torch | [test test_typing.py]",
    "role": "src",
    "loc": 190
  },
  {
    "id": "test\\test_unary_ufuncs.py",
    "summary": "No description | classes: TestUnaryUfuncs | imports: random, unittest, numbers, numpy | [test test_unary_ufuncs.py]",
    "role": "src",
    "loc": 1467
  },
  {
    "id": "test\\test_utils.py",
    "summary": "No description | classes: RandomDatasetMock, Net, ModuleListNet, Two, Noop, TestCheckpoint | imports: random, shutil, subprocess, tempfile | [test test_utils.py]",
    "role": "src",
    "loc": 926
  },
  {
    "id": "test\\test_utils_config_module.py",
    "summary": "No description | classes: TestConfigModule | imports: pickle, unittest, torch | [test test_utils_config_module.py]",
    "role": "src",
    "loc": 372
  },
  {
    "id": "test\\test_utils_filelock.py",
    "summary": "No description | classes: TestFileLock | imports: concurrent, tempfile, torch | [test test_utils_filelock.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "test\\test_view_ops.py",
    "summary": "No description | classes: TestViewOps, TestOldViewOps | functions: _generate_input, _rand_shape, _convert_t, _make_tensor | imports: random, unittest, functools, numpy | [test test_view_ops.py]",
    "role": "src",
    "loc": 1604
  },
  {
    "id": "test\\test_vulkan.py",
    "summary": "No description | classes: Conv2D, Conv2DRelu, Conv2DHardtanh, TestVulkanRewritePass | imports: unittest, torch, io | [test test_vulkan.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\test_weak.py",
    "summary": "No description | classes: DummyKey, DummyValue, WeakTest, SimpleUserDict, Exc, FailingUserDict | functions: C | imports: copy, gc, random, threading | [test test_weak.py]",
    "role": "src",
    "loc": 679
  },
  {
    "id": "test\\test_xnnpack_integration.py",
    "summary": "No description | classes: TestXNNPACKOps, Linear, LinearPrePacked, Conv2D, Conv2DPrePacked, Conv2DT | imports: io, unittest, hypothesis, torch | [test test_xnnpack_integration.py]",
    "role": "src",
    "loc": 1372
  },
  {
    "id": "test\\test_xpu.py",
    "summary": "No description | classes: TestXpu, TestXpuAutocast, TestXpuTrace | imports: subprocess, tempfile, unittest, torch | [test test_xpu.py]",
    "role": "src",
    "loc": 587
  },
  {
    "id": "test\\_test_bazel.py",
    "summary": "This test module contains a minimalistic \"smoke tests\" for the bazel build. | functions: test_sum, test_simple_compile_eager, foo | imports: torch | [test _test_bazel.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "test\\ao\\sparsity\\test_activation_sparsifier.py",
    "summary": "No description | classes: Model, TestActivationSparsifier | imports: copy, torch | [test ao sparsity test_activation_sparsifier.py]",
    "role": "src",
    "loc": 286
  },
  {
    "id": "test\\ao\\sparsity\\test_composability.py",
    "summary": "No description | classes: TestComposability, TestFxComposability | functions: _get_model_and_sparsifier_and_sparse_config, _squash_mask_calibrate_and_convert, _calculate_sparsity, _module_has_activation_post_process | imports: torch | [test ao sparsity test_composability.py]",
    "role": "src",
    "loc": 453
  },
  {
    "id": "test\\ao\\sparsity\\test_data_scheduler.py",
    "summary": "No description | classes: ImplementedDataScheduler, TestBaseDataScheduler | imports: copy, torch | [test ao sparsity test_data_scheduler.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\ao\\sparsity\\test_data_sparsifier.py",
    "summary": "No description | classes: ImplementedSparsifier, _BaseDataSparsiferTestCase, _NormDataSparsifierTestCase, TestBaseDataSparsifier, TestNormDataSparsifiers, Model | imports: copy, torch | [test ao sparsity test_data_sparsifier.py]",
    "role": "src",
    "loc": 683
  },
  {
    "id": "test\\ao\\sparsity\\test_kernels.py",
    "summary": "No description | classes: TestQuantizedSparseKernels, SparseQuantizedModel, TestQuantizedSparseLayers | functions: _sparse_layer_test_helper | imports: copy, io, numpy, torch | [test ao sparsity test_kernels.py]",
    "role": "src",
    "loc": 229
  },
  {
    "id": "test\\ao\\sparsity\\test_parametrization.py",
    "summary": "No description | classes: ModelUnderTest, TestFakeSparsity | imports: torch | [test ao sparsity test_parametrization.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "test\\ao\\sparsity\\test_qlinear_packed_params.py",
    "summary": "No description | classes: TestQlinearPackedParams | imports: tempfile, torch | [test ao sparsity test_qlinear_packed_params.py]",
    "role": "src",
    "loc": 234
  },
  {
    "id": "test\\ao\\sparsity\\test_scheduler.py",
    "summary": "No description | classes: ImplementedScheduler, TestScheduler, TestCubicScheduler | imports: torch | [test ao sparsity test_scheduler.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "test\\ao\\sparsity\\test_sparsifier.py",
    "summary": "No description | classes: TestBaseSparsifier, TestWeightNormSparsifier, TestNearlyDiagonalSparsifier | imports: torch | [test ao sparsity test_sparsifier.py]",
    "role": "src",
    "loc": 397
  },
  {
    "id": "test\\ao\\sparsity\\test_sparsity_utils.py",
    "summary": "No description | classes: TestSparsityUtilFunctions | imports: torch | [test ao sparsity test_sparsity_utils.py]",
    "role": "src",
    "loc": 133
  },
  {
    "id": "test\\ao\\sparsity\\test_structured_sparsifier.py",
    "summary": "No description | classes: SimplePruner, ImplementedPruner, BottomHalfLSTMPruner, TestSaliencyPruner, TestBaseStructuredSparsifier, SimpleConvFPGM | imports: copy, random, torch | [test ao sparsity test_structured_sparsifier.py]",
    "role": "src",
    "loc": 902
  },
  {
    "id": "test\\autograd\\test_complex.py",
    "summary": "No description | classes: TestAutogradComplex | imports: torch | [test autograd test_complex.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "test\\autograd\\test_functional.py",
    "summary": "No description | classes: TestAutogradFunctional | functions: wrap_with_logging_tensor, wrapper | imports: types, unittest, torch | [test autograd test_functional.py]",
    "role": "src",
    "loc": 1393
  },
  {
    "id": "test\\autograd\\test_logging.py",
    "summary": "No description | classes: TestAutogradLogging | imports: torch | [test autograd test_logging.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\backends\\xeon\\test_launch.py",
    "summary": "No description | classes: TestTorchrun | imports: shutil, subprocess, tempfile, unittest | [test backends xeon test_launch.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "test\\benchmark_utils\\test_benchmark_utils.py",
    "summary": "Regenerate `callgrind_artifacts.json` | classes: MyModule, _MockTimer, MockTimer, _MockCudaTimer, MockCudaTimer, _MockTimer_0 | functions: generate_callgrind_artifacts, to_entry, load_callgrind_artifacts, to_function_counts | imports: json, textwrap, timeit, unittest | [test benchmark_utils test_ben",
    "role": "src",
    "loc": 852
  },
  {
    "id": "test\\bottleneck_test\\test.py",
    "summary": "No description | imports: torch | [test bottleneck_test test.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "test\\bottleneck_test\\test_args.py",
    "summary": "No description | imports: argparse, torch | [test bottleneck_test test_args.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\bottleneck_test\\test_cuda.py",
    "summary": "No description | classes: Model | functions: main | imports: torch | [test bottleneck_test test_cuda.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "test\\cpp\\__init__.py",
    "summary": "Package initializer | [test cpp __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\cpp\\aoti_inference\\compile_model.py",
    "summary": "No description | classes: TensorSerializer, SimpleModule, MyAOTIModule | functions: make_script_module, compile_model, main | imports: torch | [test cpp aoti_inference compile_model.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "test\\cpp\\aoti_inference\\generate_lowered_cpu.py",
    "summary": "No description | classes: Serializer | functions: main | imports: copy, click, torch | [test cpp aoti_inference generate_lowered_cpu.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\cpp\\aoti_inference\\test.py",
    "summary": "No description | classes: Net, NetWithTensorConstants, Serializer | functions: generate_basic_tests, generate_test_with_additional_tensors | imports: torch | [test cpp aoti_inference test.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "test\\cpp\\api\\init_baseline.py",
    "summary": "Script to generate baseline values from PyTorch initialization algorithms | functions: emit, run, main | imports: torch | [test cpp api init_baseline.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\cpp\\api\\optim_baseline.py",
    "summary": "Script to generate baseline values from PyTorch optimization algorithms | functions: weight_init, run, closure, emit, main | imports: argparse, torch | [test cpp api optim_baseline.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "test\\cpp\\jit\\tests_setup.py",
    "summary": "No description | classes: Setup, FileSetup, Model, EvalModeForLoadedModule, SerializationInterop, TorchSaveError | functions: setup, shutdown | imports: torch | [test cpp jit tests_setup.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "test\\cpp\\jit\\__init__.py",
    "summary": "Package initializer | [test cpp jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\cpp_api_parity\\functional_impl_check.py",
    "summary": "No description | functions: run_forward, test_forward, run_cpp_test_fn_and_check_output, compute_functional_name, camel_case_to_snake_case, compute_cpp_function_call | imports: pprint, tempfile, string, torch | [test cpp_api_parity functional_impl_check.py]",
    "role": "src",
    "loc": 226
  },
  {
    "id": "test\\cpp_api_parity\\module_impl_check.py",
    "summary": "No description | functions: run_python_forward_backward, test_forward_backward, run_cpp_test_fn_and_check_output, compute_module_name, process_test_params_for_module, write_test_to_test_class | imports: pprint, tempfile, types, string | [test cpp_api_parity module_impl_check.py]",
    "role": "src",
    "loc": 283
  },
  {
    "id": "test\\cpp_api_parity\\parity_table_parser.py",
    "summary": "No description | functions: parse_parity_tracker_table, parse_parity_choice | [test cpp_api_parity parity_table_parser.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "test\\cpp_api_parity\\sample_functional.py",
    "summary": "No description | functions: sample_functional | imports: torch | [test cpp_api_parity sample_functional.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "test\\cpp_api_parity\\sample_module.py",
    "summary": "No description | classes: SampleModule | imports: torch | [test cpp_api_parity sample_module.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "test\\cpp_api_parity\\utils.py",
    "summary": "No description | functions: compile_cpp_code_inline, compute_temp_file_path, is_torch_nn_functional_test, convert_to_list, set_python_tensors_requires_grad, move_python_tensors_to_device | imports: shutil, unittest, torch | [test cpp_api_parity utils.py]",
    "role": "src",
    "loc": 226
  },
  {
    "id": "test\\cpp_api_parity\\__init__.py",
    "summary": "Package initializer | [test cpp_api_parity __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\cpp_extensions\\setup.py",
    "summary": "No description | imports: setuptools, torch | [test cpp_extensions setup.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "test\\cpp_extensions\\no_python_abi_suffix_test\\setup.py",
    "summary": "No description | imports: setuptools, torch | [test cpp_extensions no_python_abi_suffix_test setup.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\setup.py",
    "summary": "No description | classes: clean | imports: distutils, shutil, setuptools, torch | [test cpp_extensions open_registration_extension setup.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\pytorch_openreg\\_aten_impl.py",
    "summary": "No description | classes: DeviceContext | functions: impl_factory, _, _openreg_kernel_fallback, get_tensor_device, _kernel_fallback, _post_process | imports: torch, _device_daemon, _meta_parser | [test cpp_extensions open_registration_extension pytorch_openreg _aten_impl.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\pytorch_openreg\\_device_daemon.py",
    "summary": "No description | classes: Allocator, HostAllocator, DeviceAllocator, Driver, _Executor | functions: register, func | imports: ctypes, threading, torch, _meta_parser | [test cpp_extensions open_registration_extension pytorch_openreg _device_daemon.py]",
    "role": "src",
    "loc": 289
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\pytorch_openreg\\_meta_parser.py",
    "summary": "No description | classes: OpenRegTensorMeta, OpenRegTensorData | functions: safe_str, convert, validate_send_queue_args, check, prepare_for_sending, receive_after_sending | imports: pprint, torch | [test cpp_extensions open_registration_extension pytorch_openreg _meta_parser.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\pytorch_openreg\\__init__.py",
    "summary": "Package initializer | classes: _OpenRegMod | imports: torch, _aten_impl, pytorch_openreg | [test cpp_extensions open_registration_extension pytorch_openreg __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\cpp_extensions\\open_registration_extension\\test\\test_openreg.py",
    "summary": "No description | classes: TestOpenReg | imports: psutil, pytorch_openreg, torch | [test cpp_extensions open_registration_extension test test_openreg.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "test\\cpp_extensions\\python_agnostic_extension\\setup.py",
    "summary": "No description | classes: clean | functions: get_extension | imports: distutils, shutil, setuptools, torch | [test cpp_extensions python_agnostic_extension setup.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "test\\cpp_extensions\\python_agnostic_extension\\python_agnostic\\ops.py",
    "summary": "Computes the ultra-L2-norm of a list of tensors via computing the norm of norms. | functions: ultra_norm | imports: torch | [test cpp_extensions python_agnostic_extension python_agnostic ops.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "test\\cpp_extensions\\python_agnostic_extension\\python_agnostic\\__init__.py",
    "summary": "Package initializer | imports: torch | [test cpp_extensions python_agnostic_extension python_agnostic __init__.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "test\\cpp_extensions\\torch_test_cpp_extension\\__init__.py",
    "summary": "This is a device backend extension used for testing. | functions: _autoload | [test cpp_extensions torch_test_cpp_extension __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\custom_backend\\backend.py",
    "summary": "Simple model used for testing that to_backend API supports saving, loading, | classes: Model | functions: get_custom_backend_library_path, to_custom_backend, main | imports: argparse, torch | [test custom_backend backend.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "test\\custom_backend\\test_custom_backend.py",
    "summary": "No description | classes: TestCustomBackend | imports: tempfile, backend, torch | [test custom_backend test_custom_backend.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "test\\custom_operator\\model.py",
    "summary": "No description | classes: Model | functions: get_custom_op_library_path, main | imports: argparse, torch | [test custom_operator model.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "test\\custom_operator\\my_custom_ops.py",
    "summary": "No description | functions: nonzero_abstract | imports: model, torch | [test custom_operator my_custom_ops.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "test\\custom_operator\\my_custom_ops2.py",
    "summary": "No description | functions: sin_abstract | imports: model, torch | [test custom_operator my_custom_ops2.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "test\\custom_operator\\pointwise.py",
    "summary": "No description | functions: cos_abstract, tan_abstract | imports: model, torch | [test custom_operator pointwise.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\custom_operator\\test_custom_ops.py",
    "summary": "No description | classes: TestCustomOperators | imports: tempfile, unittest, model, torch | [test custom_operator test_custom_ops.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "test\\custom_operator\\test_infer_schema_annotation.py",
    "summary": "No description | classes: TestInferSchemaWithAnnotation | imports: torch | [test custom_operator test_infer_schema_annotation.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "test\\distributed\\argparse_util_test.py",
    "summary": "No description | classes: ArgParseUtilTest | imports: unittest, argparse, torch | [test distributed argparse_util_test.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "test\\distributed\\test_backends.py",
    "summary": "No description | classes: TestMiscCollectiveUtils | imports: torch | [test distributed test_backends.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "test\\distributed\\test_c10d_common.py",
    "summary": "Multigpu tests are designed to simulate the multi nodes with multi | classes: AbstractTimeoutTest, TimeoutTest, Net, DoubleGpuNet, QuadraGpuNet, ConvNet | functions: gpus_for_rank | imports: copy, pickle, tempfile, threading | [test distributed test_c10d_common.py]",
    "role": "src",
    "loc": 1685
  },
  {
    "id": "test\\distributed\\test_c10d_functional_native.py",
    "summary": "No description | classes: TestThread, TestWithNCCL, _DummyWork, ProcessGroupDummy, MyWork, PyWorkTest | functions: load_test_module, dummy_init_pg | imports: gc, threading, unittest, datetime | [test distributed test_c10d_functional_native.py]",
    "role": "src",
    "loc": 911
  },
  {
    "id": "test\\distributed\\test_c10d_gloo.py",
    "summary": "No description | classes: RendezvousEnvTest, TimeoutTest, ProcessGroupGlooTest, GlobalLocalUnusedParamModule, FindUnusedParamModule, IgnoredOutput | functions: simple_reduce_tests, simple_coalesced_reduce_tests, simple_multi_input_reduce_tests | imports: copy, operator, random, tempfile | [test dist",
    "role": "src",
    "loc": 2012
  },
  {
    "id": "test\\distributed\\test_c10d_logger.py",
    "summary": "No description | classes: C10dErrorLoggerTest | functions: with_comms, wrapper | imports: json, functools, torch | [test distributed test_c10d_logger.py]",
    "role": "src",
    "loc": 112
  },
  {
    "id": "test\\distributed\\test_c10d_nccl.py",
    "summary": "No description | classes: Env, RendezvousEnvTest, TimeoutTest, ProcessGroupNCCLNoGPUTest, ProcessGroupNCCLInitTest, ProcessGroupNCCLGroupTest | functions: check_if_test_is_skipped, wrapper | imports: copy, json, pickle, random | [test distributed test_c10d_nccl.py]",
    "role": "src",
    "loc": 4194
  },
  {
    "id": "test\\distributed\\test_c10d_object_collectives.py",
    "summary": "No description | classes: TestObjectCollectives | functions: with_comms, wrapper | imports: functools, torch | [test distributed test_c10d_object_collectives.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "test\\distributed\\test_c10d_ops_nccl.py",
    "summary": "No description | classes: ProcessGroupNCCLOpTest | imports: tempfile, torch | [test distributed test_c10d_ops_nccl.py]",
    "role": "src",
    "loc": 740
  },
  {
    "id": "test\\distributed\\test_c10d_pypg.py",
    "summary": "No description | classes: MyWork, LonelyRankProcessGroup, DummyAttrProcessGroup, AbstractDDPSingleRank, TestDDPWithWorkSubclass, TestDDPWithWorkWrapper | functions: create_work | imports: weakref, test_c10d_common, torch | [test distributed test_c10d_pypg.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "test\\distributed\\test_c10d_spawn.py",
    "summary": "No description | classes: AbstractProcessGroupShareTensorTest, TestDistributedNNFunctions | imports: tempfile, torch | [test distributed test_c10d_spawn.py]",
    "role": "src",
    "loc": 196
  },
  {
    "id": "test\\distributed\\test_c10d_spawn_gloo.py",
    "summary": "No description | classes: Net, DistributedDataParallelSingleProcessTest, TestDistributedNNFunctionsGloo | imports: copy, tempfile, test_c10d_spawn, torch | [test distributed test_c10d_spawn_gloo.py]",
    "role": "src",
    "loc": 192
  },
  {
    "id": "test\\distributed\\test_c10d_spawn_nccl.py",
    "summary": "No description | classes: NonContiguousGrad, TestDistributedNNFunctionsNccl | imports: test_c10d_spawn, torch | [test distributed test_c10d_spawn_nccl.py]",
    "role": "src",
    "loc": 155
  },
  {
    "id": "test\\distributed\\test_c10d_spawn_ucc.py",
    "summary": "No description | classes: TestDistributedNNFunctionsUcc | imports: test_c10d_spawn, torch | [test distributed test_c10d_spawn_ucc.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\distributed\\test_c10d_ucc.py",
    "summary": "No description | classes: RendezvousEnvTest, TimeoutTest, ProcessGroupUCCTest, GlobalLocalUnusedParamModule, FindUnusedParamModule, IgnoredOutput | functions: simple_reduce_tests | imports: copy, operator, random, tempfile | [test distributed test_c10d_ucc.py]",
    "role": "src",
    "loc": 823
  },
  {
    "id": "test\\distributed\\test_collective_utils.py",
    "summary": "No description | classes: TestCollectiveUtils | imports: unittest, torch | [test distributed test_collective_utils.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "test\\distributed\\test_composability.py",
    "summary": "No description | classes: MLPModule, MLPModuleEven, ComposabilityTest | functions: loss_fn | imports: copy, tempfile, torch | [test distributed test_composability.py]",
    "role": "src",
    "loc": 333
  },
  {
    "id": "test\\distributed\\test_compute_comm_reordering.py",
    "summary": "Run correctness checks in multi-proc runner, mark with minimum # GPUs to run under | classes: TestComputeCommReorderingMultiProc | functions: get_snode_runtime_for_reorder_compute_test, create_grouped_node_for_allreduce_and_its_deps | imports: unittest, torch | [test distributed test_compute_comm_re",
    "role": "src",
    "loc": 321
  },
  {
    "id": "test\\distributed\\test_control_collectives.py",
    "summary": "No description | classes: TestCollectives | functions: simple_user_func | imports: datetime, multiprocessing, torch | [test distributed test_control_collectives.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "test\\distributed\\test_data_parallel.py",
    "summary": "No description | classes: TestModule, Model, Layer, Net, Cplx, ConvNet | imports: functools, io, copy, torch | [test distributed test_data_parallel.py]",
    "role": "src",
    "loc": 786
  },
  {
    "id": "test\\distributed\\test_device_mesh.py",
    "summary": "No description | classes: DeviceMeshTestGlooBackend, DeviceMeshTest, DeviceMeshTestNDim, InitDeviceMeshTest, TestDeviceMeshGetItem, TestMeshEnv | functions: _get_device_type, _set_env_var | imports: torch | [test distributed test_device_mesh.py]",
    "role": "src",
    "loc": 824
  },
  {
    "id": "test\\distributed\\test_distributed_spawn.py",
    "summary": "No description | classes: TestDistBackendWithSpawn | imports: torch | [test distributed test_distributed_spawn.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "test\\distributed\\test_dynamo_distributed.py",
    "summary": "No description | classes: ToyModel, MutatingModel, ForcedGetAttrMod, ToyInnerModel, ToyOuterModel, MyCustomLinear | functions: reset_rng_state, init_weights, get_model, get_mutating_model, get_forced_getattr_module, get_toy_model_for_activation_checkpointing | imports: copy, functools, random, unitt",
    "role": "src",
    "loc": 1583
  },
  {
    "id": "test\\distributed\\test_fake_pg.py",
    "summary": "No description | classes: TestFakePG | imports: unittest, torch | [test distributed test_fake_pg.py]",
    "role": "src",
    "loc": 166
  },
  {
    "id": "test\\distributed\\test_functional_api.py",
    "summary": "No description | classes: TestExpand, TestPgTag, TestTraceableCollectives, TestMetaCollectives, TestGradCollectives, TestMakeFx | functions: new_subgroups, exit_if_lt_x_accelerators, with_comms, wrapper | imports: unittest, functools, torch, functorch | [test distributed test_functional_api.py]",
    "role": "src",
    "loc": 633
  },
  {
    "id": "test\\distributed\\test_inductor_collectives.py",
    "summary": "No description | classes: Model, TestCollectivesMultiProc, TestCollectivesInductor | functions: _tolist_with_constrain_as_size | imports: datetime, functools, unittest, torch | [test distributed test_inductor_collectives.py]",
    "role": "src",
    "loc": 1002
  },
  {
    "id": "test\\distributed\\test_launcher.py",
    "summary": "No description | classes: TestDistributedLaunch | functions: path | imports: torch | [test distributed test_launcher.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "test\\distributed\\test_multi_threaded_pg.py",
    "summary": "No description | classes: TestCollectivesWithWrapper, MyFunc, TestCollectivesWithBaseClass | imports: operator, threading, functools, unittest | [test distributed test_multi_threaded_pg.py]",
    "role": "src",
    "loc": 267
  },
  {
    "id": "test\\distributed\\test_nccl.py",
    "summary": "No description | classes: TestNCCL | imports: torch | [test distributed test_nccl.py]",
    "role": "src",
    "loc": 189
  },
  {
    "id": "test\\distributed\\test_pg_wrapper.py",
    "summary": "No description | classes: AbstractProcessGroupWrapperTest, ProcessGroupNCCLWrapperTest, ProcessGroupGlooWrapperTest | imports: datetime, unittest, torch, test_c10d_common | [test distributed test_pg_wrapper.py]",
    "role": "src",
    "loc": 386
  },
  {
    "id": "test\\distributed\\test_serialization.py",
    "summary": "No description | classes: MyClass, TestSerialization | imports: pickle, io, torch | [test distributed test_serialization.py]",
    "role": "src",
    "loc": 130
  },
  {
    "id": "test\\distributed\\test_store.py",
    "summary": "Multigpu tests are designed to simulate the multi nodes with multi | classes: StoreTestBase, FileStoreTest, HashStoreTest, PrefixStoreTest, PrefixFileStoreTest, TCPStoreTest | functions: gpus_for_rank | imports: datetime, socket, struct, tempfile | [test distributed test_store.py]",
    "role": "src",
    "loc": 880
  },
  {
    "id": "test\\distributed\\test_symmetric_memory.py",
    "summary": "No description | classes: SymmetricMemoryTest, SubgroupTest, SymmMemCollectiveTest, LoweringTest, SymmMemSingleProcTest | functions: requires_cuda_p2p_access | imports: unittest, torch | [test distributed test_symmetric_memory.py]",
    "role": "src",
    "loc": 875
  },
  {
    "id": "test\\distributed\\algorithms\\test_join.py",
    "summary": "Join hook for :class:`AllReducer`. | classes: AllReducerJoinHook, AllReducer, TestJoin | imports: torch | [test distributed algorithms test_join.py]",
    "role": "src",
    "loc": 412
  },
  {
    "id": "test\\distributed\\algorithms\\ddp_comm_hooks\\test_ddp_hooks.py",
    "summary": "No description | classes: Task, TestDdpCommHook, DistributedDataParallelCommHookTest | functions: gpus_for_rank | imports: torch | [test distributed algorithms ddp_comm_hooks test_ddp_hooks.py]",
    "role": "src",
    "loc": 176
  },
  {
    "id": "test\\distributed\\algorithms\\quantization\\test_quantization.py",
    "summary": "No description | classes: DistQuantizationTests | functions: _build_tensor | imports: torch | [test distributed algorithms quantization test_quantization.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "test\\distributed\\bin\\test_script.py",
    "summary": "No description | functions: main | [test distributed bin test_script.py]",
    "role": "scripts",
    "loc": 4
  },
  {
    "id": "test\\distributed\\checkpoint\\test_checkpoint.py",
    "summary": "No description | classes: TestModule, TestDistributedCheckpointing, TestStorageBase, FaultyStorageWriter, FaultyStorageReader, TestDistributedFailure | imports: torch | [test distributed checkpoint test_checkpoint.py]",
    "role": "src",
    "loc": 310
  },
  {
    "id": "test\\distributed\\checkpoint\\test_compatibility.py",
    "summary": "No description | classes: TestDCPCompatbility | imports: unittest, torch | [test distributed checkpoint test_compatibility.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "test\\distributed\\checkpoint\\test_dedup_tensors.py",
    "summary": "Test class for deduplication of tensor write items across different ranks. | classes: TestDedupTensor | functions: create_plan | imports: dataclasses, torch | [test distributed checkpoint test_dedup_tensors.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "test\\distributed\\checkpoint\\test_dtensor_checkpoint.py",
    "summary": "No description | classes: MyTestModule, DTensorPlanner | imports: torch | [test distributed checkpoint test_dtensor_checkpoint.py]",
    "role": "src",
    "loc": 237
  },
  {
    "id": "test\\distributed\\checkpoint\\test_dtensor_resharding.py",
    "summary": "Test DCP reshard for DTensor with placements changes and without world_size change and mesh_tensor change. | classes: TestDTensorReshardPlacementChange, TestDTensorReshardMeshChange | imports: torch | [test distributed checkpoint test_dtensor_resharding.py]",
    "role": "src",
    "loc": 244
  },
  {
    "id": "test\\distributed\\checkpoint\\test_file_system_checkpoint.py",
    "summary": "No description | classes: MyTestModule, MyShardedModel3, TestDistributedStateDictSaveLoad, TestDistributedStateDictSaveLoadWithSharedTensor, TestDistributedReshardOnLoad, TestDistributedStateDictSaveLoadWithCaching | functions: assert_state_dict_equal | imports: shutil, tempfile, torch | [test distr",
    "role": "src",
    "loc": 465
  },
  {
    "id": "test\\distributed\\checkpoint\\test_file_system_checkpoint_cpu.py",
    "summary": "No description | classes: MyTestModule, MyShardedModel3, BlobState, TestDistributedStateDictSaveLoad, TestDistributedStateDictSaveLoadRot13, TestDistributedStateDictSaveLoadZStandard | functions: assert_state_dict_equal | imports: tempfile, torch | [test distributed checkpoint test_file_system_check",
    "role": "src",
    "loc": 446
  },
  {
    "id": "test\\distributed\\checkpoint\\test_format_utils.py",
    "summary": "No description | classes: SimpleModelUneven, TestFormatUtils | imports: torch | [test distributed checkpoint test_format_utils.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "test\\distributed\\checkpoint\\test_fsdp_model_state.py",
    "summary": "No description | classes: FsdpModelStateCheckpoint | imports: torch | [test distributed checkpoint test_fsdp_model_state.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "test\\distributed\\checkpoint\\test_fsdp_optim_state.py",
    "summary": "No description | classes: TestDummyModel, FsdpOptimStateCheckpoint | imports: torch | [test distributed checkpoint test_fsdp_optim_state.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\distributed\\checkpoint\\test_fsdp_tp_checkpoint_conversion.py",
    "summary": "No description | classes: TestFsdpTpCheckpointConversion | imports: torch | [test distributed checkpoint test_fsdp_tp_checkpoint_conversion.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "test\\distributed\\checkpoint\\test_fsspec.py",
    "summary": "Wrapper to initialize temp directory for distributed checkpoint. | classes: MyTestModule, TestFSSpec, TestFileSystem | functions: with_temp_dir, wrapper | imports: shutil, tempfile, functools, torch | [test distributed checkpoint test_fsspec.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "test\\distributed\\checkpoint\\test_hsdp_checkpoint.py",
    "summary": "No description | classes: SimpleModel, SimpleModelUneven, TestHSDPCheckpoint | imports: copy, torch | [test distributed checkpoint test_hsdp_checkpoint.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "test\\distributed\\checkpoint\\test_nested_dict.py",
    "summary": "No description | classes: TestFlattening | imports: torch | [test distributed checkpoint test_nested_dict.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "test\\distributed\\checkpoint\\test_planner.py",
    "summary": "No description | classes: TestSavePlan, TestPlannerHelpers, TestLoadPlanner | functions: create_sharded_tensor | imports: torch | [test distributed checkpoint test_planner.py]",
    "role": "src",
    "loc": 439
  },
  {
    "id": "test\\distributed\\checkpoint\\test_save_load_api.py",
    "summary": "No description | classes: MyTestModule, TestSaveAndLoadAPI | imports: unittest, torch | [test distributed checkpoint test_save_load_api.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "test\\distributed\\checkpoint\\test_state_dict.py",
    "summary": "No description | classes: TiedEmbeddingModel, TestModel, TestStateDict, TestNoComm | imports: copy, functools, torch | [test distributed checkpoint test_state_dict.py]",
    "role": "src",
    "loc": 903
  },
  {
    "id": "test\\distributed\\checkpoint\\test_state_dict_utils.py",
    "summary": "No description | classes: TestStateDictUtils | imports: copy, io, torch | [test distributed checkpoint test_state_dict_utils.py]",
    "role": "src",
    "loc": 210
  },
  {
    "id": "test\\distributed\\checkpoint\\test_tp_checkpoint.py",
    "summary": "No description | classes: UnevenShardedModel, TestTpCheckpoint | imports: copy, torch | [test distributed checkpoint test_tp_checkpoint.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "test\\distributed\\checkpoint\\test_traverse.py",
    "summary": "Test class for util methods of _traverse | classes: TestTraverse | imports: torch | [test distributed checkpoint test_traverse.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "test\\distributed\\checkpoint\\test_utils.py",
    "summary": "No description | classes: TestMedatadaIndex, TestReaderView | functions: create_sharded_tensor | imports: io, torch | [test distributed checkpoint test_utils.py]",
    "role": "src",
    "loc": 145
  },
  {
    "id": "test\\distributed\\checkpoint\\e2e\\test_e2e_save_and_load.py",
    "summary": "No description | classes: TestDummyModel, TestStatefulObj, ModelType, TestTrainState, StateDict, Foo | functions: _train | imports: dataclasses, functools, io, torch | [test distributed checkpoint e2e test_e2e_save_and_load.py]",
    "role": "src",
    "loc": 399
  },
  {
    "id": "test\\distributed\\checkpoint\\e2e\\test_fine_tuning.py",
    "summary": "No description | classes: PreTrainedModel, FineTuningModel, TestFineTuning | imports: torch | [test distributed checkpoint e2e test_fine_tuning.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "test\\distributed\\checkpoint\\e2e\\test_fsdp_ep.py",
    "summary": "No description | classes: Dummymodel, EPModel, SecondTier, TopModel, TestFSDPWithEP | imports: torch | [test distributed checkpoint e2e test_fsdp_ep.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "test\\distributed\\checkpoint\\e2e\\test_pipeline.py",
    "summary": "No description | classes: PipelineModel, TestPipeline | imports: torch | [test distributed checkpoint e2e test_pipeline.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "test\\distributed\\checkpoint\\fsdp\\test_fsdp_dsd.py",
    "summary": "No description | classes: TestFullyShardWithDistributedStateDict | imports: copy, torch | [test distributed checkpoint fsdp test_fsdp_dsd.py]",
    "role": "src",
    "loc": 488
  },
  {
    "id": "test\\distributed\\elastic\\test_control_plane.py",
    "summary": "No description | classes: UnixHTTPConnection, UnixHTTPConnectionPool, Request, Response, WorkerServerTest | functions: local_worker_server | imports: json, pickle, socket, tempfile | [test distributed elastic test_control_plane.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "test\\distributed\\elastic\\agent\\server\\test\\api_test.py",
    "summary": "No description | classes: WorkerStateTest, WorkerGroupTest, RoleInstanceInfoTest, TestAgent, SimpleElasticAgentTest | functions: do_nothing, monres | imports: functools, signal, unittest, uuid | [test distributed elastic agent server test api_test.py]",
    "role": "src",
    "loc": 543
  },
  {
    "id": "test\\distributed\\elastic\\agent\\server\\test\\local_elastic_agent_test.py",
    "summary": "No description | classes: RankInfo, Conf, LocalElasticAgentTest | functions: init_rpc, rpc_master, rpc_worker, _happy_function, _sad_function, dummy_compute | imports: json, multiprocessing, shutil, signal | [test distributed elastic agent server test local_elastic_agent_test.py]",
    "role": "src",
    "loc": 1228
  },
  {
    "id": "test\\distributed\\elastic\\agent\\server\\test\\__init__.py",
    "summary": "Package initializer | [test distributed elastic agent server test __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\events\\lib_test.py",
    "summary": "No description | classes: EventLibTest, RdzvEventLibTest | imports: json, dataclasses, unittest, torch | [test distributed elastic events lib_test.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "test\\distributed\\elastic\\metrics\\api_test.py",
    "summary": "No description | classes: TestMetricsHandler, Parent, Child, MetricsApiTest | functions: foo_1 | imports: abc, unittest, torch | [test distributed elastic metrics api_test.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "test\\distributed\\elastic\\metrics\\__init__.py",
    "summary": "Package initializer | [test distributed elastic metrics __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\api_test.py",
    "summary": "void function | classes: RunProcResultsTest, StdTest, _StartProcessesTest, StartProcessesAsFuncTest, StartProcessesAsBinaryTest, StartProcessesListAsFuncTest | functions: echo0, echo1, echo2, echo_large, echo3, dummy_compute | imports: asyncio, ctypes, multiprocessing, shutil | [test distributed ela",
    "role": "src",
    "loc": 779
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\redirects_test.py",
    "summary": "No description | classes: RedirectsTest | imports: ctypes, shutil, tempfile, unittest | [test distributed elastic multiprocessing redirects_test.py]",
    "role": "src",
    "loc": 99
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\tail_log_test.py",
    "summary": "No description | classes: TailLogTest | functions: write | imports: io, shutil, tempfile, unittest | [test distributed elastic multiprocessing tail_log_test.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\bin\\echo1.py",
    "summary": "No description | imports: argparse | [test distributed elastic multiprocessing bin echo1.py]",
    "role": "scripts",
    "loc": 16
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\bin\\echo2.py",
    "summary": "No description | imports: argparse | [test distributed elastic multiprocessing bin echo2.py]",
    "role": "scripts",
    "loc": 12
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\bin\\echo3.py",
    "summary": "No description | imports: argparse, ctypes | [test distributed elastic multiprocessing bin echo3.py]",
    "role": "scripts",
    "loc": 15
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\bin\\test_script.py",
    "summary": "No description | [test distributed elastic multiprocessing bin test_script.py]",
    "role": "scripts",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\bin\\zombie_test.py",
    "summary": "No description | [test distributed elastic multiprocessing bin zombie_test.py]",
    "role": "scripts",
    "loc": 4
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\errors\\api_test.py",
    "summary": "No description | classes: SentinelError, ApiTest | functions: raise_exception_fn, raise_system_exit_exception_fn, good_fn, raise_child_failure_error_fn, read_resource_file | imports: json, shutil, signal, tempfile | [test distributed elastic multiprocessing errors api_test.py]",
    "role": "src",
    "loc": 194
  },
  {
    "id": "test\\distributed\\elastic\\multiprocessing\\errors\\error_handler_test.py",
    "summary": "No description | classes: GetErrorHandlerTest, ErrorHandlerTest | functions: raise_exception_fn | imports: filecmp, json, shutil, tempfile | [test distributed elastic multiprocessing errors error_handler_test.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\api_test.py",
    "summary": "No description | classes: RendezvousParametersTest, _DummyRendezvousHandler, RendezvousHandlerRegistryTest | imports: unittest, torch | [test distributed elastic rendezvous api_test.py]",
    "role": "src",
    "loc": 199
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\c10d_rendezvous_backend_test.py",
    "summary": "No description | classes: TCPStoreBackendTest, FileStoreBackendTest, CreateBackendTest | imports: tempfile, base64, datetime, unittest | [test distributed elastic rendezvous c10d_rendezvous_backend_test.py]",
    "role": "src",
    "loc": 206
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\dynamic_rendezvous_test.py",
    "summary": "No description | classes: CustomAssertMixin, RendezvousTimeoutTest, NodeDescTest, NodeDescGeneratorTest, RendezvousStateTest, FakeRendezvousBackend | functions: _ignore_exception, _wait_for, _wait_while | imports: copy, pickle, socket, threading | [test distributed elastic rendezvous dynamic_rendezv",
    "role": "src",
    "loc": 1330
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\etcd_rendezvous_backend_test.py",
    "summary": "No description | classes: EtcdRendezvousBackendTest, CreateBackendTest | imports: subprocess, threading, base64, unittest | [test distributed elastic rendezvous etcd_rendezvous_backend_test.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\etcd_rendezvous_test.py",
    "summary": "No description | classes: EtcdRendezvousTest | imports: unittest, uuid, torch | [test distributed elastic rendezvous etcd_rendezvous_test.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\etcd_server_test.py",
    "summary": "No description | classes: EtcdServerTest | imports: unittest, etcd, torch | [test distributed elastic rendezvous etcd_server_test.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\out_of_tree_rendezvous_test.py",
    "summary": "No description | classes: OutOfTreeRendezvousTest | imports: unittest, torch | [test distributed elastic rendezvous out_of_tree_rendezvous_test.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\rendezvous_backend_test.py",
    "summary": "No description | classes: RendezvousBackendTestMixin | imports: abc, torch | [test distributed elastic rendezvous rendezvous_backend_test.py]",
    "role": "src",
    "loc": 68
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\static_rendezvous_test.py",
    "summary": "No description | classes: StaticTCPRendezvousTest | imports: unittest, torch | [test distributed elastic rendezvous static_rendezvous_test.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\utils_test.py",
    "summary": "No description | classes: UtilsTest, PeriodicTimerTest | imports: socket, threading, datetime, unittest | [test distributed elastic rendezvous utils_test.py]",
    "role": "src",
    "loc": 290
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\__init__.py",
    "summary": "Package initializer | [test distributed elastic rendezvous __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\rendezvous\\out_of_tree_test_package\\src\\testbackend\\__init__.py",
    "summary": "Package initializer | functions: test_handler | [test distributed elastic rendezvous out_of_tree_test_package src testbackend __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "test\\distributed\\elastic\\timer\\api_test.py",
    "summary": "No description | classes: MockRequestQueue, MockTimerServer, TimerApiTest | imports: unittest, torch | [test distributed elastic timer api_test.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "test\\distributed\\elastic\\timer\\file_based_local_timer_test.py",
    "summary": "No description | classes: FileTimerTest, FileTimerClientTest, FileTimerServerTest | functions: func2, _request_on_interval | imports: multiprocessing, signal, unittest, uuid | [test distributed elastic timer file_based_local_timer_test.py]",
    "role": "src",
    "loc": 272
  },
  {
    "id": "test\\distributed\\elastic\\timer\\local_timer_example.py",
    "summary": "Demonstrates how to use LocalTimerServer and LocalTimerClient | classes: LocalTimerExample | functions: _happy_function, _stuck_function | imports: multiprocessing, signal, torch | [test distributed elastic timer local_timer_example.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "test\\distributed\\elastic\\timer\\local_timer_test.py",
    "summary": "No description | classes: LocalTimerTest, MultiprocessingRequestQueueTest, LocalTimerServerTest | functions: func2, _enqueue_on_interval | imports: multiprocessing, signal, unittest, torch | [test distributed elastic timer local_timer_test.py]",
    "role": "src",
    "loc": 235
  },
  {
    "id": "test\\distributed\\elastic\\timer\\__init__.py",
    "summary": "Package initializer | [test distributed elastic timer __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\utils\\distributed_test.py",
    "summary": "No description | classes: DistributedUtilTest | functions: _create_c10d_store_mp | imports: multiprocessing, socket, unittest, torch | [test distributed elastic utils distributed_test.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "test\\distributed\\elastic\\utils\\logging_test.py",
    "summary": "No description | classes: LoggingTest | imports: torch | [test distributed elastic utils logging_test.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "test\\distributed\\elastic\\utils\\util_test.py",
    "summary": "No description | classes: MockStore, StoreUtilTest, UtilTest | imports: datetime, multiprocessing, unittest, torch | [test distributed elastic utils util_test.py]",
    "role": "src",
    "loc": 201
  },
  {
    "id": "test\\distributed\\elastic\\utils\\__init__.py",
    "summary": "Package initializer | [test distributed elastic utils __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\elastic\\utils\\data\\cycling_iterator_test.py",
    "summary": "No description | classes: CyclingIteratorTest | imports: unittest, torch | [test distributed elastic utils data cycling_iterator_test.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "test\\distributed\\elastic\\utils\\data\\__init__.py",
    "summary": "Package initializer | [test distributed elastic utils data __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\flight_recorder\\test_fr_analysis.py",
    "summary": "No description | classes: FlightRecorderEventTest, FlightMatchInfoTest | functions: create_one_event | imports: tools, torch | [test distributed flight_recorder test_fr_analysis.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "test\\distributed\\fsdp\\test_checkpoint_wrapper.py",
    "summary": "No description | classes: MyModel, ModelEnforceKwarg, Model, LinearWithBatchNorm, CheckpointWrapperTest | imports: unittest, copy, functools, torch | [test distributed fsdp test_checkpoint_wrapper.py]",
    "role": "src",
    "loc": 321
  },
  {
    "id": "test\\distributed\\fsdp\\test_distributed_checkpoint.py",
    "summary": "No description | classes: TestDistributedCheckpoint | imports: torch | [test distributed fsdp test_distributed_checkpoint.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_apply.py",
    "summary": "No description | classes: TestApply | imports: torch | [test distributed fsdp test_fsdp_apply.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_backward_prefetch.py",
    "summary": "No description | classes: TestBackwardPrefetch | imports: unittest, torch | [test distributed fsdp test_fsdp_backward_prefetch.py]",
    "role": "src",
    "loc": 167
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_checkpoint.py",
    "summary": "No description | classes: SequentialModule, TestFSDPCheckpoint, CheckpointModule, ModelWithCheckpointSubmodule, TestModel, TestFSDPCheckpointSubmodule | functions: get_patched_save_on_cpu, patched_save_on_cpu, patch_save_on_cpu | imports: copy, functools, torch | [test distributed fsdp test_fsdp_che",
    "role": "src",
    "loc": 291
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_clip_grad_norm.py",
    "summary": "No description | classes: Model, TestClipGradNorm | imports: torch | [test distributed fsdp test_fsdp_clip_grad_norm.py]",
    "role": "src",
    "loc": 306
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_comm.py",
    "summary": "No description | classes: PassType, TestCommunication, ReduceModule, MLPs, ReduceModel, TestExplicitUnshard | imports: unittest, torch | [test distributed fsdp test_fsdp_comm.py]",
    "role": "src",
    "loc": 335
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_comm_hooks.py",
    "summary": "No description | classes: Net, DummyState, DummyHook, TestCommunicationHooks | imports: torch | [test distributed fsdp test_fsdp_comm_hooks.py]",
    "role": "src",
    "loc": 359
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_core.py",
    "summary": "Compare losses and parameter values after several updates when using | classes: TestParityWithDDP, TestParamInit, TestHooks, TestNoGrad, TestAutograd | imports: functools, unittest, torch | [test distributed fsdp test_fsdp_core.py]",
    "role": "src",
    "loc": 464
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_dtensor_state_dict.py",
    "summary": "No description | classes: TestDummyModel, TestDummyModelUneven, TestFSDPWithDeviceMeshAndDTensor | imports: io, copy, torch | [test distributed fsdp test_fsdp_dtensor_state_dict.py]",
    "role": "src",
    "loc": 243
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_exec_order.py",
    "summary": "Model that supports two computation paths: `layer0` -> `layer1` and | classes: Model, TestFSDPExecOrder | imports: torch | [test distributed fsdp test_fsdp_exec_order.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_fine_tune.py",
    "summary": "No description | classes: LinearUnusedInput, ModelUnusedInput, TestModule, TestFSDPFineTune | imports: copy, unittest, torch | [test distributed fsdp test_fsdp_fine_tune.py]",
    "role": "src",
    "loc": 357
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_flatten_params.py",
    "summary": "No description | classes: EmptyModule, TestFlattenParams | imports: torch | [test distributed fsdp test_fsdp_flatten_params.py]",
    "role": "src",
    "loc": 568
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_freezing_weights.py",
    "summary": "No description | classes: Model, NestedTrunkModel, FreezingMethod, TestFreezingWeights | imports: torch | [test distributed fsdp test_fsdp_freezing_weights.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_fx.py",
    "summary": "No description | classes: Model, TestSymbolicTracing | imports: torch | [test distributed fsdp test_fsdp_fx.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_grad_acc.py",
    "summary": "This configures how gradients are accumulated in :meth:`_test_grad_acc`. | classes: _GradAccConfig, _GradAccConfigs, TestGradAcc | imports: dataclasses, torch | [test distributed fsdp test_fsdp_grad_acc.py]",
    "role": "src",
    "loc": 257
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_hybrid_shard.py",
    "summary": "Patches dist.all_reduce with a new all_reduce and | classes: MyModel, ShardingStrategyMode, TestFSDPHybridShard | functions: patch_allreduce, patch_reduce_scatter | imports: functools, torch | [test distributed fsdp test_fsdp_hybrid_shard.py]",
    "role": "src",
    "loc": 386
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_ignored_modules.py",
    "summary": "No description | classes: Model, IgnoredModule, ModelWithIgnoredModules, TestFSDPIgnoredModules | imports: functools, torch | [test distributed fsdp test_fsdp_ignored_modules.py]",
    "role": "src",
    "loc": 379
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_input.py",
    "summary": "No description | classes: Model, TestInput | imports: torch | [test distributed fsdp test_fsdp_input.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_memory.py",
    "summary": "Collect memory allocated values in a result dict in MB | classes: Model, TestFSDPMemory | functions: get_cur_mem, create_model | imports: unittest, torch | [test distributed fsdp test_fsdp_memory.py]",
    "role": "src",
    "loc": 180
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_meta.py",
    "summary": "Linear layer with deterministic reset_parameters for testing. | classes: MyLinear, MyBuffer, MyModel, NestedModel, FakeLinear, Model | functions: _reset_params_if_meta, _init_with_reset_params, _init_with_torchdistX, check_fn | imports: torch, torchdistx | [test distributed fsdp test_fsdp_meta.py]",
    "role": "src",
    "loc": 328
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_misc.py",
    "summary": "No description | classes: MyModel, Mnist, MyModule, TestFSDPMiscMultiProcess, CPUGPUModule, MultiGPUModule | imports: functools, copy, torch | [test distributed fsdp test_fsdp_misc.py]",
    "role": "src",
    "loc": 934
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_mixed_precision.py",
    "summary": "A linear module with extra checks for mixed precision training. | classes: LinearMixedPrecision, MyModel, TestFSDPMixedPrecision, BatchNormNet, NonLearnableConv, TestFSDPMixedPrecisionSharded | functions: patch_reduce_scatter | imports: functools, torch, torchvision | [test distributed fsdp test_fsd",
    "role": "src",
    "loc": 1096
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_multiple_forward.py",
    "summary": "No description | classes: Model, TestMultiForward | imports: torch | [test distributed fsdp test_fsdp_multiple_forward.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_multiple_wrapping.py",
    "summary": "No description | classes: InnerModel, TestMultipleWrapping | imports: torch | [test distributed fsdp test_fsdp_multiple_wrapping.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_optim_state.py",
    "summary": "Method for communicating the optimizer state dict for internal tests. | classes: _OSDCommMethod, _ModelClass, Bias, BlockA, BlockB, NestedModel | imports: bisect, copy, torch | [test distributed fsdp test_fsdp_optim_state.py]",
    "role": "src",
    "loc": 1734
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_overlap.py",
    "summary": "No description | classes: Layer, Min10, TestForwardOverlapWorldSizeOne, TestForwardOverlapWorldSizeTwo | functions: _create_model | imports: unittest, statistics, torch | [test distributed fsdp test_fsdp_overlap.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_pure_fp16.py",
    "summary": "No description | classes: TestPureFP16 | imports: torch | [test distributed fsdp test_fsdp_pure_fp16.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_sharded_grad_scaler.py",
    "summary": "No description | classes: TestShardGradScaler, TestShardedGradScalerParityWithDDP | imports: copy, functools, unittest, torch | [test distributed fsdp test_fsdp_sharded_grad_scaler.py]",
    "role": "src",
    "loc": 308
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_state_dict.py",
    "summary": "No description | classes: Model, TestDummyModel, FSDPContainer, TestFSDPStateDict, TestFSDPStateDict4GPUs | imports: io, copy, functools, torch | [test distributed fsdp test_fsdp_state_dict.py]",
    "role": "src",
    "loc": 1153
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_tp_integration.py",
    "summary": "No description | classes: SimpleModel, TestModel, TestTPFSDPIntegration | functions: distribute_rmsnorm, prepare_input_fn, prepare_output_fn | imports: copy, torch | [test distributed fsdp test_fsdp_tp_integration.py]",
    "role": "src",
    "loc": 404
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_traversal.py",
    "summary": "No description | classes: TestTraversal | imports: torch | [test distributed fsdp test_fsdp_traversal.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_uneven.py",
    "summary": "No description | classes: TestUnevenParamShard | imports: torch | [test distributed fsdp test_fsdp_uneven.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_unshard_params.py",
    "summary": "This contains any methods common to both the sharded and non-sharded cases. | classes: TestUnshardParamsBase, TestUnshardParams, TestUnshardParamsNoShard, MyModule, TestUnshardParamsErrors | imports: torch | [test distributed fsdp test_fsdp_unshard_params.py]",
    "role": "src",
    "loc": 637
  },
  {
    "id": "test\\distributed\\fsdp\\test_fsdp_use_orig_params.py",
    "summary": "Tests multiple parameter groups. | classes: TestFSDPUseOrigParamsMultipleParamGroups, TestFSDPUseOrigParamsUnshardReshard, Model, TestFSDPUseOrigParamsParamAccess, TestFSDPUseOrigParamsWriteback, TestFSDPUseOrigParamsFQNs | imports: copy, functools, unittest, torch | [test distributed fsdp test_fsdp",
    "role": "src",
    "loc": 1209
  },
  {
    "id": "test\\distributed\\fsdp\\test_hsdp_dtensor_state_dict.py",
    "summary": "No description | classes: DenseModel, FakeMPModel, TestHSDPWithDeviceMeshAndDTensor | imports: io, copy, torch | [test distributed fsdp test_hsdp_dtensor_state_dict.py]",
    "role": "src",
    "loc": 259
  },
  {
    "id": "test\\distributed\\fsdp\\test_shard_utils.py",
    "summary": "No description | classes: TestShardUtilsDistributed, TestShardUtilsDistributedDTensor | imports: torch | [test distributed fsdp test_shard_utils.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "test\\distributed\\fsdp\\test_utils.py",
    "summary": "No description | classes: NonFrozenDataClass, FrozenDataClass, TestUtils | imports: random, dataclasses, torch | [test distributed fsdp test_utils.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "test\\distributed\\fsdp\\test_wrap.py",
    "summary": "No description | classes: BatchNormNet, LoraModel, LoraDecoder, LoraAttention, LoraMLP, WrapMethod | imports: functools, tempfile, unittest, torch | [test distributed fsdp test_wrap.py]",
    "role": "src",
    "loc": 850
  },
  {
    "id": "test\\distributed\\launcher\\api_test.py",
    "summary": "No description | classes: MockException, ElasticLaunchTest | functions: path, simple_rank_scale, function_with_bug, get_test_launch_config, elastic_launch_wrapper, _dist_sum | imports: multiprocessing, shutil, signal, tempfile | [test distributed launcher api_test.py]",
    "role": "src",
    "loc": 329
  },
  {
    "id": "test\\distributed\\launcher\\launch_test.py",
    "summary": "No description | classes: LaunchTest | functions: path | imports: shutil, tempfile, unittest, torch | [test distributed launcher launch_test.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "test\\distributed\\launcher\\test_run.py",
    "summary": "No description | classes: MockException, ElasticLaunchTest | functions: launch_in_proc, path, get_child_pids, pid_exists | imports: io, multiprocessing, runpy, shutil | [test distributed launcher test_run.py]",
    "role": "src",
    "loc": 535
  },
  {
    "id": "test\\distributed\\launcher\\__init__.py",
    "summary": "Package initializer | [test distributed launcher __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\launcher\\bin\\test_script.py",
    "summary": "No description | functions: parse_args, main | imports: argparse | [test distributed launcher bin test_script.py]",
    "role": "scripts",
    "loc": 50
  },
  {
    "id": "test\\distributed\\launcher\\bin\\test_script_init_method.py",
    "summary": "No description | functions: parse_args, main | imports: argparse, torch | [test distributed launcher bin test_script_init_method.py]",
    "role": "scripts",
    "loc": 48
  },
  {
    "id": "test\\distributed\\launcher\\bin\\test_script_is_torchelastic_launched.py",
    "summary": "This is a test script that launches as part of the test cases in | functions: parse_args, main | imports: argparse, torch | [test distributed launcher bin test_script_is_torchelastic_launched.py]",
    "role": "scripts",
    "loc": 27
  },
  {
    "id": "test\\distributed\\launcher\\bin\\test_script_local_rank.py",
    "summary": "No description | functions: parse_args, main | imports: argparse | [test distributed launcher bin test_script_local_rank.py]",
    "role": "scripts",
    "loc": 25
  },
  {
    "id": "test\\distributed\\nn\\jit\\test_instantiator.py",
    "summary": "No description | classes: MyModuleInterface, MyModule, TestInstantiator | functions: create_module | imports: torch | [test distributed nn jit test_instantiator.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "test\\distributed\\nn\\jit\\__init__.py",
    "summary": "Package initializer | [test distributed nn jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\optim\\test_apply_optimizer_in_backward.py",
    "summary": "No description | classes: TestModel, ApplyOverlappedOptimizerTest | functions: _validate_params | imports: unittest, copy, torch | [test distributed optim test_apply_optimizer_in_backward.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "test\\distributed\\optim\\test_named_optimizer.py",
    "summary": "No description | classes: TestDummyModel, NamedOptimizerTest | functions: _run_model_training | imports: unittest, torch | [test distributed optim test_named_optimizer.py]",
    "role": "src",
    "loc": 367
  },
  {
    "id": "test\\distributed\\optim\\test_zero_redundancy_optimizer.py",
    "summary": "No description | classes: TestZeroRedundancyOptimizer, SGDWithStepKWArg, SGDWithNewKey, SGDWithoutClosure, TestZeroRedundancyOptimizerSingleRank, _JoinGradInfo | functions: _get_backend_for_tests | imports: copy, numpy, torch, torchvision | [test distributed optim test_zero_redundancy_optimizer.py]",
    "role": "src",
    "loc": 1189
  },
  {
    "id": "test\\distributed\\pipelining\\model_registry.py",
    "summary": "No description | classes: ExampleCode, ModelWithKwargs, ModelWithParamAlias, MLPModule, MultiMLP, CustomLinearDx | imports: torch | [test distributed pipelining model_registry.py]",
    "role": "src",
    "loc": 183
  },
  {
    "id": "test\\distributed\\pipelining\\schedule_registry.py",
    "summary": "No description | classes: ScheduleVShaped, ScheduleUnbalanced, ScheduleWithW, ScheduleWithReorderedB | imports: torch | [test distributed pipelining schedule_registry.py]",
    "role": "src",
    "loc": 201
  },
  {
    "id": "test\\distributed\\pipelining\\test_backward.py",
    "summary": "No description | classes: StageBackwardTests | imports: copy, model_registry, torch | [test distributed pipelining test_backward.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "test\\distributed\\pipelining\\test_microbatch.py",
    "summary": "No description | classes: MicrobatchTests | imports: model_registry, torch | [test distributed pipelining test_microbatch.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "test\\distributed\\pipelining\\test_pipe.py",
    "summary": "No description | classes: ExampleCode, MultiMLP, PipeTests | imports: model_registry, torch | [test distributed pipelining test_pipe.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "test\\distributed\\pipelining\\test_schedule.py",
    "summary": "No description | classes: MockPipelineStage, ScheduleTest, TestSchedulePlan, TestScheduleLowering, TestValidateSchedule, ScheduleUtilTests | imports: copy, csv, model_registry, torch | [test distributed pipelining test_schedule.py]",
    "role": "src",
    "loc": 842
  },
  {
    "id": "test\\distributed\\pipelining\\test_schedule_multiproc.py",
    "summary": "No description | classes: CustomState, ScheduleTest | imports: copy, tempfile, model_registry, schedule_registry | [test distributed pipelining test_schedule_multiproc.py]",
    "role": "src",
    "loc": 740
  },
  {
    "id": "test\\distributed\\pipelining\\test_stage.py",
    "summary": "A simple hook for simulating mixed precision | classes: CustomState, StageTest | functions: get_dtype_change_hook, dtype_change_hook, f, get_flatten_hook, flatten_hook | imports: tempfile, model_registry, torch | [test distributed pipelining test_stage.py]",
    "role": "src",
    "loc": 249
  },
  {
    "id": "test\\distributed\\pipelining\\test_transformer.py",
    "summary": "No description | classes: MLPModule, TransformerLike, TransformerTests | imports: torch | [test distributed pipelining test_transformer.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "test\\distributed\\pipelining\\test_unflatten.py",
    "summary": "No description | classes: Block, M, UnflattenTests | imports: torch | [test distributed pipelining test_unflatten.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "test\\distributed\\pipelining\\__init__.py",
    "summary": "Package initializer | [test distributed pipelining __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\rpc\\test_faulty_agent.py",
    "summary": "No description | imports: torch | [test distributed rpc test_faulty_agent.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "test\\distributed\\rpc\\test_share_memory.py",
    "summary": "No description | classes: ShareMemoryRPCPickler, TestRPCPickler | functions: fs_sharing, worker_loop, worker_fn | imports: copyreg, torch | [test distributed rpc test_share_memory.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\distributed\\rpc\\test_tensorpipe_agent.py",
    "summary": "No description | imports: torch | [test distributed rpc test_tensorpipe_agent.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "test\\distributed\\rpc\\cuda\\test_tensorpipe_agent.py",
    "summary": "No description | imports: torch | [test distributed rpc cuda test_tensorpipe_agent.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\distributed\\tensor\\test_api.py",
    "summary": "No description | classes: MyModel, DTensorAPITest | imports: torch | [test distributed tensor test_api.py]",
    "role": "src",
    "loc": 277
  },
  {
    "id": "test\\distributed\\tensor\\test_attention.py",
    "summary": "No description | classes: RingAttentionTest | imports: unittest, torch | [test distributed tensor test_attention.py]",
    "role": "src",
    "loc": 365
  },
  {
    "id": "test\\distributed\\tensor\\test_common_rules.py",
    "summary": "No description | classes: CommonRulesTest | imports: torch | [test distributed tensor test_common_rules.py]",
    "role": "src",
    "loc": 333
  },
  {
    "id": "test\\distributed\\tensor\\test_convolution_ops.py",
    "summary": "No description | classes: DistConvolutionOpsTest | functions: _conv_fn | imports: copy, torch | [test distributed tensor test_convolution_ops.py]",
    "role": "src",
    "loc": 176
  },
  {
    "id": "test\\distributed\\tensor\\test_dtensor.py",
    "summary": "No description | classes: DummyMLP, DTensorTest, DTensorMeshTest, TestDTensorPlacementTypes, DTensorLogTest | imports: tempfile, unittest, numpy, torch | [test distributed tensor test_dtensor.py]",
    "role": "src",
    "loc": 827
  },
  {
    "id": "test\\distributed\\tensor\\test_dtensor_compile.py",
    "summary": "No description | classes: SimpleModel, FakeAttention, FakeTransformerBlock, FakeTransformer, TestDTensorCompile, TestDTensorCompileE2E | functions: extract_graph | imports: copy, functools, unittest, torch | [test distributed tensor test_dtensor_compile.py]",
    "role": "src",
    "loc": 779
  },
  {
    "id": "test\\distributed\\tensor\\test_dtensor_ops.py",
    "summary": "No description | classes: TestDTensorOps | functions: xfail, skip, skipOps, wrapped | imports: unittest, torch | [test distributed tensor test_dtensor_ops.py]",
    "role": "src",
    "loc": 564
  },
  {
    "id": "test\\distributed\\tensor\\test_embedding_ops.py",
    "summary": "No description | classes: TestEmbeddingOp | imports: torch | [test distributed tensor test_embedding_ops.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "test\\distributed\\tensor\\test_experimental_ops.py",
    "summary": "No description | classes: DistOtherOpsTest | imports: torch | [test distributed tensor test_experimental_ops.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "test\\distributed\\tensor\\test_init.py",
    "summary": "No description | classes: DTensorInitOpsTest, DTensorConstructorTest | imports: torch | [test distributed tensor test_init.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "test\\distributed\\tensor\\test_math_ops.py",
    "summary": "No description | classes: SubTest, LnTpBlock, DistMathOpsTest | imports: copy, pprint, torch | [test distributed tensor test_math_ops.py]",
    "role": "src",
    "loc": 541
  },
  {
    "id": "test\\distributed\\tensor\\test_matrix_ops.py",
    "summary": "No description | classes: DistMatrixOpsTest | functions: scale_for_fp8 | imports: unittest, torch | [test distributed tensor test_matrix_ops.py]",
    "role": "src",
    "loc": 391
  },
  {
    "id": "test\\distributed\\tensor\\test_optimizers.py",
    "summary": "No description | classes: TestDTensorOptimizer | functions: shard_fn, input_fn, output_fn | imports: copy, torch | [test distributed tensor test_optimizers.py]",
    "role": "src",
    "loc": 506
  },
  {
    "id": "test\\distributed\\tensor\\test_op_strategy.py",
    "summary": "No description | classes: TestEinsumDims, TestEinsumStrategies, TestCostModel | imports: torch | [test distributed tensor test_op_strategy.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "test\\distributed\\tensor\\test_pointwise_ops.py",
    "summary": "No description | classes: DistElementwiseOpsTest | functions: no_op, deepcopy_convert_to_dtensor, f, deepcopy_convert_from_dtensor | imports: unittest, torch | [test distributed tensor test_pointwise_ops.py]",
    "role": "src",
    "loc": 235
  },
  {
    "id": "test\\distributed\\tensor\\test_random_ops.py",
    "summary": "No description | classes: DistTensorRandomInitTest, DistTensorRandomOpTest, DistTensorRandomOpsTest3D | imports: torch | [test distributed tensor test_random_ops.py]",
    "role": "src",
    "loc": 439
  },
  {
    "id": "test\\distributed\\tensor\\test_redistribute.py",
    "summary": "No description | classes: RedistributeTest, MultiDimRedistributeTest | imports: torch | [test distributed tensor test_redistribute.py]",
    "role": "src",
    "loc": 417
  },
  {
    "id": "test\\distributed\\tensor\\test_tensor_ops.py",
    "summary": "No description | classes: DistTensorOpsTest | imports: torch | [test distributed tensor test_tensor_ops.py]",
    "role": "src",
    "loc": 533
  },
  {
    "id": "test\\distributed\\tensor\\test_utils.py",
    "summary": "No description | classes: UtilTest, TestStridedSharding, Test2DStridedLocalShard | imports: torch | [test distributed tensor test_utils.py]",
    "role": "src",
    "loc": 410
  },
  {
    "id": "test\\distributed\\tensor\\test_view_ops.py",
    "summary": "No description | classes: TestViewOps | imports: torch | [test distributed tensor test_view_ops.py]",
    "role": "src",
    "loc": 496
  },
  {
    "id": "test\\distributed\\tensor\\test_xla_integration.py",
    "summary": "No description | classes: SimpleLinear, DTensorXLAIntegrationTest | functions: with_xla, wrapper | imports: unittest, functools, numpy, torch | [test distributed tensor test_xla_integration.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "test\\distributed\\tensor\\__init__.py",
    "summary": "Package initializer | [test distributed tensor __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\tensor\\debug\\test_comm_mode.py",
    "summary": "No description | classes: WrapperModel, WrapperModelCoalesced, TestCommMode | imports: torch | [test distributed tensor debug test_comm_mode.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\distributed\\tensor\\debug\\test_comm_mode_features.py",
    "summary": "No description | classes: TestCommModeFeatures | imports: torch | [test distributed tensor debug test_comm_mode_features.py]",
    "role": "src",
    "loc": 307
  },
  {
    "id": "test\\distributed\\tensor\\debug\\test_op_coverage.py",
    "summary": "No description | classes: SimpleMLP, TestOpCoverage | imports: torch | [test distributed tensor debug test_op_coverage.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\distributed\\tensor\\experimental\\test_local_map.py",
    "summary": "No description | classes: TestLocalMap | functions: equal_allgather_forward, mm_all_gather_forward, mm_forward, mm_allreduce_forward, mm_allreduce_forward_decorated, mul_forward | imports: functools, torch | [test distributed tensor experimental test_local_map.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "test\\distributed\\tensor\\experimental\\test_register_sharding.py",
    "summary": "No description | classes: TestRegisterSharding | imports: torch | [test distributed tensor experimental test_register_sharding.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "test\\distributed\\tensor\\experimental\\test_tp_transform.py",
    "summary": "A dummy model with list of MLPs. | classes: MLPListModule, DummyModel, TensorParallelTest | imports: torch | [test distributed tensor experimental test_tp_transform.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\test_micro_pipeline_tp.py",
    "summary": "No description | classes: MicroPipelineTPTest | functions: _make_post_grad_fx, _fp8_all_gather | imports: unittest, torch, functorch | [test distributed tensor parallel test_micro_pipeline_tp.py]",
    "role": "src",
    "loc": 356
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\test_parallelize_api.py",
    "summary": "No description | classes: DummyModule, TensorParallelAPITests | imports: copy, torch | [test distributed tensor parallel test_parallelize_api.py]",
    "role": "src",
    "loc": 276
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\test_tp_examples.py",
    "summary": "No description | classes: ExpCommCounts, TestModule, DistTensorParallelExampleTest | imports: copy, torch | [test distributed tensor parallel test_tp_examples.py]",
    "role": "src",
    "loc": 472
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\test_tp_random_state.py",
    "summary": "No description | classes: TensorParallelRandomStateTests | imports: torch | [test distributed tensor parallel test_tp_random_state.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\test_tp_style.py",
    "summary": "No description | classes: TestModule, TestKwargModule, TestKwargOnlyModule, TensorParallelStyleTest | imports: copy, torch | [test distributed tensor parallel test_tp_style.py]",
    "role": "src",
    "loc": 339
  },
  {
    "id": "test\\distributed\\tensor\\parallel\\__init__.py",
    "summary": "Package initializer | [test distributed tensor parallel __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\distributed\\_composable\\test_checkpoint.py",
    "summary": "No description | classes: MemoryDelta, ToyModel, RandomModel, MultiOutputModel, MultiInputModel, MyModel | imports: unittest, copy, functools, torch | [test distributed _composable test_checkpoint.py]",
    "role": "src",
    "loc": 259
  },
  {
    "id": "test\\distributed\\_composable\\test_contract.py",
    "summary": "No description | classes: ToyModel, ModelWrapper, TestContract | imports: copy, torch | [test distributed _composable test_contract.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "test\\distributed\\_composable\\test_replicate.py",
    "summary": "No description | classes: Net, ReplicateStateDictTest, MyNet, ReplicateTest, ToyModel, ReplicateFullyShardInit | imports: copy, torch | [test distributed _composable test_replicate.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "test\\distributed\\_composable\\test_replicate_with_compiler.py",
    "summary": "No description | classes: Net, MultiProcessInductorTestCase, ReplicateTest, DDP_TP_Test | functions: compiler_fn, _compiler_fn, inner_compiler | imports: functools, unittest, copy, torch | [test distributed _composable test_replicate_with_compiler.py]",
    "role": "src",
    "loc": 349
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_autograd.py",
    "summary": "No description | classes: Module, ToContainerType, FromContainerType, TestFullyShardAutograd, TestFullyShardPostAccGradHookMultiThread, TestFullyShardPostAccGradHookMultiProcess | imports: copy, functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_autograd.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_clip_grad_norm_.py",
    "summary": "No description | classes: _TestClipGradNormBase, TestClipGradNormWorldSize2, TestClipGradNormWorldSize4 | imports: copy, functools, torch | [test distributed _composable fsdp test_fully_shard_clip_grad_norm_.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_comm.py",
    "summary": "No description | classes: TestFullyShardCollectiveOps, TestFullyShardCommunication, ModuleWithUnusedLinear, TestFullyShardPrefetch, ReduceModule, MLPs | imports: copy, functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_comm.py]",
    "role": "src",
    "loc": 1008
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_compile.py",
    "summary": "No description | classes: Mod, TestFullyShardCompileCompute, TestSubmodule, TestModule, TestFullyShardCompile | functions: _count_op_in_graph, _is_fallback_op_in_snodes | imports: copy, functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_compile.py]",
    "role": "src",
    "loc": 946
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_extensions.py",
    "summary": "No description | classes: BFloat16AllGatherTensor, TestFullyShardAllGatherExtensionsCommon, TestFullyShardAllGatherExtensionsMultiProcess, TestFullyShardAllGatherExtensionsMultiThread | functions: two_tensor_fsdp_pre_all_gather_v1, two_tensor_fsdp_pre_all_gather_v2, two_tensor_fsdp_post_all_gather |",
    "role": "src",
    "loc": 402
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_frozen.py",
    "summary": "No description | classes: MultiForwardModule, TestFullyShardFrozen | imports: copy, functools, torch | [test distributed _composable fsdp test_fully_shard_frozen.py]",
    "role": "src",
    "loc": 230
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_grad_scaler.py",
    "summary": "No description | classes: TestFullyShardGradientScaler | imports: copy, torch | [test distributed _composable fsdp test_fully_shard_grad_scaler.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_ignore_params.py",
    "summary": "No description | classes: C, B, A, Y, X, TestFullyShardIgnoreParams | functions: _append_prefix, _generate_model_and_input, _find_name_param_mappings, _discover_ddp_ignored_params, _modify_ddp_ignored_params, _get_full_tensor | imports: torch | [test distributed _composable fsdp test_fully_shard_ign",
    "role": "src",
    "loc": 232
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_init.py",
    "summary": "Tests that tensor parameters are moved to the expected device. | classes: TestFullyShardDeviceTensor, TestFullyShardDeviceDTensor, TestFullyShardMeshArg, TestFullyShardManagedModulesAndStates, TestFullyShardParamModuleInfos, TestFullyShardShardedParameterTensor | imports: copy, unittest, torch | [te",
    "role": "src",
    "loc": 1065
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_logging.py",
    "summary": "No description | classes: LoggingTests | imports: functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_logging.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_memory.py",
    "summary": "No description | classes: TestFullyShardMemory | imports: functools, gc, torch | [test distributed _composable fsdp test_fully_shard_memory.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_mixed_precision.py",
    "summary": "No description | classes: TestFullyShardMixedPrecisionTraining, ToyModule, ToyModel, TestFullyShardMixedPrecisionCasts | imports: copy, functools, torch | [test distributed _composable fsdp test_fully_shard_mixed_precision.py]",
    "role": "src",
    "loc": 489
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_overlap.py",
    "summary": "NOTE: Testing stream overlap in PyTorch CI is tricky. | classes: TestFullyShardOverlap, Matmul, LinearWithSleep | imports: copy, functools, torch | [test distributed _composable fsdp test_fully_shard_overlap.py]",
    "role": "src",
    "loc": 180
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_state.py",
    "summary": "No description | classes: TestFullyShardState | imports: copy, unittest, torch | [test distributed _composable fsdp test_fully_shard_state.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_state_dict.py",
    "summary": "No description | classes: TestFullyShardStateDictMultiProcess, TestFullyShardStateDictMultiThread | imports: copy, functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_state_dict.py]",
    "role": "src",
    "loc": 327
  },
  {
    "id": "test\\distributed\\_composable\\fsdp\\test_fully_shard_training.py",
    "summary": "No description | classes: ParamlessModule, TestFullyShardForwardInputs, TestFullyShardRegisteredParams, TestFullyShardCastAfterInit, MultiForwardModule, TestFullyShard1DTrainingCore | imports: copy, functools, unittest, torch | [test distributed _composable fsdp test_fully_shard_training.py]",
    "role": "src",
    "loc": 1209
  },
  {
    "id": "test\\distributed\\_composable\\test_composability\\test_2d_composability.py",
    "summary": "No description | classes: SimpleModel, SimpleModelUneven, TestFullyShard2DTraining, TestFullyShard2DStateDict, Test2dFSDP1ParallelIntegration, TestNew2dParallelTraining | imports: copy, functools, io, torch | [test distributed _composable test_composability test_2d_composability.py]",
    "role": "src",
    "loc": 805
  },
  {
    "id": "test\\distributed\\_composable\\test_composability\\test_pp_composability.py",
    "summary": "No description | classes: MLPModule, MLPModuleEven, AppState, PPModelChunk, ComposabilityTest | imports: torch | [test distributed _composable test_composability test_pp_composability.py]",
    "role": "src",
    "loc": 307
  },
  {
    "id": "test\\distributed\\_shard\\test_sharder.py",
    "summary": "No description | classes: CustomEmbeddingBagCollection, CustomShardedEBC, CustomSharder, MyModule, TestCustomSharder | imports: copy, torch | [test distributed _shard test_sharder.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\distributed\\_shard\\sharded_optim\\test_sharded_optim.py",
    "summary": "No description | classes: MyShardedModel, MyShardedLinear, TestShardedOptimizer | imports: copy, torch | [test distributed _shard sharded_optim test_sharded_optim.py]",
    "role": "src",
    "loc": 138
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\test_logger.py",
    "summary": "No description | classes: ShardingSpecLoggerTest | imports: torch | [test distributed _shard sharded_tensor test_logger.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\test_sharded_tensor.py",
    "summary": "No description | classes: TestShardedTensorMetadata, TestCreateTensorFromParams, TestShardParameter, TestShardTensor, DummyNNModule, TestModuleHookApi | imports: copy, io, pickle, torch | [test distributed _shard sharded_tensor test_sharded_tensor.py]",
    "role": "src",
    "loc": 2734
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\test_sharded_tensor_reshard.py",
    "summary": "No description | classes: TestReshard | imports: torch | [test distributed _shard sharded_tensor test_sharded_tensor_reshard.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\ops\\test_binary_cmp.py",
    "summary": "Test base for binary comparison functions such as torch.equal, torch.allclose etc. for ShardedTensor | classes: TestShardedTensorBinaryOps | imports: torch | [test distributed _shard sharded_tensor ops test_binary_cmp.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\ops\\test_embedding.py",
    "summary": "No description | classes: TestShardedEmbedding | imports: torch | [test distributed _shard sharded_tensor ops test_embedding.py]",
    "role": "src",
    "loc": 157
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\ops\\test_embedding_bag.py",
    "summary": "No description | classes: TestShardedEmbeddingBag | imports: torch | [test distributed _shard sharded_tensor ops test_embedding_bag.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\ops\\test_init.py",
    "summary": "Testing torch.nn.init functions for ShardedTensor | classes: TestShardedTensorNNInit | imports: torch | [test distributed _shard sharded_tensor ops test_init.py]",
    "role": "src",
    "loc": 100
  },
  {
    "id": "test\\distributed\\_shard\\sharded_tensor\\ops\\test_tensor_ops.py",
    "summary": "No description | classes: TestTensorOps | imports: copy, torch | [test distributed _shard sharded_tensor ops test_tensor_ops.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "test\\distributed\\_shard\\sharding_plan\\test_sharding_plan.py",
    "summary": "No description | classes: ChunkAllShardingPlanner, TestShardingPlan | imports: torch | [test distributed _shard sharding_plan test_sharding_plan.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "test\\distributed\\_shard\\sharding_spec\\test_sharding_spec.py",
    "summary": "No description | classes: TestShardingSpec, GridShardingSpec, TestCustomShardingSpec | imports: copy, dataclasses, torch | [test distributed _shard sharding_spec test_sharding_spec.py]",
    "role": "src",
    "loc": 539
  },
  {
    "id": "test\\distributed\\_tools\\test_fsdp2_mem_tracker.py",
    "summary": "No description | classes: TestTrackerFullyShard1DTrainingCore, TestTrackerFullyShard1DTrainingCompose | functions: _init_cublas_workspace, _reset_mem_stats | imports: functools, gc, torch | [test distributed _tools test_fsdp2_mem_tracker.py]",
    "role": "src",
    "loc": 251
  },
  {
    "id": "test\\distributed\\_tools\\test_memory_tracker.py",
    "summary": "No description | classes: TestMemoryTracker | imports: unittest, torch | [test distributed _tools test_memory_tracker.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "test\\distributed\\_tools\\test_mem_tracker.py",
    "summary": "No description | classes: DummyModel, MLPBlock, MyModule, TestMemTracker | imports: gc, unittest, torch | [test distributed _tools test_mem_tracker.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "test\\distributed\\_tools\\test_mod_tracker.py",
    "summary": "No description | classes: Foo, Mod, Bar, TestModTracker | imports: copy, torch | [test distributed _tools test_mod_tracker.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "test\\distributed\\_tools\\test_runtime_estimator.py",
    "summary": "No description | classes: ConvArgs, SimpleCNN, TestRuntimeEstimator | imports: unittest, dataclasses, torch | [test distributed _tools test_runtime_estimator.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "test\\distributed\\_tools\\test_sac_estimator.py",
    "summary": "No description | classes: Foo, TestSACEstimator | imports: unittest, torch | [test distributed _tools test_sac_estimator.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "test\\distributed\\_tools\\test_sac_ilp.py",
    "summary": "No description | classes: TestSACILP, TestOptimalCheckpointingPolicy | imports: copy, unittest, torch | [test distributed _tools test_sac_ilp.py]",
    "role": "src",
    "loc": 220
  },
  {
    "id": "test\\distributions\\test_constraints.py",
    "summary": "No description | functions: build_constraint, test_constraint, test_biject_to, test_transform_to | imports: pytest, torch | [test distributions test_constraints.py]",
    "role": "src",
    "loc": 159
  },
  {
    "id": "test\\distributions\\test_distributions.py",
    "summary": "Note [Randomized statistical tests] | classes: DistributionsTestCase, Dummy, SubClass, Rounded, ArgMax, ScipyCategorical | functions: pairwise, is_all_nan, _get_examples, _get_bad_examples | imports: numbers, unittest, random, packaging | [test distributions test_distributions.py]",
    "role": "src",
    "loc": 6306
  },
  {
    "id": "test\\distributions\\test_transforms.py",
    "summary": "No description | functions: get_transforms, reshape_transform, transform_id, generate_data, test_inv_inv, test_equality | imports: io, numbers, pytest, torch | [test distributions test_transforms.py]",
    "role": "src",
    "loc": 480
  },
  {
    "id": "test\\distributions\\test_utils.py",
    "summary": "No description | functions: test_tril_matrix_to_vec | imports: pytest, torch | [test distributions test_utils.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\dynamo\\mock_store_global_crossfile_inline.py",
    "summary": "No description | functions: set_flag_true, set_flag_false | [test dynamo mock_store_global_crossfile_inline.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\dynamo\\test_activation_checkpointing.py",
    "summary": "No description | classes: _InvalidContext, MockModule, MySubmod, MyMod, Parametrization, MLPModule | functions: checkpoint_wrapper, inner, count_ops, match_rng_op, collect_fwd_graph_outputs, _invalid_context_gen | imports: copy, functools, unittest, importlib | [test dynamo test_activation_checkpoin",
    "role": "src",
    "loc": 1134
  },
  {
    "id": "test\\dynamo\\test_after_aot.py",
    "summary": "No description | classes: TestAfterAot | functions: strip_trailing_whitespace | imports: io, shutil, tempfile, unittest | [test dynamo test_after_aot.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "test\\dynamo\\test_aot_autograd.py",
    "summary": "No description | classes: Repro, ModuleSpecialFwd, F, Test, WrapperModule, Model | functions: maybe_dupe_op, is_dynamic_shape_test | imports: copy, unittest, textwrap, torch | [test dynamo test_aot_autograd.py]",
    "role": "src",
    "loc": 1187
  },
  {
    "id": "test\\dynamo\\test_aot_autograd_cache.py",
    "summary": "No description | classes: MyMod, AOTAutogradCacheTests, AllowInGraphFunc, AOTAutogradCachePicklerTests | imports: shutil, unittest, torch | [test dynamo test_aot_autograd_cache.py]",
    "role": "src",
    "loc": 773
  },
  {
    "id": "test\\dynamo\\test_autograd_function.py",
    "summary": "No description | classes: CustomFunc1, CustomFunc3, Module1, Module2, Module3, Module4 | imports: copy, dataclasses, torch, triton | [test dynamo test_autograd_function.py]",
    "role": "src",
    "loc": 1200
  },
  {
    "id": "test\\dynamo\\test_backends.py",
    "summary": "No description | classes: Seq, Conv_Bn_Relu, TestOptimizations, NormalizeIRTests, MPSSupportedTest, TestExplainWithBackend | imports: unittest, torch, functorch | [test dynamo test_backends.py]",
    "role": "src",
    "loc": 307
  },
  {
    "id": "test\\dynamo\\test_backward_higher_order_ops.py",
    "summary": "No description | classes: MyObj, BackwardHigherOrderOpTests | functions: _multiply, _multiply_invoke | imports: functools, torch | [test dynamo test_backward_higher_order_ops.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "test\\dynamo\\test_base_hop.py",
    "summary": "No description | classes: InvokeQuantTest, BaseHOPTest | functions: normalize_graph | imports: unittest, torch, functorch | [test dynamo test_base_hop.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\dynamo\\test_base_output.py",
    "summary": "No description | classes: TestBaseOutput | functions: maybe_skip | imports: unittest, torch, diffusers | [test dynamo test_base_output.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "test\\dynamo\\test_bytecode_utils.py",
    "summary": "No description | classes: BytecodeTests, BytecodeHookTests | imports: dis, unittest, torch | [test dynamo test_bytecode_utils.py]",
    "role": "src",
    "loc": 491
  },
  {
    "id": "test\\dynamo\\test_callback.py",
    "summary": "No description | classes: CallbackTests | imports: unittest, torch | [test dynamo test_callback.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\dynamo\\test_compile.py",
    "summary": "No description | classes: ToyModel, TestEnum, Mod, InPlaceCompilationTests, PublicTorchCompilerTests | imports: inspect, io, tempfile, unittest | [test dynamo test_compile.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "test\\dynamo\\test_compiler_bisector.py",
    "summary": "No description | classes: Foo, TestCompilerBisector | imports: unittest, importlib, torch | [test dynamo test_compiler_bisector.py]",
    "role": "src",
    "loc": 187
  },
  {
    "id": "test\\dynamo\\test_comptime.py",
    "summary": "No description | classes: mylist, ComptimeTests | imports: io, torch | [test dynamo test_comptime.py]",
    "role": "src",
    "loc": 327
  },
  {
    "id": "test\\dynamo\\test_config.py",
    "summary": "No description | classes: ConfigTests | imports: torch | [test dynamo test_config.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "test\\dynamo\\test_ctx_manager.py",
    "summary": "No description | classes: CustomizedCtxManager, CustomizedCtxManagerWithGraphBreak, MyModule, CtxManagerTests, ContextlibContextManagerTests, RuntimeErrorSubclass | functions: customized_ctx_manager, customized_ctx_manager_with_graph_break | imports: traceback, unittest, torch, test_functions | [tes",
    "role": "src",
    "loc": 2435
  },
  {
    "id": "test\\dynamo\\test_cudagraphs.py",
    "summary": "No description | classes: TestAotCudagraphs | functions: composed, deco, assert_aot_autograd_counter, wrap, patch_all | imports: functools, unittest, torch | [test dynamo test_cudagraphs.py]",
    "role": "src",
    "loc": 166
  },
  {
    "id": "test\\dynamo\\test_cudagraphs_expandable_segments.py",
    "summary": "No description | imports: torch, dynamo, tools | [test dynamo test_cudagraphs_expandable_segments.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "test\\dynamo\\test_debug_utils.py",
    "summary": "No description | classes: TestDebugUtils, TestDebugUtilsDevice | imports: unittest, torch, functorch | [test dynamo test_debug_utils.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "test\\dynamo\\test_decorators.py",
    "summary": "No description | classes: SimpleLinear, SimpleModel, Point, PointTensor, Num, encoder | functions: my_custom_function | imports: functools, operator, unittest, torch | [test dynamo test_decorators.py]",
    "role": "src",
    "loc": 976
  },
  {
    "id": "test\\dynamo\\test_deviceguard.py",
    "summary": "Unit tests for the DeviceGuard class using a mock DeviceInterface. | classes: TestDeviceGuard, TestCUDADeviceGuard | imports: unittest, torch | [test dynamo test_deviceguard.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "test\\dynamo\\test_dicts.py",
    "summary": "No description | classes: SimpleDict, MethodDict, SetItemDict, ReversedDict, ODSubclass, dotdict | imports: dataclasses, gc, types, unittest | [test dynamo test_dicts.py]",
    "role": "src",
    "loc": 718
  },
  {
    "id": "test\\dynamo\\test_dynamic_shapes.py",
    "summary": "No description | functions: make_dynamic_cls | imports: unittest, torch, test_aot_autograd, test_ctx_manager | [test dynamo test_dynamic_shapes.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "test\\dynamo\\test_exc.py",
    "summary": "No description | classes: ExcTests | imports: unittest, torch | [test dynamo test_exc.py]",
    "role": "src",
    "loc": 280
  },
  {
    "id": "test\\dynamo\\test_exceptions.py",
    "summary": "No description | classes: A, B, Foo, Mock, M, MyMapping | imports: unittest, torch | [test dynamo test_exceptions.py]",
    "role": "src",
    "loc": 384
  },
  {
    "id": "test\\dynamo\\test_export.py",
    "summary": "PYTEST_DONT_REWRITE (prevents pytest from rewriting assertions, which interferes | classes: Animal, MyModule, MyBlock, Bob, Module, Foo | functions: dynamo_assume_constant_result_global_function | imports: copy, functools, inspect, io | [test dynamo test_export.py]",
    "role": "src",
    "loc": 3626
  },
  {
    "id": "test\\dynamo\\test_export_mutations.py",
    "summary": "No description | classes: Foo, MutationExportTests | imports: unittest, torch | [test dynamo test_export_mutations.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "test\\dynamo\\test_flat_apply.py",
    "summary": "No description | classes: Norm, Point, PointTensor, FlatApplyTests | functions: distance | imports: dataclasses, torch | [test dynamo test_flat_apply.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "test\\dynamo\\test_frame_init.py",
    "summary": "No description | classes: FrameInitTests | functions: target_with_varkwargs, varkwargs_code1, varkwargs_code2, target_with_varargs, varargs_code1, varargs_code2 | imports: torch | [test dynamo test_frame_init.py]",
    "role": "src",
    "loc": 109
  },
  {
    "id": "test\\dynamo\\test_functions.py",
    "summary": "No description | classes: CustomDictSubclass, MyCls, Mock, CallableClass, NotCallableClass, TensorDict | functions: constant3, call, update_global, update_global_ctx, func_with_default, make_test | imports: functools, inspect, keyword, operator | [test dynamo test_functions.py]",
    "role": "src",
    "loc": 3734
  },
  {
    "id": "test\\dynamo\\test_fx_passes_pre_grad.py",
    "summary": "No description | classes: TestModule, FxPassesPreGradTests | imports: unittest, torch | [test dynamo test_fx_passes_pre_grad.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "test\\dynamo\\test_generator.py",
    "summary": "No description | classes: GeneratorTestsBase, Counter, GeneratorTests, TestGeneratorSend, TestGeneratorClose, TestGeneratorThrow | imports: unittest, torch | [test dynamo test_generator.py]",
    "role": "src",
    "loc": 1467
  },
  {
    "id": "test\\dynamo\\test_global.py",
    "summary": "No description | classes: Pair, Variable, TestGlobals | functions: Foo, fresh_name, reset_name | imports: torch, utils, mock_store_global_crossfile_inline | [test dynamo test_global.py]",
    "role": "src",
    "loc": 203
  },
  {
    "id": "test\\dynamo\\test_graph_break_messages.py",
    "summary": "No description | classes: Foo, GraphBreakMessagesTest | imports: unittest, torch, optree | [test dynamo test_graph_break_messages.py]",
    "role": "src",
    "loc": 393
  },
  {
    "id": "test\\dynamo\\test_graph_deduplication.py",
    "summary": "No description | classes: GraphDededuplicationTests | functions: extract_graph, graph_str | imports: torch | [test dynamo test_graph_deduplication.py]",
    "role": "src",
    "loc": 468
  },
  {
    "id": "test\\dynamo\\test_graph_region_tracker.py",
    "summary": "No description | classes: GraphRegionTrackerTests | functions: get_nodes_by_name, track_same_nodes | imports: torch | [test dynamo test_graph_region_tracker.py]",
    "role": "src",
    "loc": 247
  },
  {
    "id": "test\\dynamo\\test_guard_manager.py",
    "summary": "No description | classes: Pair, Foo, Bar, A, B, GuardManagerTests | functions: id_type, equals_match, equals_match_verbose_code_parts, ge_match, ge_match_verbose_code_parts, less_match | imports: functools, unittest, weakref, torch | [test dynamo test_guard_manager.py]",
    "role": "src",
    "loc": 705
  },
  {
    "id": "test\\dynamo\\test_higher_order_ops.py",
    "summary": "No description | classes: Obj, MyModule, SomeEnum, MyClass, MyMode, Foo | functions: count_ops, find_first_node, op_count, assert_dict_matches_regex, default_args_generator | imports: functools, pprint, unittest, copy | [test dynamo test_higher_order_ops.py]",
    "role": "src",
    "loc": 5220
  },
  {
    "id": "test\\dynamo\\test_hooks.py",
    "summary": "No description | classes: ClassWithVal, MyMod, SomePyClass, Mod, HooksTests | functions: compiler_fn, global_hook_0, global_hook_1, global_hook_2 | imports: functools, unittest, torch, functorch | [test dynamo test_hooks.py]",
    "role": "src",
    "loc": 678
  },
  {
    "id": "test\\dynamo\\test_input_attr_tracking.py",
    "summary": "No description | classes: MyUserDefinedClass, TestInputAttrTracking | imports: torch | [test dynamo test_input_attr_tracking.py]",
    "role": "src",
    "loc": 250
  },
  {
    "id": "test\\dynamo\\test_interop.py",
    "summary": "No description | classes: InteropTests | functions: fn | imports: torch, functools | [test dynamo test_interop.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "test\\dynamo\\test_logging.py",
    "summary": "No description | classes: ToyModel, LoggingTests | functions: munge_shape_guards, munge, example_fn, dynamo_error_fn, inductor_error_fn, inductor_schedule_fn | imports: functools, unittest, torch, functorch | [test dynamo test_logging.py]",
    "role": "src",
    "loc": 765
  },
  {
    "id": "test\\dynamo\\test_metrics_context.py",
    "summary": "No description | classes: TestMetricsContext | imports: torch | [test dynamo test_metrics_context.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "test\\dynamo\\test_minifier.py",
    "summary": "No description | classes: MinifierTests | imports: unittest, torch | [test dynamo test_minifier.py]",
    "role": "src",
    "loc": 176
  },
  {
    "id": "test\\dynamo\\test_misc.py",
    "summary": "No description | classes: MyPickledModule, UserDefineSetAttr, MyModule, MyClass, Mod, Cfg | functions: onlyIfTranslationValidation, wrapper, closure_adder, inner | imports: abc, copy, dataclasses, dis | [test dynamo test_misc.py]",
    "role": "src",
    "loc": 9311
  },
  {
    "id": "test\\dynamo\\test_model_output.py",
    "summary": "No description | classes: TestHFPretrained, MyDataClass, Model, TestModelOutput, BertPooler, BertEncoder | functions: maybe_skip | imports: dataclasses, unittest, torch, transformers | [test dynamo test_model_output.py]",
    "role": "src",
    "loc": 296
  },
  {
    "id": "test\\dynamo\\test_modes.py",
    "summary": "No description | classes: TestMode, RewriteAddToMul, TorchDispatchModeTests, TestMode1, TestMode2, TestSubclass | imports: operator, unittest, torch, functools | [test dynamo test_modes.py]",
    "role": "src",
    "loc": 485
  },
  {
    "id": "test\\dynamo\\test_modules.py",
    "summary": "No description | classes: BasicModule, FnMember, FnMemberCmp, SubmoduleExample, IsTrainingCheck, IsEvalCheck | functions: update_global, requires_grad1, requires_grad2, make_test, test_fn, temporary_tensor_subclass | imports: copy, tempfile, traceback, types | [test dynamo test_modules.py]",
    "role": "src",
    "loc": 2498
  },
  {
    "id": "test\\dynamo\\test_nops.py",
    "summary": "No description | classes: NopTests | functions: fn1, fn2, modify, fn3 | imports: torch | [test dynamo test_nops.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "test\\dynamo\\test_optimizers.py",
    "summary": "PYTEST_DONT_REWRITE (prevents pytest from rewriting assertions, which interferes | classes: MyOptimizer, Net, End2EndTests | imports: functools, torch | [test dynamo test_optimizers.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\dynamo\\test_pgo.py",
    "summary": "No description | classes: PgoTest | imports: torch | [test dynamo test_pgo.py]",
    "role": "src",
    "loc": 123
  },
  {
    "id": "test\\dynamo\\test_pre_dispatch.py",
    "summary": "No description | classes: PreDispatchTests | imports: torch | [test dynamo test_pre_dispatch.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "test\\dynamo\\test_profiler.py",
    "summary": "No description | classes: DynamoProfilerTests | imports: unittest, torch | [test dynamo test_profiler.py]",
    "role": "src",
    "loc": 144
  },
  {
    "id": "test\\dynamo\\test_python_autograd.py",
    "summary": "create a new unique name for a variable: v0, v1, v2 | classes: Variable, TapeEntry, TestPythonAutograd | functions: fresh_name, reset_tape, grad, gather_grad, operator_mul, propagate | imports: torch | [test dynamo test_python_autograd.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "test\\dynamo\\test_python_dispatcher.py",
    "summary": "No description | classes: PythonDispatcherTests | imports: unittest, torch | [test dynamo test_python_dispatcher.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "test\\dynamo\\test_recompiles.py",
    "summary": "No description | classes: SimpleDropout, RecompileTests | imports: unittest, torch | [test dynamo test_recompiles.py]",
    "role": "src",
    "loc": 369
  },
  {
    "id": "test\\dynamo\\test_recompile_ux.py",
    "summary": "No description | classes: RecompileUxTests | imports: unittest, weakref, torch | [test dynamo test_recompile_ux.py]",
    "role": "src",
    "loc": 225
  },
  {
    "id": "test\\dynamo\\test_reconstruct.py",
    "summary": "No description | classes: DummyModule, ReconstructTest | functions: _filter_instructions | imports: dis, unittest, torch | [test dynamo test_reconstruct.py]",
    "role": "src",
    "loc": 231
  },
  {
    "id": "test\\dynamo\\test_reorder_logs.py",
    "summary": "No description | classes: IgnoreLogsTests, ReorderLogsTests | functions: f_info, f_isEnabledFor | imports: io, unittest, torch | [test dynamo test_reorder_logs.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "test\\dynamo\\test_repros.py",
    "summary": "PYTEST_DONT_REWRITE (prevents pytest from rewriting assertions, which interferes | classes: Boxes, _ReversibleFunction, ReformerEncoder, ValueNode, ListIterator, ListConfig | functions: exists, maybe, inner, is_fx_tracing_test, has_detectron2, _do_paste_mask | imports: copy, dataclasses, functools, ",
    "role": "src",
    "loc": 5165
  },
  {
    "id": "test\\dynamo\\test_resume.py",
    "summary": "No description | classes: ResumeFunctionTests | functions: fn_creator, fn, inner_fn | imports: torch | [test dynamo test_resume.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "test\\dynamo\\test_sdpa.py",
    "summary": "No description | classes: TestSDPA | functions: allow_in_graph_sdpa_params | imports: torch | [test dynamo test_sdpa.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "test\\dynamo\\test_skip_guard_eval_unsafe.py",
    "summary": "No description | classes: Foo, RunDiffGuardTests | functions: my_custom_function | imports: torch | [test dynamo test_skip_guard_eval_unsafe.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\dynamo\\test_skip_non_tensor.py",
    "summary": "No description | classes: MyModule, Foo, SkipNonTensorTests | functions: user_function, user_generator | imports: unittest, torch | [test dynamo test_skip_non_tensor.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "test\\dynamo\\test_sources.py",
    "summary": "No description | classes: CausalLMOutputWithPast, Elements, Model, SourceTests | imports: torch | [test dynamo test_sources.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\dynamo\\test_structured_trace.py",
    "summary": "No description | classes: StructuredTraceTestingFilter, ChromiumEventFilter, StructuredTracePayloadFormatter, StructuredTraceTestingFormatter, ToyModel, MySin | functions: example_fn, example_training_fn, dynamo_error_fn, inductor_error_fn, inductor_schedule_fn, replace_dynamic | imports: copy, func",
    "role": "src",
    "loc": 896
  },
  {
    "id": "test\\dynamo\\test_subclasses.py",
    "summary": "No description | classes: BaseTorchFunction, MockSubclass, AttrSubclass, DummyNDim, WrapperSubclass, SigmoidToExpSubclass | functions: traceable_subclass, _check_recompiles, get_jagged_tensor, get_view_test_cases, mk_basic, mk_leaf | imports: functools, unittest, torch, dataclasses | [test dynamo te",
    "role": "src",
    "loc": 2476
  },
  {
    "id": "test\\dynamo\\test_subgraphs.py",
    "summary": "No description | classes: SubGraphTests | functions: indirectly_unsupported | imports: unittest, torch | [test dynamo test_subgraphs.py]",
    "role": "src",
    "loc": 483
  },
  {
    "id": "test\\dynamo\\test_sys.py",
    "summary": "No description | classes: SysTests, CPythonActiveExceptionTests | imports: unittest, torch | [test dynamo test_sys.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "test\\dynamo\\test_torchrec.py",
    "summary": "No description | classes: BucketizeMod, TorchRecTests | imports: unittest, torch, torchrec | [test dynamo test_torchrec.py]",
    "role": "src",
    "loc": 150
  },
  {
    "id": "test\\dynamo\\test_trace_rules.py",
    "summary": "Track the objects, object id - name pairs, and name - dynamo wrapping rule pairs | classes: AllowedObjects, weird, TraceRuleTests, TestModuleSurviveSkipFiles | functions: dump_allowed_torch_name_rule_map, gen_allowed_objs_and_ids, is_special_functions, heuristic_record_if_in_graph_function, _is_allo",
    "role": "src",
    "loc": 438
  },
  {
    "id": "test\\dynamo\\test_unspec.py",
    "summary": "No description | classes: TestModel, UnspecTests, UnspecTestsDevice | imports: random, unittest, numpy, torch | [test dynamo test_unspec.py]",
    "role": "src",
    "loc": 648
  },
  {
    "id": "test\\dynamo\\test_utils.py",
    "summary": "No description | classes: TestUtils, TestModel, TestDynamoTimed, TestObject, TestInductorConfigParsingForLogging | imports: dataclasses, pprint, unittest, torch | [test dynamo test_utils.py]",
    "role": "src",
    "loc": 453
  },
  {
    "id": "test\\dynamo\\test_verify_correctness.py",
    "summary": "No description | classes: Seq, Conv_Bn_Relu, TestVerifyCorrectness | functions: toy_example, transform | imports: operator, torch | [test dynamo test_verify_correctness.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\dynamo\\test_view.py",
    "summary": "No description | classes: ViewTests | imports: torch | [test dynamo test_view.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "test\\dynamo\\utils.py",
    "summary": "No description | functions: inner_func, outer_func, wrapped, add, create_dummy_module_and_function, install_guard_manager_testing_hook | imports: importlib, types, torch | [test dynamo utils.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "test\\dynamo\\__init__.py",
    "summary": "Package initializer | [test dynamo __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\dynamo\\mock_modules\\mock_module1.py",
    "summary": "No description | functions: method1 | [test dynamo mock_modules mock_module1.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "test\\dynamo\\mock_modules\\mock_module2.py",
    "summary": "No description | classes: Class1 | functions: method1 | imports: torch | [test dynamo mock_modules mock_module2.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "test\\dynamo\\mock_modules\\mock_module3.py",
    "summary": "No description | functions: method1 | imports: torch | [test dynamo mock_modules mock_module3.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\dynamo\\mock_modules\\__init__.py",
    "summary": "Package initializer | [test dynamo mock_modules __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\error_messages\\storage.py",
    "summary": "No description | functions: check_error, assign | imports: torch | [test error_messages storage.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\expect\\__init__.py",
    "summary": "Package initializer | [test expect __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\export\\opinfo_schema.py",
    "summary": "Dispatch mode built on top of SchemaCheckMode that checks for incorrect op schemas | classes: PreDispatchSchemaCheckMode, TestOpInfo | imports: torch | [test export opinfo_schema.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\export\\random_dag.py",
    "summary": "Util to generate a block of Python-formatted code. | classes: Block, TestGenerator, NNModuleGenerator, GenNNModule, Unflatten, ConstantUnflatten | functions: random_dag | imports: random | [test export random_dag.py]",
    "role": "src",
    "loc": 215
  },
  {
    "id": "test\\export\\testing.py",
    "summary": "No description | functions: make_test_cls_with_mocked_export, _make_fn_with_mocked_export, _fn, expectedFailureTrainingIRToRunDecomp, expectedFailureTrainingIRToRunDecompNonStrict, expectedFailureNonStrict | imports: functools, unittest, torch, test_export | [test export testing.py]",
    "role": "src",
    "loc": 270
  },
  {
    "id": "test\\export\\test_converter.py",
    "summary": "No description | classes: _FakeTensorQueue, MSingle, MMulti, MOutputList, MOutputTuple, MOutputDict | imports: unittest, torch | [test export test_converter.py]",
    "role": "src",
    "loc": 1119
  },
  {
    "id": "test\\export\\test_cpp_serdes.py",
    "summary": "No description | functions: mocked_cpp_serdes_export, make_dynamic_cls | imports: torch, test_export, testing | [test export test_cpp_serdes.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "test\\export\\test_db.py",
    "summary": "No description | classes: ExampleTests | imports: copy, unittest, torch | [test export test_db.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "test\\export\\test_draft_export.py",
    "summary": "No description | classes: FakeTensorQueue, M, Foo, Model, TestDraftExport | imports: copy, tempfile, unittest, torch | [test export test_draft_export.py]",
    "role": "src",
    "loc": 455
  },
  {
    "id": "test\\export\\test_experimental.py",
    "summary": "No description | classes: M, MTuple, MList, MDict, Module, Net | imports: unittest, torch | [test export test_experimental.py]",
    "role": "src",
    "loc": 283
  },
  {
    "id": "test\\export\\test_export.py",
    "summary": "No description | classes: Inp1, Inp2, Inp3, Module, InvalidInputConflictWithInputConstraints, Slice | functions: returns_tensor_symint_impl, foo_impl, foo_abstract, foo_mutated, foo_functional, foo_unbacked | imports: copy, dataclasses, operator, unittest | [test export test_export.py]",
    "role": "src",
    "loc": 10530
  },
  {
    "id": "test\\export\\test_export_legacy.py",
    "summary": "No description | functions: mocked_legacy_export, mocked_legacy_export_non_strict, make_dynamic_cls | imports: torch, test_export, testing, pyjk | [test export test_export_legacy.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "test\\export\\test_export_nonstrict.py",
    "summary": "No description | functions: mocked_non_strict_export, make_dynamic_cls | imports: test_export, testing, torch | [test export test_export_nonstrict.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "test\\export\\test_export_training_ir_to_run_decomp.py",
    "summary": "No description | functions: mocked_training_ir_to_run_decomp_export_strict, mocked_training_ir_to_run_decomp_export_non_strict, make_dynamic_cls | imports: torch, test_export, testing | [test export test_export_training_ir_to_run_decomp.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "test\\export\\test_functionalized_assertions.py",
    "summary": "No description | classes: TestFuntionalAssertions | imports: torch | [test export test_functionalized_assertions.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "test\\export\\test_hop.py",
    "summary": "No description | classes: Foo, TestHOP | imports: copy, io, unittest, torch | [test export test_hop.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "test\\export\\test_lift_unlift.py",
    "summary": "No description | classes: GraphBuilder, Foo, TestLift, ConstantAttrMapTest | imports: unittest, torch | [test export test_lift_unlift.py]",
    "role": "src",
    "loc": 333
  },
  {
    "id": "test\\export\\test_passes.py",
    "summary": "PYTEST_DONT_REWRITE (prevents pytest from rewriting assertions, which interferes | classes: _AddOperatorSupport, _AtenAddOperatorSupport, Simple, SimpleWithAttrInContainer, NestedWithAttrInContainer, MoreNestedWithAttrInContainer | functions: count_call_function, _to_partition_names, _get_output_nam",
    "role": "src",
    "loc": 1238
  },
  {
    "id": "test\\export\\test_pass_infra.py",
    "summary": "No description | classes: Foo, NullPass, M, CustomModule, TestPassInfra | imports: copy, unittest, torch, functorch | [test export test_pass_infra.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "test\\export\\test_retraceability.py",
    "summary": "No description | functions: mocked_retraceability_export_strict, mocked_retraceability_export_non_strict, make_dynamic_cls | imports: test_export, testing, torch | [test export test_retraceability.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\export\\test_schema.py",
    "summary": "No description | classes: TestSchema | imports: torch | [test export test_schema.py]",
    "role": "src",
    "loc": 365
  },
  {
    "id": "test\\export\\test_serdes.py",
    "summary": "No description | functions: mocked_serder_export_strict, mocked_serder_export_non_strict, make_dynamic_cls | imports: io, test_export, testing, torch | [test export test_serdes.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\export\\test_serialize.py",
    "summary": "PYTEST_DONT_REWRITE (prevents pytest from rewriting assertions, which interferes | classes: TestModule, FooExtensionOp, ExtensionVerifier, FooExtensionHandler, Foo, MyModule | functions: get_filtered_export_db_tests | imports: copy, io, tempfile, unittest | [test export test_serialize.py]",
    "role": "src",
    "loc": 1316
  },
  {
    "id": "test\\export\\test_sparse.py",
    "summary": "No description | classes: IdNet, SumNet, EltwiseNet, ToDenseNet, AddNet, SparseActivationCOO | functions: all_sparse_layouts | imports: unittest, torch | [test export test_sparse.py]",
    "role": "src",
    "loc": 214
  },
  {
    "id": "test\\export\\test_swap.py",
    "summary": "No description | classes: NestedChild, Child1, Child2, MyModule, M1, M | imports: unittest, dataclasses, parameterized, torch | [test export test_swap.py]",
    "role": "src",
    "loc": 340
  },
  {
    "id": "test\\export\\test_tools.py",
    "summary": "No description | classes: Module, Unsupported, Supported, TestExportTools | functions: op_missing_meta | imports: torch | [test export test_tools.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "test\\export\\test_torchbind.py",
    "summary": "No description | classes: FakeFoo, FakeTensorQueue, MyModule, F2, F1, F3 | functions: _assertEqualSkipScriptObject, _check_script_obj_equal, _assertEqualScriptObject | imports: copy, unittest, torch | [test export test_torchbind.py]",
    "role": "src",
    "loc": 1330
  },
  {
    "id": "test\\export\\test_tree_utils.py",
    "summary": "No description | classes: TestTreeUtils | imports: torch | [test export test_tree_utils.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\export\\test_unflatten.py",
    "summary": "No description | classes: NestedChild, Child1, Child2, MyModule, Child, Shared | imports: copy, unittest, torch | [test export test_unflatten.py]",
    "role": "src",
    "loc": 752
  },
  {
    "id": "test\\export\\test_unflatten_training_ir.py",
    "summary": "No description | functions: mocked_training_ir_export, make_dynamic_cls | imports: test_unflatten, testing, torch | [test export test_unflatten_training_ir.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "test\\export\\test_verifier.py",
    "summary": "No description | classes: Foo, M, TestVerifier | imports: unittest, torch, functorch | [test export test_verifier.py]",
    "role": "src",
    "loc": 161
  },
  {
    "id": "test\\export\\__init__.py",
    "summary": "Package initializer | [test export __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\forward_backward_compatibility\\check_forward_backward_compatibility.py",
    "summary": "No description | functions: allow_listed, has_valid_upgraders, dont_parse, load_schemas_to_dict, process_version_map, is_core_aten_op | imports: argparse, datetime, torch | [test forward_backward_compatibility check_forward_backward_compatibility.py]",
    "role": "src",
    "loc": 276
  },
  {
    "id": "test\\forward_backward_compatibility\\dump_all_function_schemas.py",
    "summary": "No description | functions: dump | imports: argparse, torch | [test forward_backward_compatibility dump_all_function_schemas.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "test\\functorch\\attn_ft.py",
    "summary": "No description | classes: Linear, BertSelfAttention | imports: torch, functorch | [test functorch attn_ft.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "test\\functorch\\attn_positional.py",
    "summary": "No description | classes: BertSelfAttention | imports: torch | [test functorch attn_positional.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "test\\functorch\\common_utils.py",
    "summary": "No description | classes: DisableVmapFallback | functions: loop, loop2, is_valid_inplace_sample_input, memoize, wrapped, get_bdim_choices | imports: unittest, functorch_additional_op_db, torch, functorch | [test functorch common_utils.py]",
    "role": "src",
    "loc": 468
  },
  {
    "id": "test\\functorch\\discover_coverage.py",
    "summary": "No description | classes: Status, Support, Operator, OperatorSet | functions: get_public_overridable_apis, get_method_only_ops_we_care_about, get_public_overridable_ops, get_public_overridable_outplace_ops, get_public_overridable_outplace_we_care_about, get_op | imports: copy, pprint, unittest, test",
    "role": "src",
    "loc": 727
  },
  {
    "id": "test\\functorch\\functorch_additional_op_db.py",
    "summary": "No description | functions: sample_inputs_conv2d, sample_inputs_embedding, make_input, make_long_input, generator, sample_inputs_mse_loss | imports: unittest, functools, torch | [test functorch functorch_additional_op_db.py]",
    "role": "src",
    "loc": 707
  },
  {
    "id": "test\\functorch\\test_ac.py",
    "summary": "No description | classes: MemoryBudgetTest | functions: compile_with_ac, get_act_mem, get_bw_flops, create_pair, get_mem_and_flops | imports: random, unittest, torch, triton | [test functorch test_ac.py]",
    "role": "src",
    "loc": 286
  },
  {
    "id": "test\\functorch\\test_ac_knapsack.py",
    "summary": "Test class for GraphInfoProvider. | classes: TestGraphInfoProvider, TestKnapsackEvaluator | imports: torch | [test functorch test_ac_knapsack.py]",
    "role": "src",
    "loc": 289
  },
  {
    "id": "test\\functorch\\test_ac_logging.py",
    "summary": "No description | classes: TestAcLogging | imports: unittest, torch | [test functorch test_ac_logging.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "test\\functorch\\test_aotdispatch.py",
    "summary": "No description | classes: AOTTestCase, TestPythonKey, F, Foo, TracableCreateParameter, BwMutation | functions: get_base, is_in_base, skipIfDynamoInput, decorator, wrapper, extract_graph | imports: copy, unittest, functools, common_utils | [test functorch test_aotdispatch.py]",
    "role": "src",
    "loc": 5442
  },
  {
    "id": "test\\functorch\\test_control_flow.py",
    "summary": "No description | classes: Nested, SimpleWithLinear, SimpleWithPytreeCarry, NestedWithLinear, PytreeIntCarry, IntCarry | functions: to_fun, from_fun, to_fun_old, from_fun_old, _fake_map, _fake_while_loop | imports: functools, unittest, torch, functorch | [test functorch test_control_flow.py]",
    "role": "src",
    "loc": 6509
  },
  {
    "id": "test\\functorch\\test_dims.py",
    "summary": "No description | classes: Foo, TestMin, TestMinFunctorchOnly | functions: magic_trace, measure, triu, gpu_time | imports: gc, unittest, attn_ft, attn_positional | [test functorch test_dims.py]",
    "role": "src",
    "loc": 566
  },
  {
    "id": "test\\functorch\\test_eager_transforms.py",
    "summary": "No description | classes: VmapTearDownMixin, TestSliceArgnums, MLPClassifier, TestGradTransform, A, NumpyCubeNotComposable | functions: _get_weights_and_functional_call, net_func, _get_weights_and_functional_call_with_buffers, normalize_devices, construct_sum_pyop, mysum_batch_rule | imports: copy, ",
    "role": "src",
    "loc": 4109
  },
  {
    "id": "test\\functorch\\test_logging.py",
    "summary": "No description | classes: TestAOTLogging | imports: torch | [test functorch test_logging.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "test\\functorch\\test_memory_efficient_fusion.py",
    "summary": "No description | classes: T5LayerNorm, TestMemoryEfficientOpAuthoring, NoChangeTestCase, ReduceTestCase, RandomOpTestCase | functions: _num_args, gelu_bias, swish, mish, hard_sigmoid, hard_swish | imports: inspect, random, unittest, torch | [test functorch test_memory_efficient_fusion.py]",
    "role": "src",
    "loc": 277
  },
  {
    "id": "test\\functorch\\test_minifier.py",
    "summary": "No description | classes: MockModule, TestMinifier | imports: torch, functorch | [test functorch test_minifier.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "test\\functorch\\test_ops.py",
    "summary": "No description | classes: TestOperators | functions: _autograd_grad, diff_arg, is_differentiable_arg, normalize_op_input_output2, wrapped, normalize_op_input_output3 | imports: functools, unittest, common_utils, functorch_additional_op_db | [test functorch test_ops.py]",
    "role": "src",
    "loc": 2528
  },
  {
    "id": "test\\functorch\\test_parsing.py",
    "summary": "Adapted from https://github.com/arogozhnikov/einops/blob/230ac1526c1f42c9e1f7373912c7f8047496df11/tests/test_parsing.py. | classes: TestAnonymousAxis, TestParsedExpression, TestParsingUtils, MaliciousRepr, TestValidateRearrangeExpressions | functions: mock_anonymous_axis_eq | imports: unittest, func",
    "role": "src",
    "loc": 245
  },
  {
    "id": "test\\functorch\\test_rearrange.py",
    "summary": "Adapted from https://github.com/arogozhnikov/einops/blob/230ac1526c1f42c9e1f7373912c7f8047496df11/tests/test_ops.py. | classes: TestRearrange | imports: numpy, torch, functorch | [test functorch test_rearrange.py]",
    "role": "src",
    "loc": 156
  },
  {
    "id": "test\\functorch\\test_vmap.py",
    "summary": "No description | classes: EnableVmapFallbackWarnings, TestVmapAPI, TensorFactory, TestVmapBase, Namespace, TestVmapOperators | functions: get_platform_specific_sdpa, slice_inputs, reference_vmap, _vmap_test, should_allow_vmap_fallback_usage, allowVmapFallbackUsage | imports: functools, random, types",
    "role": "src",
    "loc": 4994
  },
  {
    "id": "test\\functorch\\test_vmap_registrations.py",
    "summary": "No description | classes: TestFunctorchDispatcher | functions: dispatch_registrations, filter_vmap_implementable | imports: unittest, torch | [test functorch test_vmap_registrations.py]",
    "role": "src",
    "loc": 303
  },
  {
    "id": "test\\functorch\\xfail_suggester.py",
    "summary": "No description | functions: get_failed_test, remove_device_dtype, belongs_to_base, parse_namespace, get_torch_module, parse_base | imports: torch | [test functorch xfail_suggester.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "test\\fx\\named_tup.py",
    "summary": "No description | classes: MyNamedTup | imports: torch | [test fx named_tup.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\fx\\quantization.py",
    "summary": "**This file is EXPERIMENTAL and is mostly used for testing purposes! Do not | classes: MinMaxObserver, NoObserver, Add, Relu, CopyNode, IdentityModule | functions: _minmax_scale_zeropoint, register_pattern, insert, _parent_name, matches | imports: operator, torch | [test fx quantization.py]",
    "role": "src",
    "loc": 292
  },
  {
    "id": "test\\fx\\test_common_passes.py",
    "summary": "No description | classes: TestCommonPass | functions: FactoryFunctionCall, TorchTensorCall, TakeList, ReturnList, Mutation, MutationInput | imports: torch | [test fx test_common_passes.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "test\\fx\\test_cse_pass.py",
    "summary": "check if the CSE modified graph of ``f`` | classes: TestCSEPass | functions: check | imports: random, torch | [test fx test_cse_pass.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "test\\fx\\test_dce_pass.py",
    "summary": "No description | classes: TestTracer, TestModule, ReLUImpure, TestDCE | imports: copy, unittest, torch | [test fx test_dce_pass.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "test\\fx\\test_future.py",
    "summary": "No description | classes: A, M1, M2, M3, M4 | imports: torch | [test fx test_future.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "test\\fx\\test_fx_const_fold.py",
    "summary": "No description | classes: ConstFoldTestModule, TracedThroughModule, TestConstFold | imports: operator, torch | [test fx test_fx_const_fold.py]",
    "role": "src",
    "loc": 542
  },
  {
    "id": "test\\fx\\test_fx_node_hook.py",
    "summary": "No description | classes: TestFXNodeHook | imports: torch | [test fx test_fx_node_hook.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "test\\fx\\test_fx_param_shape_control_flow.py",
    "summary": "No description | classes: MyModuleBase, MyModuleParamShape, MyModuleParamSize, MyModuleParamDim, MyModuleParamNDim, MyModuleParamNumEl | imports: unittest, torch | [test fx test_fx_param_shape_control_flow.py]",
    "role": "src",
    "loc": 113
  },
  {
    "id": "test\\fx\\test_fx_split.py",
    "summary": "No description | classes: TestModule, TestFXSplit, TestSplitByTags, TestSplitOutputType | imports: torch | [test fx test_fx_split.py]",
    "role": "src",
    "loc": 185
  },
  {
    "id": "test\\fx\\test_fx_traceback.py",
    "summary": "No description | classes: Model, TestFXNodeSource | imports: torch | [test fx test_fx_traceback.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "test\\fx\\test_fx_xform_observer.py",
    "summary": "No description | classes: M, SimpleLinearModel, TestGraphTransformObserver | imports: copy, tempfile, torch | [test fx test_fx_xform_observer.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "test\\fx\\test_gradual_type.py",
    "summary": "3x3 convolution with padding | classes: M, AnnotationsTest, BasicBlock, TypeCheckerTest | functions: conv3x3 | imports: unittest, sympy, torch, torchvision | [test fx test_gradual_type.py]",
    "role": "src",
    "loc": 973
  },
  {
    "id": "test\\fx\\test_graph_pickler.py",
    "summary": "No description | classes: GraphPicklerCpuTests, TestGraphPickler | functions: make_test_cls | imports: importlib, unittest, torch, inductor | [test fx test_graph_pickler.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "test\\fx\\test_lazy_graph_module.py",
    "summary": "No description | classes: SimpleTest, TestLazyGraphModule | imports: pickle, io, unittest, torch | [test fx test_lazy_graph_module.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "test\\fx\\test_matcher_utils.py",
    "summary": "No description | classes: WrapperModule, LargeModel, PatternModel, M, Pattern, TestMatcher | imports: torch, unittest | [test fx test_matcher_utils.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "test\\fx\\test_partitioner_order.py",
    "summary": "No description | classes: DummyDevOperatorSupport, DummyPartitioner, AddModule, TestPartitionerOrder | imports: unittest, torch | [test fx test_partitioner_order.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "test\\fx\\test_pass_infra.py",
    "summary": "No description | classes: ReplaceAddWithMulPass, ReplaceDivWithSubPass, AddModule, TestPassManager | functions: replace_mul_with_div_pass, replace_sub_with_add_pass | imports: torch | [test fx test_pass_infra.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "test\\fx\\test_shape_inference.py",
    "summary": "No description | classes: TestModule, TestShapeInference | imports: copy, unittest, torch | [test fx test_shape_inference.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "test\\fx\\test_source_matcher_utils.py",
    "summary": "No description | classes: M, FunctionalConv2d, TestSourceMatcher | imports: unittest, torch | [test fx test_source_matcher_utils.py]",
    "role": "src",
    "loc": 405
  },
  {
    "id": "test\\fx\\test_subgraph_rewriter.py",
    "summary": "No description | classes: M, Pattern, Replacement, Comparison, M1, M2 | functions: wrapped_gemm_bias_mul, wrapped_gemm_bias_mul_with_c | imports: torch | [test fx test_subgraph_rewriter.py]",
    "role": "src",
    "loc": 784
  },
  {
    "id": "test\\fx\\test_z3_gradual_types.py",
    "summary": "No description | classes: BasicBlock, TorchDynamoUseCases, HFOperations, ComposeOperationsGradualTypes, GradualTypes, BasicBlock2 | imports: operator, unittest, torch, z3 | [test fx test_z3_gradual_types.py]",
    "role": "src",
    "loc": 2013
  },
  {
    "id": "test\\higher_order_ops\\test_invoke_quant.py",
    "summary": "No description | classes: TestInvokeQuant, TestInvokeQuantEager, TestInvokeQuantAotEager, TestInvokeQuantInductor | imports: unittest, torch | [test higher_order_ops test_invoke_quant.py]",
    "role": "src",
    "loc": 179
  },
  {
    "id": "test\\higher_order_ops\\test_invoke_subgraph.py",
    "summary": "No description | classes: TestInvokeSubgraph, CustomOp, Mod, SubMod, TestInvokeSubgraphCompile | imports: unittest, torch, functorch | [test higher_order_ops test_invoke_subgraph.py]",
    "role": "src",
    "loc": 518
  },
  {
    "id": "test\\higher_order_ops\\test_with_effects.py",
    "summary": "No description | classes: M, MyLinear, MockModule, TestWithEffects | functions: extract_graph, get_fw_bw_graph, fn_req_grad, make_inputs_non_leaves | imports: unittest, functools, torch, functorch | [test higher_order_ops test_with_effects.py]",
    "role": "src",
    "loc": 754
  },
  {
    "id": "test\\inductor\\indirect_assert_helper.py",
    "summary": "No description | functions: first_arg, second_arg, same_pm_one, same_pp_one, store, upper1 | imports: torch | [test inductor indirect_assert_helper.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\inductor\\minifier_smoke.py",
    "summary": "No description | functions: func, run_internal_minifier | imports: torch | [test inductor minifier_smoke.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "test\\inductor\\opinfo_harness.py",
    "summary": "No description | imports: subprocess, torch | [test inductor opinfo_harness.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\inductor\\s429861_repro.py",
    "summary": "No description | functions: forward | imports: torch | [test inductor s429861_repro.py]",
    "role": "src",
    "loc": 4604
  },
  {
    "id": "test\\inductor\\test_aot_inductor.py",
    "summary": "No description | classes: Model, Foo, LinearModel, Repro, Module, M | functions: fail_cpu, fail_gpu | imports: tempfile, unittest, torch, triton | [test inductor test_aot_inductor.py]",
    "role": "src",
    "loc": 3791
  },
  {
    "id": "test\\inductor\\test_aot_inductor_arrayref.py",
    "summary": "No description | classes: AOTInductorTestABICompatibleCpuWithStackAllocation, AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface | functions: fail_stack_allocation, fail_minimal_arrayref_interface | imports: unittest, torch, test_aot_inductor, test_torchinductor | [test in",
    "role": "src",
    "loc": 151
  },
  {
    "id": "test\\inductor\\test_aot_inductor_custom_ops.py",
    "summary": "No description | classes: M, MyModel, Model, TestRefMode, AOTInductorTestsTemplate, Foo | functions: fn_with_incorrect_optional_tensor, fn_with_incorrect_optional_tensor_fake, fn_ret_list_of_single_tensor, _, fn_ret_single_tensor, fail_cpu | imports: unittest, torch, test_aot_inductor_utils, test_to",
    "role": "src",
    "loc": 355
  },
  {
    "id": "test\\inductor\\test_aot_inductor_package.py",
    "summary": "No description | classes: Model, Model1, Model2, TestAOTInductorPackage | functions: skipif, decorator, wrapper, compile | imports: copy, functools, io, shutil | [test inductor test_aot_inductor_package.py]",
    "role": "src",
    "loc": 443
  },
  {
    "id": "test\\inductor\\test_aot_inductor_utils.py",
    "summary": "No description | classes: WrapperModule, AOTIRunnerUtil | functions: check_model, check_model_with_multiple_inputs, code_check_count | imports: copy, shutil, tempfile, types | [test inductor test_aot_inductor_utils.py]",
    "role": "src",
    "loc": 215
  },
  {
    "id": "test\\inductor\\test_async_compile.py",
    "summary": "No description | classes: TestAsyncCompile | imports: torch | [test inductor test_async_compile.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\inductor\\test_autoheuristic.py",
    "summary": "No description | classes: AutoHeuristicTest | imports: unittest, torch | [test inductor test_autoheuristic.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "test\\inductor\\test_auto_functionalize.py",
    "summary": "No description | classes: AutoFunctionalizeTests | imports: unittest, numpy, torch | [test inductor test_auto_functionalize.py]",
    "role": "src",
    "loc": 1462
  },
  {
    "id": "test\\inductor\\test_b2b_gemm.py",
    "summary": "No description | classes: B2BGEMMTest | imports: unittest, torch | [test inductor test_b2b_gemm.py]",
    "role": "src",
    "loc": 281
  },
  {
    "id": "test\\inductor\\test_benchmarking.py",
    "summary": "No description | classes: TestBenchmarker | imports: unittest, torch | [test inductor test_benchmarking.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "test\\inductor\\test_benchmark_fusion.py",
    "summary": "No description | classes: TestCase, BenchmarkFusionTestTemplate, BenchmarkFusionCudaTest, BenchmarkingTest, BenchmarkMultiTemplateFusionCudaTest, BenchmarkFusionCpuTest | imports: torch, unittest, inductor, torchvision | [test inductor test_benchmark_fusion.py]",
    "role": "src",
    "loc": 266
  },
  {
    "id": "test\\inductor\\test_binary_folding.py",
    "summary": "No description | classes: ConvOp, LinearOp, BinaryFoldingTemplate, FreezingCpuTests, FreezingGpuTests | imports: functools, importlib, torch, inductor | [test inductor test_binary_folding.py]",
    "role": "src",
    "loc": 294
  },
  {
    "id": "test\\inductor\\test_block_analysis.py",
    "summary": "No description | classes: BlockAnalysisTest | imports: sympy, torch | [test inductor test_block_analysis.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "test\\inductor\\test_ck_backend.py",
    "summary": "Get the PATH environment variable without sccache. | classes: TestCKBackend | functions: _get_path_without_sccache | imports: unittest, test_aot_inductor_utils, torch, test_fp8 | [test inductor test_ck_backend.py]",
    "role": "src",
    "loc": 358
  },
  {
    "id": "test\\inductor\\test_codecache.py",
    "summary": "No description | classes: MyModelConv2d, SimpleFunction, MyModel, MM, TestFxGraphCache, TestCustomGraphPass | imports: pickle, shutil, tempfile, unittest | [test inductor test_codecache.py]",
    "role": "src",
    "loc": 1357
  },
  {
    "id": "test\\inductor\\test_codegen_triton.py",
    "summary": "No description | classes: DummyModule, TestCodegenTriton | imports: sympy, torch | [test inductor test_codegen_triton.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\inductor\\test_combo_kernels.py",
    "summary": "No description | classes: ComboKernelTests, ComboKernelBenchmarkTests, ComboKernelDynamicShapesTests | imports: unittest, torch, test_torchinductor | [test inductor test_combo_kernels.py]",
    "role": "src",
    "loc": 444
  },
  {
    "id": "test\\inductor\\test_compiled_autograd.py",
    "summary": "No description | classes: Module, TestModel, UnreachableBwd, ModuleWithJit, Model, MySin | functions: make_compiler_fn, _compiler_fn, _inner_compiler, hook1, hook2, hook3 | imports: dataclasses, functools, io, subprocess | [test inductor test_compiled_autograd.py]",
    "role": "src",
    "loc": 3453
  },
  {
    "id": "test\\inductor\\test_compiled_optimizers.py",
    "summary": "No description | classes: KernelCounts, CompiledOptimizerParityTests, CompiledOptimizerTests | functions: get_inputs, update_exp_avg_sq, update_param, foreach_map_adam, create_scheduler, build_opt_kwarg_db | imports: unittest, weakref, copy, torch | [test inductor test_compiled_optimizers.py]",
    "role": "src",
    "loc": 753
  },
  {
    "id": "test\\inductor\\test_compile_worker.py",
    "summary": "No description | classes: TestCompileWorker | imports: operator, torch | [test inductor test_compile_worker.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "test\\inductor\\test_config.py",
    "summary": "No description | classes: DummyModule, TestInductorConfig | functions: dummy_fn | imports: unittest, torch | [test inductor test_config.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "test\\inductor\\test_control_flow.py",
    "summary": "No description | classes: Simple, SimpleWithIntClosure, Nested, InnerModel1, InnerModel2, Parameters | functions: _prepend_product_of_values, prepend_predicates, prepend_counters | imports: unittest, torch | [test inductor test_control_flow.py]",
    "role": "src",
    "loc": 1033
  },
  {
    "id": "test\\inductor\\test_cooperative_reductions.py",
    "summary": "No description | classes: TestingHeuristics, CooperativeReductionTests, NoPersistCooperativeReductionTests, MultiKernelCooperativeReductionTests, TestFixedConfigs | imports: unittest, sympy, torch | [test inductor test_cooperative_reductions.py]",
    "role": "src",
    "loc": 261
  },
  {
    "id": "test\\inductor\\test_coordinate_descent_tuner.py",
    "summary": "self is the CoordescTuner object | classes: TestCoordinateDescentTuner | functions: mock_compare_config_prefer_larger_XBLOCK | imports: unittest, torch, triton | [test inductor test_coordinate_descent_tuner.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "test\\inductor\\test_cpp_wrapper_hipify.py",
    "summary": "No description | classes: TestCppWrapperHipify | imports: torch | [test inductor test_cpp_wrapper_hipify.py]",
    "role": "src",
    "loc": 113
  },
  {
    "id": "test\\inductor\\test_cpu_cpp_wrapper.py",
    "summary": "No description | classes: CppWrapperTemplate, TestCppWrapper, DynamicShapesCppWrapperCpuTests, BaseTest | functions: make_test_case, fn | imports: unittest, torch, test_cpu_repro, test_cpu_select_algorithm | [test inductor test_cpu_cpp_wrapper.py]",
    "role": "src",
    "loc": 361
  },
  {
    "id": "test\\inductor\\test_cpu_repro.py",
    "summary": "No description | classes: LstmModule, RecordFunctions, Model, M, MaskedConv2d, Mod | functions: _can_check_vec_metrics, check_metrics_vec_kernel_count, simd_lengths_to_test, set_num_threads | imports: copy, functools, platform, unittest | [test inductor test_cpu_repro.py]",
    "role": "src",
    "loc": 4407
  },
  {
    "id": "test\\inductor\\test_cpu_select_algorithm.py",
    "summary": "No description | classes: BaseTestSelectAlgorithm, M, Mod, TestSelectAlgorithm, _DynamicShapesTestBase, TestSelectAlgorithmDynamicShapes | functions: patches, skip_cache, wrapped, verify, _get_epilogue | imports: functools, unittest, torch, test_cpu_repro | [test inductor test_cpu_select_algorithm.p",
    "role": "src",
    "loc": 2369
  },
  {
    "id": "test\\inductor\\test_cudacodecache.py",
    "summary": "No description | classes: TestCUDACodeCache | imports: ctypes, unittest, torch | [test inductor test_cudacodecache.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "test\\inductor\\test_cudagraph_trees.py",
    "summary": "Replace sys.stderr with a temporary StringIO | classes: capture_stderr, TestCase, Model, Mod, CloneCounterMode, AliasMod | functions: get_compile_fn, cdata, get_all_cudagraph_segments, all_live_blocks, all_live_block_count | imports: functools, gc, importlib, unittest | [test inductor test_cudagraph",
    "role": "src",
    "loc": 1854
  },
  {
    "id": "test\\inductor\\test_cudagraph_trees_expandable_segments.py",
    "summary": "No description | imports: torch, test_cudagraph_trees, tools | [test inductor test_cudagraph_trees_expandable_segments.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "test\\inductor\\test_cuda_repro.py",
    "summary": "No description | classes: Repro, MyModule, Model, SelfAttention, ToyModel, CudaReproTests | imports: functools, gc, unittest, torch | [test inductor test_cuda_repro.py]",
    "role": "src",
    "loc": 1424
  },
  {
    "id": "test\\inductor\\test_custom_lowering.py",
    "summary": "No description | classes: TestCustomLowering | imports: functools, unittest, torch | [test inductor test_custom_lowering.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "test\\inductor\\test_custom_post_grad_passes.py",
    "summary": "No description | classes: TestCustomPassBase, _CustomPass, _ConvReLU, TestPostGradCustomPrePostPass | functions: change_cos_pass | imports: operator, torch | [test inductor test_custom_post_grad_passes.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "test\\inductor\\test_cutlass_backend.py",
    "summary": "Get the PATH environment variable without sccache. | classes: MyModel, TestCutlassBackend | functions: _get_path_without_sccache | imports: sysconfig, unittest, torch, test_aot_inductor_utils | [test inductor test_cutlass_backend.py]",
    "role": "src",
    "loc": 957
  },
  {
    "id": "test\\inductor\\test_debug_trace.py",
    "summary": "No description | classes: Model, ToyModel, TestDebugTrace | functions: filesize | imports: shutil, tempfile, unittest, torch | [test inductor test_debug_trace.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "test\\inductor\\test_decompose_mem_bound_mm.py",
    "summary": "No description | classes: MyModule, MyModule2, MyModule3, TestDecomposeMemMM | imports: torch | [test inductor test_decompose_mem_bound_mm.py]",
    "role": "src",
    "loc": 343
  },
  {
    "id": "test\\inductor\\test_dependencies.py",
    "summary": "No description | classes: DummyModule, TestDependencies | imports: torch | [test inductor test_dependencies.py]",
    "role": "src",
    "loc": 142
  },
  {
    "id": "test\\inductor\\test_distributed_patterns.py",
    "summary": "No description | classes: CustomObj, DistributedPatternTests | functions: init_fake_distributed, all_gather, reduce_scatter, fw_pre_hook, fw_post_hook, bw_pre_hook | imports: dataclasses, functools, torch | [test inductor test_distributed_patterns.py]",
    "role": "src",
    "loc": 391
  },
  {
    "id": "test\\inductor\\test_efficient_conv_bn_eval.py",
    "summary": "No description | classes: ConvOp, MultiUserConvOp, EfficientConvBNEvalTemplate, EfficientConvBNEvalCpuTests, EfficientConvBNEvalGpuTests | imports: copy, importlib, torch, inductor | [test inductor test_efficient_conv_bn_eval.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "test\\inductor\\test_extension_backend.py",
    "summary": "No description | classes: BaseExtensionBackendTests, ExtensionBackendTests | imports: unittest, torch, extension_backends, filelock | [test inductor test_extension_backend.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "test\\inductor\\test_external_callables.py",
    "summary": "No description | classes: MatMulModule, TestInductorExternalCallable | functions: matmul_cpu, matmul_dup, matmul_cuda | imports: unittest, torch | [test inductor test_external_callables.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "test\\inductor\\test_flex_attention.py",
    "summary": "Temporarily set the float32 matmul precision and restore it after the context is exited. | classes: SimpleAttention, ApplyMask, Repro, Attention, Model, TestModule | functions: temp_float32_matmul_precision, rmse, create_attention, create_block_mask_test, _causal, _rel_bias | imports: functools, ran",
    "role": "src",
    "loc": 4430
  },
  {
    "id": "test\\inductor\\test_flex_decoding.py",
    "summary": "No description | classes: TestFlexDecoding | functions: create_attention, create_block_mask_test, _causal, _generate_windowed, _windowed, _get_windowed_sdpa_mask | imports: functools, unittest, torch | [test inductor test_flex_decoding.py]",
    "role": "src",
    "loc": 1480
  },
  {
    "id": "test\\inductor\\test_foreach.py",
    "summary": "No description | classes: ForeachTests | functions: foreach_map_wrapper, wrapper, add_op, addrecip_op, addcmul_op, recipaddmul_op | imports: unittest, torch, test_torchinductor | [test inductor test_foreach.py]",
    "role": "src",
    "loc": 844
  },
  {
    "id": "test\\inductor\\test_fp8.py",
    "summary": "No description | classes: TestFP8Types, TestFP8Lowering | functions: _to_fp8_saturated, _amax_to_scale, _quantize_tensorwise, _quantize_rowwise, _fix_fp8_dtype_for_rocm | imports: functools, unittest, torch | [test inductor test_fp8.py]",
    "role": "src",
    "loc": 622
  },
  {
    "id": "test\\inductor\\test_fused_attention.py",
    "summary": "No description | classes: Model, TestSDPAPatternRewriterTemplate, SDPAPatternRewriterCudaTests, SDPAPatternRewriterCudaDynamicTests, SDPAPatternRewriterCpuTests, SDPAPatternRewriterCpuDynamicTests | functions: checkpoint_wrapper, inner | imports: functools, torch | [test inductor test_fused_attentio",
    "role": "src",
    "loc": 962
  },
  {
    "id": "test\\inductor\\test_fuzzer.py",
    "summary": "No description | classes: MyException, TestConfigFuzzer | functions: create_simple_test_model_cpu, test_fn, create_simple_test_model_gpu | imports: unittest, torch | [test inductor test_fuzzer.py]",
    "role": "src",
    "loc": 176
  },
  {
    "id": "test\\inductor\\test_fx_fusion.py",
    "summary": "No description | classes: TestModule, TestFxFusion | functions: chain_passes, parent_pass, count_call, count_call_function, count_call_method | imports: torch | [test inductor test_fx_fusion.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "test\\inductor\\test_gpu_cpp_wrapper.py",
    "summary": "No description | classes: GpuWrapperTemplate, TestGpuWrapper, DynamicShapesGpuWrapperGpuTests, BaseTest | functions: make_test_case, fn | imports: unittest, torch, test_combo_kernels, test_foreach | [test inductor test_gpu_cpp_wrapper.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "test\\inductor\\test_graph_transform_observer.py",
    "summary": "No description | classes: TestGraphTransformObserver | imports: glob, shutil, tempfile, torch | [test inductor test_graph_transform_observer.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\inductor\\test_group_batch_fusion.py",
    "summary": "No description | classes: TestHighwaySelfGating, MyModule, MyModule2, MyModule3, MyModule4, MyModule5 | imports: unittest, torch, deeplearning | [test inductor test_group_batch_fusion.py]",
    "role": "src",
    "loc": 1148
  },
  {
    "id": "test\\inductor\\test_halide.py",
    "summary": "No description | classes: HalideTests | functions: make_halide | imports: functools, textwrap, unittest, torch | [test inductor test_halide.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "test\\inductor\\test_indexing.py",
    "summary": "No description | classes: TestIndexingSimplification, ExprPrinterTests | imports: unittest, sympy, torch | [test inductor test_indexing.py]",
    "role": "src",
    "loc": 348
  },
  {
    "id": "test\\inductor\\test_inductor_annotations.py",
    "summary": "No description | classes: InductorAnnotationTestCase | imports: torch | [test inductor test_inductor_annotations.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "test\\inductor\\test_inductor_freezing.py",
    "summary": "No description | classes: TestCase, ConvBN, ConvBNHardswish, ConvFunctionalBN, ConvMultiBN, ConvMultiFunctionalBN | imports: functools, importlib, unittest, weakref | [test inductor test_inductor_freezing.py]",
    "role": "src",
    "loc": 760
  },
  {
    "id": "test\\inductor\\test_inductor_utils.py",
    "summary": "No description | classes: TestBench | imports: functools, torch | [test inductor test_inductor_utils.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "test\\inductor\\test_inplace_padding.py",
    "summary": "No description | classes: InplacePaddingTest | functions: num_inplace_padding | imports: unittest, torch, inductor, triton | [test inductor test_inplace_padding.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "test\\inductor\\test_inplacing_pass.py",
    "summary": "No description | classes: MySin, TestReinplacingPassCorrectness | functions: num_reinplacing_failures, miss_inplaced_bytes, sin, sin_cos, sin_kernel, sin_triton | imports: torch, functorch, triton | [test inductor test_inplacing_pass.py]",
    "role": "src",
    "loc": 358
  },
  {
    "id": "test\\inductor\\test_kernel_benchmark.py",
    "summary": "No description | classes: TestKernelBenchmark | imports: subprocess, unittest, torch | [test inductor test_kernel_benchmark.py]",
    "role": "src",
    "loc": 388
  },
  {
    "id": "test\\inductor\\test_layout_optim.py",
    "summary": "No description | classes: Model2Conv, Model, MyModel, TestLayoutOptim | imports: copy, random, torch | [test inductor test_layout_optim.py]",
    "role": "src",
    "loc": 268
  },
  {
    "id": "test\\inductor\\test_loop_ordering.py",
    "summary": "No description | classes: MockScheduler, ImplDetailTest, Model, LoopOrderingTest | imports: unittest, numpy, torch, triton | [test inductor test_loop_ordering.py]",
    "role": "src",
    "loc": 386
  },
  {
    "id": "test\\inductor\\test_max_autotune.py",
    "summary": "No description | classes: FailChoiceCaller, FakeChoiceCaller, M, ToyModel, TestMaxAutotune, Model | functions: _get_func_call, _get_kernel_launch, benchmark_choice | imports: unittest, torch, threading | [test inductor test_max_autotune.py]",
    "role": "src",
    "loc": 1210
  },
  {
    "id": "test\\inductor\\test_memory.py",
    "summary": "The default compiled graph is | classes: Foo, TestOperatorReorderForPeakMemory | imports: unittest, torch | [test inductor test_memory.py]",
    "role": "src",
    "loc": 195
  },
  {
    "id": "test\\inductor\\test_memory_planning.py",
    "summary": "No description | classes: Foo, TestMemoryPlanning | imports: unittest, torch, test_aot_inductor | [test inductor test_memory_planning.py]",
    "role": "src",
    "loc": 101
  },
  {
    "id": "test\\inductor\\test_metrics.py",
    "summary": "No description | classes: TestMetrics | imports: torch | [test inductor test_metrics.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "test\\inductor\\test_minifier.py",
    "summary": "No description | classes: MinifierTests | imports: unittest, torch | [test inductor test_minifier.py]",
    "role": "src",
    "loc": 250
  },
  {
    "id": "test\\inductor\\test_minifier_isolate.py",
    "summary": "No description | classes: MinifierIsolateTests | imports: unittest, torch | [test inductor test_minifier_isolate.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "test\\inductor\\test_minifier_utils.py",
    "summary": "No description | classes: SimpleModel, M, MinifierUtilsTests | imports: torch | [test inductor test_minifier_utils.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "test\\inductor\\test_mkldnn_pattern_matcher.py",
    "summary": "No description | classes: TestPatternMatcherBase, M, Model, Model_v1, Model_v2, Model_v3 | functions: get_default_quantizer, cal_conv_generated_kernel_number | imports: copy, unittest, torch | [test inductor test_mkldnn_pattern_matcher.py]",
    "role": "src",
    "loc": 3774
  },
  {
    "id": "test\\inductor\\test_mmdecomp.py",
    "summary": "Creates rand dense or nested tensor with given shape and type. | classes: TestDecomp | functions: rand_math_tensor, init_tensor, run_comp_nocomp, torch_mm, torch_addmm, torch_bmm | imports: unittest, torch | [test inductor test_mmdecomp.py]",
    "role": "src",
    "loc": 144
  },
  {
    "id": "test\\inductor\\test_move_constructors_to_cuda.py",
    "summary": "No description | classes: TestMoveConstructorsToCuda | imports: functools, unittest, torch | [test inductor test_move_constructors_to_cuda.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\inductor\\test_mps_basic.py",
    "summary": "No description | classes: MPSBasicTests | imports: importlib, torch, inductor | [test inductor test_mps_basic.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "test\\inductor\\test_multi_kernel.py",
    "summary": "No description | classes: TransformerSnippet, MultiKernelTest | functions: _contains_multi_kernel_code, make_cpp_wrapper_test, fn | imports: unittest, torch | [test inductor test_multi_kernel.py]",
    "role": "src",
    "loc": 229
  },
  {
    "id": "test\\inductor\\test_op_completeness.py",
    "summary": "No description | classes: TestOpCompleteness | imports: unittest, torch | [test inductor test_op_completeness.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "test\\inductor\\test_op_dtype_prop.py",
    "summary": "No description | classes: TestCase | imports: importlib, torch | [test inductor test_op_dtype_prop.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "test\\inductor\\test_ordered_set.py",
    "summary": "No description | classes: PassThru, BadCmp, ReprWrapper, HashCountingInt, Tracer, A | functions: check_pass_thru, baditer, gooditer, R, L, powerset | imports: copy, gc, operator, pickle | [test inductor test_ordered_set.py]",
    "role": "src",
    "loc": 1468
  },
  {
    "id": "test\\inductor\\test_padding.py",
    "summary": "It's very common that a transformer model will do a matmul and then | classes: LinearAndSoftmax, TestCaseBase, PerfTestBetweenGoodAndBadShape, Model, PerfTestWithAndWithoutPadding, Attention | functions: get_optim, gen_transformer_inputs, geninp, forward_and_backward_pass | imports: copy, functools,",
    "role": "src",
    "loc": 583
  },
  {
    "id": "test\\inductor\\test_pad_mm.py",
    "summary": "No description | classes: Model, PadMMTest | imports: unittest, torch | [test inductor test_pad_mm.py]",
    "role": "src",
    "loc": 459
  },
  {
    "id": "test\\inductor\\test_pattern_matcher.py",
    "summary": "No description | classes: Model, _CustomPass, TestPatternMatcher | imports: copy, unittest, torch | [test inductor test_pattern_matcher.py]",
    "role": "src",
    "loc": 1384
  },
  {
    "id": "test\\inductor\\test_perf.py",
    "summary": "No description | classes: TestCase, NumBytesMetricTests, FusionTests, SchedulerFusionTests, TilingTests, Foo | functions: compile_but_use_eager, inner_compile, count_numel, count_numel_train, T, TI | imports: unittest, functorch, torch, triton | [test inductor test_perf.py]",
    "role": "src",
    "loc": 948
  },
  {
    "id": "test\\inductor\\test_profiler.py",
    "summary": "No description | classes: DynamoProfilerTests | imports: json, tempfile, unittest, torch | [test inductor test_profiler.py]",
    "role": "src",
    "loc": 215
  },
  {
    "id": "test\\inductor\\test_provenance_tracing.py",
    "summary": "No description | classes: Model, TestProvenanceTracingArtifact, TestProvenanceTracingNodeMapping | imports: json, shutil, tempfile, torch | [test inductor test_provenance_tracing.py]",
    "role": "src",
    "loc": 231
  },
  {
    "id": "test\\inductor\\test_scatter_optimization.py",
    "summary": "No description | classes: TestScatterOpt | imports: copy, unittest, torch | [test inductor test_scatter_optimization.py]",
    "role": "src",
    "loc": 147
  },
  {
    "id": "test\\inductor\\test_select_algorithm.py",
    "summary": "No description | classes: TestSelectAlgorithm | functions: patches, skip_cache, wrapped | imports: functools, unittest, torch | [test inductor test_select_algorithm.py]",
    "role": "src",
    "loc": 303
  },
  {
    "id": "test\\inductor\\test_smoke.py",
    "summary": "No description | classes: MLP, SmokeTest | functions: _test_f | imports: unittest, torch | [test inductor test_smoke.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "test\\inductor\\test_snode_runtime.py",
    "summary": "No description | classes: TestCase, UnsupportedTests, ComputeBoundedTests, MemoryBoundedTests, TestCommAnalysis | functions: compile_but_use_eager, inner_compile, calculate_runtime, T | imports: unittest, torch | [test inductor test_snode_runtime.py]",
    "role": "src",
    "loc": 265
  },
  {
    "id": "test\\inductor\\test_split_cat_fx_aten_passes.py",
    "summary": "No description | classes: TestSplitCat, TestSelectCat, TestSplitCatAten | imports: torch, deeplearning | [test inductor test_split_cat_fx_aten_passes.py]",
    "role": "src",
    "loc": 141
  },
  {
    "id": "test\\inductor\\test_split_cat_fx_passes.py",
    "summary": "No description | classes: TestSplitCatFxPasses | functions: patch | imports: torch | [test inductor test_split_cat_fx_passes.py]",
    "role": "src",
    "loc": 1368
  },
  {
    "id": "test\\inductor\\test_standalone_compile.py",
    "summary": "No description | classes: MyModule, MyModule2, MyModule3, TestStandaloneInductor | imports: torch | [test inductor test_standalone_compile.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "test\\inductor\\test_torchbind.py",
    "summary": "No description | classes: M, TestTorchbind | imports: torch | [test inductor test_torchbind.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "test\\inductor\\test_torchinductor.py",
    "summary": "No description | classes: TestCase, ToTuple, InputGen, SweepInputs2, skip_if_cpp_wrapper, Net | functions: _large_cumprod_input, define_custom_op_for_test, define_custom_op_2_for_test, define_custom_op_3_for_test, register_ops_with_aoti_compile, get_divisible_by_16 | imports: copy, dataclasses, func",
    "role": "src",
    "loc": 11491
  },
  {
    "id": "test\\inductor\\test_torchinductor_codegen_config_overrides.py",
    "summary": "No description | classes: CodegenInductorTest | imports: importlib, unittest, torch | [test inductor test_torchinductor_codegen_config_overrides.py]",
    "role": "src",
    "loc": 102
  },
  {
    "id": "test\\inductor\\test_torchinductor_codegen_dynamic_shapes.py",
    "summary": "No description | classes: DynamicShapesCodegenCpuTests, DynamicShapesCodegenGPUTests | functions: check_codegen, copy_fn, compile_fx_wrapper, run | imports: importlib, torch, inductor | [test inductor test_torchinductor_codegen_dynamic_shapes.py]",
    "role": "src",
    "loc": 392
  },
  {
    "id": "test\\inductor\\test_torchinductor_dynamic_shapes.py",
    "summary": "No description | classes: DynamicShapesCpuTests, DynamicShapesGPUTests, TestWrapperCodegen, TestInductorDynamic | functions: make_dynamic_cls | imports: importlib, operator, unittest, functools | [test inductor test_torchinductor_dynamic_shapes.py]",
    "role": "src",
    "loc": 871
  },
  {
    "id": "test\\inductor\\test_torchinductor_opinfo.py",
    "summary": "No description | classes: HasRngOp, TestInductorOpInfo | functions: print_seen, fmt_dtypes, sort_key, maybe_truncate, format_op, get_skips_and_xfails | imports: atexit, functools, unittest, torch | [test inductor test_torchinductor_opinfo.py]",
    "role": "src",
    "loc": 969
  },
  {
    "id": "test\\inductor\\test_torchinductor_strided_blocks.py",
    "summary": "Runs the module through Inductor, comparing to eager reference. | classes: BlockPointerTestBase, CommonTemplate, TritonBlockPointerTestCPU, TritonBlockPointerTestGPU | functions: run_and_compare, flatten_tensors, count_code | imports: importlib, unittest, torch, test_torchinductor | [test inductor t",
    "role": "src",
    "loc": 787
  },
  {
    "id": "test\\inductor\\test_triton_cpu_backend.py",
    "summary": "No description | classes: SweepInputsCpuTritonTest, CpuTritonTests | imports: torch, test_torchinductor | [test inductor test_triton_cpu_backend.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "test\\inductor\\test_triton_extension_backend.py",
    "summary": "Test creating a backend for inductor with Triton scheduling. | classes: TritonExtensionBackendTests | functions: mock_triton_hash_with_backend | imports: random, string, unittest, torch | [test inductor test_triton_extension_backend.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "test\\inductor\\test_triton_heuristics.py",
    "summary": "No description | classes: TestTritonHeuristics, TestArgumentCloneAndRestore | imports: unittest, torch, triton | [test inductor test_triton_heuristics.py]",
    "role": "src",
    "loc": 214
  },
  {
    "id": "test\\inductor\\test_triton_kernels.py",
    "summary": "No description | classes: AddOne, C, D, KernelTests, MutationTests, _CustomPass | functions: _triton_get_ast_equal_to_str, make_mutation_test, test_fn, helper_id, helper_add_and_out | imports: functools, torch, triton, functorch | [test inductor test_triton_kernels.py]",
    "role": "src",
    "loc": 3454
  },
  {
    "id": "test\\inductor\\test_triton_syntax.py",
    "summary": "No description | classes: TestTritonSyntacticallyValid | imports: torch | [test inductor test_triton_syntax.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "test\\inductor\\test_triton_wrapper.py",
    "summary": "No description | classes: TestTritonWrapper | imports: subprocess, torch | [test inductor test_triton_wrapper.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "test\\inductor\\test_unbacked_symints.py",
    "summary": "No description | classes: CustomSliceSubclass, Model, TestUnbackedSymints | imports: functools, unittest, torch | [test inductor test_unbacked_symints.py]",
    "role": "src",
    "loc": 353
  },
  {
    "id": "test\\inductor\\test_utils.py",
    "summary": "No description | classes: TestUtils | imports: sympy, torch | [test inductor test_utils.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "test\\inductor\\test_xpu_basic.py",
    "summary": "No description | classes: XpuBasicTests | imports: importlib, torch, inductor | [test inductor test_xpu_basic.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "test\\inductor\\__init__.py",
    "summary": "Package initializer | [test inductor __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\inductor\\extension_backends\\cpp\\extension_codegen_backend.py",
    "summary": "No description | classes: ExtensionWrapperCodegen, ExtensionCppWrapperCodegen, ExtensionScheduling | imports: torch | [test inductor extension_backends cpp extension_codegen_backend.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\inductor\\extension_backends\\triton\\device_interface.py",
    "summary": "No description | classes: DeviceProperties, Event, device, Worker, DeviceInterface | imports: torch | [test inductor extension_backends triton device_interface.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "test\\inductor\\extension_backends\\triton\\extension_codegen_backend.py",
    "summary": "No description | classes: ExtensionWrapperCodegen, ExtensionScheduling, CPUDeviceOpOverrides | imports: torch | [test inductor extension_backends triton extension_codegen_backend.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\jit\\mydecorator.py",
    "summary": "Decorator used in test_decorator.py. We define it in a | functions: my_decorator, wrapped_func | imports: functools | [test jit mydecorator.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "test\\jit\\myexception.py",
    "summary": "Define exceptions used in test_exception.py. We define them in a | classes: MyKeyError | [test jit myexception.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\jit\\myfunction_a.py",
    "summary": "Helper function used in test_decorator.py. We define it in a | functions: my_function_a | imports: jit | [test jit myfunction_a.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "test\\jit\\myfunction_b.py",
    "summary": "Helper function used in test_decorator.py. We define it in a | functions: my_function_b, my_function_c | imports: jit | [test jit myfunction_b.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "test\\jit\\test_alias_analysis.py",
    "summary": "No description | classes: Mod, MultiTmpFile, ModuleWrapper, MyModuleCUTest, TestAliasAnalysis | imports: torch | [test jit test_alias_analysis.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "test\\jit\\test_async.py",
    "summary": "No description | classes: Mod, Traced, TupleCl, TestListFutureModule, TestModuleWrapper, DifferentOutputModule | imports: torch | [test jit test_async.py]",
    "role": "src",
    "loc": 417
  },
  {
    "id": "test\\jit\\test_aten_pow.py",
    "summary": "No description | classes: TestAtenPow | imports: torch | [test jit test_aten_pow.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "test\\jit\\test_attr.py",
    "summary": "No description | classes: A, MyTuple, TestGetDefaultAttr | imports: torch | [test jit test_attr.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\jit\\test_autodiff.py",
    "summary": "No description | classes: TestAutodiffJit | imports: torch | [test jit test_autodiff.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "test\\jit\\test_autodiff_subgraph_slicing.py",
    "summary": "No description | classes: M, TestAutodiffSubgraphSlicing | imports: unittest, torch | [test jit test_autodiff_subgraph_slicing.py]",
    "role": "src",
    "loc": 454
  },
  {
    "id": "test\\jit\\test_await.py",
    "summary": "No description | classes: C, Tree, TestAwait | imports: io, torch | [test jit test_await.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "test\\jit\\test_backends.py",
    "summary": "A simple Module used to test to_backend lowering machinery. | classes: BasicModule, JitBackendTestCase, BasicModuleTest, BasicModuleUnavailableTest, NestedModule, NestedModuleTest | functions: to_test_backend, to_test_backend_multi, to_test_backend_selective, _to_test_backend | imports: io, unittest",
    "role": "src",
    "loc": 607
  },
  {
    "id": "test\\jit\\test_backend_nnapi.py",
    "summary": "No description | classes: TestNnapiBackend | imports: unittest, torch, test_nnapi | [test jit test_backend_nnapi.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "test\\jit\\test_batch_mm.py",
    "summary": "No description | classes: TestBatchMM | imports: torch | [test jit test_batch_mm.py]",
    "role": "src",
    "loc": 261
  },
  {
    "id": "test\\jit\\test_builtins.py",
    "summary": "No description | classes: HasA, HasB, Mod, TestBuiltins, TestTensorBuiltins | imports: inspect, unittest, torch | [test jit test_builtins.py]",
    "role": "src",
    "loc": 221
  },
  {
    "id": "test\\jit\\test_class_type.py",
    "summary": "No description | classes: Foo, FooTest, MyMod, FooNestedTest, FooNestedTest2, NoMethod | imports: io, unittest, torch, jit | [test jit test_class_type.py]",
    "role": "src",
    "loc": 1223
  },
  {
    "id": "test\\jit\\test_complex.py",
    "summary": "No description | classes: ComplexModule, TestComplex | imports: cmath, textwrap, torch | [test jit test_complex.py]",
    "role": "src",
    "loc": 523
  },
  {
    "id": "test\\jit\\test_complexity.py",
    "summary": "No description | classes: TestComplexity | functions: num_ifs_loops, num_non_tensor_nodes | imports: unittest, torch | [test jit test_complexity.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "test\\jit\\test_convert_activation.py",
    "summary": "No description | classes: Test3, TestFunctionalToInplaceActivation, TestInplaceToFunctionalActivation | imports: unittest, torch, torchvision | [test jit test_convert_activation.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "test\\jit\\test_cuda.py",
    "summary": "No description | classes: Result, Model, TestCUDA | imports: gc, unittest, torch | [test jit test_cuda.py]",
    "role": "src",
    "loc": 504
  },
  {
    "id": "test\\jit\\test_custom_operators.py",
    "summary": "No description | classes: TestCustomOperators | functions: canonical | imports: unittest, torch | [test jit test_custom_operators.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "test\\jit\\test_dataclasses.py",
    "summary": "No description | classes: Point, MixupScheme, MixupParams, MixupScheme2, MixupParams2, MixupParams3 | imports: unittest, dataclasses, hypothesis, torch | [test jit test_dataclasses.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "test\\jit\\test_data_parallel.py",
    "summary": "No description | classes: Mpy, Mpy1, Mpy2, Msm, Msm1, TestDataParallel | imports: unittest, torch | [test jit test_data_parallel.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "test\\jit\\test_dce.py",
    "summary": "No description | classes: Net, Thing1, Thing2, TestDCE | imports: torch | [test jit test_dce.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "test\\jit\\test_decorator.py",
    "summary": "No description | classes: TestDecorator | imports: unittest, torch, jit | [test jit test_decorator.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "test\\jit\\test_device_analysis.py",
    "summary": "No description | classes: TestDeviceAnalysis | imports: unittest, torch, torchvision | [test jit test_device_analysis.py]",
    "role": "src",
    "loc": 252
  },
  {
    "id": "test\\jit\\test_dtype_analysis.py",
    "summary": "No description | classes: TestDtypeBase, TestDtypeAnalysis, TestDtypeCustomRules | imports: unittest, torch | [test jit test_dtype_analysis.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "test\\jit\\test_enum.py",
    "summary": "No description | classes: IntEnum, FloatEnum, StringEnum, TensorEnum, Color, Foo | imports: torch | [test jit test_enum.py]",
    "role": "src",
    "loc": 258
  },
  {
    "id": "test\\jit\\test_exception.py",
    "summary": "No description | classes: Foo, MyValueError, TestException | imports: torch, jit | [test jit test_exception.py]",
    "role": "src",
    "loc": 150
  },
  {
    "id": "test\\jit\\test_freezing.py",
    "summary": "No description | classes: M, SubModule, SubModule2, TestModule, FreezeMe, Obj | functions: removeExceptions | imports: io, unittest, torch, torchvision | [test jit test_freezing.py]",
    "role": "src",
    "loc": 2706
  },
  {
    "id": "test\\jit\\test_functional_blocks.py",
    "summary": "No description | classes: TestFunctionalBlocks | imports: torch | [test jit test_functional_blocks.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "test\\jit\\test_fuser_common.py",
    "summary": "No description | classes: TestFuserCommon | imports: torch | [test jit test_fuser_common.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "test\\jit\\test_generator.py",
    "summary": "No description | classes: Foo, TestGenerator | imports: io, unittest, torch | [test jit test_generator.py]",
    "role": "src",
    "loc": 130
  },
  {
    "id": "test\\jit\\test_graph_rewrite_passes.py",
    "summary": "No description | classes: FunctionalLinear, Matmul, TestGraphRewritePasses | imports: torch | [test jit test_graph_rewrite_passes.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "test\\jit\\test_hash.py",
    "summary": "No description | classes: TestHash | imports: torch | [test jit test_hash.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\jit\\test_hooks.py",
    "summary": "No description | classes: TestHooks | imports: unittest, torch, jit | [test jit test_hooks.py]",
    "role": "src",
    "loc": 290
  },
  {
    "id": "test\\jit\\test_hooks_modules.py",
    "summary": "No description | classes: SubmoduleNoForwardInputs, ModuleNoForwardInputs, SubmoduleForwardSingleInput, ModuleForwardSingleInput, ModuleDirectforwardSubmodCall, SuboduleForwardMultipleInputs | functions: create_module_no_forward_input, pre_hook, forward_hook, create_submodule_no_forward_input, creat",
    "role": "src",
    "loc": 352
  },
  {
    "id": "test\\jit\\test_ignorable_args.py",
    "summary": "No description | classes: TestIgnorableArgs | imports: torch | [test jit test_ignorable_args.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "test\\jit\\test_ignore_context_manager.py",
    "summary": "No description | classes: A, B, C, TestIgnoreContextManager | imports: unittest, torch | [test jit test_ignore_context_manager.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "test\\jit\\test_isinstance.py",
    "summary": "No description | classes: TestIsinstance | imports: torch | [test jit test_isinstance.py]",
    "role": "src",
    "loc": 267
  },
  {
    "id": "test\\jit\\test_jit_utils.py",
    "summary": "No description | classes: TestJitUtils | imports: textwrap, torch | [test jit test_jit_utils.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\jit\\test_list_dict.py",
    "summary": "No description | classes: TestList, M, TestDict, FeatureVector, Tup, Config | imports: inspect, types, unittest, textwrap | [test jit test_list_dict.py]",
    "role": "src",
    "loc": 2244
  },
  {
    "id": "test\\jit\\test_logging.py",
    "summary": "No description | classes: ModuleThatLogs, ModuleThatTimes, TestLogging | imports: torch | [test jit test_logging.py]",
    "role": "src",
    "loc": 99
  },
  {
    "id": "test\\jit\\test_misc.py",
    "summary": "No description | classes: M, OneTwoModule, FooMod, BarMod, TestMisc | imports: unittest, torch, jit | [test jit test_misc.py]",
    "role": "src",
    "loc": 380
  },
  {
    "id": "test\\jit\\test_models.py",
    "summary": "No description | classes: MnistNet, DCGANGenerator, DCGANDiscriminator, TransformerNet, ConvLayer, ResidualBlock | imports: unittest, torch, torchvision | [test jit test_models.py]",
    "role": "src",
    "loc": 609
  },
  {
    "id": "test\\jit\\test_modules.py",
    "summary": "No description | classes: Net, TestModules | imports: torch | [test jit test_modules.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "test\\jit\\test_module_apis.py",
    "summary": "No description | classes: DefaultStateDictModule, CustomStateDictModule, ParentModule, TestModuleAPIs | imports: torch | [test jit test_module_apis.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "test\\jit\\test_module_containers.py",
    "summary": "No description | classes: A, B, C, Inner, Inner2, Inner3 | imports: torch | [test jit test_module_containers.py]",
    "role": "src",
    "loc": 581
  },
  {
    "id": "test\\jit\\test_module_interface.py",
    "summary": "No description | classes: OrigModule, NewModule, ModuleInterface, TestNotModuleInterfaceCall, OneTwoModule, OneTwoClass | imports: torch | [test jit test_module_interface.py]",
    "role": "src",
    "loc": 519
  },
  {
    "id": "test\\jit\\test_optimize_for_mobile_preserve_debug_info.py",
    "summary": "No description | classes: TestConv1d, TestPrepackedLinearBeforeInlineAndConv2dOp, TestFuseActivationLinearConv2d, TestOptimizeForMobilePreserveDebugInfo | imports: torch | [test jit test_optimize_for_mobile_preserve_debug_info.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "test\\jit\\test_op_decompositions.py",
    "summary": "No description | classes: TestOpDecompositions | imports: torch | [test jit test_op_decompositions.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\jit\\test_parametrization.py",
    "summary": "No description | classes: Symmetric, TestParametrization | imports: torch | [test jit test_parametrization.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "test\\jit\\test_pdt.py",
    "summary": "No description | classes: TestPDTModel, NestedPDTInner, NestedModulePDTWrapper, NestedModulePDTInner, NestedModulePDTOuter, NestedFunctionInForward | imports: torch | [test jit test_pdt.py]",
    "role": "src",
    "loc": 806
  },
  {
    "id": "test\\jit\\test_peephole.py",
    "summary": "No description | classes: ConvDim, ConvDimMutate, TestPeephole | imports: unittest, torch | [test jit test_peephole.py]",
    "role": "src",
    "loc": 685
  },
  {
    "id": "test\\jit\\test_profiler.py",
    "summary": "No description | classes: TestProfiler | imports: torch | [test jit test_profiler.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "test\\jit\\test_python_bindings.py",
    "summary": "No description | classes: TestPythonBindings | imports: torch | [test jit test_python_bindings.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "test\\jit\\test_python_builtins.py",
    "summary": "No description | classes: TestPythonBuiltinOP | functions: get_fn | imports: random, tempfile, textwrap, torch | [test jit test_python_builtins.py]",
    "role": "src",
    "loc": 351
  },
  {
    "id": "test\\jit\\test_python_ir.py",
    "summary": "No description | classes: TestPythonIr | imports: unittest, numpy, torch | [test jit test_python_ir.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "test\\jit\\test_recursive_script.py",
    "summary": "No description | classes: M, MyModule, M1, M2, M3, MyScriptClass | imports: types, typing_extensions, torch, pdb | [test jit test_recursive_script.py]",
    "role": "src",
    "loc": 606
  },
  {
    "id": "test\\jit\\test_remove_mutation.py",
    "summary": "No description | classes: OpMod, TestRemoveMutation | imports: torch | [test jit test_remove_mutation.py]",
    "role": "src",
    "loc": 250
  },
  {
    "id": "test\\jit\\test_save_load.py",
    "summary": "No description | classes: Foo, ContainsBoth, MyInterface, ImplementInterface, MyCoolNamedTuple, MyMod | functions: script_module_to_buffer | imports: io, torch, psutil | [test jit test_save_load.py]",
    "role": "src",
    "loc": 918
  },
  {
    "id": "test\\jit\\test_save_load_for_op_version.py",
    "summary": "No description | classes: MyModule, MyModuleFloat, MyModuleInt, Module, TestSaveLoadForOpVersion | imports: io, hypothesis, torch | [test jit test_save_load_for_op_version.py]",
    "role": "src",
    "loc": 505
  },
  {
    "id": "test\\jit\\test_scriptmod_ann.py",
    "summary": "No description | classes: M, TestScriptModuleInstanceAttributeTypeAnnotation | imports: torch | [test jit test_scriptmod_ann.py]",
    "role": "src",
    "loc": 294
  },
  {
    "id": "test\\jit\\test_script_profile.py",
    "summary": "No description | classes: Sequence, TestScriptProfile | imports: torch | [test jit test_script_profile.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "test\\jit\\test_slice.py",
    "summary": "No description | classes: Bar, Foo, TestSlice | imports: torch | [test jit test_slice.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "test\\jit\\test_sparse.py",
    "summary": "No description | classes: SparseTensorModule, TestSparse | imports: io, unittest, torch | [test jit test_sparse.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\jit\\test_string_formatting.py",
    "summary": "No description | classes: TestStringFormatting | imports: torch | [test jit test_string_formatting.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "test\\jit\\test_symbolic_shape_analysis.py",
    "summary": "No description | classes: CatMod, TwoConvs, TestSymbolicShapeAnalysis | imports: operator, unittest, textwrap, torch | [test jit test_symbolic_shape_analysis.py]",
    "role": "src",
    "loc": 680
  },
  {
    "id": "test\\jit\\test_tensor_creation_ops.py",
    "summary": "A suite of tests for ops that create tensors. | classes: TestTensorCreationOps | imports: torch | [test jit test_tensor_creation_ops.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "test\\jit\\test_tensor_methods.py",
    "summary": "No description | classes: TestTensorMethods | imports: torch | [test jit test_tensor_methods.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\jit\\test_torchbind.py",
    "summary": "No description | classes: NonJitableClass, CustomWrapper, FooBar, FooBar1234, FooBar4321, TryTracing | imports: copy, io, unittest, torch | [test jit test_torchbind.py]",
    "role": "src",
    "loc": 365
  },
  {
    "id": "test\\jit\\test_tracer.py",
    "summary": "No description | classes: Recurrence, MyModule, MyClass, M, TracedInlineDecision, InplaceFn | imports: copy, io, unittest, torch | [test jit test_tracer.py]",
    "role": "src",
    "loc": 2149
  },
  {
    "id": "test\\jit\\test_types.py",
    "summary": "No description | classes: Foo, M, MyPythonClass, Sub, ModuleWithIgnoredAttr, ModuleUsesIgnoredAttr | imports: inspect, textwrap, torch, jit | [test jit test_types.py]",
    "role": "src",
    "loc": 274
  },
  {
    "id": "test\\jit\\test_type_sharing.py",
    "summary": "No description | classes: M, A, B, Caller, Traced, AB | imports: io, torch | [test jit test_type_sharing.py]",
    "role": "src",
    "loc": 478
  },
  {
    "id": "test\\jit\\test_typing.py",
    "summary": "No description | classes: _NamedTupleBadMemberType, BaseModule, Submodule, LowestModule, TestTyping | imports: torch | [test jit test_typing.py]",
    "role": "src",
    "loc": 491
  },
  {
    "id": "test\\jit\\test_union.py",
    "summary": "No description | classes: Color, A, M, TestUnion | imports: io, textwrap, torch | [test jit test_union.py]",
    "role": "src",
    "loc": 815
  },
  {
    "id": "test\\jit\\test_union_pep604.py",
    "summary": "No description | classes: Color, A, M, TestUnion | imports: io, unittest, textwrap, torch | [test jit test_union_pep604.py]",
    "role": "src",
    "loc": 812
  },
  {
    "id": "test\\jit\\test_unsupported_ops.py",
    "summary": "No description | classes: TestUnsupportedOps | imports: unittest, torch | [test jit test_unsupported_ops.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "test\\jit\\test_upgraders.py",
    "summary": "No description | classes: TestUpgraders | imports: io, zipfile, torch | [test jit test_upgraders.py]",
    "role": "src",
    "loc": 269
  },
  {
    "id": "test\\jit\\test_warn.py",
    "summary": "No description | classes: TestWarn | imports: io, torch | [test jit test_warn.py]",
    "role": "src",
    "loc": 113
  },
  {
    "id": "test\\jit\\test_with.py",
    "summary": "This class implements a basic context manager interface for use in | classes: Context, NoEnterNoExit, BadEnter, BadExit, ExitIncorrectTypes, NoGradModule | imports: torch | [test jit test_with.py]",
    "role": "src",
    "loc": 491
  },
  {
    "id": "test\\jit\\__init__.py",
    "summary": "Package initializer | [test jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\jit\\fixtures_srcs\\fixtures_src.py",
    "summary": "No description | classes: TestVersionedDivTensorExampleV7, TestVersionedLinspaceV7, TestVersionedLinspaceOutV7, TestVersionedLogspaceV8, TestVersionedLogspaceOutV8, TestVersionedGeluV9 | imports: torch | [test jit fixtures_srcs fixtures_src.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "test\\jit\\fixtures_srcs\\generate_models.py",
    "summary": "No description | functions: get_fixtures_path, get_all_models, model_exist, get_operator_list, get_output_model_version, generate_models | imports: io, zipfile, test, torch | [test jit fixtures_srcs generate_models.py]",
    "role": "src",
    "loc": 197
  },
  {
    "id": "test\\jit\\fixtures_srcs\\test_upgrader_models_generation.py",
    "summary": "No description | classes: TestUpgraderModelGeneration | imports: torch, test | [test jit fixtures_srcs test_upgrader_models_generation.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "test\\jit\\fixtures_srcs\\__init__.py",
    "summary": "Package initializer | [test jit fixtures_srcs __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\jit\\xnnpack\\test_xnnpack_delegate.py",
    "summary": "No description | classes: Module, AddModule, AddSpliceModule, TestXNNPackBackend | imports: unittest, torch | [test jit xnnpack test_xnnpack_delegate.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "test\\jit\\_imported_class_test\\bar.py",
    "summary": "No description | classes: FooSameName | imports: torch | [test jit _imported_class_test bar.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\jit\\_imported_class_test\\foo.py",
    "summary": "No description | classes: FooSameName | imports: torch | [test jit _imported_class_test foo.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\jit\\_imported_class_test\\__init__.py",
    "summary": "Package initializer | [test jit _imported_class_test __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\jit\\_imported_class_test\\very\\__init__.py",
    "summary": "Package initializer | [test jit _imported_class_test very __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\jit\\_imported_class_test\\very\\very\\nested.py",
    "summary": "No description | classes: FooUniqueName | imports: torch | [test jit _imported_class_test very very nested.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\jit\\_imported_class_test\\very\\very\\__init__.py",
    "summary": "Package initializer | [test jit _imported_class_test very very __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\jit_hooks\\model.py",
    "summary": "No description | functions: main | imports: argparse, torch, jit | [test jit_hooks model.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "test\\lazy\\test_bindings.py",
    "summary": "No description | functions: test_metrics | imports: torch | [test lazy test_bindings.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\lazy\\test_debug_util.py",
    "summary": "No description | classes: DebugUtilTest | imports: tempfile, unittest, torch | [test lazy test_debug_util.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "test\\lazy\\test_extract_compiled_graph.py",
    "summary": "No description | classes: ModuleConstScale, ModuleSub, ModuleAddcmul, ModuleReturnMulti, ModuleReturnDupTensor, ModuleInplaceUpdate | functions: force_fallback_ctx_mgr, nop_ctx_mgr, gen_rand_args, allclose, unwrap, verify_reusing_compiled_graph | imports: unittest, torch, copy, dis | [test lazy test",
    "role": "src",
    "loc": 133
  },
  {
    "id": "test\\lazy\\test_functionalization.py",
    "summary": "No description | classes: Model, LazyFuncionalizationTest | imports: torch | [test lazy test_functionalization.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "test\\lazy\\test_generator.py",
    "summary": "No description | classes: LazyGeneratorTest | imports: torch | [test lazy test_generator.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "test\\lazy\\test_meta_kernel.py",
    "summary": "No description | classes: TestMetaKernel | imports: torch | [test lazy test_meta_kernel.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\lazy\\test_reuse_ir.py",
    "summary": "No description | classes: TestLazyReuseIr | functions: get_test_device | imports: unittest, torch | [test lazy test_reuse_ir.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "test\\lazy\\test_step_closures.py",
    "summary": "No description | classes: ClosuresTest | imports: threading, torch | [test lazy test_step_closures.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "test\\lazy\\test_ts_opinfo.py",
    "summary": "No description | classes: TestLazyTensor, TestLazyOpInfo, TestLazyDynamicOps | functions: get_test_device, remove_suffixes, init_lists, clone_move | imports: functools, unittest, yaml, torch | [test lazy test_ts_opinfo.py]",
    "role": "src",
    "loc": 289
  },
  {
    "id": "test\\lazy\\__init__.py",
    "summary": "Package initializer | [test lazy __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\mobile\\test_bytecode.py",
    "summary": "No description | classes: MyTestModule, testVariousModelVersions | imports: fnmatch, io, shutil, tempfile | [test mobile test_bytecode.py]",
    "role": "src",
    "loc": 252
  },
  {
    "id": "test\\mobile\\test_lite_script_module.py",
    "summary": "No description | classes: MyTestModule, A, B, Foo, MyTestModuleForListWithModuleClass, MyTestModuleForDictWithModuleClass | imports: inspect, io, tempfile, torch | [test mobile test_lite_script_module.py]",
    "role": "src",
    "loc": 471
  },
  {
    "id": "test\\mobile\\test_lite_script_type.py",
    "summary": "No description | classes: MyTestModule, Foo, Bar, Baz, TestLiteScriptModule | imports: io, unittest, torch | [test mobile test_lite_script_type.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "test\\mobile\\test_quantize_fx_lite_script_module.py",
    "summary": "No description | classes: M, TestLiteFuseFx | imports: torch | [test mobile test_quantize_fx_lite_script_module.py]",
    "role": "src",
    "loc": 74
  },
  {
    "id": "test\\mobile\\test_upgraders.py",
    "summary": "No description | classes: TestLiteScriptModule | imports: io, torch | [test mobile test_upgraders.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "test\\mobile\\test_upgrader_codegen.py",
    "summary": "No description | classes: TestLiteScriptModule | imports: tempfile, torch, torchgen | [test mobile test_upgrader_codegen.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "test\\mobile\\custom_build\\prepare_model.py",
    "summary": "This is a script for end-to-end mobile custom build test purpose. It prepares | imports: yaml, torchvision, torch | [test mobile custom_build prepare_model.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "test\\mobile\\lightweight_dispatch\\tests_setup.py",
    "summary": "Save a model and dump all the ops | classes: ModelWithDTypeDeviceLayoutPinMemory, ModelWithTensorOptional, ModelWithScalarList, ModelWithFloatList, ModelWithListOfOptionalTensors, ModelWithArrayOfInt | functions: save_model, wrapper_save | imports: functools, shutil, io, torch | [test mobile lightwe",
    "role": "src",
    "loc": 123
  },
  {
    "id": "test\\mobile\\model_test\\android_api_module.py",
    "summary": "No description | classes: AndroidAPIModule | imports: torch | [test mobile model_test android_api_module.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\mobile\\model_test\\builtin_ops.py",
    "summary": "No description | classes: TSBuiltinOpsModule, TSCollectionOpsModule | imports: torch | [test mobile model_test builtin_ops.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "test\\mobile\\model_test\\gen_test_model.py",
    "summary": "No description | functions: calcOpsCoverage, getModuleFromName, runModule, generateAllModels, generateModel, main | imports: io, yaml, android_api_module, builtin_ops | [test mobile model_test gen_test_model.py]",
    "role": "src",
    "loc": 203
  },
  {
    "id": "test\\mobile\\model_test\\math_ops.py",
    "summary": "No description | classes: PointwiseOpsModule, ReductionOpsModule, ComparisonOpsModule, OtherMathOpsModule, SpectralOpsModule, BlasLapackOpsModule | imports: torch | [test mobile model_test math_ops.py]",
    "role": "src",
    "loc": 402
  },
  {
    "id": "test\\mobile\\model_test\\nn_ops.py",
    "summary": "No description | classes: NNConvolutionModule, NNPoolingModule, NNPaddingModule, NNNormalizationModule, NNActivationModule, NNRecurrentModule | imports: torch | [test mobile model_test nn_ops.py]",
    "role": "src",
    "loc": 367
  },
  {
    "id": "test\\mobile\\model_test\\quantization_ops.py",
    "summary": "No description | classes: GeneralQuantModule, M, DynamicQuantModule, StaticQuantModule, FusedQuantModule | imports: torch | [test mobile model_test quantization_ops.py]",
    "role": "src",
    "loc": 194
  },
  {
    "id": "test\\mobile\\model_test\\sampling_ops.py",
    "summary": "No description | classes: SamplingOpsModule | imports: torch | [test mobile model_test sampling_ops.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\mobile\\model_test\\tensor_ops.py",
    "summary": "No description | classes: TensorOpsModule, TensorCreationOpsModule, TensorIndexingOpsModule, TensorTypingOpsModule, TensorViewOpsModule | imports: torch | [test mobile model_test tensor_ops.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "test\\mobile\\model_test\\torchvision_models.py",
    "summary": "No description | classes: MobileNetV2Module, MobileNetV2VulkanModule, Resnet18Module | imports: torchvision, torch | [test mobile model_test torchvision_models.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\mobile\\model_test\\update_production_ops.py",
    "summary": "This is a script to aggregate production ops from xplat/pytorch_models/build/all_mobile_model_configs.yaml. | imports: yaml | [test mobile model_test update_production_ops.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\mobile\\nnc\\aot_test_model.py",
    "summary": "No description | classes: NeuralNetwork | imports: torch | [test mobile nnc aot_test_model.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "test\\nn\\test_convolution.py",
    "summary": "No description | classes: TestConvolutionNN, TestConvolutionNNDeviceType | imports: unittest, torch, scipy | [test nn test_convolution.py]",
    "role": "src",
    "loc": 3525
  },
  {
    "id": "test\\nn\\test_dropout.py",
    "summary": "No description | classes: TestDropoutNN, TestDropoutNNDeviceType | imports: random, unittest, torch | [test nn test_dropout.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "test\\nn\\test_embedding.py",
    "summary": "No description | classes: TestEmbeddingNN, TestEmbeddingNNDeviceType | imports: random, unittest, torch | [test nn test_embedding.py]",
    "role": "src",
    "loc": 1381
  },
  {
    "id": "test\\nn\\test_init.py",
    "summary": "No description | classes: TestNNInit | imports: random, string, unittest, functools | [test nn test_init.py]",
    "role": "src",
    "loc": 441
  },
  {
    "id": "test\\nn\\test_lazy_modules.py",
    "summary": "No description | classes: LazyModule, TestModule, MyNetwork, TestLazyModules | imports: pickle, unittest, torch | [test nn test_lazy_modules.py]",
    "role": "src",
    "loc": 719
  },
  {
    "id": "test\\nn\\test_load_state_dict.py",
    "summary": "No description | classes: CustomState, MyModule, TestLoadStateDict, MyLoadTensor, MyLoadTensor2, MyBrokenLoadTensor | functions: load_torch_function_handler, module_load | imports: unittest, copy, torch, numpy | [test nn test_load_state_dict.py]",
    "role": "src",
    "loc": 502
  },
  {
    "id": "test\\nn\\test_module_hooks.py",
    "summary": "No description | classes: Net, ToyModel, KwargModel, FailsInForwardModel, DummyContextManager, TestModule | functions: forward_hook, forward_pre_hook, full_backward_hook, full_backward_pre_hook, kwarg_forward_pre_hook, kwarg_forward_hook | imports: gc, pickle, unittest, weakref | [test nn test_modul",
    "role": "src",
    "loc": 1324
  },
  {
    "id": "test\\nn\\test_multihead_attention.py",
    "summary": "No description | classes: TestMultiheadAttentionNN, TestMultiheadAttentionNNDeviceType | imports: random, unittest, torch, numpy | [test nn test_multihead_attention.py]",
    "role": "src",
    "loc": 805
  },
  {
    "id": "test\\nn\\test_packed_sequence.py",
    "summary": "No description | classes: PackedSequenceTest | imports: random, torch | [test nn test_packed_sequence.py]",
    "role": "src",
    "loc": 404
  },
  {
    "id": "test\\nn\\test_parametrization.py",
    "summary": "No description | classes: Skew, Orthogonal, Resize, NoResize, FirstZero, LastZero | imports: pickle, copy, torch | [test nn test_parametrization.py]",
    "role": "src",
    "loc": 1395
  },
  {
    "id": "test\\nn\\test_pooling.py",
    "summary": "No description | classes: TestAvgPool, TestPoolingNN, TestPoolingNNDeviceType | imports: operator, random, subprocess, unittest | [test nn test_pooling.py]",
    "role": "src",
    "loc": 1716
  },
  {
    "id": "test\\nn\\test_pruning.py",
    "summary": "No description | classes: TestPruningNN | imports: pickle, unittest, torch | [test nn test_pruning.py]",
    "role": "src",
    "loc": 642
  },
  {
    "id": "test\\onnx\\autograd_helper.py",
    "summary": "No description | classes: CustomFunction | imports: torch | [test onnx autograd_helper.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "test\\onnx\\onnx_test_common.py",
    "summary": "No description | classes: _TestONNXRuntime, DecorateMeta | functions: run_model_test, assert_dynamic_shapes, parameterize_class_name, run_ort, _try_clone_model, _try_clone_inputs | imports: copy, dataclasses, io, unittest | [test onnx onnx_test_common.py]",
    "role": "src",
    "loc": 608
  },
  {
    "id": "test\\onnx\\pytorch_test_common.py",
    "summary": "No description | classes: TorchModelType, ExportTestCase | functions: _skipper, decorator, wrapper, skipIfUnsupportedMinOpsetVersion, skip_dec, skipIfUnsupportedMaxOpsetVersion | imports: functools, random, unittest, numpy | [test onnx pytorch_test_common.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "test\\onnx\\test_autograd_funs.py",
    "summary": "No description | classes: SingleOut, Caller, MultiOut, PartialOut, Child, Parent | imports: pytorch_test_common, onnx_test_common, torch | [test onnx test_autograd_funs.py]",
    "role": "src",
    "loc": 168
  },
  {
    "id": "test\\onnx\\test_custom_ops.py",
    "summary": "No description | classes: MyClip, MyModule, MyRelu, TestCustomAutogradFunction, M, TestExportAsContribOps | imports: onnx_test_common, pytorch_test_common, torch | [test onnx test_custom_ops.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "test\\onnx\\test_fx_passes.py",
    "summary": "No description | classes: TestFxPasses | imports: torch | [test onnx test_fx_passes.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "test\\onnx\\test_fx_type_promotion.py",
    "summary": "No description | classes: TestGeneratedTypePromotionRuleSet | imports: torch | [test onnx test_fx_type_promotion.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "test\\onnx\\test_lazy_import.py",
    "summary": "No description | classes: TestLazyONNXPackages | imports: subprocess, tempfile, pytorch_test_common, torch | [test onnx test_lazy_import.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "test\\onnx\\test_models.py",
    "summary": "No description | classes: TestModels | functions: toC | imports: unittest, pytorch_test_common, model_defs, torchvision | [test onnx test_models.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "test\\onnx\\test_models_onnxruntime.py",
    "summary": "No description | classes: RoIHeadsModule, MyModule, TestModelsONNXRuntime | functions: exportTest, _get_image, _get_test_images, _get_features, _init_test_generalized_rcnn_transform, _init_test_rpn | imports: unittest, onnx_test_common, parameterized, PIL | [test onnx test_models_onnxruntime.py]",
    "role": "src",
    "loc": 391
  },
  {
    "id": "test\\onnx\\test_models_quantized_onnxruntime.py",
    "summary": "No description | classes: _TopPredictor, TestQuantizedModelsONNXRuntime | functions: _get_test_image_tensor | imports: unittest, onnx_test_common, parameterized, PIL | [test onnx test_models_quantized_onnxruntime.py]",
    "role": "src",
    "loc": 74
  },
  {
    "id": "test\\onnx\\test_onnxscript_no_runtime.py",
    "summary": "Test the support on onnxscript in PyTorch-ONNX converter. | classes: NestedLoopsModel, TestONNXScriptExport | imports: io, onnx, onnxscript, torch | [test onnx test_onnxscript_no_runtime.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "test\\onnx\\test_onnxscript_runtime.py",
    "summary": "Test the support on onnxscript in PyTorch-ONNX converter with onnxruntime. | classes: N, M, TestONNXScriptRuntime | imports: onnx_test_common, onnxscript, torch | [test onnx test_onnxscript_runtime.py]",
    "role": "src",
    "loc": 101
  },
  {
    "id": "test\\onnx\\test_onnx_opset.py",
    "summary": "No description | classes: MyModule, MyModuleDynamic, DynamicSliceModel, MyModel, MyDynamicModel, TestONNXOpset | functions: check_onnx_opset_operator, check_onnx_opsets_operator | imports: io, onnx, pytorch_test_common, torch | [test onnx test_onnx_opset.py]",
    "role": "src",
    "loc": 573
  },
  {
    "id": "test\\onnx\\test_op_consistency.py",
    "summary": "Test consistency between the output values of torch.onnx exported operators | classes: SingleOpModel, TestOnnxModelOutputConsistency | functions: _should_skip_xfail_test_sample, _get_test_class_name | imports: copy, onnx_test_common, parameterized, torch | [test onnx test_op_consistency.py]",
    "role": "src",
    "loc": 259
  },
  {
    "id": "test\\onnx\\test_pytorch_jit_onnx.py",
    "summary": "Abstract base class for test cases. | classes: _TestJITIRToONNX | functions: _jit_graph_to_onnx_model, MakeTestCase | imports: onnxruntime, pytorch_test_common, torch | [test onnx test_pytorch_jit_onnx.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "test\\onnx\\test_pytorch_onnx_no_runtime.py",
    "summary": "Tests for onnx export that don't run the exported model. | classes: AddmmModel, Foo, TraceMe, ModuleToExport, WarningTest, PythonModule | functions: export_to_onnx | imports: io, unittest, numpy, onnx | [test onnx test_pytorch_onnx_no_runtime.py]",
    "role": "src",
    "loc": 982
  },
  {
    "id": "test\\onnx\\test_pytorch_onnx_onnxruntime.py",
    "summary": "No description | classes: Fuse, ConvTBC, Reshape, MyModel, Data, DictModelOutput | functions: _init_test_generalized_rcnn_transform, _init_test_rpn, _construct_tensor_for_quantization_test, _parameterized_class_attrs_and_values, _parametrize_rnn_args | imports: functools, io, unittest, numpy | [test",
    "role": "src",
    "loc": 11464
  },
  {
    "id": "test\\onnx\\test_pytorch_onnx_onnxruntime_cuda.py",
    "summary": "No description | classes: GeluModel, LayerNormModel, FusionModel, LinearModel, MyModule, Model | imports: unittest, onnx_test_common, onnxruntime, parameterized | [test onnx test_pytorch_onnx_onnxruntime_cuda.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "test\\onnx\\test_pytorch_onnx_shape_inference.py",
    "summary": "No description | classes: TestONNXShapeInference, CustomInverse, TestONNXCustomOpShapeInference | functions: expect_tensor, verify, as_graphcontext, g_op | imports: io, numpy, onnx, pytorch_test_common | [test onnx test_pytorch_onnx_shape_inference.py]",
    "role": "src",
    "loc": 436
  },
  {
    "id": "test\\onnx\\test_symbolic_helper.py",
    "summary": "Unit tests on `torch.onnx.symbolic_helper`. | classes: TestHelperFunctions | imports: torch | [test onnx test_symbolic_helper.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "test\\onnx\\test_utility_funs.py",
    "summary": "Remove test environment prefix added to module. | classes: _BaseTestCase, EinsumModule, SkipConnectionModule, TestUnconvertibleOps, MyModule, SplitModule | functions: _remove_test_environment_prefix_from_scope_name | imports: copy, functools, io, onnx | [test onnx test_utility_funs.py]",
    "role": "src",
    "loc": 1616
  },
  {
    "id": "test\\onnx\\test_verification.py",
    "summary": "No description | classes: UnexportableModel, SupportedModel, TestVerification, Model, TestVerificationOnWrongExport, TestFindMismatch | imports: io, tempfile, unittest, numpy | [test onnx test_verification.py]",
    "role": "src",
    "loc": 246
  },
  {
    "id": "test\\onnx\\verify.py",
    "summary": "No description | classes: ShortCircuit, Recover, AddContext, Errors | functions: colonize, verify, _nested_map, _map, _iter_filter, _iter | imports: difflib, io, numpy, onnx | [test onnx verify.py]",
    "role": "src",
    "loc": 423
  },
  {
    "id": "test\\onnx\\dynamo\\test_dynamo_with_onnxruntime_backend.py",
    "summary": "No description | classes: MLP, LlamaAttentionWrapper, LlamaDecoderWrapper, LlamaModelWrapper, TestDynamoWithONNXRuntime | functions: make_aot_ort | imports: copy, dataclasses, unittest, onnxruntime | [test onnx dynamo test_dynamo_with_onnxruntime_backend.py]",
    "role": "src",
    "loc": 676
  },
  {
    "id": "test\\onnx\\exporter\\test_api.py",
    "summary": "Simple API tests for the ONNX exporter. | classes: SampleModel, SampleModelTwoInputs, SampleModelForDynamicShapes, NestedModelForDynamicShapes, ScriptModule, Nested | imports: numpy, onnxscript, torch | [test onnx exporter test_api.py]",
    "role": "src",
    "loc": 376
  },
  {
    "id": "test\\onnx\\exporter\\test_building.py",
    "summary": "Unit tests for the _building module. | classes: TestOpRecorder | imports: numpy, onnxscript, torch | [test onnx exporter test_building.py]",
    "role": "src",
    "loc": 130
  },
  {
    "id": "test\\onnx\\exporter\\test_capture_strategies.py",
    "summary": "Unit tests for the _capture_strategies module. | classes: Model, ExportStrategiesTest | imports: torch | [test onnx exporter test_capture_strategies.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "test\\onnx\\exporter\\test_core.py",
    "summary": "Unit tests for the _core module. | classes: TorchTensorTest | imports: numpy, torch | [test onnx exporter test_core.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "test\\onnx\\exporter\\test_dynamic_shapes.py",
    "summary": "Unit tests for the _dynamic_shapes module. | classes: SampleModelForDynamicShapes, NestedModelForDynamicShapes, SingnatureOnlyLlamaModel, TestDynamicShapes | imports: tempfile, onnx, torch | [test onnx exporter test_dynamic_shapes.py]",
    "role": "src",
    "loc": 581
  },
  {
    "id": "test\\onnx\\exporter\\test_hf_models_e2e.py",
    "summary": "Unit LLM tests for the onnx dynamo exporter. | classes: DynamoExporterHfModelsTest | functions: _prepare_llm_model_gptj_to_test | imports: transformers, torch | [test onnx exporter test_hf_models_e2e.py]",
    "role": "src",
    "loc": 198
  },
  {
    "id": "test\\onnx\\exporter\\test_ir_passes.py",
    "summary": "Unit tests for the _ir_passes module. | classes: ONNXIRPassesTest | imports: torch | [test onnx exporter test_ir_passes.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\onnx\\exporter\\test_small_models_e2e.py",
    "summary": "Unit tests for the onnx dynamo exporter. | classes: Model, MulModule, CondModel, Submodule, VisionModel, TopKModel | imports: torchvision, transformers, torch | [test onnx exporter test_small_models_e2e.py]",
    "role": "src",
    "loc": 426
  },
  {
    "id": "test\\onnx\\exporter\\test_tensors.py",
    "summary": "Unit tests for the _tensors module. | classes: SymbolicTensorTest | imports: onnxscript, torch | [test onnx exporter test_tensors.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "test\\onnx\\internal\\test_diagnostics.py",
    "summary": "No description | classes: _SarifLogBuilder, _RuleCollectionForTest, TestDynamoOnnxDiagnostics, CustomAdd, M, TestTorchScriptOnnxDiagnostics | functions: _assert_has_diagnostics, assert_all_diagnostics, assert_diagnostic | imports: dataclasses, io, torch, unittest | [test onnx internal test_diagnosti",
    "role": "src",
    "loc": 537
  },
  {
    "id": "test\\onnx\\internal\\test_registraion.py",
    "summary": "Unit tests for the internal registration wrapper module. | classes: TestGlobalHelpers, TestOverrideDict, TestRegistrationDecorators | imports: torch | [test onnx internal test_registraion.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "test\\onnx\\model_defs\\dcgan.py",
    "summary": "No description | classes: _netG, _netD | functions: weights_init | imports: torch | [test onnx model_defs dcgan.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "test\\onnx\\model_defs\\emb_seq.py",
    "summary": "No description | classes: EmbeddingNetwork1, EmbeddingNetwork2 | imports: torch | [test onnx model_defs emb_seq.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "test\\onnx\\model_defs\\lstm_flattening_result.py",
    "summary": "No description | classes: LstmFlatteningResult, LstmFlatteningResultWithSeqLength, LstmFlatteningResultWithoutSeqLength | imports: torch | [test onnx model_defs lstm_flattening_result.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "test\\onnx\\model_defs\\mnist.py",
    "summary": "No description | classes: MNIST | imports: torch | [test onnx model_defs mnist.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "test\\onnx\\model_defs\\op_test.py",
    "summary": "No description | classes: DummyNet, ConcatNet, PermuteNet, PReluNet, FakeQuantNet | imports: torch | [test onnx model_defs op_test.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "test\\onnx\\model_defs\\rnn_model_with_packed_sequence.py",
    "summary": "No description | classes: RnnModelWithPackedSequence, RnnModelWithPackedSequenceWithoutState, RnnModelWithPackedSequenceWithState | imports: torch | [test onnx model_defs rnn_model_with_packed_sequence.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "test\\onnx\\model_defs\\squeezenet.py",
    "summary": "No description | classes: Fire, SqueezeNet | imports: torch | [test onnx model_defs squeezenet.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "test\\onnx\\model_defs\\srresnet.py",
    "summary": "No description | classes: ResidualBlock, UpscaleBlock, SRResNet | functions: _initialize_orthogonal | imports: torch | [test onnx model_defs srresnet.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "test\\onnx\\model_defs\\super_resolution.py",
    "summary": "No description | classes: SuperResolutionNet | imports: torch | [test onnx model_defs super_resolution.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\onnx\\model_defs\\word_language_model.py",
    "summary": "Container module with an encoder, a recurrent module, and a decoder. | classes: RNNModel, RNNModelWithTensorHidden, RNNModelWithTupleHidden | imports: torch | [test onnx model_defs word_language_model.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "test\\onnx\\model_defs\\__init__.py",
    "summary": "Package initializer | imports: op_test, squeezenet, srresnet, super_resolution | [test onnx model_defs __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\onnx\\torchlib\\error_reproduction.py",
    "summary": "Error reproduction utilities for op consistency tests. | functions: create_reproduction_report, create_mismatch_report, save_error_report | imports: difflib, platform, traceback, numpy | [test onnx torchlib error_reproduction.py]",
    "role": "src",
    "loc": 211
  },
  {
    "id": "test\\onnx\\torchlib\\ops_test_common.py",
    "summary": "Common utils for testing operators. | classes: DecorateMeta, OrtAbortedError | functions: xfail, skip, add_decorate_info, wrapped, duplicate_opinfo, duplicate_opinfo_for_prims | imports: copy, dataclasses, multiprocessing, pprint | [test onnx torchlib ops_test_common.py]",
    "role": "src",
    "loc": 543
  },
  {
    "id": "test\\onnx\\torchlib\\ops_test_data.py",
    "summary": "Test op correctness by comparing with PyTorch results. | classes: TorchLibOpInfo | functions: _amin_amax_input_wrangler, _avg_pool_input_wrangler, _cross_entropy_input_wrangler, _dropout_input_wrangler, _einsum_input_wrangler, _embedding_input_wrangler | imports: copy, dataclasses, functools, typing",
    "role": "src",
    "loc": 548
  },
  {
    "id": "test\\onnx\\torchlib\\test_ops.py",
    "summary": "Test op correctness by comparing with PyTorch results. | classes: TestFunctionValidity, TestOutputConsistencyFullGraph | functions: dtypes_except, _should_skip_xfail_test_sample, run_test_output_match | imports: error_reproduction, numpy, onnx, onnxruntime | [test onnx torchlib test_ops.py]",
    "role": "src",
    "loc": 278
  },
  {
    "id": "test\\optim\\test_lrscheduler.py",
    "summary": "No description | classes: SchedulerTestNet, LambdaLRTestObject, MultiStepLR, ScaleFn, TestLRScheduler | imports: copy, pickle, tempfile, types | [test optim test_lrscheduler.py]",
    "role": "src",
    "loc": 2290
  },
  {
    "id": "test\\optim\\test_optim.py",
    "summary": "No description | classes: TestDifferentiableOptimizer | functions: _diff_fn, _multistep_backprop_diff_hyperparams_fn | imports: torch | [test optim test_optim.py]",
    "role": "src",
    "loc": 637
  },
  {
    "id": "test\\optim\\test_swa_utils.py",
    "summary": "No description | classes: SWATestDNN, SWATestCNN, TestSWAUtils | imports: pickle, torch | [test optim test_swa_utils.py]",
    "role": "src",
    "loc": 271
  },
  {
    "id": "test\\package\\common.py",
    "summary": "No description | classes: PackageTestCase | imports: tempfile, torch | [test package common.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\package\\generate_bc_packages.py",
    "summary": "Function to create packages for testing backwards compatiblity | functions: generate_bc_packages | imports: torch, package_a | [test package generate_bc_packages.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "test\\package\\module_a.py",
    "summary": "No description | [test package module_a.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "test\\package\\module_a_remapped_path.py",
    "summary": "No description | [test package module_a_remapped_path.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "test\\package\\test_analyze.py",
    "summary": "Dependency analysis API tests. | classes: TestAnalyze | imports: torch, common, test_trace_dep | [test package test_analyze.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "test\\package\\test_dependency_api.py",
    "summary": "No description | classes: BrokenImporter, TestDependencyAPI | imports: importlib, io, textwrap, unittest | [test package test_dependency_api.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "test\\package\\test_dependency_hooks.py",
    "summary": "Dependency management hooks API tests. | classes: TestDependencyHooks | imports: io, torch, common | [test package test_dependency_hooks.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "test\\package\\test_digraph.py",
    "summary": "Test the DiGraph structure we use to represent dependencies in PackageExporter | classes: TestDiGraph | imports: torch, common | [test package test_digraph.py]",
    "role": "src",
    "loc": 100
  },
  {
    "id": "test\\package\\test_directory_reader.py",
    "summary": "Tests use of DirectoryReader as accessor for opened packages. | classes: DirectoryReaderTest | imports: zipfile, tempfile, textwrap, unittest | [test package test_directory_reader.py]",
    "role": "src",
    "loc": 219
  },
  {
    "id": "test\\package\\test_glob_group.py",
    "summary": "No description | classes: TestGlobGroup | imports: torch, common | [test package test_glob_group.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "test\\package\\test_importer.py",
    "summary": "No description | classes: DummyImporter, DummyClass, TestImporter | imports: io, torch, common, package_a | [test package test_importer.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\package\\test_load_bc_packages.py",
    "summary": "Tests for checking loading has backwards compatiblity | classes: TestLoadBCPackages | imports: unittest, torch, common | [test package test_load_bc_packages.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "test\\package\\test_mangling.py",
    "summary": "No description | classes: TestMangling | imports: io, torch, common, package_a | [test package test_mangling.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "test\\package\\test_misc.py",
    "summary": "No description | classes: LoaderThatRemapsModuleA, FinderThatRemapsModuleA, TestMisc | imports: inspect, platform, io, textwrap | [test package test_misc.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "test\\package\\test_model.py",
    "summary": "End-to-end tests packaging an entire model. | classes: ModelTest | imports: io, textwrap, unittest, torch | [test package test_model.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "test\\package\\test_package_fx.py",
    "summary": "No description | classes: SimpleTest, SpecialGraphModule, TestModule, TestPackageFX | imports: io, torch, common, package_a | [test package test_package_fx.py]",
    "role": "src",
    "loc": 141
  },
  {
    "id": "test\\package\\test_package_script.py",
    "summary": "No description | classes: Submod, TopMod, ModWithTorchVision, InlineMod, TorchVisionTestInline, M | imports: io, textwrap, unittest, torch | [test package test_package_script.py]",
    "role": "src",
    "loc": 605
  },
  {
    "id": "test\\package\\test_repackage.py",
    "summary": "Tests for repackaging. | classes: TestRepackage | imports: io, torch, common, package_d | [test package test_repackage.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "test\\package\\test_resources.py",
    "summary": "Tests for access APIs for packaged resources. | classes: TestResources | imports: io, textwrap, unittest, torch | [test package test_resources.py]",
    "role": "src",
    "loc": 112
  },
  {
    "id": "test\\package\\test_save_load.py",
    "summary": "Core save_* and loading API tests. | classes: TestSaveLoad | imports: pickle, io, textwrap, unittest | [test package test_save_load.py]",
    "role": "src",
    "loc": 210
  },
  {
    "id": "test\\package\\__init__.py",
    "summary": "Package initializer | [test package __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\package\\package_a\\fake_interface.py",
    "summary": "No description | classes: ModuleInterface, OrigModule, NewModule, UsesInterface | imports: torch | [test package package_a fake_interface.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\package\\package_a\\fake_script_class.py",
    "summary": "Intended to be scripted. | classes: MyScriptClass, IdListFeature, UsesIdListFeature | functions: uses_script_class | imports: torch | [test package package_a fake_script_class.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "test\\package\\package_a\\long_name.py",
    "summary": "No description | functions: add_function, function_with_a_long_name_256charsplus_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    "role": "src",
    "loc": 6
  },
  {
    "id": "test\\package\\package_a\\std_sys_module_hacks.py",
    "summary": "No description | classes: Module | imports: torch | [test package package_a std_sys_module_hacks.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\package\\package_a\\std_sys_module_hacks_3_13.py",
    "summary": "No description | classes: Module | imports: torch | [test package package_a std_sys_module_hacks_3_13.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\package\\package_a\\subpackage.py",
    "summary": "No description | classes: PackageASubpackageObject | functions: leaf_function | [test package package_a subpackage.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\package\\package_a\\test_all_leaf_modules_tracer.py",
    "summary": "No description | classes: TestAllLeafModulesTracer | imports: torch | [test package package_a test_all_leaf_modules_tracer.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\package\\package_a\\test_module.py",
    "summary": "No description | classes: ModWithSubmod, ModWithTensor, ModWithSubmodAndTensor, ModWithTwoSubmodsAndTensor, ModWithMultipleSubmods, SimpleTest | functions: a_non_torch_leaf | imports: torch | [test package package_a test_module.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "test\\package\\package_a\\test_nn_module.py",
    "summary": "No description | classes: TestNnModule | imports: torch | [test package package_a test_nn_module.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "test\\package\\package_a\\use_dunder_package.py",
    "summary": "No description | functions: is_from_package | [test package package_a use_dunder_package.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "test\\package\\package_a\\use_torch_package_importer.py",
    "summary": "No description | imports: torch_package_importer | [test package package_a use_torch_package_importer.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\package\\package_a\\__init__.py",
    "summary": "Package initializer | classes: PackageAObject | [test package package_a __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\package\\package_b\\subpackage_1.py",
    "summary": "No description | classes: PackageBSubpackage1Object_0 | [test package package_b subpackage_1.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\package\\package_b\\subpackage_2.py",
    "summary": "No description | classes: PackageBSubpackage2Object_0 | functions: dynamic_import_test | [test package package_b subpackage_2.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\package\\package_b\\__init__.py",
    "summary": "Package initializer | classes: PackageBObject | [test package package_b __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "test\\package\\package_b\\subpackage_0\\__init__.py",
    "summary": "Package initializer | [test package package_b subpackage_0 __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "test\\package\\package_b\\subpackage_0\\subsubpackage_0\\__init__.py",
    "summary": "Package initializer | classes: PackageBSubsubpackage0Object_0 | [test package package_b subpackage_0 subsubpackage_0 __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\package\\package_c\\test_module.py",
    "summary": "No description | classes: TorchVisionTest | functions: a_non_torch_leaf | imports: torch, torchvision | [test package package_c test_module.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "test\\package\\package_c\\__init__.py",
    "summary": "Package initializer | classes: PackageCObject | [test package package_c __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\package\\package_d\\imports_directly.py",
    "summary": "No description | classes: ImportsDirectlyFromSubSubPackage | imports: torch, subpackage_0 | [test package package_d imports_directly.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "test\\package\\package_d\\imports_indirectly.py",
    "summary": "No description | classes: ImportsIndirectlyFromSubPackage | imports: torch, subpackage_0 | [test package package_d imports_indirectly.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "test\\package\\package_d\\__init__.py",
    "summary": "Package initializer | [test package package_d __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\package\\package_d\\subpackage_0\\__init__.py",
    "summary": "Package initializer | imports: subsubpackage_0 | [test package package_d subpackage_0 __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "test\\package\\package_d\\subpackage_0\\subsubpackage_0\\__init__.py",
    "summary": "Package initializer | [test package package_d subpackage_0 subsubpackage_0 __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "test\\package\\test_trace_dep\\__init__.py",
    "summary": "Package initializer | classes: SumMod | imports: yaml, torch | [test package test_trace_dep __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "test\\profiler\\test_cpp_thread.py",
    "summary": "No description | classes: PythonProfilerEventHandler, CppThreadTestCUDA, CppThreadTestXPU | functions: blueprint | imports: unittest, torch, caffe2 | [test profiler test_cpp_thread.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "test\\profiler\\test_execution_trace.py",
    "summary": "No description | classes: TestExecutionTrace | imports: tqdm, json, tempfile, unittest | [test profiler test_execution_trace.py]",
    "role": "src",
    "loc": 554
  },
  {
    "id": "test\\profiler\\test_kineto.py",
    "summary": "No description | classes: SimpleKinetoInitializationTest | imports: subprocess, unittest, torch | [test profiler test_kineto.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "test\\profiler\\test_memory_profiler.py",
    "summary": "No description | classes: TestMemoryProfiler, ScaleLayer, LazyLinear, RecordInputOutputDispatchMode, TestIdentifyGradients, TestDataFlow | imports: functools, gc, textwrap, unittest | [test profiler test_memory_profiler.py]",
    "role": "src",
    "loc": 1282
  },
  {
    "id": "test\\profiler\\test_profiler.py",
    "summary": "No description | classes: MyFunc, TestProfilerCUDA, TestProfilerITT, DummyModule, Task, A | imports: gc, json, mmap, pickle | [test profiler test_profiler.py]",
    "role": "src",
    "loc": 2460
  },
  {
    "id": "test\\profiler\\test_profiler_tree.py",
    "summary": "No description | classes: TorchFunctionTensor, TorchDispatchTensor, ProfilerTree, MyModule, TestProfilerTree | imports: functools, textwrap, traceback, unittest | [test profiler test_profiler_tree.py]",
    "role": "src",
    "loc": 1015
  },
  {
    "id": "test\\profiler\\test_record_function.py",
    "summary": "No description | classes: IDPIterator, IDPDelegator, TestRecordFunction | imports: tqdm, torch | [test profiler test_record_function.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "test\\profiler\\test_torch_tidy.py",
    "summary": "No description | classes: SimpleNet, Sentinel, TestTorchTidyProfiler | functions: find_node_with_name, find_node_with_regex | imports: tqdm, gc, textwrap, unittest | [test profiler test_torch_tidy.py]",
    "role": "src",
    "loc": 719
  },
  {
    "id": "test\\quantization\\__init__.py",
    "summary": "Package initializer | [test quantization __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\ao_migration\\common.py",
    "summary": "No description | classes: AOMigrationTestCase | imports: importlib, torch | [test quantization ao_migration common.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "test\\quantization\\ao_migration\\test_ao_migration.py",
    "summary": "No description | classes: TestAOMigrationNNQuantized, TestAOMigrationNNIntrinsic | imports: common, torch | [test quantization ao_migration test_ao_migration.py]",
    "role": "src",
    "loc": 319
  },
  {
    "id": "test\\quantization\\ao_migration\\test_quantization.py",
    "summary": "Modules and functions related to the | classes: TestAOMigrationQuantization | imports: common | [test quantization ao_migration test_quantization.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "test\\quantization\\ao_migration\\test_quantization_fx.py",
    "summary": "No description | classes: TestAOMigrationQuantizationFx | imports: common | [test quantization ao_migration test_quantization_fx.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "test\\quantization\\ao_migration\\__init__.py",
    "summary": "Package initializer | [test quantization ao_migration __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\bc\\test_backward_compatibility.py",
    "summary": "No description | classes: LSTMModule, Model, TestSerialization | functions: remove_prefix, get_filenames | imports: unittest, torch | [test quantization bc test_backward_compatibility.py]",
    "role": "src",
    "loc": 478
  },
  {
    "id": "test\\quantization\\bc\\__init__.py",
    "summary": "Package initializer | [test quantization bc __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\core\\test_backend_config.py",
    "summary": "No description | classes: TestBackendConfig | imports: torch | [test quantization core test_backend_config.py]",
    "role": "src",
    "loc": 273
  },
  {
    "id": "test\\quantization\\core\\test_docs.py",
    "summary": "The tests in this section import code from the quantization docs and check that | classes: TestQuantizationDocs | imports: torch, unittest | [test quantization core test_docs.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "test\\quantization\\core\\test_quantized_functional.py",
    "summary": "No description | classes: TestQuantizedFunctionalOps | imports: torch, numpy, hypothesis | [test quantization core test_quantized_functional.py]",
    "role": "src",
    "loc": 198
  },
  {
    "id": "test\\quantization\\core\\test_quantized_module.py",
    "summary": "No description | classes: _FusedModule_two_input_args, TestStaticQuantizedModule, TestDynamicQuantizedModule, TestReferenceQuantizedModule | imports: torch, hypothesis, copy, io | [test quantization core test_quantized_module.py]",
    "role": "src",
    "loc": 1719
  },
  {
    "id": "test\\quantization\\core\\test_quantized_op.py",
    "summary": "No description | classes: PointwisePostOp, QuantizableLSTMSplitGates, MultiheadAttentionModel, TestQuantizedOps, TestDynamicQuantizedOps, TestQuantizedLinear | functions: avoid_vpmaddubsw_overflow_linear, qlinear_ref, pool_output_shape, _get_random_tensor_and_q_params | imports: copy, numpy, operato",
    "role": "src",
    "loc": 6867
  },
  {
    "id": "test\\quantization\\core\\test_quantized_tensor.py",
    "summary": "Calculate the dynamic quantization parameters (scale, zero_point) | classes: Foo, M, SimpleQTensor, TestQuantizedTensor | functions: _calculate_dynamic_qparams, param_search_greedy, _compress_uniform_simplified | imports: numpy, random, torch, io | [test quantization core test_quantized_tensor.py]",
    "role": "src",
    "loc": 1343
  },
  {
    "id": "test\\quantization\\core\\test_top_level_apis.py",
    "summary": "No description | classes: TestDefaultObservers, TestQConfig | imports: torch | [test quantization core test_top_level_apis.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "test\\quantization\\core\\test_utils.py",
    "summary": "No description | classes: Sub, M, UInt4OrInt4Tensor, TestUtils | imports: torch | [test quantization core test_utils.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "test\\quantization\\core\\test_workflow_module.py",
    "summary": "Object addresses stay constant if and only if all modifications are in-place | classes: TestObserver, _ReferenceHistogramObserver, TestRecordHistogramObserver, TestHistogramObserver, TestFakeQuantize, Model | functions: _get_buffer_ids | imports: copy, io, unittest, numpy | [test quantization core t",
    "role": "src",
    "loc": 1245
  },
  {
    "id": "test\\quantization\\core\\test_workflow_ops.py",
    "summary": "No description | classes: Model, TestFakeQuantizeOps, TestFusedObsFakeQuant | functions: _fake_quantize_per_tensor_affine_reference, _fake_quantize_per_tensor_affine_grad_reference, _fake_quantize_learnable_per_tensor_affine_grad_reference, _quantize_per_tensor, _fake_quantize_learnable_per_channel_",
    "role": "src",
    "loc": 1090
  },
  {
    "id": "test\\quantization\\core\\__init__.py",
    "summary": "Package initializer | [test quantization core __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\core\\experimental\\apot_fx_graph_mode_ptq.py",
    "summary": "No description | functions: calibrate, prepare_ptq_linear | imports: torch, torchvision, copy | [test quantization core experimental apot_fx_graph_mode_ptq.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "test\\quantization\\core\\experimental\\apot_fx_graph_mode_qat.py",
    "summary": "No description | functions: prepare_qat_linear | imports: torchvision, torch, copy | [test quantization core experimental apot_fx_graph_mode_qat.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "test\\quantization\\core\\experimental\\quantization_util.py",
    "summary": "Computes and stores the average and current value | classes: AverageMeter | functions: accuracy, evaluate, load_model, print_size_of_model, prepare_data_loaders, training_loop | imports: torch, torchvision | [test quantization core experimental quantization_util.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_adaround_eager.py",
    "summary": "No description | classes: FeedForwardWrapper, LinearChain, ConvChain, TestAdaround | functions: forward_wrapper, forward | imports: copy, torch | [test quantization core experimental test_adaround_eager.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_bits.py",
    "summary": "No description | classes: Int16Tensor, TestBits | imports: torch | [test quantization core experimental test_bits.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_fake_quantize.py",
    "summary": "Tests fake quantize calculate_qparams() method | classes: TestFakeQuantize | imports: torch, unittest | [test quantization core experimental test_fake_quantize.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_float8.py",
    "summary": "No description | classes: TestFloat8Dtype, TestFloat8DtypeCPUOnly | functions: _int_bits_to_float, simulate_fp8_precision, _round_e8m0_rne | imports: struct, unittest, torch | [test quantization core experimental test_float8.py]",
    "role": "src",
    "loc": 345
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_linear.py",
    "summary": "Test linear_APoT_fn by comparing to uniform linear | classes: TestNonUniformObserver | imports: torch, unittest | [test quantization core experimental test_linear.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_nonuniform_observer.py",
    "summary": "Test case 1: calculate_qparams | classes: TestNonUniformObserver | imports: torch, unittest | [test quantization core experimental test_nonuniform_observer.py]",
    "role": "src",
    "loc": 141
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_quantized_tensor.py",
    "summary": "Tests int_repr on APoTQuantizer with random tensor2quantize | classes: TestQuantizedTensor | imports: torch, unittest | [test quantization core experimental test_quantized_tensor.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "test\\quantization\\core\\experimental\\test_quantizer.py",
    "summary": "Tests quantize_APoT result on random 1-dim tensor | classes: TestQuantizer | imports: torch, unittest, random | [test quantization core experimental test_quantizer.py]",
    "role": "src",
    "loc": 150
  },
  {
    "id": "test\\quantization\\eager\\test_bias_correction_eager.py",
    "summary": "No description | classes: LinearChain, ConvChain, TestBiasCorrectionEager | imports: torch, copy | [test quantization eager test_bias_correction_eager.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "test\\quantization\\eager\\test_equalize_eager.py",
    "summary": "No description | classes: ChainModule, M, TestEqualizeEager | imports: torch, copy | [test quantization eager test_equalize_eager.py]",
    "role": "src",
    "loc": 150
  },
  {
    "id": "test\\quantization\\eager\\test_fuse_eager.py",
    "summary": "No description | classes: TestFuseEager | imports: copy, torch | [test quantization eager test_fuse_eager.py]",
    "role": "src",
    "loc": 394
  },
  {
    "id": "test\\quantization\\eager\\test_model_numerics.py",
    "summary": "No description | classes: TestModelNumericsEager | imports: torch | [test quantization eager test_model_numerics.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "test\\quantization\\eager\\test_numeric_suite_eager.py",
    "summary": "No description | classes: SubModule, ModelWithSubModules, ModelWithFunctionals, TestNumericSuiteEager | imports: unittest, torch, torchvision | [test quantization eager test_numeric_suite_eager.py]",
    "role": "src",
    "loc": 482
  },
  {
    "id": "test\\quantization\\eager\\test_quantize_eager_ptq.py",
    "summary": "No description | classes: M, RefM, TestQuantizeEagerOps, EmbeddingBagWithLinear, CustomModule, ObservedCustomModule | imports: torch, hypothesis, numpy | [test quantization eager test_quantize_eager_ptq.py]",
    "role": "src",
    "loc": 1234
  },
  {
    "id": "test\\quantization\\eager\\test_quantize_eager_qat.py",
    "summary": "Conv-BN fusion implemented with explicit folding. Useful | classes: _ReferenceConvBnNd, _ReferenceConvBn2d, M, TestQuantizeEagerQAT, TestQuantizeEagerQATNumerics | imports: copy, torch, hypothesis, functools | [test quantization eager test_quantize_eager_qat.py]",
    "role": "src",
    "loc": 932
  },
  {
    "id": "test\\quantization\\eager\\__init__.py",
    "summary": "Package initializer | [test quantization eager __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\fx\\test_equalize_fx.py",
    "summary": "No description | classes: TestBranchingWithoutEqualizationModel, TestBranchingWithEqualizationModel, M, TestEqualizeFx | imports: torch, copy, numpy, hypothesis | [test quantization fx test_equalize_fx.py]",
    "role": "src",
    "loc": 725
  },
  {
    "id": "test\\quantization\\fx\\test_model_report_fx.py",
    "summary": "Returns a model that has been prepared for callibration and corresponding model_report | classes: ThreeOps, TwoThreeOps, ConvLinearModel, QATConvLinearReluModel, TestFxModelReportDetector, NestedModifiedSingleLayerLinear | functions: _get_prepped_for_calibration_model_helper | imports: torch | [test",
    "role": "src",
    "loc": 1235
  },
  {
    "id": "test\\quantization\\fx\\test_numeric_suite_fx.py",
    "summary": "No description | classes: LinearReluFunctional, LinearFunctional, LinearReluLinearFunctional, AddMulFunctional, AllConvAndLinearFusionModules, AllConvFunctional | functions: _wrapped_hardswish, _wrapped_hardswish_fp16, _wrapped_sigmoid, _wrapped_linear, get_all_quant_patterns | imports: copy, operat",
    "role": "src",
    "loc": 2383
  },
  {
    "id": "test\\quantization\\fx\\test_quantize_fx.py",
    "summary": "No description | classes: BinaryOp, BinaryOpNonQuantizedInput, BinaryOpRelu, M, LinearRelu, MyConvReLU | functions: _user_func_with_complex_return_type | imports: torch, hypothesis, copy, operator | [test quantization fx test_quantize_fx.py]",
    "role": "src",
    "loc": 8236
  },
  {
    "id": "test\\quantization\\fx\\test_subgraph_rewriter.py",
    "summary": "No description | classes: M, Pattern, Replacement, Comparison, M1, M2 | imports: torch | [test quantization fx test_subgraph_rewriter.py]",
    "role": "src",
    "loc": 353
  },
  {
    "id": "test\\quantization\\fx\\__init__.py",
    "summary": "Package initializer | [test quantization fx __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\jit\\test_deprecated_jit_quant.py",
    "summary": "No description | classes: FooBar, Linear, TestDeprecatedJitQuantized | imports: torch | [test quantization jit test_deprecated_jit_quant.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "test\\quantization\\jit\\test_fusion_passes.py",
    "summary": "No description | classes: MAdd, MAddOut, MAddScalar, MAddScalarOut, TestFusionPasses | imports: torch | [test quantization jit test_fusion_passes.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "test\\quantization\\jit\\test_ondevice_quantization.py",
    "summary": "No description | classes: myMod, MyConvLinearModule, OnDevicePTQUtils, TestOnDeviceDynamicPTQInsertObservers, TestOnDeviceDynamicPTQInsertQuantDequant, TestOnDeviceDynamicPTQFinalize | imports: io, torch | [test quantization jit test_ondevice_quantization.py]",
    "role": "src",
    "loc": 453
  },
  {
    "id": "test\\quantization\\jit\\test_quantize_jit.py",
    "summary": "No description | classes: M, TestModule, SubModule, CustomConv, CustomBn, FunctionalLinear | imports: io, unittest, torch | [test quantization jit test_quantize_jit.py]",
    "role": "src",
    "loc": 3212
  },
  {
    "id": "test\\quantization\\jit\\__init__.py",
    "summary": "Package initializer | [test quantization jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\quantization\\pt2e\\test_duplicate_dq.py",
    "summary": "No description | classes: Conv2dWithObsSharingOps, Conv2dWithSharedDQ, ModuleForDifferentQconfig, TestHelperModules, BackendAQuantizer, TestDuplicateDQPass | imports: copy, unittest, torch | [test quantization pt2e test_duplicate_dq.py]",
    "role": "src",
    "loc": 270
  },
  {
    "id": "test\\quantization\\pt2e\\test_graph_utils.py",
    "summary": "No description | classes: M, TestGraphUtils | imports: copy, unittest, torch | [test quantization pt2e test_graph_utils.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "test\\quantization\\pt2e\\test_metadata_porting.py",
    "summary": "No description | classes: Conv2dWithObsSharingOps, TestHelperModules, BackendAQuantizer, MatmulWithConstInput, TestMetaDataPorting | functions: _tag_partitions | imports: copy, unittest, torch | [test quantization pt2e test_metadata_porting.py]",
    "role": "src",
    "loc": 456
  },
  {
    "id": "test\\quantization\\pt2e\\test_numeric_debugger.py",
    "summary": "No description | classes: TestNumericDebugger | imports: copy, unittest, torch | [test quantization pt2e test_numeric_debugger.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "test\\quantization\\pt2e\\test_quantize_pt2e.py",
    "summary": "No description | classes: BackendAQuantizer, M, DtypeActQuantizer, BadQuantizer, TestQuantizer, TestQuantizer1 | imports: torch | [test quantization pt2e test_quantize_pt2e.py]",
    "role": "src",
    "loc": 2224
  },
  {
    "id": "test\\quantization\\pt2e\\test_quantize_pt2e_qat.py",
    "summary": "No description | classes: _BaseConvBnModel, PT2EQATTestCase, M, M2, TestQuantizePT2EQAT_ConvBn_Base, TestQuantizePT2EQAT_ConvBn1d | functions: _is_conv_node, _get_conv_bn_getitem_nodes | imports: copy, operator, unittest, torch | [test quantization pt2e test_quantize_pt2e_qat.py]",
    "role": "src",
    "loc": 963
  },
  {
    "id": "test\\quantization\\pt2e\\test_representation.py",
    "summary": "No description | classes: M, TestPT2ERepresentation | imports: copy, torch | [test quantization pt2e test_representation.py]",
    "role": "src",
    "loc": 258
  },
  {
    "id": "test\\quantization\\pt2e\\test_x86inductor_quantizer.py",
    "summary": "No description | classes: NodePosType, SingleConv2dModule, Conv2dUnaryModule, Conv2dAddModule, Conv2dAddReLUModule, Conv2dSingleOpPowModule | imports: copy, torch | [test quantization pt2e test_x86inductor_quantizer.py]",
    "role": "src",
    "loc": 2448
  },
  {
    "id": "test\\quantization\\pt2e\\test_xnnpack_quantizer.py",
    "summary": "No description | classes: Sub, M, RNNDynamicModel, TestXNNPACKQuantizer, TestXNNPACKQuantizerModels | imports: copy, operator, torch, torchvision | [test quantization pt2e test_xnnpack_quantizer.py]",
    "role": "src",
    "loc": 924
  },
  {
    "id": "test\\scripts\\cuda_memcheck_common.py",
    "summary": "Whenever the simple parser is unable to parse the report, this exception will be raised | classes: ParseError, Report, Error | functions: parse | [test scripts cuda_memcheck_common.py]",
    "role": "scripts",
    "loc": 83
  },
  {
    "id": "test\\scripts\\run_cuda_memcheck.py",
    "summary": "This script runs cuda-memcheck on the specified unit test. Each test case | classes: ProgressbarStub | functions: is_ignored_only, is_cpu_only | imports: argparse, asyncio, multiprocessing, subprocess | [test scripts run_cuda_memcheck.py]",
    "role": "scripts",
    "loc": 154
  },
  {
    "id": "test\\strobelight\\examples\\cli_function_profiler_example.py",
    "summary": "No description | functions: fn, work, work2 | imports: torch | [test strobelight examples cli_function_profiler_example.py]",
    "role": "examples",
    "loc": 24
  },
  {
    "id": "test\\strobelight\\examples\\compile_time_profile_example.py",
    "summary": "No description | functions: fn, work, func4 | imports: torch | [test strobelight examples compile_time_profile_example.py]",
    "role": "examples",
    "loc": 19
  },
  {
    "id": "test\\torch_np\\check_tests_conform.py",
    "summary": "Check a test file for common issues with pytest->pytorch conversion. | functions: check, is_comment, report_violation | imports: textwrap | [test torch_np check_tests_conform.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "test\\torch_np\\conftest.py",
    "summary": "No description | classes: Inaccessible | functions: pytest_configure, pytest_addoption, pytest_sessionstart, pytest_generate_tests, pytest_collection_modifyitems | imports: pytest, torch, numpy | [test torch_np conftest.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "test\\torch_np\\test_basic.py",
    "summary": "Base for smoke tests of one-arg functions: (array_like) -> (array_like) | classes: TestOneArr, TestOneArrAndAxis, TestOneArrAndAxesTuple, TestOneArrAndShape, TestOneArrToScalar, TestShapeLikeToArray | imports: functools, inspect, unittest, numpy | [test torch_np test_basic.py]",
    "role": "src",
    "loc": 438
  },
  {
    "id": "test\\torch_np\\test_binary_ufuncs.py",
    "summary": "No description | classes: TestBinaryUfuncBasic | imports: numpy, torch | [test torch_np test_binary_ufuncs.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "test\\torch_np\\test_dtype.py",
    "summary": "No description | classes: TestConvertDType | imports: unittest, numpy, torch | [test torch_np test_dtype.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "test\\torch_np\\test_function_base.py",
    "summary": "No description | classes: TestAppend | imports: pytest, torch, numpy | [test torch_np test_function_base.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "test\\torch_np\\test_ndarray_methods.py",
    "summary": "No description | classes: TestIndexing, TestReshape, TestTranspose, TestRavel, TestNonzero, TestArgmaxArgminCommon | imports: unittest, numpy, pytest, torch | [test torch_np test_ndarray_methods.py]",
    "role": "src",
    "loc": 527
  },
  {
    "id": "test\\torch_np\\test_nep50_examples.py",
    "summary": "Test examples for NEP 50. | classes: TestNEP50Table, TestCompareToNumpy | imports: unittest, numpy, torch, pytest | [test torch_np test_nep50_examples.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "test\\torch_np\\test_random.py",
    "summary": "Light smoke test switching between numpy to pytorch random streams. | classes: TestScalarReturn, TestShuffle, TestChoice, TestNumpyGlobal | functions: control_stream | imports: functools, numpy, pytest, torch | [test torch_np test_random.py]",
    "role": "src",
    "loc": 109
  },
  {
    "id": "test\\torch_np\\test_reductions.py",
    "summary": "No description | classes: TestFlatnonzero, TestAny, TestAll, TestMean, TestSum, TestGenericReductions | imports: unittest, numpy, pytest, torch | [test torch_np test_reductions.py]",
    "role": "src",
    "loc": 385
  },
  {
    "id": "test\\torch_np\\test_scalars_0D_arrays.py",
    "summary": "Basic tests to assert and illustrate the  behavior around the decision to use 0D | classes: TestArrayScalars, TestIsScalar | imports: torch, numpy | [test torch_np test_scalars_0D_arrays.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "test\\torch_np\\test_ufuncs_basic.py",
    "summary": "Poking around ufunc casting/broadcasting/dtype/out behavior. | classes: TestUnaryUfuncs, TestBinaryUfuncs, TestNdarrayDunderVsUfunc, TestUfuncDtypeKwd | imports: operator, unittest, pytest, torch | [test torch_np test_ufuncs_basic.py]",
    "role": "src",
    "loc": 302
  },
  {
    "id": "test\\torch_np\\test_unary_ufuncs.py",
    "summary": "No description | classes: TestUnaryUfuncs | imports: numpy, torch | [test torch_np test_unary_ufuncs.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "test\\torch_np\\__init__.py",
    "summary": "Package initializer | [test torch_np __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_dlpack.py",
    "summary": "No description | classes: TestDLPack | imports: functools, unittest, numpy, pytest | [test torch_np numpy_tests core test_dlpack.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_dtype.py",
    "summary": "No description | classes: TestBuiltin, TestDtypeAttributeDeletion, TestPickling, TestPromotion, TestMisc, dt | functions: assert_dtype_equal, assert_dtype_not_equal | imports: functools, operator, pickle, types | [test torch_np numpy_tests core test_dtype.py]",
    "role": "src",
    "loc": 290
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_einsum.py",
    "summary": "No description | classes: TestEinsum, TestEinsumPath, TestMisc | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests core test_einsum.py]",
    "role": "src",
    "loc": 1021
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_getlimits.py",
    "summary": "Test functions for limits module. | classes: TestPythonFloat, TestHalf, TestSingle, TestDouble, TestFinfo, TestIinfo | functions: assert_ma_equal | imports: functools, unittest, numpy, pytest | [test torch_np numpy_tests core test_getlimits.py]",
    "role": "src",
    "loc": 185
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_indexing.py",
    "summary": "No description | classes: TupleSubclass, SequenceLike, TestIndexing, TestBroadcastedAssignments, TestFancyIndexingCast, TestMultiIndexingAutomated | imports: functools, operator, unittest, pytest | [test torch_np numpy_tests core test_indexing.py]",
    "role": "src",
    "loc": 832
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_multiarray.py",
    "summary": "No description | classes: subclass, frominterface, MyArr, TestFlag, TestHash, TestAttributes | functions: runstring, temppath, _aligned_zeros, _mean, _var, _std | imports: builtins, ctypes, functools, io | [test torch_np numpy_tests core test_multiarray.py]",
    "role": "src",
    "loc": 5392
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_numeric.py",
    "summary": "No description | classes: TestResize, TestNonarrayArgs, TestIsscalar, TestBoolScalar, TestBoolArray, TestBoolCmp | imports: functools, platform, numpy, pytest | [test torch_np numpy_tests core test_numeric.py]",
    "role": "src",
    "loc": 2394
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_numerictypes.py",
    "summary": "No description | classes: TestCommonType, TestIsSubDType, TestBitName, TestDocStrings, TestScalarTypeNames | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests core test_numerictypes.py]",
    "role": "src",
    "loc": 120
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_scalarinherit.py",
    "summary": "Test printing of scalar types. | classes: A, B, C, D, B0, C0 | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests core test_scalarinherit.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_scalarmath.py",
    "summary": "No description | classes: TestTypes, TestBaseMath, TestPower, TestModulus, TestComplexDivision, TestConversion | functions: floordiv_and_mod, _signs, recursionlimit | imports: functools, operator, unittest, numpy | [test torch_np numpy_tests core test_scalarmath.py]",
    "role": "src",
    "loc": 732
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_scalar_ctors.py",
    "summary": "Test the scalar constructors, which also do type-coercion | classes: TestFromString, TestFromInt, TestArrayFromScalar | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests core test_scalar_ctors.py]",
    "role": "src",
    "loc": 91
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_scalar_methods.py",
    "summary": "Test the scalar constructors, which also do type-coercion | classes: TestAsIntegerRatio, TestIsInteger, TestClassGetItem, TestClassGetitemMisc, TestBitCount | imports: fractions, functools, types, unittest | [test torch_np numpy_tests core test_scalar_methods.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "test\\torch_np\\numpy_tests\\core\\test_shape_base.py",
    "summary": "No description | classes: TestAtleast1d, TestAtleast2d, TestAtleast3d, TestHstack, TestVstack, TestConcatenate | imports: functools, unittest, numpy, pytest | [test torch_np numpy_tests core test_shape_base.py]",
    "role": "src",
    "loc": 669
  },
  {
    "id": "test\\torch_np\\numpy_tests\\fft\\test_helper.py",
    "summary": "Test functions for fftpack.helper module | classes: TestFFTShift, TestFFTFreq, TestRFFTFreq, TestIRFFTN | imports: torch, numpy | [test torch_np numpy_tests fft test_helper.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "test\\torch_np\\numpy_tests\\fft\\test_pocketfft.py",
    "summary": "No description | classes: TestFFTShift, TestFFT1D, TestFFTThreadSafe | functions: fft1 | imports: functools, queue, threading, unittest | [test torch_np numpy_tests fft test_pocketfft.py]",
    "role": "src",
    "loc": 331
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_arraypad.py",
    "summary": "No description | classes: TestConstant | imports: unittest, torch, numpy | [test torch_np numpy_tests lib test_arraypad.py]",
    "role": "src",
    "loc": 554
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_arraysetops.py",
    "summary": "Test functions for 1D array set operations. | classes: Test, TestSetOps, TestUnique | imports: unittest, numpy, pytest, torch | [test torch_np numpy_tests lib test_arraysetops.py]",
    "role": "src",
    "loc": 699
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_function_base.py",
    "summary": "No description | classes: TestRot90, TestFlip, TestAny, TestAll, TestCopy, TestAverage | functions: get_mat, _make_complex, _foo1, _foo2 | imports: functools, operator, fractions, unittest | [test torch_np numpy_tests lib test_function_base.py]",
    "role": "src",
    "loc": 3122
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_histograms.py",
    "summary": "No description | classes: TestHistogram, TestHistogramOptimBinNums, TestHistogramdd | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests lib test_histograms.py]",
    "role": "src",
    "loc": 765
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_index_tricks.py",
    "summary": "No description | classes: TestRavelUnravelIndex, TestGrid, TestConcatenator, TestNdenumerate, TestIndexExpression, TestIx_ | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests lib test_index_tricks.py]",
    "role": "src",
    "loc": 468
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_shape_base_.py",
    "summary": "hack in keepdims behavior into a function taking an axis | classes: TestTakeAlongAxis, TestPutAlongAxis, TestApplyAlongAxis, TestApplyOverAxes, TestExpandDims, TestArraySplit | functions: _add_keepdims, wrapped, compare_results | imports: functools, unittest, pytest, torch | [test torch_np numpy_tes",
    "role": "src",
    "loc": 682
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_twodim_base.py",
    "summary": "Test functions for matrix module | classes: TestEye, TestDiag, TestFliplr, TestFlipud, TestHistogram2d, TestTri | functions: get_mat | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests lib test_twodim_base.py]",
    "role": "src",
    "loc": 476
  },
  {
    "id": "test\\torch_np\\numpy_tests\\lib\\test_type_check.py",
    "summary": "No description | classes: TestCommonType, TestMintypecode, TestIsscalar, TestReal, TestImag, TestIscomplex | functions: assert_all | imports: functools, unittest, pytest, torch | [test torch_np numpy_tests lib test_type_check.py]",
    "role": "src",
    "loc": 348
  },
  {
    "id": "test\\torch_np\\numpy_tests\\linalg\\test_linalg.py",
    "summary": "Test functions for linalg module | classes: LinalgCase, LinalgTestCase, LinalgSquareTestCase, LinalgNonsquareTestCase, HermitianTestCase, LinalgGeneralizedSquareTestCase | functions: consistent_subclass, assert_almost_equal, get_real_dtype, get_complex_dtype, get_rtol, apply_tag | imports: functools",
    "role": "src",
    "loc": 1728
  },
  {
    "id": "test\\typing\\fail\\arithmetic_ops.py",
    "summary": "No description | imports: typing_extensions, torch | [test typing fail arithmetic_ops.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "test\\typing\\fail\\creation_ops.py",
    "summary": "No description | imports: torch | [test typing fail creation_ops.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "test\\typing\\fail\\disabled_bitwise_ops.py",
    "summary": "No description | imports: torch | [test typing fail disabled_bitwise_ops.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\typing\\fail\\random.py",
    "summary": "No description | imports: torch | [test typing fail random.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "test\\typing\\fail\\torch_size.py",
    "summary": "No description | imports: torch | [test typing fail torch_size.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "test\\typing\\pass\\arithmetic_ops.py",
    "summary": "This class demonstrates what is possible by overriding every magic method | classes: Binary | imports: typing_extensions, torch | [test typing pass arithmetic_ops.py]",
    "role": "src",
    "loc": 343
  },
  {
    "id": "test\\typing\\pass\\creation_ops.py",
    "summary": "No description | imports: typing_extensions, torch, numpy | [test typing pass creation_ops.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "test\\typing\\pass\\cuda_steam.py",
    "summary": "No description | functions: foo | imports: torch | [test typing pass cuda_steam.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "test\\typing\\pass\\disabled_jit.py",
    "summary": "No description | classes: Color | imports: typing_extensions, pytest, torch | [test typing pass disabled_jit.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "test\\typing\\pass\\distributions.py",
    "summary": "No description | imports: typing_extensions, torch | [test typing pass distributions.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "test\\typing\\pass\\math_ops.py",
    "summary": "No description | imports: torch | [test typing pass math_ops.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "test\\typing\\pass\\torch_size.py",
    "summary": "No description | classes: ZeroIndex | imports: typing_extensions, torch | [test typing pass torch_size.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "test\\typing\\reveal\\module_list.py",
    "summary": "No description | classes: FooModule, BarModule | imports: torch | [test typing reveal module_list.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "test\\typing\\reveal\\namedtuple.py",
    "summary": "No description | imports: torch | [test typing reveal namedtuple.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "test\\typing\\reveal\\opt_size.py",
    "summary": "No description | imports: torch | [test typing reveal opt_size.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\typing\\reveal\\size.py",
    "summary": "No description | imports: torch | [test typing reveal size.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "test\\typing\\reveal\\tensor_constructors.py",
    "summary": "No description | imports: torch, numpy | [test typing reveal tensor_constructors.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "test\\typing\\reveal\\tensor_copy.py",
    "summary": "No description | imports: torch | [test typing reveal tensor_copy.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\typing\\reveal\\tensor_sampling.py",
    "summary": "No description | imports: torch | [test typing reveal tensor_sampling.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "test\\typing\\reveal\\torch_optim.py",
    "summary": "No description | functions: foo | imports: torch | [test typing reveal torch_optim.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "test\\xpu\\test_conv.py",
    "summary": "No description | classes: TestConvolutionNNDeviceType | imports: unittest, torch, scipy | [test xpu test_conv.py]",
    "role": "src",
    "loc": 1139
  },
  {
    "id": "test\\xpu\\test_gemm.py",
    "summary": "No description | classes: TestBasicGEMM | imports: random, functools, numpy, torch | [test xpu test_gemm.py]",
    "role": "src",
    "loc": 944
  },
  {
    "id": "third_party\\build_bundled.py",
    "summary": "No description | functions: collect_license, create_bundled, identify_license, squeeze | imports: argparse | [third_party build_bundled.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "third_party\\generate-cpuinfo-wrappers.py",
    "summary": "No description | [third_party generate-cpuinfo-wrappers.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "third_party\\generate-xnnpack-wrappers.py",
    "summary": "No description | functions: handle_singleline_parse, update_sources, gen_wrappers, main | [third_party generate-xnnpack-wrappers.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "tools\\build_libtorch.py",
    "summary": "No description | imports: argparse, tools | [tools build_libtorch.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "tools\\build_pytorch_libs.py",
    "summary": "No description | functions: _get_vc_env, _overlay_windows_vcvars, _create_build_env, read_nccl_pin, checkout_nccl, build_pytorch | imports: platform, subprocess, glob, setup_helpers | [tools build_pytorch_libs.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "tools\\build_with_debinfo.py",
    "summary": "No description | functions: check_output, parse_args, get_lib_extension, create_symlinks, has_build_ninja, is_devel_setup | imports: subprocess, argparse | [tools build_with_debinfo.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "tools\\download_mnist.py",
    "summary": "No description | functions: report_download_progress, download, unzip, main | imports: argparse, gzip, urllib | [tools download_mnist.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "tools\\extract_scripts.py",
    "summary": "No description | classes: Script | functions: extract, main | imports: argparse, typing_extensions, yaml | [tools extract_scripts.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "tools\\generate_torch_version.py",
    "summary": "No description | functions: get_sha, get_tag, get_torch_version | imports: argparse, subprocess, setuptools | [tools generate_torch_version.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "tools\\gen_vulkan_spv.py",
    "summary": "No description | classes: UniqueKeyLoader, SPVGenerator, ShaderInfo | functions: extract_filename, extract_leading_whitespace, escape, preprocess, getName, isDescriptorLine | imports: argparse, array, codecs, copy | [tools gen_vulkan_spv.py]",
    "role": "src",
    "loc": 585
  },
  {
    "id": "tools\\nightly.py",
    "summary": "Checks out the nightly development version of PyTorch and installs pre-built | classes: PipSource, Formatter, Venv | functions: timer, timed, decorator, wrapper, git, logging_base_dir | imports: argparse, atexit, functools, shlex | [tools nightly.py]",
    "role": "src",
    "loc": 808
  },
  {
    "id": "tools\\nightly_hotpatch.py",
    "summary": "Parses command-line arguments using argparse. | functions: parse_arguments, get_pytorch_path, handle_import_error, download_patch, apply_patch, main | imports: argparse, shutil, subprocess, tempfile | [tools nightly_hotpatch.py]",
    "role": "src",
    "loc": 180
  },
  {
    "id": "tools\\nvcc_fix_deps.py",
    "summary": "Tool to fix the nvcc's dependecy file output | functions: resolve_include, repair_depfile, extract_include_arg, extract_one | imports: subprocess | [tools nvcc_fix_deps.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "tools\\render_junit.py",
    "summary": "No description | functions: parse_junit_reports, parse_file, convert_junit_to_testcases, render_tests, parse_args, main | imports: argparse, junitparser, rich | [tools render_junit.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "tools\\substitute.py",
    "summary": "No description | functions: main | imports: argparse | [tools substitute.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "tools\\update_masked_docs.py",
    "summary": "This script updates the file torch/masked/_docs.py that contains | functions: main | imports: torch | [tools update_masked_docs.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "tools\\vscode_settings.py",
    "summary": "No description | functions: deep_update, main | imports: json5, json | [tools vscode_settings.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "tools\\__init__.py",
    "summary": "Package initializer | [tools __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\alerts\\create_alerts.py",
    "summary": "No description | classes: JobStatus | functions: fetch_hud_data, map_job_data, is_job_failed, is_job_skipped, get_failed_jobs, classify_jobs | imports: argparse, json, difflib, requests | [tools alerts create_alerts.py]",
    "role": "src",
    "loc": 258
  },
  {
    "id": "tools\\alerts\\__init__.py",
    "summary": "Package initializer | [tools alerts __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\amd_build\\build_amd.py",
    "summary": "No description | functions: is_hip_clang, remove_hcc | imports: argparse, hipify | [tools amd_build build_amd.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "tools\\autograd\\context.py",
    "summary": "No description | functions: with_native_function_with_differentiability_info, wrapper, with_native_function_with_differentiability_info_and_key | imports: functools, torchgen | [tools autograd context.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "tools\\autograd\\gen_annotated_fn_args.py",
    "summary": "For procedural tests needed for __torch_function__, we use this function | functions: gen_annotated, gen_annotated_args, _get_kwargs_func_exclusion_list, _add_out_arg, main | imports: argparse, textwrap, torchgen, gen_python_functions | [tools autograd gen_annotated_fn_args.py]",
    "role": "src",
    "loc": 112
  },
  {
    "id": "tools\\autograd\\gen_autograd.py",
    "summary": "To run this file by hand from the root of the PyTorch | functions: gen_autograd, gen_autograd_python, main | imports: argparse, torchgen, gen_autograd_functions, gen_inplace_or_view_type | [tools autograd gen_autograd.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "tools\\autograd\\gen_autograd_functions.py",
    "summary": "No description | functions: get_infos_with_derivatives_list, gen_autograd_functions_lib, gen_autograd_functions_python, process_function, save_var, emit_derivative | imports: torchgen, gen_inplace_or_view_type | [tools autograd gen_autograd_functions.py]",
    "role": "src",
    "loc": 894
  },
  {
    "id": "tools\\autograd\\gen_inplace_or_view_type.py",
    "summary": "No description | functions: is_tensor_type, is_tensor_list_type, unpacked_name, inverse_view_name, extract_bindings, unpack_args | imports: torchgen, context, gen_trace_type, gen_view_funcs | [tools autograd gen_inplace_or_view_type.py]",
    "role": "src",
    "loc": 532
  },
  {
    "id": "tools\\autograd\\gen_python_functions.py",
    "summary": "No description | functions: should_generate_py_binding, get_pycname, is_noarg, is_py_variable_method, is_py_torch_function, is_py_nn_function | imports: yaml, torchgen, gen_inplace_or_view_type, gen_trace_type | [tools autograd gen_python_functions.py]",
    "role": "src",
    "loc": 1039
  },
  {
    "id": "tools\\autograd\\gen_trace_type.py",
    "summary": "No description | functions: should_trace, format_trace_op_name, format_trace_inputs, dispatch_trace_input, format_prerecord_trace, format_postrecord_trace | imports: torchgen | [tools autograd gen_trace_type.py]",
    "role": "src",
    "loc": 376
  },
  {
    "id": "tools\\autograd\\gen_variable_factories.py",
    "summary": "No description | functions: fully_qualified_type, maybe_optional_type, gen_variable_factories, is_factory_function, process_function | imports: torchgen | [tools autograd gen_variable_factories.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "tools\\autograd\\gen_variable_type.py",
    "summary": "VariableType.h and VariableType.cpp body | functions: gen_variable_type, wrapper_registrations, gen_wrapper_registration, gen_variable_type_func, emit_body, gen_differentiable_input | imports: torchgen, context, gen_inplace_or_view_type, gen_trace_type | [tools autograd gen_variable_type.py]",
    "role": "src",
    "loc": 1837
  },
  {
    "id": "tools\\autograd\\gen_view_funcs.py",
    "summary": "No description | functions: view_func_name, is_symint_or_tensor, remove_const_ref, returns_multi_tensor, generate_state_getter_setter, process_function | imports: torchgen, gen_inplace_or_view_type | [tools autograd gen_view_funcs.py]",
    "role": "src",
    "loc": 255
  },
  {
    "id": "tools\\autograd\\load_derivatives.py",
    "summary": "No description | functions: add_view_copy_derivatives, load_derivatives, cpp_arguments, create_derivative, create_forward_derivative, postprocess_forward_derivatives | imports: yaml, torchgen | [tools autograd load_derivatives.py]",
    "role": "src",
    "loc": 783
  },
  {
    "id": "tools\\autograd\\__init__.py",
    "summary": "Package initializer | [tools autograd __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\code_analyzer\\gen_operators_yaml.py",
    "summary": "No description | functions: canonical_opnames, make_filter_from_options, is_model_included, is_new_style_rule, verify_all_specified_present, find_missing_items | imports: argparse, json, yaml, gen_op_registration_allowlist | [tools code_analyzer gen_operators_yaml.py]",
    "role": "src",
    "loc": 445
  },
  {
    "id": "tools\\code_analyzer\\gen_oplist.py",
    "summary": "No description | functions: extract_all_operators, extract_training_operators, throw_if_any_op_includes_overloads, gen_supported_mobile_models, main | imports: argparse, json, functools, yaml | [tools code_analyzer gen_oplist.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "tools\\code_analyzer\\gen_op_registration_allowlist.py",
    "summary": "This util is invoked from cmake to produce the op registration allowlist param | functions: canonical_name, load_op_dep_graph, load_root_ops, gen_transitive_closure, gen_transitive_closure_str | imports: argparse, yaml | [tools code_analyzer gen_op_registration_allowlist.py]",
    "role": "src",
    "loc": 68
  },
  {
    "id": "tools\\code_coverage\\oss_coverage.py",
    "summary": "No description | functions: report_coverage | imports: package | [tools code_coverage oss_coverage.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "tools\\code_coverage\\package\\__init__.py",
    "summary": "Package initializer | [tools code_coverage package __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\code_coverage\\package\\oss\\cov_json.py",
    "summary": "No description | functions: get_json_report | imports: tool, util, init, run | [tools code_coverage package oss cov_json.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "tools\\code_coverage\\package\\oss\\init.py",
    "summary": "No description | functions: initialization, add_arguments_oss, parse_arguments, get_test_list_by_type, get_test_list, empty_list_if_none | imports: argparse, util, utils | [tools code_coverage package oss init.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "tools\\code_coverage\\package\\oss\\run.py",
    "summary": "No description | functions: clang_run, gcc_run | imports: tool, util, utils | [tools code_coverage package oss run.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "tools\\code_coverage\\package\\oss\\utils.py",
    "summary": "No description | functions: get_oss_binary_folder, get_oss_shared_library, get_oss_binary_file, get_llvm_tool_path, get_pytorch_folder, detect_compiler_type | imports: subprocess, util | [tools code_coverage package oss utils.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "tools\\code_coverage\\package\\oss\\__init__.py",
    "summary": "Package initializer | [tools code_coverage package oss __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\clang_coverage.py",
    "summary": "No description | functions: create_corresponding_folder, run_target, merge_target, export_target, merge, export | imports: subprocess, util, utils, oss | [tools code_coverage package tool clang_coverage.py]",
    "role": "src",
    "loc": 147
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\gcc_coverage.py",
    "summary": "No description | functions: update_gzip_dict, run_target, export | imports: subprocess, oss, util, utils | [tools code_coverage package tool gcc_coverage.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\print_report.py",
    "summary": "No description | functions: key_by_percentage, key_by_name, is_intrested_file, is_this_type_of_tests, print_test_by_type, print_test_condition | imports: subprocess, oss, util | [tools code_coverage package tool print_report.py]",
    "role": "src",
    "loc": 193
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\summarize_jsons.py",
    "summary": "No description | functions: transform_file_name, is_intrested_file, get_json_obj, parse_json, parse_jsons, update_coverage | imports: json, util, parser, print_report | [tools code_coverage package tool summarize_jsons.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\utils.py",
    "summary": "No description | functions: run_cpp_test, get_tool_path_by_platform | imports: subprocess, util, caffe2, oss | [tools code_coverage package tool utils.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\__init__.py",
    "summary": "Package initializer | [tools code_coverage package tool __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\parser\\coverage_record.py",
    "summary": "No description | classes: CoverageRecord | [tools code_coverage package tool parser coverage_record.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\parser\\gcov_coverage_parser.py",
    "summary": "Accepts a parsed json produced by gcov --json-format -- typically, | classes: GcovCoverageParser | imports: coverage_record | [tools code_coverage package tool parser gcov_coverage_parser.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\parser\\llvm_coverage_parser.py",
    "summary": "Accepts a parsed json produced by llvm-cov export -- typically, | classes: LlvmCoverageParser | imports: coverage_record, llvm_coverage_segment | [tools code_coverage package tool parser llvm_coverage_parser.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\parser\\llvm_coverage_segment.py",
    "summary": "Creates LlvmCoverageSegment from a list of lists in llvm export json. | classes: LlvmCoverageSegment | functions: parse_segments | [tools code_coverage package tool parser llvm_coverage_segment.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "tools\\code_coverage\\package\\tool\\parser\\__init__.py",
    "summary": "Package initializer | [tools code_coverage package tool parser __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\code_coverage\\package\\util\\setting.py",
    "summary": "No description | classes: TestType, Test, Option, TestPlatform, CompilerType | [tools code_coverage package util setting.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "tools\\code_coverage\\package\\util\\utils.py",
    "summary": "No description | functions: convert_time, print_time, print_log, print_error, remove_file, remove_folder | imports: shutil, setting, package, caffe2 | [tools code_coverage package util utils.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "tools\\code_coverage\\package\\util\\utils_init.py",
    "summary": "No description | functions: remove_files, create_folders, add_arguments_utils, have_option, get_options | imports: argparse, setting, utils | [tools code_coverage package util utils_init.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "tools\\code_coverage\\package\\util\\__init__.py",
    "summary": "Package initializer | [tools code_coverage package util __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\coverage_plugins_package\\setup.py",
    "summary": "No description | imports: setuptools | [tools coverage_plugins_package setup.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "tools\\coverage_plugins_package\\src\\coverage_plugins\\jit_plugin.py",
    "summary": "This coverage plug-in attempts to cover JIT'd functions and methods that were previously missed in code coverage. Any | classes: JitPlugin | functions: is_not_builtin_class, coverage_init | imports: inspect, coverage | [tools coverage_plugins_package src coverage_plugins jit_plugin.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "tools\\coverage_plugins_package\\src\\coverage_plugins\\__init__.py",
    "summary": "Package initializer | [tools coverage_plugins_package src coverage_plugins __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\dynamo\\verify_dynamo.py",
    "summary": "No description | classes: VerifyDynamoError, Module | functions: check_python, check_torch, get_cuda_version, get_rocm_version, check_cuda, check_rocm | imports: subprocess, traceback, torch | [tools dynamo verify_dynamo.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "tools\\flight_recorder\\fr_trace.py",
    "summary": "Flight Recorder Trace Analyzer | functions: main | imports: pickle, tools | [tools flight_recorder fr_trace.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "tools\\flight_recorder\\components\\builder.py",
    "summary": "No description | functions: tabulate, build_groups_memberships, build_collectives, build_db | imports: argparse, ast, tools, tabulate | [tools flight_recorder components builder.py]",
    "role": "src",
    "loc": 394
  },
  {
    "id": "tools\\flight_recorder\\components\\config_manager.py",
    "summary": "A helper class to manage the script configuration. | classes: JobConfig | imports: argparse, tools | [tools flight_recorder components config_manager.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "tools\\flight_recorder\\components\\fr_logger.py",
    "summary": "No description | classes: FlightRecorderLogger | [tools flight_recorder components fr_logger.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "tools\\flight_recorder\\components\\loader.py",
    "summary": "No description | functions: read_dump, _determine_prefix, read_dir | imports: argparse, gc, pickle, tools | [tools flight_recorder components loader.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "tools\\flight_recorder\\components\\types.py",
    "summary": "No description | classes: Ref, TypeInfo, MatchState, MatchInfo, Group, Membership | imports: tools | [tools flight_recorder components types.py]",
    "role": "src",
    "loc": 487
  },
  {
    "id": "tools\\flight_recorder\\components\\utils.py",
    "summary": "No description | functions: format_frame, format_frames, match_one_event, match_coalesced_groups, visualize_ops, check_size_alltoall | imports: argparse, tools, tabulate | [tools flight_recorder components utils.py]",
    "role": "src",
    "loc": 263
  },
  {
    "id": "tools\\gdb\\pytorch-gdb.py",
    "summary": "Context-manager to temporarily disable all gdb breakpoints, useful if | classes: DisableBreakpoints, TensorRepr, IntArrayRefRepr, DispatchKeysetRepr | imports: textwrap, gdb | [tools gdb pytorch-gdb.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "tools\\github\\github_utils.py",
    "summary": "GitHub Utilities | functions: gh_fetch_url_and_headers, gh_fetch_url, _gh_fetch_json_any, gh_fetch_json_dict, gh_fetch_commit | imports: json, urllib | [tools github github_utils.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "tools\\github\\__init__.py",
    "summary": "Package initializer | [tools github __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\iwyu\\fixup.py",
    "summary": "No description | functions: main | [tools iwyu fixup.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "tools\\jit\\gen_unboxing.py",
    "summary": "No description | classes: ComputeUnboxingFunctions, ComputeCodegenUnboxedKernels | functions: gen_unboxing, key_func, main | imports: argparse, dataclasses, yaml, torchgen | [tools jit gen_unboxing.py]",
    "role": "src",
    "loc": 232
  },
  {
    "id": "tools\\jit\\__init__.py",
    "summary": "Package initializer | [tools jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\jit\\test\\test_gen_unboxing.py",
    "summary": "No description | classes: TestGenUnboxing | imports: tempfile, unittest, tools | [tools jit test test_gen_unboxing.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "tools\\jit\\test\\__init__.py",
    "summary": "Package initializer | [tools jit test __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\linter\\__init__.py",
    "summary": "Package initializer | [tools linter __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\linter\\adapters\\actionlint_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: run_command, check_file | imports: argparse, concurrent, json, subprocess | [tools linter adapters actionlint_linter.py]",
    "role": "src",
    "loc": 146
  },
  {
    "id": "tools\\linter\\adapters\\bazel_linter.py",
    "summary": "This linter ensures that users don't set a SHA hash checksum in Bazel for the http_archive. | classes: LintSeverity, LintMessage | functions: is_required_checksum, get_disallowed_checksums, check_bazel, main | imports: argparse, json, shlex, subprocess | [tools linter adapters bazel_linter.py]",
    "role": "src",
    "loc": 160
  },
  {
    "id": "tools\\linter\\adapters\\black_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, as_posix, _run_command, run_command, check_file, main | imports: argparse, concurrent, json, subprocess | [tools linter adapters black_linter.py]",
    "role": "src",
    "loc": 204
  },
  {
    "id": "tools\\linter\\adapters\\clangformat_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, as_posix, _run_command, run_command, check_file, main | imports: argparse, concurrent, json, subprocess | [tools linter adapters clangformat_linter.py]",
    "role": "src",
    "loc": 224
  },
  {
    "id": "tools\\linter\\adapters\\clangtidy_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: scm_root, get_python_include_dir, eprint, as_posix, run_command, clang_search_dirs | imports: argparse, concurrent, json, shutil | [tools linter adapters clangtidy_linter.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "tools\\linter\\adapters\\cmake_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: run_command, check_file | imports: argparse, concurrent, json, subprocess | [tools linter adapters cmake_linter.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "tools\\linter\\adapters\\docstring_linter.py",
    "summary": "No description | classes: DocstringLinter | functions: _is_def, indent_to_dedent | imports: token, functools, _linter, tokenize | [tools linter adapters docstring_linter.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "tools\\linter\\adapters\\exec_linter.py",
    "summary": "EXEC: Ensure that source files are not executable. | classes: LintSeverity, LintMessage | functions: check_file | imports: argparse, json | [tools linter adapters exec_linter.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "tools\\linter\\adapters\\flake8_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, as_posix, _test_results_re, _run_command, run_command, get_issue_severity | imports: argparse, json, subprocess | [tools linter adapters flake8_linter.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "tools\\linter\\adapters\\gha_linter.py",
    "summary": "TODO | classes: LintSeverity, LintMessage | imports: argparse, json, ruamel | [tools linter adapters gha_linter.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "tools\\linter\\adapters\\grep_linter.py",
    "summary": "Generic linter that greps for a pattern and optionally suggests replacements. | classes: LintSeverity, LintMessage | functions: eprint, as_posix, run_command, lint_file, main | imports: argparse, json, subprocess | [tools linter adapters grep_linter.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "tools\\linter\\adapters\\import_linter.py",
    "summary": "Checks files to make sure there are no imports from disallowed third party | classes: LintSeverity, LintMessage | functions: check_file | imports: argparse, json, token, _linter | [tools linter adapters import_linter.py]",
    "role": "src",
    "loc": 382
  },
  {
    "id": "tools\\linter\\adapters\\lintrunner_version_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: toVersionString | imports: json, subprocess | [tools linter adapters lintrunner_version_linter.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "tools\\linter\\adapters\\mypy_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, as_posix, run_command, check_mypy_installed, in_github_actions, check_files | imports: argparse, json, subprocess | [tools linter adapters mypy_linter.py]",
    "role": "src",
    "loc": 230
  },
  {
    "id": "tools\\linter\\adapters\\nativefunctions_linter.py",
    "summary": "Verify that it is possible to round-trip native_functions.yaml via ruamel under some | classes: LintSeverity, LintMessage | imports: argparse, json, io, ruamel | [tools linter adapters nativefunctions_linter.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "tools\\linter\\adapters\\newlines_linter.py",
    "summary": "NEWLINE: Checks files to make sure there are no trailing newlines. | classes: LintSeverity, LintMessage | functions: check_file | imports: argparse, json | [tools linter adapters newlines_linter.py]",
    "role": "src",
    "loc": 141
  },
  {
    "id": "tools\\linter\\adapters\\no_merge_conflict_csv_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, check_file, main | imports: argparse, concurrent, json | [tools linter adapters no_merge_conflict_csv_linter.py]",
    "role": "src",
    "loc": 91
  },
  {
    "id": "tools\\linter\\adapters\\no_workflows_on_fork.py",
    "summary": "This a linter that ensures that jobs that can be triggered by push, | classes: LintSeverity, LintMessage | functions: load_yaml, gen_lint_message, check_file | imports: argparse, concurrent, json, yaml | [tools linter adapters no_workflows_on_fork.py]",
    "role": "src",
    "loc": 180
  },
  {
    "id": "tools\\linter\\adapters\\pip_init.py",
    "summary": "Initializer script that installs stuff to pip. | functions: run_command | imports: argparse, shutil, subprocess | [tools linter adapters pip_init.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "tools\\linter\\adapters\\pyfmt_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: eprint, as_posix, format_error_message, run_isort, run_usort, run_black | imports: argparse, concurrent, fnmatch, json | [tools linter adapters pyfmt_linter.py]",
    "role": "src",
    "loc": 185
  },
  {
    "id": "tools\\linter\\adapters\\ruff_linter.py",
    "summary": "Adapter for https://github.com/charliermarsh/ruff. | classes: LintSeverity, LintMessage | functions: eprint, as_posix, _run_command, run_command, add_default_options, explain_rule | imports: argparse, concurrent, dataclasses, json | [tools linter adapters ruff_linter.py]",
    "role": "src",
    "loc": 397
  },
  {
    "id": "tools\\linter\\adapters\\s3_init.py",
    "summary": "Compute the SHA256 hash of a file and return it as a hex string. | functions: compute_file_sha256, report_download_progress, check, download | imports: argparse, hashlib, json, platform | [tools linter adapters s3_init.py]",
    "role": "src",
    "loc": 163
  },
  {
    "id": "tools\\linter\\adapters\\set_linter.py",
    "summary": "No description | classes: SetLinter, TokenLine, PythonLines | imports: dataclasses, token, functools, _linter | [tools linter adapters set_linter.py]",
    "role": "src",
    "loc": 145
  },
  {
    "id": "tools\\linter\\adapters\\shellcheck_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage | functions: run_command, check_files | imports: argparse, json, shutil, subprocess | [tools linter adapters shellcheck_linter.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "tools\\linter\\adapters\\testowners_linter.py",
    "summary": "Test ownership was introduced in https://github.com/pytorch/pytorch/issues/66232. | classes: LintSeverity, LintMessage | functions: get_pytorch_labels, check_labels, check_file, main | imports: argparse, json, urllib | [tools linter adapters testowners_linter.py]",
    "role": "src",
    "loc": 133
  },
  {
    "id": "tools\\linter\\adapters\\test_has_main_linter.py",
    "summary": "This lint verifies that every Python test file (file that matches test_*.py or | classes: HasMainVisiter, LintSeverity, LintMessage | functions: check_file, main | imports: argparse, json, multiprocessing, libcst | [tools linter adapters test_has_main_linter.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "tools\\linter\\adapters\\update_s3.py",
    "summary": "Uploads a new binary to s3 and updates its hash in the config file. | functions: compute_file_sha256, main | imports: argparse, hashlib, json, boto3 | [tools linter adapters update_s3.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "tools\\linter\\adapters\\workflow_consistency_linter.py",
    "summary": "Checks for consistency of jobs between different GitHub workflows. | classes: LintSeverity, LintMessage | functions: glob_yamls, load_yaml, is_workflow, print_lint_message | imports: argparse, json, yaml | [tools linter adapters workflow_consistency_linter.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "tools\\linter\\adapters\\_linter.py",
    "summary": "No description | classes: LintSeverity, LintMessage, LintResult, ParseError, ArgumentParser, OmittedLines | functions: is_name, is_op, bracket_pairs, set_logging_level | imports: argparse, dataclasses, json, token | [tools linter adapters _linter.py]",
    "role": "src",
    "loc": 367
  },
  {
    "id": "tools\\linter\\clang_tidy\\generate_build_files.py",
    "summary": "No description | functions: run_cmd, update_submodules, gen_compile_commands, run_autogen, generate_build_files | imports: subprocess | [tools linter clang_tidy generate_build_files.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "tools\\linter\\clang_tidy\\__init__.py",
    "summary": "Package initializer | [tools linter clang_tidy __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\lite_interpreter\\gen_selected_mobile_ops_header.py",
    "summary": "No description | functions: extract_root_operators, get_selected_kernel_dtypes_code, write_selected_mobile_ops, write_selected_mobile_ops_with_all_dtypes, main | imports: argparse, yaml, torchgen | [tools lite_interpreter gen_selected_mobile_ops_header.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "tools\\lite_interpreter\\__init__.py",
    "summary": "Package initializer | [tools lite_interpreter __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\lldb\\deploy_debugger.py",
    "summary": "No description | imports: lldb | [tools lldb deploy_debugger.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "tools\\lldb\\pytorch_lldb.py",
    "summary": "Context-manager to temporarily disable all lldb breakpoints, useful if | classes: DisableBreakpoints | functions: get_target, IntArrayRef_summary, DispatchKeyset_summary, Tensor_summary, __lldb_init_module | imports: lldb | [tools lldb pytorch_lldb.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "tools\\onnx\\gen_diagnostics.py",
    "summary": "Generates PyTorch ONNX Export Diagnostic rules for C++, Python and documentations. | functions: _kebab_case_to_snake_case, _kebab_case_to_pascal_case, _format_rule_for_python_class, _format_rule_for_python_field, _format_rule_for_cpp, gen_diagnostics_python | imports: argparse, string, subprocess, t",
    "role": "src",
    "loc": 207
  },
  {
    "id": "tools\\onnx\\update_default_opset_version.py",
    "summary": "Updates the default value of opset_version. | functions: read_sub_write, main | imports: argparse, datetime, subprocess, onnx | [tools onnx update_default_opset_version.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "tools\\packaging\\build_wheel.py",
    "summary": "No description | classes: Builder | functions: run_cmd, interpreter_version, venv, parse_args, main | imports: argparse, subprocess, tempfile | [tools packaging build_wheel.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "tools\\packaging\\split_wheel.py",
    "summary": "Script to build split pytorch wheels | functions: requirements_installed, setup_py, split_build, parse_args, main | imports: argparse, subprocess, setuptools | [tools packaging split_wheel.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "tools\\pyi\\gen_pyi.py",
    "summary": "Get declarations (grouped by name) which should be generated | functions: get_py_torch_functions, should_bind_function, should_bind_method, sig_for_ops, generate_type_hints, get_max_pool_dispatch | imports: argparse, importlib, pprint, unittest | [tools pyi gen_pyi.py]",
    "role": "src",
    "loc": 1298
  },
  {
    "id": "tools\\pyi\\__init__.py",
    "summary": "Package initializer | [tools pyi __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\setup_helpers\\cmake.py",
    "summary": "Manages CMake. | classes: CMake | functions: _mkdir_p | imports: multiprocessing, platform, sysconfig, distutils | [tools setup_helpers cmake.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "tools\\setup_helpers\\cmake_utils.py",
    "summary": "This is refactored from cmake.py to avoid circular imports issue with env.py, | functions: convert_cmake_value_to_python_value, get_cmake_cache_variables_from_file | [tools setup_helpers cmake_utils.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "tools\\setup_helpers\\env.py",
    "summary": "Checks build type. The build type will be given in :attr:`cmake_build_type_env`. If :attr:`cmake_build_type_env` | classes: BuildType | functions: check_env_flag, check_negative_env_flag, gather_paths, lib_paths_from_base | imports: platform, struct, cmake_utils | [tools setup_helpers env.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "tools\\setup_helpers\\gen.py",
    "summary": "No description | imports: torchgen | [tools setup_helpers gen.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "tools\\setup_helpers\\generate_code.py",
    "summary": "No description | functions: generate_code, get_selector_from_legacy_operator_selection_list, get_selector, main | imports: argparse, yaml, tools, torchgen | [tools setup_helpers generate_code.py]",
    "role": "src",
    "loc": 196
  },
  {
    "id": "tools\\setup_helpers\\generate_linker_script.py",
    "summary": "No description | functions: gen_linker_script | imports: subprocess | [tools setup_helpers generate_linker_script.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "tools\\setup_helpers\\gen_unboxing.py",
    "summary": "No description | imports: tools | [tools setup_helpers gen_unboxing.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "tools\\setup_helpers\\gen_version_header.py",
    "summary": "Parses a version string into (major, minor, patch) version numbers. | functions: parse_version, apply_replacements, main | imports: argparse | [tools setup_helpers gen_version_header.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "tools\\setup_helpers\\__init__.py",
    "summary": "Package initializer | functions: which | [tools setup_helpers __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "tools\\shared\\logging_utils.py",
    "summary": "No description | functions: pluralize, duration_to_str | [tools shared logging_utils.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "tools\\shared\\module_loader.py",
    "summary": "No description | functions: import_module | imports: importlib, types | [tools shared module_loader.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "tools\\shared\\__init__.py",
    "summary": "Package initializer | imports: module_loader | [tools shared __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "tools\\stats\\check_disabled_tests.py",
    "summary": "Return a list of disabled tests that should be re-enabled and those that are still | functions: process_report, get_test_reports, get_disabled_test_name, prepare_record, save_results, main | imports: argparse, json, xml, tempfile | [tools stats check_disabled_tests.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "tools\\stats\\export_test_times.py",
    "summary": "No description | functions: main | imports: tools | [tools stats export_test_times.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "tools\\stats\\import_test_stats.py",
    "summary": "No description | functions: get_disabled_issues, fetch_and_cache, is_cached_file_valid, get_test_times, get_test_class_times, get_disabled_tests | imports: datetime, json, shutil, urllib | [tools stats import_test_stats.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "tools\\stats\\monitor.py",
    "summary": "A Python script that logging the system-level utilization usage in json format. | classes: UsageData, GpuData, SharedResource, UsageLogger | functions: parse_args, main | imports: argparse, copy, dataclasses, signal | [tools stats monitor.py]",
    "role": "src",
    "loc": 434
  },
  {
    "id": "tools\\stats\\sccache_stats_to_benchmark_format.py",
    "summary": "No description | functions: flatten_data, main | imports: glob, json | [tools stats sccache_stats_to_benchmark_format.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "tools\\stats\\test_dashboard.py",
    "summary": "No description | functions: get_job_name, get_build_name, get_test_config, get_td_exclusions, group_test_cases, get_reruns | imports: json, functools, tempfile, requests | [tools stats test_dashboard.py]",
    "role": "src",
    "loc": 173
  },
  {
    "id": "tools\\stats\\upload_artifacts.py",
    "summary": "No description | functions: get_artifacts | imports: argparse, tempfile, tools | [tools stats upload_artifacts.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "tools\\stats\\upload_dynamo_perf_stats.py",
    "summary": "No description | functions: get_perf_stats, generate_partition_key | imports: argparse, csv, hashlib, json | [tools stats upload_dynamo_perf_stats.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "tools\\stats\\upload_external_contrib_stats.py",
    "summary": "No description | functions: _fetch_url, fetch_json, get_external_pr_data | imports: argparse, datetime, json, urllib | [tools stats upload_external_contrib_stats.py]",
    "role": "src",
    "loc": 141
  },
  {
    "id": "tools\\stats\\upload_metrics.py",
    "summary": "Adds stats that should be emitted with every metric by the current process. | classes: EnvVarMetric | functions: add_global_metric, emit_metric | imports: datetime, inspect, uuid, tools | [tools stats upload_metrics.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "tools\\stats\\upload_sccache_stats.py",
    "summary": "No description | functions: get_sccache_stats | imports: argparse, json, tempfile, tools | [tools stats upload_sccache_stats.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "tools\\stats\\upload_stats_lib.py",
    "summary": "No description | functions: get_s3_resource, _get_request_headers, _get_artifact_urls, _download_artifact, download_s3_artifacts, download_gha_artifacts | imports: gzip, io, json, zipfile | [tools stats upload_stats_lib.py]",
    "role": "src",
    "loc": 201
  },
  {
    "id": "tools\\stats\\upload_test_stats.py",
    "summary": "Convert a test report xml file into a JSON-serializable list of test cases. | functions: parse_xml_report, process_xml_element, get_tests, get_tests_for_circleci, summarize_test_cases, get_key | imports: argparse, xml, multiprocessing, tempfile | [tools stats upload_test_stats.py]",
    "role": "src",
    "loc": 210
  },
  {
    "id": "tools\\stats\\upload_test_stats_intermediate.py",
    "summary": "No description | imports: argparse, tools | [tools stats upload_test_stats_intermediate.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "tools\\stats\\upload_test_stats_running_jobs.py",
    "summary": "No description | functions: get_bucket, delete_obj, put_object, do_upload, get_workflow_ids, read_s3 | imports: functools, tools | [tools stats upload_test_stats_running_jobs.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "tools\\stats\\utilization_stats_lib.py",
    "summary": "No description | classes: UtilizationStats, UtilizationMetadata, GpuUsage, RecordData, UtilizationRecord, OssCiSegmentV1 | functions: getDataModelVersion, getTsNow | imports: dataclasses, datetime, dataclasses_json | [tools stats utilization_stats_lib.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "tools\\stats\\__init__.py",
    "summary": "Package initializer | [tools stats __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\stats\\upload_utilization_stats\\test_upload_utilization_stats.py",
    "summary": "No description | classes: TestSegmentGenerator | functions: get_base_test_records, getTimestampStr, getCurrentTimestampStr | imports: unittest, datetime, tools | [tools stats upload_utilization_stats test_upload_utilization_stats.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "tools\\stats\\upload_utilization_stats\\upload_utilization_stats.py",
    "summary": "generates test segment from utilization records, currently it only generate segments on python commands level | classes: SegmentGenerator, UtilizationDbConverter, UploadUtilizationData | functions: unzip_file, parse_args | imports: argparse, json, zipfile, dataclasses | [tools stats upload_utilizati",
    "role": "src",
    "loc": 375
  },
  {
    "id": "tools\\test\\gen_operators_yaml_test.py",
    "summary": "No description | classes: GenOperatorsYAMLTest | functions: _mock_options, _mock_load_op_dep_graph | imports: argparse, json, unittest, gen_operators_yaml | [tools test gen_operators_yaml_test.py]",
    "role": "src",
    "loc": 219
  },
  {
    "id": "tools\\test\\gen_oplist_test.py",
    "summary": "No description | classes: GenOplistTest | imports: unittest, tools | [tools test gen_oplist_test.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "tools\\test\\linter_test_case.py",
    "summary": "No description | classes: LinterTestCase | imports: io, json, unittest, tools | [tools test linter_test_case.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "tools\\test\\test_cmake.py",
    "summary": "Sets/clears an environment variable within a Python context. | classes: TestCMake | functions: env_var, set_env_var | imports: unittest, tools | [tools test test_cmake.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "tools\\test\\test_codegen.py",
    "summary": "No description | classes: TestCreateDerivative, TestGenAutogradFunctions, TestGenSchemaRegistration, TestGenNativeFunctionDeclaration, TestNativeFunctionGeneratrion, TestStaticDispatchGeneratrion | imports: dataclasses, unittest, yaml, tools | [tools test test_codegen.py]",
    "role": "src",
    "loc": 467
  },
  {
    "id": "tools\\test\\test_codegen_model.py",
    "summary": "No description | classes: TestCodegenModel, TestAnnotation | imports: textwrap, unittest, expecttest, yaml | [tools test test_codegen_model.py]",
    "role": "src",
    "loc": 173
  },
  {
    "id": "tools\\test\\test_create_alerts.py",
    "summary": "No description | classes: TestGitHubPR | imports: unittest, tools | [tools test test_create_alerts.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "tools\\test\\test_docstring_linter.py",
    "summary": "No description | classes: TestDocstringLinter | imports: tools, linter_test_case | [tools test test_docstring_linter.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "tools\\test\\test_executorch_custom_ops.py",
    "summary": "Could use torch.testing._internal.common_utils to reduce boilerplate. | classes: TestComputeNativeFunctionStub, TestGenCustomOpsHeader | functions: _get_native_function_from_yaml | imports: tempfile, unittest, expecttest, torchgen | [tools test test_executorch_custom_ops.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "tools\\test\\test_executorch_gen.py",
    "summary": "No description | classes: TestParseNativeYaml, TestParseKernelYamlFiles, TestGenFunctionsDeclarations, TestComputeCodegenUnboxedKernels | imports: tempfile, unittest, yaml, torchgen | [tools test test_executorch_gen.py]",
    "role": "src",
    "loc": 593
  },
  {
    "id": "tools\\test\\test_executorch_signatures.py",
    "summary": "No description | classes: ExecutorchCppSignatureTest | imports: unittest, torchgen | [tools test test_executorch_signatures.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "tools\\test\\test_executorch_types.py",
    "summary": "Test torchgen.executorch.api.cpp | classes: ExecutorchCppTest | imports: unittest, torchgen | [tools test test_executorch_types.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "tools\\test\\test_executorch_unboxing.py",
    "summary": "Could use torch.testing._internal.common_utils to reduce boilerplate. | classes: TestUnboxing | functions: aten_argumenttype_type_wrapper | imports: unittest, types, torchgen | [tools test test_executorch_unboxing.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "tools\\test\\test_gen_backend_stubs.py",
    "summary": "No description | classes: TestGenBackendStubs | imports: tempfile, unittest, expecttest, torchgen | [tools test test_gen_backend_stubs.py]",
    "role": "src",
    "loc": 252
  },
  {
    "id": "tools\\test\\test_selective_build.py",
    "summary": "No description | classes: TestSelectiveBuild, TestExecuTorchSelectiveBuild | imports: unittest, torchgen | [tools test test_selective_build.py]",
    "role": "src",
    "loc": 293
  },
  {
    "id": "tools\\test\\test_set_linter.py",
    "summary": "No description | classes: TestSetLinter | functions: python_lines | imports: token, tokenize, tools, linter_test_case | [tools test test_set_linter.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "tools\\test\\test_test_run.py",
    "summary": "No description | classes: TestTestRun, TestShardedTest | imports: unittest, tools | [tools test test_test_run.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "tools\\test\\test_test_selections.py",
    "summary": "No description | classes: TestCalculateShards | functions: gen_class_times | imports: functools, random, unittest, tools | [tools test test_test_selections.py]",
    "role": "src",
    "loc": 499
  },
  {
    "id": "tools\\test\\test_upload_stats_lib.py",
    "summary": "No description | classes: TestUploadStats | imports: gzip, inspect, json, unittest | [tools test test_upload_stats_lib.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "tools\\test\\test_upload_test_stats.py",
    "summary": "No description | classes: TestUploadTestStats | imports: unittest, tools | [tools test test_upload_test_stats.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "tools\\test\\test_utils.py",
    "summary": "No description | classes: TestNamespaceHelper | imports: unittest, torchgen | [tools test test_utils.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "tools\\test\\test_vulkan_codegen.py",
    "summary": "No description | classes: TestVulkanSPVCodegen | imports: tempfile, unittest, tools | [tools test test_vulkan_codegen.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "tools\\test\\heuristics\\test_heuristics.py",
    "summary": "No description | classes: TestHistoricalClassFailureCorrelation, TestParsePrevTests, TestFilePath | functions: mocked_file, gen_historical_class_failures | imports: io, json, unittest, tools | [tools test heuristics test_heuristics.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "tools\\test\\heuristics\\test_interface.py",
    "summary": "No description | classes: Heuristic, TestTD, TestTestPrioritizations, TestAggregatedHeuristics, TestAggregatedHeuristicsTestStats, TestJsonParsing | imports: unittest, tools | [tools test heuristics test_interface.py]",
    "role": "src",
    "loc": 522
  },
  {
    "id": "tools\\test\\heuristics\\test_utils.py",
    "summary": "No description | classes: TestHeuristicsUtils | imports: unittest, tools | [tools test heuristics test_utils.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "tools\\test\\heuristics\\__init__.py",
    "summary": "Package initializer | [tools test heuristics __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\testing\\clickhouse.py",
    "summary": "No description | functions: get_clickhouse_client, query_clickhouse, convert_to_json_list | imports: json, functools, clickhouse_connect | [tools testing clickhouse.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "tools\\testing\\discover_tests.py",
    "summary": "No description | functions: parse_test_module, discover_tests, skip_test_p | imports: glob | [tools testing discover_tests.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "tools\\testing\\do_target_determination_for_s3.py",
    "summary": "No description | functions: import_results, main | imports: json, tools | [tools testing do_target_determination_for_s3.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "tools\\testing\\explicit_ci_jobs.py",
    "summary": "Add job 'job' under 'type' and 'workflow_name' to 'workflow' in place. Also | functions: add_job, get_filtered_circleci_config, commit_ci | imports: argparse, fnmatch, subprocess, textwrap | [tools testing explicit_ci_jobs.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "tools\\testing\\modulefinder_determinator.py",
    "summary": "No description | functions: should_run_test, test_impact_of_file, log_test_reason, get_dep_modules, parse_test_module, print_to_stderr | imports: modulefinder | [tools testing modulefinder_determinator.py]",
    "role": "src",
    "loc": 146
  },
  {
    "id": "tools\\testing\\test_run.py",
    "summary": "TestRun defines the set of tests that should be run together in a single pytest invocation. | classes: TestRun, ShardedTest | imports: copy, functools | [tools testing test_run.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "tools\\testing\\test_selections.py",
    "summary": "No description | classes: ShardJob | functions: get_with_pytest_shard, get_duration, get_duration_for_classes, shard, _get_min_sharded_job, _shard_serial | imports: subprocess, tools | [tools testing test_selections.py]",
    "role": "src",
    "loc": 198
  },
  {
    "id": "tools\\testing\\update_slow_tests.py",
    "summary": "No description | functions: git_api, make_pr, approve_pr, make_comment, add_labels, search_for_open_pr | imports: json, subprocess, requests, clickhouse | [tools testing update_slow_tests.py]",
    "role": "src",
    "loc": 189
  },
  {
    "id": "tools\\testing\\upload_artifacts.py",
    "summary": "No description | functions: get_s3_resource, zip_artifact, upload_to_s3_artifacts, zip_and_upload_artifacts, trigger_upload_test_stats_intermediate_workflow | imports: glob, zipfile, functools, boto3 | [tools testing upload_artifacts.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "tools\\testing\\__init__.py",
    "summary": "Package initializer | [tools testing __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "tools\\testing\\target_determination\\determinator.py",
    "summary": "No description | functions: get_test_prioritizations | imports: tools | [tools testing target_determination determinator.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "tools\\testing\\target_determination\\gen_artifact.py",
    "summary": "No description | functions: gen_ci_artifact | imports: json | [tools testing target_determination gen_artifact.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\correlated_with_historical_failures.py",
    "summary": "No description | classes: CorrelatedWithHistoricalFailures | imports: tools | [tools testing target_determination heuristics correlated_with_historical_failures.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\edited_by_pr.py",
    "summary": "No description | classes: EditedByPR | functions: _get_modified_tests | imports: tools | [tools testing target_determination heuristics edited_by_pr.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\filepath.py",
    "summary": "No description | classes: Filepath | functions: is_valid_keyword, get_keywords, sanitize_name, file_matches_keyword, get_freq_dict | imports: functools, tools | [tools testing target_determination heuristics filepath.py]",
    "role": "src",
    "loc": 109
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\historical_class_failure_correlation.py",
    "summary": "This heuristic prioritizes test classes that have historically tended to fail | classes: HistoricalClassFailurCorrelation | functions: _get_historical_test_class_correlations, _get_ratings_for_tests, _rank_correlated_tests | imports: json, tools | [tools testing target_determination heuristics histo",
    "role": "src",
    "loc": 68
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\historical_edited_files.py",
    "summary": "No description | classes: HistorialEditedFiles | imports: tools | [tools testing target_determination heuristics historical_edited_files.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\interface.py",
    "summary": "Describes the results of whether heuristics consider a test relevant or not. | classes: TestPrioritizations, AggregatedHeuristics, HeuristicInterface | imports: abc, copy, tools | [tools testing target_determination heuristics interface.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\llm.py",
    "summary": "No description | classes: LLM | imports: json, tools | [tools testing target_determination heuristics llm.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\mentioned_in_pr.py",
    "summary": "No description | classes: MentionedInPR | imports: tools | [tools testing target_determination heuristics mentioned_in_pr.py]",
    "role": "src",
    "loc": 50
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\previously_failed_in_pr.py",
    "summary": "No description | classes: PreviouslyFailedInPR | functions: get_previous_failures, _parse_prev_failing_test_files, gen_additional_test_failures_file, read_additional_test_failures_file | imports: json, tools | [tools testing target_determination heuristics previously_failed_in_pr.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\profiling.py",
    "summary": "No description | classes: Profiling | imports: tools | [tools testing target_determination heuristics profiling.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\public_bindings.py",
    "summary": "No description | classes: PublicBindings | imports: tools | [tools testing target_determination heuristics public_bindings.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\utils.py",
    "summary": "No description | functions: python_test_file_to_test_name, get_pr_number, get_merge_base, query_changed_files, get_git_commit_info, get_issue_or_pr_body | imports: json, subprocess, functools, urllib | [tools testing target_determination heuristics utils.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "tools\\testing\\target_determination\\heuristics\\__init__.py",
    "summary": "Package initializer | imports: tools | [tools testing target_determination heuristics __init__.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\functional.py",
    "summary": "broadcast_tensors(*tensors) -> List of Tensors | functions: broadcast_tensors, broadcast_shapes, split, einsum, parse_subscript, meshgrid | imports: operator, torch | [torch functional.py]",
    "role": "src",
    "loc": 1780
  },
  {
    "id": "torch\\hub.py",
    "summary": "No description | classes: _Faketqdm | functions: _add_to_sys_path, _import_module, _remove_if_exists, _git_archive_link, _load_attr_from_module, _get_torch_home | imports: errno, hashlib, json, shutil | [torch hub.py]",
    "role": "src",
    "loc": 689
  },
  {
    "id": "torch\\library.py",
    "summary": "A class to create libraries that can be used to register new operators or | classes: Library | functions: fallthrough_kernel, _del_library, _scoped_library, define, _, wrap | imports: functools, inspect, traceback, weakref | [torch library.py]",
    "role": "src",
    "loc": 1302
  },
  {
    "id": "torch\\overrides.py",
    "summary": "Python implementation of ``__torch_function__`` | classes: TorchFunctionMode, BaseTorchFunctionMode | functions: _disable_user_warnings, wrapper, get_ignored_functions, get_default_nowrap_functions, get_testing_overrides, wrap_torch_function | imports: functools, types, torch | [torch overrides.py]",
    "role": "src",
    "loc": 1939
  },
  {
    "id": "torch\\quasirandom.py",
    "summary": "The :class:`torch.quasirandom.SobolEngine` is an engine for generating | classes: SobolEngine | imports: torch | [torch quasirandom.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "torch\\random.py",
    "summary": "Sets the random number generator state. | functions: set_rng_state, get_rng_state, manual_seed, seed, _seed_custom_device, initial_seed | imports: torch | [torch random.py]",
    "role": "src",
    "loc": 150
  },
  {
    "id": "torch\\return_types.py",
    "summary": "No description | functions: pytree_register_structseq, structseq_flatten, structseq_flatten_with_keys, structseq_unflatten | imports: inspect, torch | [torch return_types.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\serialization.py",
    "summary": "No description | classes: _SerializationLocal, SourceChangeWarning, LoadEndianness, set_default_mmap_options, safe_globals, skip_data | functions: _default_to_weights_only, mkdtemp, get_default_load_endianness, set_default_load_endianness, get_crc32_options, set_crc32_options | imports: copyreg, dif",
    "role": "src",
    "loc": 1578
  },
  {
    "id": "torch\\storage.py",
    "summary": "No description | classes: _StorageBase, UntypedStorage, TypedStorage, _LegacyStorageMeta, _LegacyStorage | functions: _share_memory_lock_protected, wrapper, _load_from_bytes, _new_dtypes, _dtype_to_storage_type_map, _storage_type_to_dtype_map | imports: copy, functools, io, threading | [torch storag",
    "role": "src",
    "loc": 1206
  },
  {
    "id": "torch\\torch_version.py",
    "summary": "A string with magic powers to compare to both Version and iterables! | classes: TorchVersion | imports: torch | [torch torch_version.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\types.py",
    "summary": "No description | classes: Storage | imports: builtins, typing_extensions, torch | [torch types.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "torch\\_appdirs.py",
    "summary": "This file is directly from | classes: AppDirs | functions: user_data_dir, site_data_dir, user_config_dir, site_config_dir, user_cache_dir, user_state_dir | imports: platform, winreg, win32com, win32api | [torch _appdirs.py]",
    "role": "src",
    "loc": 531
  },
  {
    "id": "torch\\_classes.py",
    "summary": "No description | classes: _ClassNamespace, _Classes | imports: types, torch | [torch _classes.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\_compile.py",
    "summary": "APIs related to torch.compile which lazily import torch._dynamo to avoid | functions: _disable_dynamo, inner | imports: functools, typing_extensions, torch | [torch _compile.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\_custom_ops.py",
    "summary": "Register a new custom operator | functions: custom_op, inner, impl, impl_abstract, impl_save_for_backward, impl_backward | imports: inspect, torch | [torch _custom_ops.py]",
    "role": "src",
    "loc": 270
  },
  {
    "id": "torch\\_deploy.py",
    "summary": "No description | functions: _save_storages, persistent_id, _load_storages, persistent_load, _get_package | imports: io, torch | [torch _deploy.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "torch\\_environment.py",
    "summary": "No description | functions: is_fbcode | [torch _environment.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\_guards.py",
    "summary": "No description | classes: CompileId, TraceId, GuardSource, GuardBuilderBase, SLoc, ShapeGuard | functions: compile_context, tracing, detect_fake_mode, active_fake_mode | imports: dataclasses, functools, threading, traceback | [torch _guards.py]",
    "role": "src",
    "loc": 754
  },
  {
    "id": "torch\\_jit_internal.py",
    "summary": "The weak_script annotation needs to be here instead of inside torch/jit/ so it | classes: BroadcastingListCls, SourceLoader, env, closure_lookup, FunctionModifiers, _IgnoreContextManager | functions: is_final, is_scripting, _qualified_name, createResolutionCallbackFromEnv, lookupInModule, parseNeste",
    "role": "src",
    "loc": 1023
  },
  {
    "id": "torch\\_linalg_utils.py",
    "summary": "Various linear algebra utility methods for internal use. | functions: is_sparse, get_floating_dtype, matmul, bform, qform, basis | imports: torch | [torch _linalg_utils.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\_lobpcg.py",
    "summary": "Locally Optimal Block Preconditioned Conjugate Gradient methods. | classes: LOBPCGAutogradFunction, LOBPCG | functions: _symeig_backward_complete_eigenspace, _polynomial_coefficients_given_roots, _polynomial_value, _matrix_polynomial_value, transition, _vector_polynomial_value | imports: torch | [to",
    "role": "src",
    "loc": 851
  },
  {
    "id": "torch\\_lowrank.py",
    "summary": "Implement various linear algebra algorithms for low rank matrices. | functions: get_approximate_basis, svd_lowrank, _svd_lowrank, pca_lowrank | imports: torch | [torch _lowrank.py]",
    "role": "src",
    "loc": 225
  },
  {
    "id": "torch\\_meta_registrations.py",
    "summary": "No description | classes: GridSamplerInterpolation | functions: register_meta, wrapper, register, elementwise_meta, toRealValueType, check_inplace_broadcast | imports: functools, typing_extensions, torch | [torch _meta_registrations.py]",
    "role": "src",
    "loc": 6027
  },
  {
    "id": "torch\\_namedtensor_internals.py",
    "summary": "No description | functions: check_serializing_named_tensor, build_dim_map, unzip_namedshape, namer_api_name, is_ellipsis, single_ellipsis_index | [torch _namedtensor_internals.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\_ops.py",
    "summary": "Base class for OpOverload (which represents C++ ATen operators) and HigherOrderOperator | classes: OperatorBase, HigherOrderOperator, _ModeStackStateForPreDispatch, OpOverload, TorchBindOpOverload, OpOverloadPacket | functions: dl_open_guard, resolve_key, _to_flat_tuple, _compute_keyset, _get_tensor",
    "role": "src",
    "loc": 917
  },
  {
    "id": "torch\\_python_dispatcher.py",
    "summary": "No description | classes: PythonDispatcher | imports: torch | [torch _python_dispatcher.py]",
    "role": "src",
    "loc": 146
  },
  {
    "id": "torch\\_size_docs.py",
    "summary": "Adds docstrings to torch.Size functions | functions: add_docstr_all | imports: torch | [torch _size_docs.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\_sources.py",
    "summary": "Wrapper around inspect.getsourcelines and inspect.getsourcefile. | classes: SourceContext, ParsedDef | functions: get_source_lines_and_file, normalize_source_lines, remove_prefix, make_source_context, fake_range, parse_def | imports: ast, functools, inspect, textwrap | [torch _sources.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "torch\\_storage_docs.py",
    "summary": "Adds docstrings to Storage functions | functions: add_docstr_all | imports: torch | [torch _storage_docs.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\_streambase.py",
    "summary": "No description | classes: _StreamBase, _EventBase | imports: typing_extensions, torch | [torch _streambase.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\_tensor.py",
    "summary": "No description | classes: Tensor | functions: _handle_torch_function_and_wrap_type_error_to_not_implemented, wrapped, _rebuild_from_type, _rebuild_from_type_v2, _dtype_to_typestr, _convert | imports: copyreg, functools, copy, numbers | [torch _tensor.py]",
    "role": "src",
    "loc": 1418
  },
  {
    "id": "torch\\_tensor_docs.py",
    "summary": "Adds docstrings to Tensor functions | functions: add_docstr_all | imports: torch | [torch _tensor_docs.py]",
    "role": "src",
    "loc": 5405
  },
  {
    "id": "torch\\_tensor_str.py",
    "summary": "Set options for printing. Items shamelessly taken from NumPy | classes: __PrinterOptions, _Formatter | functions: set_printoptions, get_printoptions, printoptions, tensor_totype, _scalar_str, _vector_str | imports: dataclasses, textwrap, torch | [torch _tensor_str.py]",
    "role": "src",
    "loc": 561
  },
  {
    "id": "torch\\_thread_safe_fork.py",
    "summary": "No description | [torch _thread_safe_fork.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_torch_docs.py",
    "summary": "Adds docstrings to functions defined in the torch._C module. | functions: parse_kwargs, merge_dicts | imports: torch | [torch _torch_docs.py]",
    "role": "src",
    "loc": 11032
  },
  {
    "id": "torch\\_utils.py",
    "summary": "str subclass that returns itself in repr | classes: KeyErrorMessage, ExceptionWrapper, _ClassPropertyDescriptor, _LazySeedTracker, CallbackRegistry | functions: _type, _to, _get_async_or_non_blocking, _get_restore_location, _rebuild_tensor, get_tensor_metadata | imports: copyreg, functools, importli",
    "role": "src",
    "loc": 761
  },
  {
    "id": "torch\\_utils_internal.py",
    "summary": "No description | functions: get_file_path, get_file_path_2, get_writable_path, prepare_multiprocessing_environment, resolve_library_path, throw_abstract_impl_not_imported_error | imports: functools, tempfile, typing_extensions, torch | [torch _utils_internal.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torch\\_VF.py",
    "summary": "This makes the functions in torch._C._VariableFunctions available as | classes: VFModule | imports: types, torch | [torch _VF.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\_vmap_internals.py",
    "summary": "No description | functions: _validate_and_get_batch_size, _num_outputs, _as_tuple, _create_batched_inputs, _unwrap_batched, _validate_outputs | imports: functools, typing_extensions, torch | [torch _vmap_internals.py]",
    "role": "src",
    "loc": 190
  },
  {
    "id": "torch\\_weights_only_unpickler.py",
    "summary": "No description | classes: _safe_globals, Unpickler | functions: _add_safe_globals, _get_safe_globals, _clear_safe_globals, _remove_safe_globals, _get_user_allowed_globals, _tensor_rebuild_functions | imports: functools, _codecs, pickle, struct | [torch _weights_only_unpickler.py]",
    "role": "src",
    "loc": 475
  },
  {
    "id": "torch\\__config__.py",
    "summary": "Return a human-readable string with descriptions of the | functions: show, _cxx_flags, parallel_info | imports: torch | [torch __config__.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\__future__.py",
    "summary": "Sets whether to assign new tensors to the parameters instead of changing the | functions: set_overwrite_module_params_on_conversion, get_overwrite_module_params_on_conversion, set_swap_module_params_on_conversion, get_swap_module_params_on_conversion | [torch __future__.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "torch\\__init__.py",
    "summary": "The torch package contains data structures for multi-dimensional | classes: SymInt, SymFloat, SymBool, ByteStorage, DoubleStorage, FloatStorage | functions: _running_with_deploy, _load_dll_libraries, _preload_cuda_deps, _load_global_deps, sym_not, sym_float | imports: builtins, ctypes, glob, importl",
    "role": "src",
    "loc": 1963
  },
  {
    "id": "torch\\accelerator\\_utils.py",
    "summary": "No description | functions: _get_device_index | imports: torch | [torch accelerator _utils.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\accelerator\\__init__.py",
    "summary": "This package introduces support for the current :ref:`accelerator<accelerators>` in python. | functions: device_count, is_available, current_accelerator, current_device_index, set_device_index, current_stream | imports: typing_extensions, torch, _utils | [torch accelerator __init__.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\amp\\autocast_mode.py",
    "summary": "Instances of :class:`autocast` serve as context managers or decorators that | classes: autocast | functions: is_autocast_available, autocast_decorator, decorate_autocast, _enter_autocast, _exit_autocast, _cast | imports: functools, torch, numpy | [torch amp autocast_mode.py]",
    "role": "src",
    "loc": 440
  },
  {
    "id": "torch\\amp\\grad_scaler.py",
    "summary": "Lazily serves copies of a tensor to requested devices. | classes: _MultiDeviceReplicator, OptState, GradScaler | functions: _refresh_per_optimizer_state | imports: inspect, torch | [torch amp grad_scaler.py]",
    "role": "src",
    "loc": 515
  },
  {
    "id": "torch\\amp\\__init__.py",
    "summary": "Package initializer | imports: autocast_mode, grad_scaler | [torch amp __init__.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\ao\\__init__.py",
    "summary": "Package initializer | functions: __getattr__ | imports: types, torch, importlib | [torch ao __init__.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\ao\\nn\\__init__.py",
    "summary": "Package initializer | functions: __getattr__ | imports: types, torch, importlib | [torch ao nn __init__.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\__init__.py",
    "summary": "Package initializer | functions: __getattr__ | imports: types, modules, importlib | [torch ao nn intrinsic __init__.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\modules\\fused.py",
    "summary": "No description | classes: _FusedModule, ConvReLU1d, ConvReLU2d, ConvReLU3d, LinearReLU, ConvBn1d | imports: torch | [torch ao nn intrinsic modules fused.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\modules\\__init__.py",
    "summary": "Package initializer | imports: fused | [torch ao nn intrinsic modules __init__.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\qat\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn intrinsic qat __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\qat\\modules\\conv_fused.py",
    "summary": "No description | classes: _ConvBnNd, ConvBn1d, ConvBnReLU1d, ConvReLU1d, ConvBn2d, ConvBnReLU2d | functions: update_bn_stats, freeze_bn_stats | imports: torch | [torch ao nn intrinsic qat modules conv_fused.py]",
    "role": "src",
    "loc": 834
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\qat\\modules\\linear_fused.py",
    "summary": "A LinearBn1d module is a module fused from Linear and BatchNorm1d, attached | classes: LinearBn1d | imports: torch | [torch ao nn intrinsic qat modules linear_fused.py]",
    "role": "src",
    "loc": 144
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\qat\\modules\\linear_relu.py",
    "summary": "A LinearReLU module fused from Linear and ReLU modules, attached with | classes: LinearReLU | imports: torch | [torch ao nn intrinsic qat modules linear_relu.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\qat\\modules\\__init__.py",
    "summary": "Package initializer | imports: conv_fused, linear_fused, linear_relu | [torch ao nn intrinsic qat modules __init__.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn intrinsic quantized __init__.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn intrinsic quantized dynamic __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\dynamic\\modules\\linear_relu.py",
    "summary": "A LinearReLU module fused from Linear and ReLU modules that can be used | classes: LinearReLU | imports: torch | [torch ao nn intrinsic quantized dynamic modules linear_relu.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\dynamic\\modules\\__init__.py",
    "summary": "Package initializer | imports: linear_relu | [torch ao nn intrinsic quantized dynamic modules __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\modules\\bn_relu.py",
    "summary": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU | classes: BNReLU2d, BNReLU3d | imports: torch | [torch ao nn intrinsic quantized modules bn_relu.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_add.py",
    "summary": "A ConvAdd2d module is a fused module of Conv2d and Add | classes: ConvAdd2d, ConvAddReLU2d | imports: torch | [torch ao nn intrinsic quantized modules conv_add.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py",
    "summary": "A ConvReLU1d module is a fused module of Conv1d and ReLU | classes: ConvReLU1d, ConvReLU2d, ConvReLU3d | imports: torch | [torch ao nn intrinsic quantized modules conv_relu.py]",
    "role": "src",
    "loc": 220
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\modules\\linear_relu.py",
    "summary": "A LinearReLU module fused from Linear and ReLU modules | classes: LinearReLU, LinearLeakyReLU, LinearTanh | imports: torch | [torch ao nn intrinsic quantized modules linear_relu.py]",
    "role": "src",
    "loc": 155
  },
  {
    "id": "torch\\ao\\nn\\intrinsic\\quantized\\modules\\__init__.py",
    "summary": "Package initializer | imports: bn_relu, conv_add, conv_relu, linear_relu | [torch ao nn intrinsic quantized modules __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\ao\\nn\\qat\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn qat __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\qat\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn qat dynamic __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\qat\\dynamic\\modules\\linear.py",
    "summary": "A linear module attached with FakeQuantize modules for weight, | classes: Linear | imports: torch | [torch ao nn qat dynamic modules linear.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\ao\\nn\\qat\\dynamic\\modules\\__init__.py",
    "summary": "Package initializer | imports: linear | [torch ao nn qat dynamic modules __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\ao\\nn\\qat\\modules\\conv.py",
    "summary": "No description | classes: _ConvNd, Conv1d, Conv2d, Conv3d | imports: torch | [torch ao nn qat modules conv.py]",
    "role": "src",
    "loc": 273
  },
  {
    "id": "torch\\ao\\nn\\qat\\modules\\embedding_ops.py",
    "summary": "An embedding bag module attached with FakeQuantize modules for weight, | classes: Embedding, EmbeddingBag | imports: torch | [torch ao nn qat modules embedding_ops.py]",
    "role": "src",
    "loc": 221
  },
  {
    "id": "torch\\ao\\nn\\qat\\modules\\linear.py",
    "summary": "A linear module attached with FakeQuantize modules for weight, | classes: Linear | imports: torch | [torch ao nn qat modules linear.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\ao\\nn\\qat\\modules\\__init__.py",
    "summary": "Package initializer | imports: conv, embedding_ops, linear | [torch ao nn qat modules __init__.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\ao\\nn\\quantizable\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn quantizable __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\quantizable\\modules\\activation.py",
    "summary": "No description | classes: MultiheadAttention | imports: torch | [torch ao nn quantizable modules activation.py]",
    "role": "src",
    "loc": 457
  },
  {
    "id": "torch\\ao\\nn\\quantizable\\modules\\rnn.py",
    "summary": "We will recreate all the RNN modules as we require the modules to be decomposed | classes: LSTMCell, _LSTMSingleLayer, _LSTMLayer, LSTM | imports: numbers, torch | [torch ao nn quantizable modules rnn.py]",
    "role": "src",
    "loc": 510
  },
  {
    "id": "torch\\ao\\nn\\quantizable\\modules\\__init__.py",
    "summary": "Package initializer | imports: activation, rnn | [torch ao nn quantizable modules __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\ao\\nn\\quantized\\functional.py",
    "summary": "Functional interface (quantized). | functions: avg_pool2d, avg_pool3d, adaptive_avg_pool2d, adaptive_avg_pool3d, conv1d, conv2d | imports: torch, modules | [torch ao nn quantized functional.py]",
    "role": "src",
    "loc": 654
  },
  {
    "id": "torch\\ao\\nn\\quantized\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn quantized __init__.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torch\\ao\\nn\\quantized\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn quantized dynamic __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\quantized\\dynamic\\modules\\conv.py",
    "summary": "Dynamically quantized convolution modules. | classes: Conv1d, Conv2d, Conv3d, ConvTranspose1d, ConvTranspose2d, ConvTranspose3d | imports: torch | [torch ao nn quantized dynamic modules conv.py]",
    "role": "src",
    "loc": 434
  },
  {
    "id": "torch\\ao\\nn\\quantized\\dynamic\\modules\\linear.py",
    "summary": "A dynamic quantized linear module with floating point tensor as inputs and outputs. | classes: Linear | imports: torch | [torch ao nn quantized dynamic modules linear.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py",
    "summary": "No description | classes: PackedParameter, RNNBase, LSTM, GRU, RNNCellBase, RNNCell | functions: _apply_permutation, apply_permutation, pack_weight_bias | imports: numbers, typing_extensions, torch | [torch ao nn quantized dynamic modules rnn.py]",
    "role": "src",
    "loc": 1133
  },
  {
    "id": "torch\\ao\\nn\\quantized\\dynamic\\modules\\__init__.py",
    "summary": "Package initializer | imports: conv, linear, rnn | [torch ao nn quantized dynamic modules __init__.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\activation.py",
    "summary": "Applies the element-wise function: | classes: ReLU6, Hardswish, ELU, LeakyReLU, Sigmoid, Softmax | imports: torch | [torch ao nn quantized modules activation.py]",
    "role": "src",
    "loc": 263
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\batchnorm.py",
    "summary": "No description | classes: _BatchNorm, BatchNorm2d, BatchNorm3d | imports: torch | [torch ao nn quantized modules batchnorm.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\conv.py",
    "summary": "Quantized convolution modules. | classes: _ConvNd, Conv1d, Conv2d, Conv3d, _ConvTransposeNd, ConvTranspose1d | functions: _reverse_repeat_padding | imports: torch, utils | [torch ao nn quantized modules conv.py]",
    "role": "src",
    "loc": 1050
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\dropout.py",
    "summary": "This is the quantized equivalent of :class:`~torch.nn.Dropout`. | classes: Dropout | imports: torch | [torch ao nn quantized modules dropout.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\embedding_ops.py",
    "summary": "No description | classes: EmbeddingPackedParams, Embedding, EmbeddingBag | imports: torch, utils | [torch ao nn quantized modules embedding_ops.py]",
    "role": "src",
    "loc": 345
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\functional_modules.py",
    "summary": "State collector class for float operations. | classes: FloatFunctional, FXFloatFunctional, QFunctional | imports: torch | [torch ao nn quantized modules functional_modules.py]",
    "role": "src",
    "loc": 213
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\linear.py",
    "summary": "No description | classes: LinearPackedParams, Linear | imports: torch, utils | [torch ao nn quantized modules linear.py]",
    "role": "src",
    "loc": 255
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\normalization.py",
    "summary": "This is the quantized version of :class:`~torch.nn.LayerNorm`. | classes: LayerNorm, GroupNorm, InstanceNorm1d, InstanceNorm2d, InstanceNorm3d | imports: torch | [torch ao nn quantized modules normalization.py]",
    "role": "src",
    "loc": 299
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\rnn.py",
    "summary": "A quantized long short-term memory (LSTM). | classes: LSTM | imports: torch | [torch ao nn quantized modules rnn.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\utils.py",
    "summary": "Wrapper for quantized modules than can be lowered from reference modules. | classes: WeightedQuantizedModule | functions: _get_weight_observer, _needs_weight_clamping, _clamp_weights, _quantize_weight, _ntuple_from_first, parse | imports: abc, torch | [torch ao nn quantized modules utils.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torch\\ao\\nn\\quantized\\modules\\__init__.py",
    "summary": "Quantizes an incoming tensor | classes: Quantize, DeQuantize | imports: torch, activation, batchnorm, conv | [torch ao nn quantized modules __init__.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\__init__.py",
    "summary": "Package initializer | imports: modules | [torch ao nn quantized reference __init__.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\conv.py",
    "summary": "A reference version of nn.quantized.Conv2d | classes: _ConvNd, Conv1d, Conv2d, Conv3d, _ConvTransposeNd, ConvTranspose1d | imports: torch, utils | [torch ao nn quantized reference modules conv.py]",
    "role": "src",
    "loc": 453
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\linear.py",
    "summary": "A reference quantized linear module that fits into the FX | classes: Linear | imports: torch, utils | [torch ao nn quantized reference modules linear.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\rnn.py",
    "summary": "No description | classes: RNNCellBase, RNNCell, LSTMCell, GRUCell, RNNBase, LSTM | functions: _apply_permutation, _get_weight_and_quantization_params, get_quantized_weight, _get_quantize_and_dequantized_weight | imports: torch, utils | [torch ao nn quantized reference modules rnn.py]",
    "role": "src",
    "loc": 743
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\sparse.py",
    "summary": "A reference quantized Embedding module that fits into the | classes: Embedding, EmbeddingBag | imports: torch, utils | [torch ao nn quantized reference modules sparse.py]",
    "role": "src",
    "loc": 145
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\utils.py",
    "summary": "No description | classes: ReferenceQuantizedModule | functions: _quantize_weight_decomposed, _dequantize_weight_decomposed, _quantize_weight, _quantize_and_dequantize_weight_decomposed, _quantize_and_dequantize_weight, _save_weight_qparams | imports: torch | [torch ao nn quantized reference modules ",
    "role": "src",
    "loc": 393
  },
  {
    "id": "torch\\ao\\nn\\quantized\\reference\\modules\\__init__.py",
    "summary": "Package initializer | imports: conv, linear, rnn, sparse | [torch ao nn quantized reference modules __init__.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\ao\\nn\\sparse\\__init__.py",
    "summary": "Package initializer | [torch ao nn sparse __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\ao\\nn\\sparse\\quantized\\linear.py",
    "summary": "No description | classes: LinearPackedParams, Linear | imports: torch | [torch ao nn sparse quantized linear.py]",
    "role": "src",
    "loc": 219
  },
  {
    "id": "torch\\ao\\nn\\sparse\\quantized\\utils.py",
    "summary": "No description | classes: LinearBlockSparsePattern | functions: _is_valid_linear_block_sparse_pattern | imports: threading | [torch ao nn sparse quantized utils.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\ao\\nn\\sparse\\quantized\\__init__.py",
    "summary": "Package initializer | imports: torch, linear | [torch ao nn sparse quantized __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\ao\\nn\\sparse\\quantized\\dynamic\\linear.py",
    "summary": "A dynamically quantized sparse linear module with float tensor as inputs and outputs. | classes: Linear | imports: torch | [torch ao nn sparse quantized dynamic linear.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "torch\\ao\\nn\\sparse\\quantized\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: linear | [torch ao nn sparse quantized dynamic __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\ao\\ns\\_numeric_suite.py",
    "summary": "Base class for stats logging | classes: Logger, ShadowLogger, OutputLogger, Shadow | functions: _find_match, compare_weights, _get_logger_dict_helper, get_prefix, get_logger_dict, _convert_tuple_to_list | imports: torch | [torch ao ns _numeric_suite.py]",
    "role": "src",
    "loc": 445
  },
  {
    "id": "torch\\ao\\ns\\_numeric_suite_fx.py",
    "summary": "This module contains tooling to compare weights and activations | classes: OutputLogger, OutputComparisonLogger, NSTracer | functions: _extract_weights_one_model, _extract_weights_impl, extract_weights, _add_loggers_one_model, _add_loggers_impl, add_loggers | imports: torch, fx | [torch ao ns _numer",
    "role": "src",
    "loc": 853
  },
  {
    "id": "torch\\ao\\ns\\__init__.py",
    "summary": "Package initializer | [torch ao ns __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\ns\\fx\\graph_matcher.py",
    "summary": "Iterates through the graph of gm, starting with the output nodes | classes: _NSGraphMatchableSubgraphsIterator, GraphMatchingException, SubgraphTypeRelationship | functions: _get_output_nodes, _get_subgraph_relationship_type, _get_name_for_subgraph, _get_node_target_type, get_matching_subgraph_pairs",
    "role": "src",
    "loc": 350
  },
  {
    "id": "torch\\ao\\ns\\fx\\graph_passes.py",
    "summary": "No description | functions: _maybe_get_fqn, _insert_logger_after_node, add_loggers_to_model, load_arg, _insert_quantize_per_tensor_node, _insert_dtype_cast_after_node | imports: torch, ns_types, utils | [torch ao ns fx graph_passes.py]",
    "role": "src",
    "loc": 904
  },
  {
    "id": "torch\\ao\\ns\\fx\\mappings.py",
    "summary": "No description | functions: get_base_name_to_sets_of_related_ops, get_base_name_for_op, add_op_to_sets_of_related_ops, get_node_type_to_io_type_map, get_unmatchable_types_map | imports: operator, torch, ns_types | [torch ao ns fx mappings.py]",
    "role": "src",
    "loc": 625
  },
  {
    "id": "torch\\ao\\ns\\fx\\ns_types.py",
    "summary": "No description | classes: NSSingleResultValuesType, NSSubgraph | imports: torch | [torch ao ns fx ns_types.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\ao\\ns\\fx\\n_shadows_utils.py",
    "summary": "Output propagation (modeled from shape propagation). | classes: OutputProp, M | functions: _get_attr_name, _get_attr_wrapper_name, _get_dedup_subgraphs, _order_nodes, _get_logger_for_subgraph, create_submodule_from_subgraph | imports: copy, operator, torch, tabulate | [torch ao ns fx n_shadows_utils",
    "role": "src",
    "loc": 964
  },
  {
    "id": "torch\\ao\\ns\\fx\\pattern_utils.py",
    "summary": "No description | functions: get_type_a_related_to_b, get_reversed_fusions, end_node_matches_reversed_fusion | imports: torch, ns_types | [torch ao ns fx pattern_utils.py]",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\ao\\ns\\fx\\qconfig_multi_mapping.py",
    "summary": "This class, used with the prepare_n_shadows_model API, stores a list of :class:`torch.ao.quantization.QConfigMapping`s | classes: QConfigMultiMapping | functions: _remove_duplicates_and_none | imports: copy, torch | [torch ao ns fx qconfig_multi_mapping.py]",
    "role": "src",
    "loc": 173
  },
  {
    "id": "torch\\ao\\ns\\fx\\utils.py",
    "summary": "No description | classes: NodeInputOrOutputType | functions: get_node_first_input_and_output_type, get_node_input_qparams, _get_scale_zp_from_function_args, return_first_non_observer_node, get_number_of_non_param_args, get_arg_indices_of_inputs_to_log | imports: operator, torch, ns_types | [torch ao",
    "role": "src",
    "loc": 425
  },
  {
    "id": "torch\\ao\\ns\\fx\\weight_utils.py",
    "summary": "No description | functions: mod_weight_detach, mod_0_weight_detach, mod_weight_bias_0, get_lstm_weight, get_qlstm_weight, get_conv_mod_weight | imports: torch, ns_types, utils | [torch ao ns fx weight_utils.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\ao\\ns\\fx\\__init__.py",
    "summary": "Package initializer | [torch ao ns fx __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_mappings.py",
    "summary": "No description | functions: get_static_sparse_quantized_mapping, get_dynamic_sparse_quantized_mapping | imports: torch | [torch ao pruning _mappings.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\ao\\pruning\\__init__.py",
    "summary": "Package initializer | imports: _mappings, scheduler, sparsifier | [torch ao pruning __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\ao\\pruning\\scheduler\\base_scheduler.py",
    "summary": "No description | classes: _enable_get_sl_call, BaseScheduler | imports: weakref, functools, torch | [torch ao pruning scheduler base_scheduler.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\ao\\pruning\\scheduler\\cubic_scheduler.py",
    "summary": "Sets the sparsity level of each parameter group to the final sl | classes: CubicSL | functions: _clamp | imports: base_scheduler | [torch ao pruning scheduler cubic_scheduler.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "torch\\ao\\pruning\\scheduler\\lambda_scheduler.py",
    "summary": "Sets the sparsity level of each parameter group to the final sl | classes: LambdaSL | imports: base_scheduler | [torch ao pruning scheduler lambda_scheduler.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\ao\\pruning\\scheduler\\__init__.py",
    "summary": "Package initializer | [torch ao pruning scheduler __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\sparsifier\\base_sparsifier.py",
    "summary": "Base class for all sparsifiers. | classes: BaseSparsifier | imports: abc, copy, torch, utils | [torch ao pruning sparsifier base_sparsifier.py]",
    "role": "src",
    "loc": 290
  },
  {
    "id": "torch\\ao\\pruning\\sparsifier\\nearly_diagonal_sparsifier.py",
    "summary": "Nearly Diagonal Sparsifier | classes: NearlyDiagonalSparsifier | imports: torch | [torch ao pruning sparsifier nearly_diagonal_sparsifier.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\ao\\pruning\\sparsifier\\utils.py",
    "summary": "Parametrization for the weights. Should be attached to the 'weight' or | classes: FakeSparsity | functions: module_contains_param, swap_module, module_to_fqn, fqn_to_module, get_arg_info_from_tensor_fqn | imports: torch | [torch ao pruning sparsifier utils.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "torch\\ao\\pruning\\sparsifier\\weight_norm_sparsifier.py",
    "summary": "Weight-Norm Sparsifier | classes: WeightNormSparsifier | functions: _flat_idx_to_2d | imports: operator, functools, torch, base_sparsifier | [torch ao pruning sparsifier weight_norm_sparsifier.py]",
    "role": "src",
    "loc": 206
  },
  {
    "id": "torch\\ao\\pruning\\sparsifier\\__init__.py",
    "summary": "Package initializer | [torch ao pruning sparsifier __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\__init__.py",
    "summary": "Package initializer | [torch ao pruning _experimental __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\activation_sparsifier\\activation_sparsifier.py",
    "summary": "The Activation sparsifier class aims to sparsify/prune activations in a neural | classes: ActivationSparsifier | imports: copy, torch | [torch ao pruning _experimental activation_sparsifier activation_sparsifier.py]",
    "role": "src",
    "loc": 383
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\activation_sparsifier\\__init__.py",
    "summary": "Package initializer | [torch ao pruning _experimental activation_sparsifier __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_scheduler\\base_data_scheduler.py",
    "summary": "No description | classes: _enable_get_sp_call, BaseDataScheduler | imports: abc, weakref, functools, torch | [torch ao pruning _experimental data_scheduler base_data_scheduler.py]",
    "role": "src",
    "loc": 144
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_scheduler\\__init__.py",
    "summary": "Package initializer | imports: base_data_scheduler | [torch ao pruning _experimental data_scheduler __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\base_data_sparsifier.py",
    "summary": "No description | classes: _Container, BaseDataSparsifier | imports: abc, copy, torch | [torch ao pruning _experimental data_sparsifier base_data_sparsifier.py]",
    "role": "src",
    "loc": 261
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\data_norm_sparsifier.py",
    "summary": "L1-Norm Sparsifier | classes: DataNormSparsifier | imports: operator, functools, torch, base_data_sparsifier | [torch ao pruning _experimental data_sparsifier data_norm_sparsifier.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\quantization_utils.py",
    "summary": "Fetches Embedding and EmbeddingBag modules from the model | functions: _fetch_all_embeddings, post_training_sparse_quantize | imports: torch | [torch ao pruning _experimental data_sparsifier quantization_utils.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\__init__.py",
    "summary": "Package initializer | imports: base_data_sparsifier, data_norm_sparsifier | [torch ao pruning _experimental data_sparsifier __init__.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\benchmarks\\dlrm_utils.py",
    "summary": "The SparseDLRM model is a wrapper around the DLRM_Net model that tries | classes: SparseDLRM | functions: get_valid_name, get_dlrm_model, dlrm_wrap, make_test_data_loader, fetch_model | imports: zipfile, numpy, dlrm_data_pytorch, dlrm_s_pytorch | [torch ao pruning _experimental data_sparsifier bench",
    "role": "benchmarks",
    "loc": 154
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\benchmarks\\evaluate_disk_savings.py",
    "summary": "Create a DataNormSparsifier and the attach it to the model embedding layers | functions: create_attach_sparsifier, save_model_states, sparsify_model | imports: argparse, copy, zipfile, pandas | [torch ao pruning _experimental data_sparsifier benchmarks evaluate_disk_savings.py]",
    "role": "benchmarks",
    "loc": 144
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\benchmarks\\evaluate_forward_time.py",
    "summary": "The purpose of this function is to time the forward run of the model. | functions: run_forward, make_sample_test_batch, measure_forward_pass | imports: argparse, numpy, pandas, dlrm_s_pytorch | [torch ao pruning _experimental data_sparsifier benchmarks evaluate_forward_time.py]",
    "role": "benchmarks",
    "loc": 88
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\benchmarks\\evaluate_model_metrics.py",
    "summary": "Perform inference and evaluation on the test dataset. | functions: inference_and_evaluation, evaluate_metrics | imports: argparse, numpy, pandas, sklearn | [torch ao pruning _experimental data_sparsifier benchmarks evaluate_model_metrics.py]",
    "role": "benchmarks",
    "loc": 110
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\lightning\\__init__.py",
    "summary": "Package initializer | [torch ao pruning _experimental data_sparsifier lightning __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\lightning\\callbacks\\data_sparsity.py",
    "summary": "Lightning callback that enables post-training sparsity. | classes: PostTrainingDataSparsity, TrainingAwareDataSparsity | imports: copy, pytorch_lightning, _data_sparstity_utils, torch | [torch ao pruning _experimental data_sparsifier lightning callbacks data_sparsity.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\lightning\\callbacks\\_data_sparstity_utils.py",
    "summary": "Attaches a data sparsifier to all the layers of the module. | functions: _attach_model_to_data_sparsifier, _get_valid_name, _log_sparsified_level | imports: torch | [torch ao pruning _experimental data_sparsifier lightning callbacks _data_sparstity_utils.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\lightning\\callbacks\\__init__.py",
    "summary": "Package initializer | [torch ao pruning _experimental data_sparsifier lightning callbacks __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\data_sparsifier\\lightning\\tests\\test_callbacks.py",
    "summary": "No description | classes: DummyModel, DummyLightningModule, StepSLScheduler, TestPostTrainingCallback, TestTrainingAwareCallback | functions: _make_lightning_module | imports: importlib, unittest, torch, pytorch_lightning | [torch ao pruning _experimental data_sparsifier lightning tests test_callbac",
    "role": "tests",
    "loc": 240
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\base_structured_sparsifier.py",
    "summary": "Base class for structured pruning. | classes: BaseStructuredSparsifier | functions: _get_supported_structured_pruning_modules, _get_supported_activation_functions, _get_supported_activation_modules, _get_default_structured_pruning_patterns | imports: operator, torch, match_utils, parametrization | [",
    "role": "src",
    "loc": 269
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\FPGM_pruner.py",
    "summary": "Filter Pruning via Geometric Median (FPGM) Structured Pruner | classes: FPGMPruner | imports: torch, base_structured_sparsifier | [torch ao pruning _experimental pruner FPGM_pruner.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\lstm_saliency_pruner.py",
    "summary": "Prune packed LSTM weights based on saliency. | classes: LSTMSaliencyPruner | imports: torch, base_structured_sparsifier | [torch ao pruning _experimental pruner lstm_saliency_pruner.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\match_utils.py",
    "summary": "Contains utility functions to check if a pattern is in the graph and return the matching nodes | functions: _match, apply_match | imports: torch | [torch ao pruning _experimental pruner match_utils.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\parametrization.py",
    "summary": "Parametrization for Structured Pruning. Like FakeSparsity, this should be attached to | classes: FakeStructuredSparsity, BiasHook | functions: module_contains_param | imports: torch | [torch ao pruning _experimental pruner parametrization.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\prune_functions.py",
    "summary": "Collection of conversion functions for linear / conv2d structured pruning | functions: _remove_bias_handles, _get_adjusted_next_layer_bias, _prune_module_bias, _propagate_module_bias, _prune_linear_helper, prune_linear | imports: torch, parametrization | [torch ao pruning _experimental pruner prune_",
    "role": "src",
    "loc": 378
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\saliency_pruner.py",
    "summary": "Prune rows based on the saliency (L1 norm) of each row. | classes: SaliencyPruner | imports: base_structured_sparsifier | [torch ao pruning _experimental pruner saliency_pruner.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\ao\\pruning\\_experimental\\pruner\\__init__.py",
    "summary": "Package initializer | imports: base_structured_sparsifier, FPGM_pruner, lstm_saliency_pruner, parametrization | [torch ao pruning _experimental pruner __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\ao\\quantization\\fake_quantize.py",
    "summary": "Implements modules  used to perform fake quantization. | classes: FakeQuantizeBase, FakeQuantize, FixedQParamsFakeQuantize, FusedMovingAvgObsFakeQuantize | functions: _is_per_channel, _is_per_tensor, _is_symmetric_quant, _is_float_qparams, _is_fake_quant_script_module, disable_fake_quant | imports: ",
    "role": "src",
    "loc": 515
  },
  {
    "id": "torch\\ao\\quantization\\fuser_method_mappings.py",
    "summary": "Return the fused the conv and bn modules. | functions: fuse_conv_bn, fuse_conv_bn_relu, fuse_linear_bn, fuse_convtranspose_bn, _sequential_wrapper2, fuser_method | imports: torch | [torch ao quantization fuser_method_mappings.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "torch\\ao\\quantization\\fuse_modules.py",
    "summary": "No description | functions: _get_module, _set_module, fuse_known_modules, _fuse_modules_helper, _fuse_modules, fuse_modules | imports: copy, torch | [torch ao quantization fuse_modules.py]",
    "role": "src",
    "loc": 168
  },
  {
    "id": "torch\\ao\\quantization\\observer.py",
    "summary": "This module implements observers which are used to collect statistics about | classes: _PartialWrapper, ObserverBase, UniformQuantizationObserverBase, MinMaxObserver, MovingAverageMinMaxObserver, PerChannelMinMaxObserver | functions: _with_args, _with_callable_args, get_block_size, _is_observer_scri",
    "role": "src",
    "loc": 1642
  },
  {
    "id": "torch\\ao\\quantization\\qconfig.py",
    "summary": "Describes how to quantize a layer or a part of the network by providing | classes: QConfig, QConfigDynamic | functions: get_default_qconfig, get_default_qat_qconfig, get_default_qconfig_dict, get_default_qat_qconfig_dict, _assert_valid_qconfig, _add_module_to_qconfig_obs_ctr | imports: copy, typing_",
    "role": "src",
    "loc": 584
  },
  {
    "id": "torch\\ao\\quantization\\qconfig_mapping.py",
    "summary": "Mapping from model ops to :class:`torch.ao.quantization.QConfig` s. | classes: QConfigMapping | functions: _get_default_qconfig_mapping, get_default_qconfig_mapping, get_default_qat_qconfig_mapping, _get_symmetric_qnnpack_qconfig_mapping, _get_symmetric_qnnpack_qat_qconfig_mapping, _get_default_qcon",
    "role": "src",
    "loc": 299
  },
  {
    "id": "torch\\ao\\quantization\\quantization_mappings.py",
    "summary": "These modules cannot have observers inserted by default. | functions: no_observer_set, get_default_static_quant_module_mappings, get_default_static_quant_reference_module_mappings, get_embedding_static_quant_module_mappings, get_default_static_sparse_quant_module_mappings, get_static_quant_module_cl",
    "role": "src",
    "loc": 294
  },
  {
    "id": "torch\\ao\\quantization\\quantize.py",
    "summary": "Defines the default custom config dict. | functions: get_default_custom_config_dict, _propagate_qconfig_helper, propagate_qconfig_, _observer_forward_hook, _observer_forward_pre_hook, _register_activation_post_process_hook | imports: copy, inspect, torch, utils | [torch ao quantization quantize.py]",
    "role": "src",
    "loc": 652
  },
  {
    "id": "torch\\ao\\quantization\\quantize_fx.py",
    "summary": "Store preserved attributes to the model.meta so that it can be preserved during deepcopy | functions: attach_preserved_attrs_to_model, _check_is_graph_module, _attach_meta_to_node_if_not_exist, _swap_ff_with_fxff, _fuse_fx, _prepare_fx | imports: copy, torch, backend_config, fx | [torch ao quantizat",
    "role": "src",
    "loc": 508
  },
  {
    "id": "torch\\ao\\quantization\\quantize_jit.py",
    "summary": "No description | functions: _check_is_script_module, _check_forward_method, script_qconfig, script_qconfig_dict, fuse_conv_bn_jit, _prepare_jit | imports: torch | [torch ao quantization quantize_jit.py]",
    "role": "src",
    "loc": 348
  },
  {
    "id": "torch\\ao\\quantization\\quantize_pt2e.py",
    "summary": "Prepare a model for post training quantization | functions: prepare_pt2e, prepare_qat_pt2e, _quant_node_constraint, convert_pt2e | imports: torch, pt2e, quantize_fx | [torch ao quantization quantize_pt2e.py]",
    "role": "src",
    "loc": 175
  },
  {
    "id": "torch\\ao\\quantization\\quant_type.py",
    "summary": "No description | classes: QuantType | functions: _get_quant_type_to_str, _quant_type_from_str | [torch ao quantization quant_type.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\ao\\quantization\\stubs.py",
    "summary": "Quantize stub module, before calibration, this is same as an observer, | classes: QuantStub, DeQuantStub, QuantWrapper | imports: torch | [torch ao quantization stubs.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\ao\\quantization\\utils.py",
    "summary": "Utils shared by different modes of quantization (eager/graph) | classes: MatchAllNode | functions: check_node, get_combined_dict, is_per_tensor, is_per_channel, getattr_from_fqn, to_underlying_dtype | imports: functools, inspect, torch | [torch ao quantization utils.py]",
    "role": "src",
    "loc": 662
  },
  {
    "id": "torch\\ao\\quantization\\_correct_bias.py",
    "summary": "Mean Logger for a Shadow module. | classes: MeanShadowLogger | functions: get_module, parent_child_names, get_param, bias_correction | imports: torch | [torch ao quantization _correct_bias.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torch\\ao\\quantization\\_equalize.py",
    "summary": "No description | functions: set_module_weight, set_module_bias, has_bias, get_module_weight, get_module_bias, max_over_ndim | imports: copy, torch | [torch ao quantization _equalize.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "torch\\ao\\quantization\\_learnable_fake_quantize.py",
    "summary": "Generalized extension of the FakeQuantize module in fake_quantize.py. | classes: _LearnableFakeQuantize | imports: torch | [torch ao quantization _learnable_fake_quantize.py]",
    "role": "src",
    "loc": 168
  },
  {
    "id": "torch\\ao\\quantization\\__init__.py",
    "summary": "This observer is used to describe an observer whose quantization parameters | classes: _DerivedObserverOrFakeQuantize | functions: default_eval_fn | imports: torch, fake_quantize, fuse_modules, fuser_method_mappings | [torch ao quantization __init__.py]",
    "role": "src",
    "loc": 214
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\backend_config.py",
    "summary": "An enum that represents different ways of how an operator/operator pattern | classes: ObservationType, DTypeWithConstraints, DTypeConfig, BackendConfig, BackendPatternConfig | imports: dataclasses, torch | [torch ao quantization backend_config backend_config.py]",
    "role": "src",
    "loc": 623
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\executorch.py",
    "summary": "Return all configs related to linear modules and ops. | functions: _get_linear_configs, _get_conv_configs, _get_binary_ops_configs, _get_share_qparams_ops_configs, _get_bn_configs, _get_cat_configs | imports: operator, torch, _common_operator_config_utils, backend_config | [torch ao quantization bac",
    "role": "src",
    "loc": 412
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\fbgemm.py",
    "summary": "Return the `BackendConfig` for PyTorch's native FBGEMM backend. | functions: get_fbgemm_backend_config | imports: torch, _common_operator_config_utils, backend_config | [torch ao quantization backend_config fbgemm.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\native.py",
    "summary": "Return the `BackendConfig` for PyTorch Native backend (fbgemm/qnnpack) with various additional fp16 ops. | functions: get_test_only_legacy_native_backend_config, get_native_backend_config, get_native_backend_config_dict, get_test_only_legacy_native_backend_config_dict | imports: torch, _common_opera",
    "role": "src",
    "loc": 192
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\observation_type.py",
    "summary": "No description | [torch ao quantization backend_config observation_type.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\onednn.py",
    "summary": "Given the linear, bn and leaky_relu modules, fuses them and returns the fused module | functions: _fuse_linear_bn_leaky_relu, _fuse_conv_add_left, _conv_add_root_node_getter_left, _conv_add_extra_inputs_getter_left, _fuse_conv_bn_add_left, _conv_bn_add_root_node_getter_left | imports: operator, torc",
    "role": "src",
    "loc": 466
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\qnnpack.py",
    "summary": "Return the `BackendConfig` for PyTorch's native QNNPACK backend. | functions: get_qnnpack_backend_config | imports: torch, _common_operator_config_utils, backend_config | [torch ao quantization backend_config qnnpack.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\tensorrt.py",
    "summary": "Return the `BackendConfig` for the TensorRT backend. | functions: get_tensorrt_backend_config, get_tensorrt_backend_config_dict | imports: torch, _common_operator_config_utils, backend_config | [torch ao quantization backend_config tensorrt.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\utils.py",
    "summary": "No description | functions: get_pattern_to_dtype_configs, get_qat_module_classes, get_fused_module_classes, get_pattern_to_input_type_to_index, get_root_module_to_quantized_reference_module, get_fuser_method_mapping | imports: torch, backend_config | [torch ao quantization backend_config utils.py]",
    "role": "src",
    "loc": 246
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\x86.py",
    "summary": "Return the `BackendConfig` for PyTorch's native x86 backend. | functions: get_x86_backend_config | imports: torch, _common_operator_config_utils, backend_config | [torch ao quantization backend_config x86.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\_common_operator_config_utils.py",
    "summary": "No description | functions: _get_binary_op_configs, _get_linear_configs, _get_conv_configs, _get_cat_config, _get_ln_configs, _get_default_op_configs | imports: copy, operator, torch, backend_config | [torch ao quantization backend_config _common_operator_config_utils.py]",
    "role": "src",
    "loc": 669
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\_qnnpack_pt2e.py",
    "summary": "No description | functions: get_linear_configs, get_conv_configs, get_pooling_configs, root_node_getter, get_relu_configs, get_binary_op_configs | imports: operator, torch | [torch ao quantization backend_config _qnnpack_pt2e.py]",
    "role": "src",
    "loc": 120
  },
  {
    "id": "torch\\ao\\quantization\\backend_config\\__init__.py",
    "summary": "Package initializer | imports: backend_config, executorch, fbgemm, native | [torch ao quantization backend_config __init__.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\adaround_fake_quantize.py",
    "summary": "This is a FakeQuantizer that enables an adaptive rounding fake quantizer. | classes: AdaroundFakeQuantizer | imports: torch | [torch ao quantization experimental adaround_fake_quantize.py]",
    "role": "src",
    "loc": 123
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\adaround_loss.py",
    "summary": "Adaptive Rounding Loss functions described in https://arxiv.org/pdf/2004.10568.pdf | classes: AdaptiveRoundingLoss | imports: numpy, torch | [torch ao quantization experimental adaround_loss.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\adaround_optimization.py",
    "summary": "No description | classes: AdaptiveRoundingOptimizer | imports: copy, torch | [torch ao quantization experimental adaround_optimization.py]",
    "role": "src",
    "loc": 229
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\APoT_tensor.py",
    "summary": "No description | classes: TensorAPoT | imports: torch | [torch ao quantization experimental APoT_tensor.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\apot_utils.py",
    "summary": "This file contains utility functions to convert values | functions: float_to_apot, quant_dequant_util, apot_to_float | [torch ao quantization experimental apot_utils.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\fake_quantize.py",
    "summary": "No description | classes: APoTFakeQuantize | imports: torch | [torch ao quantization experimental fake_quantize.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\fake_quantize_function.py",
    "summary": "No description | classes: fake_quantize_function | imports: torch | [torch ao quantization experimental fake_quantize_function.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\linear.py",
    "summary": "A quantized linear module with quantized tensor as inputs and outputs | classes: LinearAPoT | imports: numpy, torch | [torch ao quantization experimental linear.py]",
    "role": "src",
    "loc": 131
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\observer.py",
    "summary": "This module implements nonuniform observers used to collect statistics about | classes: APoTObserver | imports: matplotlib, torch | [torch ao quantization experimental observer.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\qconfig.py",
    "summary": "No description | imports: torch | [torch ao quantization experimental qconfig.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\ao\\quantization\\experimental\\quantizer.py",
    "summary": "No description | classes: APoTQuantizer | functions: quantize_APoT, dequantize_APoT, quant_dequant_APoT | imports: numpy, torch | [torch ao quantization experimental quantizer.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "torch\\ao\\quantization\\fx\\convert.py",
    "summary": "Replace activation_post_process module call node with quantize and | functions: _replace_observer_with_quantize_dequantize_node_decomposed, add_dequantize_op_kwargs, _replace_observer_with_quantize_dequantize_node, _replace_observer_or_dequant_stub_with_dequantize_node, _is_conversion_supported, _ha",
    "role": "src",
    "loc": 948
  },
  {
    "id": "torch\\ao\\quantization\\fx\\custom_config.py",
    "summary": "No description | classes: StandaloneModuleConfigEntry, PrepareCustomConfig, ConvertCustomConfig, FuseCustomConfig | imports: dataclasses, torch | [torch ao quantization fx custom_config.py]",
    "role": "src",
    "loc": 450
  },
  {
    "id": "torch\\ao\\quantization\\fx\\fuse.py",
    "summary": "No description | functions: fuse, load_arg, default_root_node_getter, _find_matches, apply_match | imports: torch, custom_config, fuse_handler, match_utils | [torch ao quantization fx fuse.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "torch\\ao\\quantization\\fx\\fuse_handler.py",
    "summary": "Base handler class for the fusion patterns | classes: FuseHandler, DefaultFuseHandler | functions: _get_fusion_pattern_to_fuse_handler_cls | imports: abc, torch, custom_config, match_utils | [torch ao quantization fx fuse_handler.py]",
    "role": "src",
    "loc": 101
  },
  {
    "id": "torch\\ao\\quantization\\fx\\graph_module.py",
    "summary": "No description | classes: FusedGraphModule, ObservedGraphModule, ObservedStandaloneGraphModule, QuantizedGraphModule | functions: _is_observed_module, _get_observed_graph_module_attr, _is_observed_standalone_module, _save_packed_weight | imports: copy, torch | [torch ao quantization fx graph_module.",
    "role": "src",
    "loc": 168
  },
  {
    "id": "torch\\ao\\quantization\\fx\\lower_to_fbgemm.py",
    "summary": "Lower a quantized reference model (with reference quantized operator patterns) | functions: lower_to_fbgemm | imports: torch, _lower_to_native_backend | [torch ao quantization fx lower_to_fbgemm.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\ao\\quantization\\fx\\lower_to_qnnpack.py",
    "summary": "Lower a quantized reference model (with reference quantized operator patterns) | functions: lower_to_qnnpack | imports: torch, _lower_to_native_backend | [torch ao quantization fx lower_to_qnnpack.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\ao\\quantization\\fx\\lstm_utils.py",
    "summary": "Return an observed `torch.ao.nn.quantizable.LSTM` created from a `torch.nn.LSTM` | functions: _get_lstm_with_individually_observed_parts, make_qconfig, _get_reference_quantized_lstm_module | imports: copy, operator, torch | [torch ao quantization fx lstm_utils.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "torch\\ao\\quantization\\fx\\match_utils.py",
    "summary": "Matches a node in fx against a pattern | functions: _is_match, _find_matches, _recursive_record_node_in_match_map, record_match, is_standalone_module | imports: torch, graph_module, quantize_handler | [torch ao quantization fx match_utils.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "torch\\ao\\quantization\\fx\\pattern_utils.py",
    "summary": "No description | functions: _register_fusion_pattern, insert, get_default_fusion_patterns, _register_quant_pattern, get_default_quant_patterns, get_default_output_activation_post_process_map | imports: copy, torch | [torch ao quantization fx pattern_utils.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\ao\\quantization\\fx\\prepare.py",
    "summary": "No description | functions: _get_observer_kwargs, _get_qspec_for_arg, _create_obs_or_fq_from_qspec, _needs_obs_or_fq, _is_activation_post_process_node, _get_dtype_and_is_dynamic | imports: copy, dataclasses, torch, _equalize | [torch ao quantization fx prepare.py]",
    "role": "src",
    "loc": 1682
  },
  {
    "id": "torch\\ao\\quantization\\fx\\qconfig_mapping_utils.py",
    "summary": "No description | functions: _maybe_adjust_qconfig_for_module_name_object_type_order, _update_qconfig_for_fusion, _generate_node_name_to_qconfig, _check_is_valid_config_dict, _compare_prepare_convert_qconfig_mappings, _is_qconfig_supported_by_dtype_configs | imports: torch | [torch ao quantization fx",
    "role": "src",
    "loc": 317
  },
  {
    "id": "torch\\ao\\quantization\\fx\\quantize_handler.py",
    "summary": "Base handler class for the quantizer patterns | classes: QuantizeHandler, ConfigurableQuantizeHandler, BinaryOpQuantizeHandler, CatQuantizeHandler, ConvReluQuantizeHandler, LinearReLUQuantizeHandler | functions: _default_root_node_getter, _get_quantize_handler_cls, _get_pattern_to_quantize_handlers ",
    "role": "src",
    "loc": 163
  },
  {
    "id": "torch\\ao\\quantization\\fx\\tracer.py",
    "summary": "No description | classes: ScopeContextManager, QuantizationTracer | imports: torch | [torch ao quantization fx tracer.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\ao\\quantization\\fx\\utils.py",
    "summary": "Returns if node arg is weight | classes: ObservedGraphModuleAttrs | functions: node_arg_is_weight, node_arg_is_bias, get_custom_module_class_keys, get_linear_prepack_op_for_dtype, get_qconv_prepack_op, get_new_attr_name_with_prefix | imports: copy, operator, dataclasses, torch | [torch ao quantizati",
    "role": "src",
    "loc": 791
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_decomposed.py",
    "summary": "No description | classes: FakeQuantPerChannel | functions: _quant_min_max_bounds_check, quantize_per_tensor, quantize_per_tensor_meta, quantize_per_tensor_tensor, quantize_per_tensor_tensor_meta, quantize_per_tensor_tensor2 | imports: torch | [torch ao quantization fx _decomposed.py]",
    "role": "src",
    "loc": 992
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_equalize.py",
    "summary": "Observer for tracking the running min/max values of input columns, and | classes: _InputEqualizationObserver, _WeightEqualizationObserver, EqualizationQConfig | functions: reshape_scale, calculate_equalization_scale, fused_module_supports_equalization, nn_module_supports_equalization, custom_module_",
    "role": "src",
    "loc": 701
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_lower_to_native_backend.py",
    "summary": "No description | functions: _is_node_in_list, is_fixed_qparams_node, is_default_node, is_copy_node, is_general_tensor_shape_node, is_other_node | imports: operator, torch, utils | [torch ao quantization fx _lower_to_native_backend.py]",
    "role": "src",
    "loc": 1079
  },
  {
    "id": "torch\\ao\\quantization\\fx\\__init__.py",
    "summary": "Package initializer | imports: convert, fuse, prepare | [torch ao quantization fx __init__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_model_report\\detector.py",
    "summary": "This class contains the QConfig information for a single module. | classes: DetectorQConfigInfo, DetectorBase, PerChannelDetector, DynamicStaticDetector, InputWeightEqualizationDetector, OutlierDetector | imports: abc, torch | [torch ao quantization fx _model_report detector.py]",
    "role": "src",
    "loc": 1210
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_model_report\\model_report.py",
    "summary": "The ModelReport class aims to provide users an easy way to diagnose issues that they run into | classes: ModelReport | imports: torch | [torch ao quantization fx _model_report model_report.py]",
    "role": "src",
    "loc": 451
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_model_report\\model_report_observer.py",
    "summary": "This observer is used to record additional information regarding keeping track | classes: ModelReportObserver | imports: torch | [torch ao quantization fx _model_report model_report_observer.py]",
    "role": "src",
    "loc": 191
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_model_report\\model_report_visualizer.py",
    "summary": "The ModelReportVisualizer class aims to provide users a way to visualize some of the statistics | classes: ModelReportVisualizer | imports: torch, tabulate, matplotlib | [torch ao quantization fx _model_report model_report_visualizer.py]",
    "role": "src",
    "loc": 503
  },
  {
    "id": "torch\\ao\\quantization\\fx\\_model_report\\__init__.py",
    "summary": "Package initializer | [torch ao quantization fx _model_report __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\duplicate_dq_pass.py",
    "summary": "No description | classes: DuplicateDQPass | functions: _maybe_duplicate_dq, maybe_replace_node | imports: operator, torch | [torch ao quantization pt2e duplicate_dq_pass.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\export_utils.py",
    "summary": "Class to wrap a callable in an :class:`torch.nn.Module`. Use this if you | classes: _WrapperModule | functions: model_is_exported, _replace_dropout, dropout_train, dropout_eval, _replace_batchnorm, bn_train | imports: types, torch, utils | [torch ao quantization pt2e export_utils.py]",
    "role": "src",
    "loc": 185
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\graph_utils.py",
    "summary": "No description | functions: _create_equivalent_types_dict, get_equivalent_types, update_equivalent_types_dict, _partitions_sequential, _get_matching_types, _valid_type_sequence | imports: operator, torch | [torch ao quantization pt2e graph_utils.py]",
    "role": "src",
    "loc": 146
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\port_metadata_pass.py",
    "summary": "Port metadata for nodes added by quantization flow. | classes: PortNodeMetaForQDQ | functions: _add_metadata, _has_quant_annotation, _find_choose_qparams_node, _port_metadata_for_input_quant_nodes, _port_metadata_for_output_quant_nodes | imports: torch | [torch ao quantization pt2e port_metadata_pas",
    "role": "src",
    "loc": 185
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\prepare.py",
    "summary": "Find the root node for the sharing tree | functions: _find_root_edge_or_node, _union, _update_shared_with, _unwrap_shared_qspec, _has_same_attr, _get_edge_or_node_to_qspec | imports: torch | [torch ao quantization pt2e prepare.py]",
    "role": "src",
    "loc": 428
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\qat_utils.py",
    "summary": "Optional example inputs for quantized and folded conv-bn patterns | functions: _get_quantized_conv_bn_example_inputs_kwargs, _get_conv_bn_pattern, _conv_bn_pattern, _get_qat_conv_bn_pattern, _qat_conv_bn_pattern, _get_qat_conv_bn_pattern_no_conv_bias | imports: copy, dataclasses, operator, torch | [",
    "role": "src",
    "loc": 809
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\utils.py",
    "summary": "Assuming dest is one of the ops inserted by quant workflow, this function | functions: _is_connected, _find_q_dq_node_for_user, _is_sym_size_node, _filter_sym_size_users, _is_valid_annotation, _get_tensor_constant_from_node | imports: operator, types, torch | [torch ao quantization pt2e utils.py]",
    "role": "src",
    "loc": 453
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\_affine_quantization.py",
    "summary": "No description | classes: AffineQuantizedMinMaxObserver | functions: _is_float8_type, _get_and_check_qmin_qmax, _get_reduction_params, _register_custom_op, decorator, choose_qparams_affine_with_min_max | imports: abc, torch | [torch ao quantization pt2e _affine_quantization.py]",
    "role": "src",
    "loc": 664
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\_numeric_debugger.py",
    "summary": "Base class for capturing output values for nodes in a GraphModule, it only captures | classes: OutputLogger, QuantizationComparisonResult, NodeAccuracySummary | functions: generate_numeric_debug_handle, _find_max_id, _assign_debug_handle, _detach, _tensor_shape_equals, _loss_fn | imports: copy, data",
    "role": "src",
    "loc": 263
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\__init__.py",
    "summary": "Package initializer | [torch ao quantization pt2e __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\representation\\rewrite.py",
    "summary": "Data needed for rewrite, this includes example inputs, pattern and replacement functions | classes: _RewriteInfo | functions: _qdq_quantized_linear, _reference_quantized_linear, _qdq_dynamic_quantized_linear, _reference_dynamic_quantized_linear, _qdq_quantized_conv2d, _reference_quantized_conv2d | i",
    "role": "src",
    "loc": 686
  },
  {
    "id": "torch\\ao\\quantization\\pt2e\\representation\\__init__.py",
    "summary": "Package initializer | imports: rewrite | [torch ao quantization pt2e representation __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\composable_quantizer.py",
    "summary": "ComposableQuantizer allows users to combine more than one quantizer into a single quantizer. | classes: ComposableQuantizer | imports: quantizer, torch | [torch ao quantization quantizer composable_quantizer.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\embedding_quantizer.py",
    "summary": "No description | classes: EmbeddingQuantizer | functions: get_embedding_operators_config | imports: copy, torch | [torch ao quantization quantizer embedding_quantizer.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\quantizer.py",
    "summary": "Base class for different types of quantization specs that allows users to | classes: QuantizationSpecBase, QuantizationSpec, FixedQParamsQuantizationSpec, SharedQuantizationSpec, DerivedQuantizationSpec, QuantizationAnnotation | imports: abc, dataclasses, torch | [torch ao quantization quantizer qua",
    "role": "src",
    "loc": 128
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\utils.py",
    "summary": "No description | functions: _annotate_input_qspec_map, _annotate_output_qspec, _node_only_used_for_sym_size, _get_module_name_filter, module_name_filter, _normalize_path | imports: torch | [torch ao quantization quantizer utils.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\x86_inductor_quantizer.py",
    "summary": "Determine whether to skip annotation for a list of nodes. | classes: _X86InductorQuantizationAnnotation, _CurrentQuantizationMode, X86InductorQuantizer | functions: _skip_annotate, _create_module_name_filter, check_all_nodes_from_module, _create_operator_type_filter, operator_type_filter, _global_co",
    "role": "src",
    "loc": 1341
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\xnnpack_quantizer.py",
    "summary": "!!! DEPRECATED !!! | classes: XNNPACKQuantizer | functions: _get_dynamo_graph, _get_linear_patterns, linear_op, _supported_symmetric_quantized_operators, _get_supported_symmetric_config_and_operators, get_symmetric_quantization_config | imports: copy, functools, torch | [torch ao quantization quanti",
    "role": "src",
    "loc": 364
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\xnnpack_quantizer_utils.py",
    "summary": "No description | classes: QuantizationConfig, OperatorConfig | functions: register_annotator, decorator, _is_annotated, _mark_nodes_as_annotated, get_input_act_qspec, get_output_act_qspec | imports: dataclasses, torch | [torch ao quantization quantizer xnnpack_quantizer_utils.py]",
    "role": "src",
    "loc": 921
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\xpu_inductor_quantizer.py",
    "summary": "XPUInductorQuantizer is a class designed to facilitate | classes: XPUInductorQuantizer | functions: get_default_xpu_inductor_quantization_config | imports: functools, torch | [torch ao quantization quantizer xpu_inductor_quantizer.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\ao\\quantization\\quantizer\\__init__.py",
    "summary": "Package initializer | imports: quantizer | [torch ao quantization quantizer __init__.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\autograd\\anomaly_mode.py",
    "summary": "Autograd anomaly mode. | classes: detect_anomaly, set_detect_anomaly | imports: torch | [torch autograd anomaly_mode.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\autograd\\forward_ad.py",
    "summary": "Namedtuple returned by :func:`unpack_dual` containing the primal and tangent components of the dual tensor. | classes: UnpackedDualTensor, dual_level, _set_fwd_grad_enabled | functions: enter_dual_level, exit_dual_level, _maybe_load_decompositions, make_dual, unpack_dual | imports: torch, grad_mode ",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\autograd\\function.py",
    "summary": "No description | classes: FunctionCtx, _HookMixin, BackwardCFunction, FunctionMeta, _SingleLevelFunction, Function | functions: _is_setup_context_defined, once_differentiable, wrapper, fake_requires_grad, _nested_map, _map | imports: functools, inspect, typing_extensions, torch | [torch autograd fun",
    "role": "src",
    "loc": 676
  },
  {
    "id": "torch\\autograd\\functional.py",
    "summary": "No description | functions: _as_tuple_nocheck, _as_tuple, _tuple_postprocess, _grad_preprocess, _grad_postprocess, _validate_v | imports: torch | [torch autograd functional.py]",
    "role": "src",
    "loc": 920
  },
  {
    "id": "torch\\autograd\\gradcheck.py",
    "summary": "Error raised by :func:`gradcheck` and :func:`gradgradcheck`. | classes: GradcheckError | functions: _is_sparse_compressed_tensor, _is_sparse_any_tensor, _is_float_or_complex_tensor, _allocate_jacobians_with_inputs, _allocate_jacobians_with_outputs, _iter_tensors | imports: functools, typing_extensio",
    "role": "src",
    "loc": 1816
  },
  {
    "id": "torch\\autograd\\grad_mode.py",
    "summary": "Context-manager that disables gradient calculation. | classes: no_grad, enable_grad, set_grad_enabled, inference_mode, set_multithreading_enabled, _force_original_view_tracking | functions: _enter_inference_mode, _exit_inference_mode | imports: torch | [torch autograd grad_mode.py]",
    "role": "src",
    "loc": 305
  },
  {
    "id": "torch\\autograd\\graph.py",
    "summary": "No description | classes: Node, GradientEdge, saved_tensors_hooks, save_on_cpu, _MultiHandle, _Handle | functions: _get_grad_fn_or_grad_acc, get_gradient_edge, increment_version, disable_saved_tensors_hooks, register_multi_grad_hook, get_inner_hook | imports: abc, functools, threading, typing_extens",
    "role": "src",
    "loc": 632
  },
  {
    "id": "torch\\autograd\\profiler.py",
    "summary": "No description | classes: _ContextDecorator, _ProfilerStats, profile, record_function, emit_itt, emit_nvtx | functions: _set_is_profiler_enabled, _run_on_profiler_start, _run_on_profiler_stop, load_nvprof, parse_nvprof_trace | imports: uuid, dataclasses, torch, functools | [torch autograd profiler.p",
    "role": "src",
    "loc": 974
  },
  {
    "id": "torch\\autograd\\profiler_legacy.py",
    "summary": "DEPRECATED: use torch.profiler instead. | classes: profile | functions: _parse_legacy_records, _get_record_key | imports: typing_extensions, torch | [torch autograd profiler_legacy.py]",
    "role": "src",
    "loc": 258
  },
  {
    "id": "torch\\autograd\\profiler_util.py",
    "summary": "A list of Events (for pretty printing). | classes: EventList, FormattedTimesMixin, Interval, FunctionEvent, FunctionEventAvg, StringTable | functions: _format_time, _format_time_share, _format_memory, _attr_formatter, _filter_stack_entry, _filter_name | imports: bisect, operator, typing_extensions, ",
    "role": "src",
    "loc": 921
  },
  {
    "id": "torch\\autograd\\variable.py",
    "summary": "No description | classes: VariableMeta, Variable | imports: torch | [torch autograd variable.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\autograd\\__init__.py",
    "summary": "``torch.autograd`` provides classes and functions implementing automatic differentiation of arbitrary scalar valued func | functions: _calculate_shape, _make_grads, _tensor_or_tensors_to_tuple, backward, grad, vjp | imports: torch, anomaly_mode, function, grad_mode | [torch autograd __init__.py]",
    "role": "src",
    "loc": 504
  },
  {
    "id": "torch\\autograd\\_functions\\tensor.py",
    "summary": "No description | classes: Type, Resize | imports: operator, functools, typing_extensions, torch | [torch autograd _functions tensor.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\autograd\\_functions\\utils.py",
    "summary": "No description | functions: maybe_view, maybe_unexpand, check_onnx_broadcast | imports: operator, functools | [torch autograd _functions utils.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\autograd\\_functions\\__init__.py",
    "summary": "Package initializer | imports: tensor | [torch autograd _functions __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\backends\\__init__.py",
    "summary": "Package initializer | classes: ContextProp, PropModule | functions: disable_global_flags, flags_frozen, __allow_nonbracketed_mutation | imports: types, torch | [torch backends __init__.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\backends\\cpu\\__init__.py",
    "summary": "Return cpu capability as a string value. | functions: get_cpu_capability | imports: torch | [torch backends cpu __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\backends\\cuda\\__init__.py",
    "summary": "Return whether PyTorch is built with CUDA support. | classes: cuFFTPlanCacheAttrContextProp, cuFFTPlanCache, cuFFTPlanCacheManager, cuBLASModule | functions: is_built, preferred_linalg_library, preferred_blas_library, preferred_rocm_fa_library, flash_sdp_enabled, enable_flash_sdp | imports: typing_e",
    "role": "src",
    "loc": 411
  },
  {
    "id": "torch\\backends\\cudnn\\rnn.py",
    "summary": "No description | classes: Unserializable | functions: get_cudnn_mode, init_dropout_state | imports: torch | [torch backends cudnn rnn.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\backends\\cudnn\\__init__.py",
    "summary": "Package initializer | classes: CudnnModule | functions: _init, version, is_available, is_acceptable, set_flags, flags | imports: torch | [torch backends cudnn __init__.py]",
    "role": "src",
    "loc": 161
  },
  {
    "id": "torch\\backends\\cusparselt\\__init__.py",
    "summary": "Package initializer | functions: _init, version, is_available, get_max_alg_id | imports: torch | [torch backends cusparselt __init__.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\backends\\kleidiai\\__init__.py",
    "summary": "Return whether PyTorch is built with KleidiAI support. | functions: is_available | imports: torch | [torch backends kleidiai __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\backends\\mha\\__init__.py",
    "summary": "Returns whether fast path for TransformerEncoder and MultiHeadAttention | functions: get_fastpath_enabled, set_fastpath_enabled | imports: torch | [torch backends mha __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\backends\\mkl\\__init__.py",
    "summary": "On-demand oneMKL verbosing functionality. | classes: verbose | functions: is_available | imports: torch | [torch backends mkl __init__.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\backends\\mkldnn\\__init__.py",
    "summary": "On-demand oneDNN (former MKL-DNN) verbosing functionality. | classes: verbose, MkldnnModule | functions: is_available, set_flags, flags | imports: torch | [torch backends mkldnn __init__.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\backends\\mps\\__init__.py",
    "summary": "Return whether PyTorch is built with MPS support. | functions: is_built, is_available, is_macos_or_newer, is_macos13_or_newer, _init | imports: functools, torch | [torch backends mps __init__.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\backends\\nnpack\\__init__.py",
    "summary": "Return whether PyTorch is built with NNPACK support. | functions: is_available, set_flags, flags | imports: torch | [torch backends nnpack __init__.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\backends\\openmp\\__init__.py",
    "summary": "Return whether PyTorch is built with OpenMP support. | functions: is_available | imports: torch | [torch backends openmp __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\backends\\opt_einsum\\__init__.py",
    "summary": "Return a bool indicating if opt_einsum is currently available. | classes: OptEinsumModule | functions: is_available, get_opt_einsum, _set_enabled, _get_enabled, _set_strategy, _get_strategy | imports: functools, torch, opt_einsum | [torch backends opt_einsum __init__.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\backends\\quantized\\__init__.py",
    "summary": "Package initializer | classes: _QEngineProp, _SupportedQEnginesProp, QuantizedEngine | functions: _get_qengine_id, _get_qengine_str | imports: types, torch | [torch backends quantized __init__.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\backends\\xeon\\run_cpu.py",
    "summary": "This is a script for launching PyTorch inference on Intel(R) Xeon(R) Scalable Processors with optimal configurations. | classes: _CPUinfo, _Launcher | functions: _add_memory_allocator_params, _add_multi_instance_params, _add_kmp_iomp_params, create_args, main | imports: glob, platform, subprocess, a",
    "role": "src",
    "loc": 804
  },
  {
    "id": "torch\\backends\\xeon\\__init__.py",
    "summary": "Package initializer | [torch backends xeon __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\backends\\xnnpack\\__init__.py",
    "summary": "Package initializer | classes: _XNNPACKEnabled, XNNPACKEngine | imports: types, torch | [torch backends xnnpack __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\backends\\_coreml\\preprocess.py",
    "summary": "No description | classes: ScalarType, CoreMLComputeUnit, CoreMLQuantizationMode | functions: TensorSpec, CompileSpec, _check_enumerated_shape, _convert_to_mil_type, preprocess | imports: hashlib, json, coremltools, torch | [torch backends _coreml preprocess.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\backends\\_coreml\\__init__.py",
    "summary": "Package initializer | [torch backends _coreml __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\backends\\_nnapi\\prepare.py",
    "summary": "Torch Module that wraps an NNAPI Compilation. | classes: NnapiModule, NnapiInterfaceWrapper, ShapeComputeModule | functions: convert_model_to_nnapi, process_for_nnapi | imports: torch | [torch backends _nnapi prepare.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\backends\\_nnapi\\serializer.py",
    "summary": "No description | classes: NNAPI_OperandCode, NNAPI_OperationCode, NNAPI_FuseCode, OperandValueSourceType, TorchScalarTypes, ConvPoolArgs2d | functions: approx_equal, tensor_size, change_element, broadcast_shapes, get_conv_pool_shape, fix_shape | imports: array, functools, operator, struct | [torch b",
    "role": "src",
    "loc": 1802
  },
  {
    "id": "torch\\backends\\_nnapi\\__init__.py",
    "summary": "Package initializer | [torch backends _nnapi __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\compiler\\config.py",
    "summary": "This is the top-level configuration module for the compiler, containing | imports: torch | [torch compiler config.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\compiler\\_cache.py",
    "summary": "Type of cache | classes: CacheArtifactType, CacheArtifact, CacheInfo, CacheArtifactManager | imports: dataclasses, pickle, torch | [torch compiler _cache.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torch\\compiler\\__init__.py",
    "summary": "See :func:`torch.compile` for details on the arguments for this function. | functions: compile, reset, allow_in_graph, substitute_in_graph, list_backends, assume_constant_result | imports: typing_extensions, torch, _cache | [torch compiler __init__.py]",
    "role": "src",
    "loc": 339
  },
  {
    "id": "torch\\contrib\\_tensorboard_vis.py",
    "summary": "No description | functions: dump_tensorboard_summary, visualize, visualize_graph_executor, visualize_rec, inline_graph, name_for | imports: functools, torch, tensorflow | [torch contrib _tensorboard_vis.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "torch\\contrib\\__init__.py",
    "summary": "Package initializer | [torch contrib __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\cpu\\__init__.py",
    "summary": "This package implements abstractions found in ``torch.cuda`` | classes: Stream, Event, StreamContext | functions: _is_avx2_supported, _is_avx512_supported, _is_avx512_bf16_supported, _is_vnni_supported, _is_amx_tile_supported, _is_amx_fp16_supported | imports: torch | [torch cpu __init__.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\cpu\\amp\\autocast_mode.py",
    "summary": "See :class:`torch.autocast`. | classes: autocast | imports: typing_extensions, torch | [torch cpu amp autocast_mode.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\cpu\\amp\\grad_scaler.py",
    "summary": "See :class:`torch.amp.GradScaler`. | classes: GradScaler | imports: typing_extensions, torch | [torch cpu amp grad_scaler.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\cpu\\amp\\__init__.py",
    "summary": "Package initializer | imports: autocast_mode, grad_scaler | [torch cpu amp __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\csrc\\jit\\tensorexpr\\codegen_external.py",
    "summary": "No description | functions: num_leading_spaces, deindent, gen_external, main | imports: argparse, torchgen | [torch csrc jit tensorexpr codegen_external.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\csrc\\jit\\tensorexpr\\scripts\\bisect.py",
    "summary": "No description | functions: test, bisect, keep_going | imports: subprocess, click | [torch csrc jit tensorexpr scripts bisect.py]",
    "role": "scripts",
    "loc": 54
  },
  {
    "id": "torch\\csrc\\lazy\\test_mnist.py",
    "summary": "No description | classes: Net | functions: train | imports: torchvision, torch | [torch csrc lazy test_mnist.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\cuda\\comm.py",
    "summary": "No description | imports: torch | [torch cuda comm.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\cuda\\error.py",
    "summary": "No description | [torch cuda error.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\cuda\\gds.py",
    "summary": "Wrapper around cuFile. | classes: GdsFile | functions: _dummy_fn, fn, gds_register_buffer, gds_deregister_buffer | imports: torch | [torch cuda gds.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torch\\cuda\\graphs.py",
    "summary": "Wrapper around a CUDA graph. | classes: CUDAGraph, graph, Graphed | functions: is_current_stream_capturing, graph_pool_handle, make_graphed_callables, make_graphed_autograd_function, functionalized, make_graphed_forward | imports: gc, torch, _utils | [torch cuda graphs.py]",
    "role": "src",
    "loc": 364
  },
  {
    "id": "torch\\cuda\\jiterator.py",
    "summary": "Create a jiterator-generated cuda kernel for an elementwise op. | classes: _CodeParser, _JittedFunction | functions: _create_jit_fn, _create_multi_output_jit_fn | imports: torch | [torch cuda jiterator.py]",
    "role": "src",
    "loc": 139
  },
  {
    "id": "torch\\cuda\\memory.py",
    "summary": "This package adds support for device memory management implemented in CUDA. | classes: _CUDAAllocator, CUDAPluggableAllocator, MemPoolContext, MemPool | functions: _host_allocator, _free_mutex, caching_allocator_alloc, caching_allocator_delete, caching_allocator_enable, set_per_process_memory_fracti",
    "role": "src",
    "loc": 845
  },
  {
    "id": "torch\\cuda\\nccl.py",
    "summary": "No description | functions: is_available, version, unique_id, init_rank, _check_sequence_type, all_reduce | imports: torch | [torch cuda nccl.py]",
    "role": "src",
    "loc": 120
  },
  {
    "id": "torch\\cuda\\nvtx.py",
    "summary": "This package adds support for NVIDIA Tools Extension (NVTX) used in profiling. | classes: _NVTXStub | functions: range_push, range_pop, range_start, range_end, _device_range_start, _device_range_end | imports: torch | [torch cuda nvtx.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torch\\cuda\\profiler.py",
    "summary": "No description | functions: init, start, stop, profile | imports: tempfile, torch | [torch cuda profiler.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "torch\\cuda\\random.py",
    "summary": "Return the random number generator state of the specified GPU as a ByteTensor. | functions: get_rng_state, get_rng_state_all, set_rng_state, cb, set_rng_state_all, manual_seed | imports: torch | [torch cuda random.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "torch\\cuda\\sparse.py",
    "summary": "No description | [torch cuda sparse.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\cuda\\streams.py",
    "summary": "Wrapper around a CUDA stream. | classes: Stream, ExternalStream, Event | imports: ctypes, torch | [torch cuda streams.py]",
    "role": "src",
    "loc": 182
  },
  {
    "id": "torch\\cuda\\tunable.py",
    "summary": "This module exposes a TunableOp interface. | functions: enable, is_enabled, tuning_enable, tuning_is_enabled, record_untuned_enable, record_untuned_is_enabled | imports: concurrent, glob, multiprocessing, shutil | [torch cuda tunable.py]",
    "role": "src",
    "loc": 411
  },
  {
    "id": "torch\\cuda\\_gpu_trace.py",
    "summary": "No description | functions: register_callback_for_event_creation, register_callback_for_event_deletion, register_callback_for_event_record, register_callback_for_event_wait, register_callback_for_memory_allocation, register_callback_for_memory_deallocation | imports: torch | [torch cuda _gpu_trace.p",
    "role": "src",
    "loc": 50
  },
  {
    "id": "torch\\cuda\\_memory_viz.py",
    "summary": "No description | classes: Bytes | functions: _frame_fmt, _frame_filter, _frames_fmt, _block_extra_legacy, _block_extra, format_flamegraph | imports: base64, io, json, operator | [torch cuda _memory_viz.py]",
    "role": "src",
    "loc": 615
  },
  {
    "id": "torch\\cuda\\_sanitizer.py",
    "summary": "This module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different stre | classes: AccessType, Access, SynchronizationError, UnsynchronizedAccessError, CUDASanitizerErrors, TensorInfo | functions: zip_by_key, zip_arguments, enable_cuda_sanitizer | imp",
    "role": "src",
    "loc": 530
  },
  {
    "id": "torch\\cuda\\_utils.py",
    "summary": "Get the device index from :attr:`device`, which can be a torch.device object, a Python integer, or ``None``. | functions: _get_device_index | imports: torch | [torch cuda _utils.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\cuda\\__init__.py",
    "summary": "This package adds support for CUDA tensor types. | classes: _amdsmi_cdll_hook, DeferredCudaCallError, cudaStatus, CudaError, _DeviceGuard, device | functions: _exchange_device, _maybe_exchange_device, _is_compiled, _nvml_based_avail, is_available, is_bf16_supported | imports: importlib, threading, t",
    "role": "src",
    "loc": 1383
  },
  {
    "id": "torch\\cuda\\amp\\autocast_mode.py",
    "summary": "See :class:`torch.autocast`. | classes: autocast | functions: _cast, custom_fwd, custom_bwd | imports: functools, typing_extensions, torch | [torch cuda amp autocast_mode.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "torch\\cuda\\amp\\common.py",
    "summary": "No description | functions: amp_definitely_not_available | imports: importlib, torch | [torch cuda amp common.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\cuda\\amp\\grad_scaler.py",
    "summary": "See :class:`torch.amp.GradScaler`. | classes: GradScaler | imports: typing_extensions, torch | [torch cuda amp grad_scaler.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\cuda\\amp\\__init__.py",
    "summary": "Package initializer | imports: autocast_mode, common, grad_scaler | [torch cuda amp __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\distributed\\argparse_util.py",
    "summary": "Get argument values from ``PET_{dest}`` before defaulting to the given ``default`` value. | classes: env, check_env | imports: argparse | [torch distributed argparse_util.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\distributed\\c10d_logger.py",
    "summary": "No description | functions: _get_or_create_logger, _get_logging_handler, _get_msg_dict, _exception_logger, wrapper, _time_logger | imports: functools, typing_extensions, torch | [torch distributed c10d_logger.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\distributed\\collective_utils.py",
    "summary": "A set of primitive functions for performing collective ops. | classes: SyncPayload | functions: broadcast, all_gather, all_gather_object_enforce_type | imports: dataclasses, torch | [torch distributed collective_utils.py]",
    "role": "src",
    "loc": 167
  },
  {
    "id": "torch\\distributed\\constants.py",
    "summary": "No description | imports: datetime, torch | [torch distributed constants.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\distributed\\device_mesh.py",
    "summary": "No description | classes: _DeviceMeshStub, _MeshEnv, DeviceMesh | functions: _init_device_mesh_stub, _get_device_handle, init_device_mesh | imports: threading, functools, torch, numpy | [torch distributed device_mesh.py]",
    "role": "src",
    "loc": 786
  },
  {
    "id": "torch\\distributed\\distributed_c10d.py",
    "summary": "Distributed Collective Communication (c10d). | classes: Backend, BackendConfig, _reduce_op, P2POp, _CollOp, _World | functions: _export_c_types, supports_complex, _get_default_timeout, _check_valid_timeout, _get_object_coll_device, _get_pg_default_device | imports: ctypes, hashlib, io, pickle | [tor",
    "role": "src",
    "loc": 4302
  },
  {
    "id": "torch\\distributed\\launch.py",
    "summary": "Module ``torch.distributed.launch``. | functions: parse_args, launch, main | imports: typing_extensions, torch | [torch distributed launch.py]",
    "role": "src",
    "loc": 148
  },
  {
    "id": "torch\\distributed\\logging_handlers.py",
    "summary": "No description | [torch distributed logging_handlers.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\remote_device.py",
    "summary": "Represents a device on a remote worker. | classes: _remote_device | imports: torch | [torch distributed remote_device.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "torch\\distributed\\rendezvous.py",
    "summary": "Register a new rendezvous handler. | functions: register_rendezvous_handler, _query_to_dict, _get_use_libuv_from_query_dict, _rendezvous_helper, rendezvous, _create_store_from_options | imports: urllib, numbers, datetime, torch | [torch distributed rendezvous.py]",
    "role": "src",
    "loc": 209
  },
  {
    "id": "torch\\distributed\\run.py",
    "summary": "Module ``torch.distributed.run``. | functions: get_args_parser, parse_args, parse_min_max_nnodes, determine_local_world_size, get_rdzv_endpoint, get_use_env | imports: uuid, argparse, importlib, torch | [torch distributed run.py]",
    "role": "src",
    "loc": 699
  },
  {
    "id": "torch\\distributed\\utils.py",
    "summary": "Turn argument list into separate key list and value list (unpack_kwargs does the opposite). | functions: _pack_kwargs, _cast_forward_inputs, cast_fn, _unpack_kwargs, _recursive_to, to_map | imports: dataclasses, traceback, torch | [torch distributed utils.py]",
    "role": "src",
    "loc": 303
  },
  {
    "id": "torch\\distributed\\_checkpointable.py",
    "summary": "Interface for checkpointable objects. | classes: _Checkpointable | imports: torch | [torch distributed _checkpointable.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\distributed\\_composable_state.py",
    "summary": "No description | classes: _State | functions: _insert_module_state, _get_module_state | imports: weakref, torch | [torch distributed _composable_state.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\distributed\\_functional_collectives.py",
    "summary": "A Tensor wrapper subclass that is used to trigger a call to wait | classes: AsyncCollectiveTensor, _FromTorchTensor | functions: is_torchdynamo_compiling, wait_tensor, broadcast, all_reduce, all_gather_tensor, all_gather_tensor_autograd | imports: torch | [torch distributed _functional_collectives.p",
    "role": "src",
    "loc": 930
  },
  {
    "id": "torch\\distributed\\_functional_collectives_impl.py",
    "summary": "No description | functions: _broadcast, _all_reduce, _all_reduce_coalesced, _all_gather_into_tensor, _all_gather_into_tensor_coalesced, _reduce_scatter_tensor | imports: torch | [torch distributed _functional_collectives_impl.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\distributed\\_serialization.py",
    "summary": "Save the object to a file-like object in a streaming fashion compatible with | classes: _Entry, _PseudoZipFile | functions: _streaming_save, _streaming_load | imports: pickle, dataclasses, io, torch | [torch distributed _serialization.py]",
    "role": "src",
    "loc": 123
  },
  {
    "id": "torch\\distributed\\_state_dict_utils.py",
    "summary": "No description | classes: CompanionMismatch, _TensorInfo | functions: _identity_func, _all_gather_sharded_tensor, _iterate_state_dict, _gather_state_dict, sharded_tensor_func, dtensor_func | imports: copy, io, weakref, torch | [torch distributed _state_dict_utils.py]",
    "role": "src",
    "loc": 655
  },
  {
    "id": "torch\\distributed\\__init__.py",
    "summary": "Supports using PDB from inside a multiprocessing child process. | classes: _DistributedPdb, _ProcessGroupStub | functions: is_available, breakpoint | imports: pdb, traceback, torch, device_mesh | [torch distributed __init__.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torch\\distributed\\algorithms\\join.py",
    "summary": "This defines a join hook, which provides two entry points in the join context manager. | classes: JoinHook, Joinable, _JoinConfig, Join | imports: abc, types, torch | [torch distributed algorithms join.py]",
    "role": "src",
    "loc": 279
  },
  {
    "id": "torch\\distributed\\algorithms\\__init__.py",
    "summary": "Package initializer | imports: join | [torch distributed algorithms __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\ddp_zero_hook.py",
    "summary": "Perform a local optimizer step using the gradients provided by ``bucket``. | functions: _perform_local_step, _broadcast_bucket, _save_ddp_bucket_info, _hook_with_zero_step_setup, hook_with_zero_step, hook_with_zero_fn | imports: weakref, torch | [torch distributed algorithms ddp_comm_hooks ddp_zero_",
    "role": "src",
    "loc": 353
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\debugging_hooks.py",
    "summary": "Return a future that wraps the input, so it is a no-op that does not incur any communication overheads. | functions: noop_hook | imports: torch | [torch distributed algorithms ddp_comm_hooks debugging_hooks.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\default_hooks.py",
    "summary": "Average the input gradient tensor by allreduce and returns a future. | functions: _allreduce_fut, allreduce_hook, _compress_hook, decompress, fp16_compress_hook, bf16_compress_hook | imports: torch | [torch distributed algorithms ddp_comm_hooks default_hooks.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\mixed_precision_hooks.py",
    "summary": "State to manage DDP mixed precision in backward / gradient communication. | classes: _AllreduceUpcastHookState | functions: _reducer_allreduce_and_upcast_hook, wait_for_stream_cb | imports: dataclasses, torch | [torch distributed algorithms ddp_comm_hooks mixed_precision_hooks.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\optimizer_overlap_hooks.py",
    "summary": "Holds state for running optimizer in-line after DDP communication hook. | classes: _OptimizerHookState, _OptimInBackwardHookState | functions: _apply_optim_in_backward_hook, apply_optim_in_backward_hook, wait_for_optim_stream_callback, _hook_then_optimizer, hook_then_optimizer_wrapper, optimizer_ste",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\post_localSGD_hook.py",
    "summary": "Store state for all-reducing gradients globally until given step, then locally after. | classes: PostLocalSGDState | functions: post_localSGD_hook | imports: torch | [torch distributed algorithms ddp_comm_hooks post_localSGD_hook.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\powerSGD_hook.py",
    "summary": "Store both the algorithm's hyperparameters and internal state for all gradients during training. | classes: PowerSGDState | functions: _orthogonalize, _orthogonalize_gram_schmidt, _should_compress, _report_compression_stats, powerSGD_hook, maybe_batched_tensors_to_compress | imports: torch, numpy | ",
    "role": "src",
    "loc": 597
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\quantization_hooks.py",
    "summary": "No description | functions: _quantize_per_tensor_backend, _dequantize_per_tensor_backend, _quantize_per_channel_backend, _dequantize_per_channel_backend, _get_allgather_out_list, quantization_pertensor_hook | imports: torch | [torch distributed algorithms ddp_comm_hooks quantization_hooks.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\distributed\\algorithms\\ddp_comm_hooks\\__init__.py",
    "summary": "Enumerate ``ddp_comm_hooks`` and ``ddp_comm_hook_wrapper`` communucation hook types. | classes: DDPCommHookType | functions: _ddp_comm_hook_wrapper, _powerSGD_comm_hook_wrapper, register_ddp_comm_hook | imports: functools, torch | [torch distributed algorithms ddp_comm_hooks __init__.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\distributed\\algorithms\\model_averaging\\averagers.py",
    "summary": "Base class for all model averagers. | classes: ModelAverager, PeriodicModelAverager | imports: abc, torch | [torch distributed algorithms model_averaging averagers.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "torch\\distributed\\algorithms\\model_averaging\\hierarchical_model_averager.py",
    "summary": "Runs hierarchical model averaging (`hierarchical SGD <https://arxiv.org/pdf/2010.12998.pdf>`_). | classes: HierarchicalModelAverager | imports: torch | [torch distributed algorithms model_averaging hierarchical_model_averager.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\distributed\\algorithms\\model_averaging\\utils.py",
    "summary": "Averages all the given parameters. | functions: average_parameters, get_params_to_average, average_parameters_or_parameter_groups | imports: torch | [torch distributed algorithms model_averaging utils.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\distributed\\algorithms\\model_averaging\\__init__.py",
    "summary": "Package initializer | [torch distributed algorithms model_averaging __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\algorithms\\_checkpoint\\checkpoint_wrapper.py",
    "summary": "Wrap a module for activation offloading to CPU. | classes: CheckpointImpl, ActivationWrapper, OffloadWrapper, CheckpointWrapper | functions: offload_wrapper, checkpoint_wrapper, apply_activation_checkpointing | imports: abc, functools, torch | [torch distributed algorithms _checkpoint checkpoint_wra",
    "role": "src",
    "loc": 258
  },
  {
    "id": "torch\\distributed\\algorithms\\_checkpoint\\__init__.py",
    "summary": "Package initializer | [torch distributed algorithms _checkpoint __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\algorithms\\_comm_hooks\\default_hooks.py",
    "summary": "Stores state needed to perform the default communication algorithm within a communication hook. | classes: DefaultState, LowPrecisionState | functions: _decompress, allreduce_hook, reduce_scatter_hook, _low_precision_hook, fp16_compress_hook, bf16_compress_hook | imports: functools, torch | [torch d",
    "role": "src",
    "loc": 149
  },
  {
    "id": "torch\\distributed\\algorithms\\_comm_hooks\\__init__.py",
    "summary": "Package initializer | [torch distributed algorithms _comm_hooks __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\algorithms\\_optimizer_overlap\\optimizer_overlap.py",
    "summary": "No description | classes: OverlappedOptimizer, _OverlappedStandardOptimizer | functions: register_overlapped, decorator, _as_overlapped_optim | imports: inspect, abc, torch | [torch distributed algorithms _optimizer_overlap optimizer_overlap.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\distributed\\algorithms\\_optimizer_overlap\\__init__.py",
    "summary": "Package initializer | imports: optimizer_overlap | [torch distributed algorithms _optimizer_overlap __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\distributed\\algorithms\\_quantization\\quantization.py",
    "summary": "Different quantization methods for auto_quantize API are identified here. | classes: DQuantType | functions: _fp32_to_fp16_with_clamp, _quantize_tensor, _quantize_tensor_list, _dequantize_tensor, _dequantize_tensor_list, auto_quantize | imports: functools, torch | [torch distributed algorithms _quan",
    "role": "src",
    "loc": 123
  },
  {
    "id": "torch\\distributed\\algorithms\\_quantization\\__init__.py",
    "summary": "Package initializer | [torch distributed algorithms _quantization __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\autograd\\__init__.py",
    "summary": "Context object to wrap forward and backward passes when using | classes: context | functions: is_available | imports: torch | [torch distributed autograd __init__.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\distributed\\benchmarks\\benchmark_ddp_rpc.py",
    "summary": "The model consists of a sparse part and a dense part. | classes: HybridModel | functions: _retrieve_embedding_parameters, _print_header, _print_benchmark, _print_cont, _run_printable, _run_trainer | imports: argparse, io, random, shlex | [torch distributed benchmarks benchmark_ddp_rpc.py]",
    "role": "benchmarks",
    "loc": 256
  },
  {
    "id": "torch\\distributed\\checkpoint\\api.py",
    "summary": "Exception raised if failure was detected as part of a checkpoint load or save. | classes: CheckpointException | functions: _wrap_exception, _is_wrapped_exception | imports: traceback | [torch distributed checkpoint api.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\distributed\\checkpoint\\default_planner.py",
    "summary": "No description | classes: DefaultSavePlanner, DefaultLoadPlanner, _EmptyStateDictLoadPlanner | functions: create_default_local_load_plan, create_default_global_load_plan, create_default_local_save_plan, create_default_global_save_plan, _create_default_local_metadata, _check_box_overlap | imports: da",
    "role": "src",
    "loc": 490
  },
  {
    "id": "torch\\distributed\\checkpoint\\filesystem.py",
    "summary": "This is the per entry storage info. | classes: _StorageInfo, _StoragePrefix, _TensorLoader, _SerialCpuLoader, _OverlappingCpuLoader, NoCloseWriter | functions: _generate_uuid, _item_size, _split_by_size_and_type, _write_item, _write_files_from_queue | imports: dataclasses, io, operator, pickle | [to",
    "role": "src",
    "loc": 723
  },
  {
    "id": "torch\\distributed\\checkpoint\\format_utils.py",
    "summary": "StorageReader for reading a Torch Save file. This reader will read the entire checkpoint | classes: BroadcastingTorchSaveReader, DynamicMetaLoadPlanner, FormatMode | functions: dcp_to_torch_save, torch_save_to_dcp | imports: argparse, torch | [torch distributed checkpoint format_utils.py]",
    "role": "src",
    "loc": 224
  },
  {
    "id": "torch\\distributed\\checkpoint\\logger.py",
    "summary": "Extracts log data from dcp method args | functions: _msg_dict_from_dcp_method_args, _get_msg_dict, _dcp_method_logger, decorator, wrapper | imports: functools, typing_extensions, uuid, torch | [torch distributed checkpoint logger.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\distributed\\checkpoint\\logging_handlers.py",
    "summary": "No description | imports: torch | [torch distributed checkpoint logging_handlers.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\distributed\\checkpoint\\metadata.py",
    "summary": "Each chunk is expected to have the same properties of the TensorStorageMetadata | classes: ChunkStorageMetadata, _MEM_FORMAT_ENCODING, TensorProperties, TensorStorageMetadata, BytesStorageMetadata, StorageMeta | imports: dataclasses, torch | [torch distributed checkpoint metadata.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "torch\\distributed\\checkpoint\\optimizer.py",
    "summary": "No description | classes: _ReaderWithOffset | functions: _gen_rank_device, _create_colwise_spec, _is_nested_tensor, _alloc_tensor, _get_state_dict_2d_layout, load_sharded_optimizer_state_dict | imports: dataclasses, torch | [torch distributed checkpoint optimizer.py]",
    "role": "src",
    "loc": 303
  },
  {
    "id": "torch\\distributed\\checkpoint\\planner.py",
    "summary": "No description | classes: WriteItemType, LoadItemType, TensorWriteData, WriteItem, ReadItem, SavePlan | imports: abc, io, operator, dataclasses | [torch distributed checkpoint planner.py]",
    "role": "src",
    "loc": 320
  },
  {
    "id": "torch\\distributed\\checkpoint\\planner_helpers.py",
    "summary": "Compare the two Save plans and return True if they are equal. | functions: _compare_save_plans, _merge_delta_local_plans, _create_chunk_from_tensor, _chunk_for_shard, _sharded_tensor_metadata, _create_write_items_for_dtensor | imports: io, torch, metadata, planner | [torch distributed checkpoint pla",
    "role": "src",
    "loc": 389
  },
  {
    "id": "torch\\distributed\\checkpoint\\resharding.py",
    "summary": "Check if two shards overlap. | functions: _check_shard_metadata_pair_overlap, _shards_get_overlap_region_wrt_saved_tensor | imports: torch | [torch distributed checkpoint resharding.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\distributed\\checkpoint\\staging.py",
    "summary": "This protocol is meant to provide customization and extensibility for dcp.async_save, allowing users | classes: AsyncStager, BlockingAsyncStager | imports: typing_extensions, torch | [torch distributed checkpoint staging.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\distributed\\checkpoint\\stateful.py",
    "summary": "Stateful protocol for objects that can be checkpointed and restored. | classes: Stateful | imports: typing_extensions | [torch distributed checkpoint stateful.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\distributed\\checkpoint\\state_dict.py",
    "summary": "This dataclass specifies how get_state_dict/set_state_dict will work. | classes: StateDictOptions, _StateDictInfo, _EXTRA_STATE | functions: _gc_context, _get_fqns, _iterate_valid_model_state, recurse, _verify_options, fsdp_state_dict_type_without_warning | imports: functools, gc, dataclasses, torch",
    "role": "src",
    "loc": 1230
  },
  {
    "id": "torch\\distributed\\checkpoint\\state_dict_loader.py",
    "summary": "This method is deprecated. Please switch to 'load'. | functions: load_state_dict, load, _load_state_dict, local_step, global_step, read_data | imports: typing_extensions, torch, _storage_utils, default_planner | [torch distributed checkpoint state_dict_loader.py]",
    "role": "src",
    "loc": 263
  },
  {
    "id": "torch\\distributed\\checkpoint\\state_dict_saver.py",
    "summary": "This method is deprecated. Please switch to 'save'. | functions: save_state_dict, save, async_save, _stateful_to_state_dict, _save_state_dict, local_step | imports: inspect, concurrent, typing_extensions, torch | [torch distributed checkpoint state_dict_saver.py]",
    "role": "src",
    "loc": 280
  },
  {
    "id": "torch\\distributed\\checkpoint\\storage.py",
    "summary": "No description | classes: WriteResult, StorageWriter, StorageReader | imports: abc, dataclasses, torch | [torch distributed checkpoint storage.py]",
    "role": "src",
    "loc": 219
  },
  {
    "id": "torch\\distributed\\checkpoint\\utils.py",
    "summary": "This is a wrapper around PG that provides a series of features around object collectives. | classes: _DistWrapper, _ReaderView | functions: _get_failure_dict, _all_gather_keys, _assert_same_keys, _find_shard, find_tensor_shard, find_state_dict_object | imports: cProfile, inspect, io, functools | [to",
    "role": "src",
    "loc": 375
  },
  {
    "id": "torch\\distributed\\checkpoint\\_checkpointer.py",
    "summary": "This base class specefies a high level API for saving and loading | classes: _Checkpointer | imports: concurrent, torch | [torch distributed checkpoint _checkpointer.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\distributed\\checkpoint\\_dedup_save_plans.py",
    "summary": "Removes duplicate entries from appearing on multiple SavePlans. For each duplicate across | functions: dedup_save_plans | imports: dataclasses, torch | [torch distributed checkpoint _dedup_save_plans.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\distributed\\checkpoint\\_dedup_tensors.py",
    "summary": "No description | functions: init_logger, dedup_tensors | imports: dataclasses, torch | [torch distributed checkpoint _dedup_tensors.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\distributed\\checkpoint\\_extension.py",
    "summary": "Extensions provide modular additions to functionality within distributed checkpointing, | classes: Extension, StreamTransformExtension, Writer, Reader, ZStandard, ExtensionRegistry | imports: abc, io, typing_extensions, torch | [torch distributed checkpoint _extension.py]",
    "role": "src",
    "loc": 165
  },
  {
    "id": "torch\\distributed\\checkpoint\\_fsspec_filesystem.py",
    "summary": "No description | classes: FileSystem, FsspecWriter, FsspecReader | imports: io, fsspec, torch | [torch distributed checkpoint _fsspec_filesystem.py]",
    "role": "src",
    "loc": 120
  },
  {
    "id": "torch\\distributed\\checkpoint\\_nested_dict.py",
    "summary": "Flatten ``state_dict`` made of nested dicts and lists into a top level dictionary. | functions: flatten_state_dict, flat_copy, unflatten_state_dict | imports: torch, _traverse | [torch distributed checkpoint _nested_dict.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\distributed\\checkpoint\\_sharded_tensor_utils.py",
    "summary": "Transform ``state_dict`` by flattening all nested ShardedTensor instances found. | functions: _flatten_sharded_tensors, rewrite_dict | imports: copy, torch, _traverse, utils | [torch distributed checkpoint _sharded_tensor_utils.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\distributed\\checkpoint\\_storage_utils.py",
    "summary": "No description | functions: _storage_setup | imports: filesystem, storage, _fsspec_filesystem | [torch distributed checkpoint _storage_utils.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\distributed\\checkpoint\\_traverse.py",
    "summary": "No description | functions: _keep_visiting_tensors, traverse_state_dict, _is_terminal, _traverse_obj, traverse_state_dict_v_2_3, set_element | imports: torch | [torch distributed checkpoint _traverse.py]",
    "role": "src",
    "loc": 160
  },
  {
    "id": "torch\\distributed\\checkpoint\\_version.py",
    "summary": "No description | [torch distributed checkpoint _version.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\distributed\\checkpoint\\__init__.py",
    "summary": "Package initializer | imports: api, default_planner, filesystem, metadata | [torch distributed checkpoint __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\distributed\\checkpoint\\examples\\async_checkpointing_example.py",
    "summary": "No description | classes: InjectedException, Model | functions: _init_model, _print, _input, run | imports: shutil, traceback, torch | [torch distributed checkpoint examples async_checkpointing_example.py]",
    "role": "examples",
    "loc": 98
  },
  {
    "id": "torch\\distributed\\checkpoint\\examples\\fsdp_checkpoint_example.py",
    "summary": "The following example demonstrates how to use Pytorch Distributed Checkpoint to save a FSDP model. | functions: opt_at, init_model, print_params, run_fsdp_checkpoint_example | imports: shutil, torch | [torch distributed checkpoint examples fsdp_checkpoint_example.py]",
    "role": "examples",
    "loc": 87
  },
  {
    "id": "torch\\distributed\\checkpoint\\examples\\stateful_example.py",
    "summary": "No description | classes: Model | functions: _make_stateful, _train, _init_model, run | imports: shutil, torch | [torch distributed checkpoint examples stateful_example.py]",
    "role": "examples",
    "loc": 76
  },
  {
    "id": "torch\\distributed\\elastic\\control_plane.py",
    "summary": "No description | functions: _worker_server, worker_main | imports: torch | [torch distributed elastic control_plane.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\distributed\\elastic\\__init__.py",
    "summary": "Torchelastic agent and user worker failover contract: | [torch distributed elastic __init__.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\distributed\\elastic\\agent\\__init__.py",
    "summary": "Package initializer | [torch distributed elastic agent __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\elastic\\agent\\server\\api.py",
    "summary": "Blueprint information about a particular type of worker. | classes: WorkerSpec, Worker, WorkerState, WorkerGroup, _RoleInstanceInfo, RunResult | functions: _get_fq_hostname | imports: abc, json, signal, socket | [torch distributed elastic agent server api.py]",
    "role": "src",
    "loc": 767
  },
  {
    "id": "torch\\distributed\\elastic\\agent\\server\\health_check_server.py",
    "summary": "Interface for health check monitoring server, which can be extended | classes: HealthCheckServer | functions: create_healthcheck_server | imports: torch | [torch distributed elastic agent server health_check_server.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\distributed\\elastic\\agent\\server\\local_elastic_agent.py",
    "summary": "An implementation of :py:class:`torchelastic.agent.server.ElasticAgent` that handles host-local workers. | classes: LocalElasticAgent | imports: json, signal, socket, uuid | [torch distributed elastic agent server local_elastic_agent.py]",
    "role": "src",
    "loc": 350
  },
  {
    "id": "torch\\distributed\\elastic\\agent\\server\\__init__.py",
    "summary": "The elastic agent is the control plane of torchelastic. | imports: api, local_elastic_agent | [torch distributed elastic agent server __init__.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\distributed\\elastic\\events\\api.py",
    "summary": "Known identifiers of the event producers. | classes: EventSource, Event, NodeState, RdzvEvent | imports: json, dataclasses | [torch distributed elastic events api.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\distributed\\elastic\\events\\handlers.py",
    "summary": "No description | functions: get_logging_handler | [torch distributed elastic events handlers.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\distributed\\elastic\\events\\__init__.py",
    "summary": "Module contains events processing mechanisms that are integrated with the standard python logging. | functions: _get_or_create_logger, record, record_rdzv_event, construct_and_record_rdzv_event | imports: inspect, socket, traceback, torch | [torch distributed elastic events __init__.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torch\\distributed\\elastic\\metrics\\api.py",
    "summary": "No description | classes: MetricsConfig, MetricHandler, ConsoleMetricHandler, NullMetricHandler, MetricStream | functions: configure, getStream, _get_metric_name, prof, wrap, wrapper | imports: abc, functools, typing_extensions | [torch distributed elastic metrics api.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "torch\\distributed\\elastic\\metrics\\__init__.py",
    "summary": "Metrics API. | functions: initialize_metrics | imports: api, torch | [torch distributed elastic metrics __init__.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\api.py",
    "summary": "Exception is raised inside the torchelastic agent process by the termination handler | classes: SignalException, Std, LogsDest, LogsSpecs, DefaultLogsSpecs, RunProcsResult | functions: _terminate_process_handler, _get_kill_signal, _get_default_signal, _validate_full_rank, to_map, get_std_cm | import",
    "role": "src",
    "loc": 732
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\redirects.py",
    "summary": "No description | functions: get_libc, _c_std, _python_std, redirect, _redirect | imports: ctypes, functools | [torch distributed elastic multiprocessing redirects.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\tail_log.py",
    "summary": "Tail the given log files. | classes: TailLog | functions: tail_logfile | imports: concurrent, threading | [torch distributed elastic multiprocessing tail_log.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\__init__.py",
    "summary": "Library that launches and manages ``n`` copies of worker subprocesses either specified by a function or a binary. | functions: start_processes | imports: torch | [torch distributed elastic multiprocessing __init__.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\errors\\error_handler.py",
    "summary": "Write the provided exception object along with some other metadata about | classes: ErrorHandler | imports: faulthandler, json, traceback | [torch distributed elastic multiprocessing errors error_handler.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\errors\\handlers.py",
    "summary": "No description | functions: get_error_handler | imports: torch | [torch distributed elastic multiprocessing errors handlers.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py",
    "summary": "Each host in a distributed PyTorch job runs with a single TorchElastic agent, | classes: ProcessFailure, ChildFailedError | functions: record, wrap, wrapper | imports: json, signal, socket, dataclasses | [torch distributed elastic multiprocessing errors __init__.py]",
    "role": "src",
    "loc": 294
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\subprocess_handler\\handlers.py",
    "summary": "No description | functions: get_subprocess_handler | imports: torch | [torch distributed elastic multiprocessing subprocess_handler handlers.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\subprocess_handler\\subprocess_handler.py",
    "summary": "Convenience wrapper around python's ``subprocess.Popen``. Keeps track of | classes: SubprocessHandler | functions: _get_default_signal | imports: signal, subprocess | [torch distributed elastic multiprocessing subprocess_handler subprocess_handler.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\distributed\\elastic\\multiprocessing\\subprocess_handler\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed elastic multiprocessing subprocess_handler __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\api.py",
    "summary": "Represents the base type for rendezvous errors. | classes: RendezvousError, RendezvousClosedError, RendezvousTimeoutError, RendezvousConnectionError, RendezvousStateError, RendezvousGracefulExitError | imports: socket, abc, dataclasses, torch | [torch distributed elastic rendezvous api.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\c10d_rendezvous_backend.py",
    "summary": "Represents a C10d-backed rendezvous backend. | classes: C10dRendezvousBackend | functions: _create_tcp_store, _create_file_store, create_backend | imports: binascii, tempfile, base64, datetime | [torch distributed elastic rendezvous c10d_rendezvous_backend.py]",
    "role": "src",
    "loc": 187
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\dynamic_rendezvous.py",
    "summary": "Represent a backend that holds the rendezvous state. | classes: RendezvousBackend, RendezvousTimeout, RendezvousSettings, _NodeDesc, _NodeDescGenerator, _RendezvousState | functions: get_method_name, _remove_participant_epilogue, _should_keep_alive, _get_timeout, create_handler | imports: inspect, p",
    "role": "src",
    "loc": 1101
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\etcd_rendezvous.py",
    "summary": "Create a new ``etcd.Client`` from the specified ``RendezvousParameters``. | classes: EtcdRendezvousRetryableFailure, EtcdRendezvousRetryImmediately, EtcdRendezvousHandler, EtcdRendezvous | functions: _create_etcd_client, create_rdzv_handler | imports: json, threading, etcd, torch | [torch distribute",
    "role": "src",
    "loc": 768
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\etcd_rendezvous_backend.py",
    "summary": "Represents an etcd-based rendezvous backend. | classes: EtcdRendezvousBackend | functions: _create_etcd_client, create_backend | imports: binascii, base64, urllib3, etcd | [torch distributed elastic rendezvous etcd_rendezvous_backend.py]",
    "role": "src",
    "loc": 159
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\etcd_server.py",
    "summary": ".. note:: tested on etcd server v3.4.3. | classes: EtcdServer | functions: find_free_port, stop_etcd | imports: atexit, shlex, shutil, socket | [torch distributed elastic rendezvous etcd_server.py]",
    "role": "src",
    "loc": 197
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\etcd_store.py",
    "summary": "Implement a c10 Store interface by piggybacking on the rendezvous etcd instance. | classes: EtcdStore | functions: cas_delay | imports: datetime, random, base64, torch | [torch distributed elastic rendezvous etcd_store.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\registry.py",
    "summary": "No description | functions: _create_static_handler, _create_etcd_handler, _create_etcd_v2_handler, _create_c10d_handler, _register_default_handlers, _register_out_of_tree_handlers | imports: api, dynamic_rendezvous, importlib_metadata, importlib | [torch distributed elastic rendezvous registry.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\static_tcp_rendezvous.py",
    "summary": "Static rendezvous that is a wrapper around the TCPStore. | classes: StaticTCPRendezvous | functions: create_rdzv_handler | imports: datetime, torch | [torch distributed elastic rendezvous static_tcp_rendezvous.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\utils.py",
    "summary": "Extract key-value pairs from a rendezvous configuration string. | classes: _Context, _PeriodicTimer | functions: _parse_rendezvous_config, _try_parse_port, parse_rendezvous_endpoint, _matches_machine_hostname, _delay | imports: ipaddress, random, socket, weakref | [torch distributed elastic rendezvo",
    "role": "src",
    "loc": 202
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\_etcd_stub.py",
    "summary": "Custom exception to indicate that the real etcd module is required. | classes: EtcdStubError, EtcdAlreadyExist, EtcdCompareFailed, EtcdKeyNotFound, EtcdWatchTimedOut, EtcdEventIndexCleared | [torch distributed elastic rendezvous _etcd_stub.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\distributed\\elastic\\rendezvous\\__init__.py",
    "summary": "In the context of Torch Distributed Elastic we use the term *rendezvous* to | imports: api, registry | [torch distributed elastic rendezvous __init__.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\distributed\\elastic\\timer\\api.py",
    "summary": "Data object representing a countdown timer acquisition and release | classes: TimerRequest, TimerClient, RequestQueue, TimerServer | functions: configure, expires | imports: abc, threading, inspect | [torch distributed elastic timer api.py]",
    "role": "src",
    "loc": 235
  },
  {
    "id": "torch\\distributed\\elastic\\timer\\debug_info_logging.py",
    "summary": "No description | functions: log_debug_info_for_expired_timers | imports: torch | [torch distributed elastic timer debug_info_logging.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\distributed\\elastic\\timer\\file_based_local_timer.py",
    "summary": "Data object representing a countdown timer acquisition and release | classes: FileTimerRequest, FileTimerClient, FileTimerServer | functions: _retry, wrapper | imports: io, json, select, signal | [torch distributed elastic timer file_based_local_timer.py]",
    "role": "src",
    "loc": 355
  },
  {
    "id": "torch\\distributed\\elastic\\timer\\local_timer.py",
    "summary": "Client side of ``LocalTimerServer``. This client is meant to be used | classes: LocalTimerClient, MultiprocessingRequestQueue, LocalTimerServer | imports: multiprocessing, signal, queue, api | [torch distributed elastic timer local_timer.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\distributed\\elastic\\timer\\__init__.py",
    "summary": "Expiration timers are set up on the same process as the agent and | imports: api, file_based_local_timer, local_timer | [torch distributed elastic timer __init__.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\api.py",
    "summary": "Defines simple macros for caffe2.distributed.launch cmd args substitution | classes: macros | functions: get_env_variable_or_raise, get_socket_with_port | imports: socket, string | [torch distributed elastic utils api.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\distributed.py",
    "summary": "No description | functions: create_c10d_store, _check_full_rank, get_free_port, get_socket_with_port | imports: datetime, socket, torch | [torch distributed elastic utils distributed.py]",
    "role": "src",
    "loc": 139
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\logging.py",
    "summary": "Util function to set up a simple logger that writes | functions: get_logger, _setup_logger, _derive_module_name | imports: inspect, torch | [torch distributed elastic utils logging.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\log_level.py",
    "summary": "Return default log level for pytorch. | functions: get_log_level | [torch distributed elastic utils log_level.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\store.py",
    "summary": "This sets the timeout and then restores the old timeout when the context | functions: store_timeout, get_all, synchronize, _try_detecting_missing_ranks, _find_missing_ranks, _checkin | imports: datetime, torch | [torch distributed elastic utils store.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\__init__.py",
    "summary": "Package initializer | imports: api | [torch distributed elastic utils __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\data\\cycling_iterator.py",
    "summary": "An iterator decorator that cycles through the | classes: CyclingIterator | imports: typing_extensions | [torch distributed elastic utils data cycling_iterator.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\data\\elastic_distributed_sampler.py",
    "summary": "Sampler that restricts data loading to a subset of | classes: ElasticDistributedSampler | imports: torch | [torch distributed elastic utils data elastic_distributed_sampler.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\distributed\\elastic\\utils\\data\\__init__.py",
    "summary": "Package initializer | imports: cycling_iterator, elastic_distributed_sampler | [torch distributed elastic utils data __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\distributed\\examples\\memory_tracker_example.py",
    "summary": "No description | functions: run_one_model | imports: torchvision, torch | [torch distributed examples memory_tracker_example.py]",
    "role": "examples",
    "loc": 18
  },
  {
    "id": "torch\\distributed\\fsdp\\api.py",
    "summary": "This file includes public APIs for FSDP such as the classes used for the | classes: ShardingStrategy, BackwardPrefetch, MixedPrecision, CPUOffload, StateDictType, StateDictConfig | imports: dataclasses, torch | [torch distributed fsdp api.py]",
    "role": "src",
    "loc": 340
  },
  {
    "id": "torch\\distributed\\fsdp\\fully_sharded_data_parallel.py",
    "summary": "Represents the type of key in an optimizer state-dict. | classes: OptimStateKeyType, UnshardHandle, FullyShardedDataParallel | functions: _get_grad_norm, _get_param_to_fqn, _get_fqn_to_param | imports: copy, functools, traceback, torch | [torch distributed fsdp fully_sharded_data_parallel.py]",
    "role": "src",
    "loc": 1910
  },
  {
    "id": "torch\\distributed\\fsdp\\sharded_grad_scaler.py",
    "summary": "Lazily serves tensor to request device. This class extends | classes: _GeneralMultiDeviceReplicator, ShardedGradScaler | functions: _refresh_per_optimizer_state, _is_supported_device | imports: torch | [torch distributed fsdp sharded_grad_scaler.py]",
    "role": "src",
    "loc": 276
  },
  {
    "id": "torch\\distributed\\fsdp\\wrap.py",
    "summary": "This defines an abstract base class that represents a policy for applying | classes: _Policy, ModuleWrapPolicy, CustomPolicy, _ConfigAutoWrap | functions: _post_order_apply, _post_order_apply_inner, _construct_wrap_fn, fn, _run_mixed_precision_override_policy, always_wrap_policy | imports: copy, abc",
    "role": "src",
    "loc": 470
  },
  {
    "id": "torch\\distributed\\fsdp\\_common_utils.py",
    "summary": "This file includes private common utilities for FSDP. | classes: _FSDPDeviceHandle, _UninitializedDeviceHandle, _FSDPState, TrainingState, HandleTrainingState | functions: _get_module_fsdp_state, _get_module_fsdp_state_if_fully_sharded_module, _is_composable, _module_handle, _has_fsdp_params, _get_s",
    "role": "src",
    "loc": 406
  },
  {
    "id": "torch\\distributed\\fsdp\\_debug_utils.py",
    "summary": "It is used for composable fully_shard() code path, it returns | classes: Type, SimpleProfiler | functions: _get_sharded_module_tree_with_module_name_to_fqns, module_fn, return_fn | imports: torch | [torch distributed fsdp _debug_utils.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "torch\\distributed\\fsdp\\_dynamo_utils.py",
    "summary": "Annotates the submodules in ``module`` 's tree, except those in | functions: _annotate_modules_for_dynamo | imports: torch | [torch distributed fsdp _dynamo_utils.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\distributed\\fsdp\\_exec_order_utils.py",
    "summary": "Used internally for execution order validation. | classes: _ExecOrderWarnStatus, _ExecOrderData | imports: torch | [torch distributed fsdp _exec_order_utils.py]",
    "role": "src",
    "loc": 308
  },
  {
    "id": "torch\\distributed\\fsdp\\_flat_param.py",
    "summary": "No description | classes: HandleShardingStrategy, ParamInfo, SharedParamInfo, _ShardParamInfo, FlatParamShardMetadata, _FlatParameterMeta | functions: _unsafe_setattr_param, _unsafe_setattr_tensor, _safe_setattr_tensor_or_param, _convert_to_params, _is_truly_contiguous, _detach_if_needed | imports: ",
    "role": "src",
    "loc": 2276
  },
  {
    "id": "torch\\distributed\\fsdp\\_fsdp_extensions.py",
    "summary": "This enables some customizable hooks to enable composability with tensor | classes: FSDPExtensions | functions: _set_fsdp_extensions, _ext_pre_flatten_transform, _ext_post_unflatten_transform, _ext_chunk_tensor, _ext_chunk_dtensor, _ext_pre_load_state_dict_transform | imports: abc, torch | [torch di",
    "role": "src",
    "loc": 153
  },
  {
    "id": "torch\\distributed\\fsdp\\_init_utils.py",
    "summary": "No description | functions: _init_process_group_state, _init_process_group_state_for_hybrid_shard, _is_valid_hybrid_shard_pg_type, _is_valid_hybrid_shard_device_mesh, _init_intra_node_process_group, _init_inter_node_process_group | imports: torch, torchdistx | [torch distributed fsdp _init_utils.py]",
    "role": "src",
    "loc": 970
  },
  {
    "id": "torch\\distributed\\fsdp\\_limiter_utils.py",
    "summary": "This tracks all pending frees corresponding to inflight all-gathers. The | classes: _FreeEventQueue | imports: torch | [torch distributed fsdp _limiter_utils.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\distributed\\fsdp\\_optim_utils.py",
    "summary": "No description | classes: FSDPParamInfo, _ConsolidatedOptimState, _PosDimTensorInfo, _OptimStateKey, StateInfo | functions: sorted_items, _unflatten_optim_state, _is_zero_dim_tensor, _communicate_optim_state, _unflatten_communicated_optim_state, _broadcast_processed_state | imports: copy, functools,",
    "role": "src",
    "loc": 1733
  },
  {
    "id": "torch\\distributed\\fsdp\\_runtime_utils.py",
    "summary": "Returns a tuple containing: | classes: _PrefetchMode | functions: _get_fsdp_root_states_with_modules, _get_fsdp_root_states, _is_fsdp_root, _lazy_init, _check_flat_params_on_expected_device, _share_state_and_init_handle_attrs | imports: functools, torch | [torch distributed fsdp _runtime_utils.py]",
    "role": "src",
    "loc": 1304
  },
  {
    "id": "torch\\distributed\\fsdp\\_shard_utils.py",
    "summary": "No description | functions: _get_remote_device_str, _create_chunk_sharded_tensor, _create_chunk_dtensor, _all_gather_dtensor | imports: copy, torch | [torch distributed fsdp _shard_utils.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "torch\\distributed\\fsdp\\_state_dict_utils.py",
    "summary": "No description | functions: _should_unshard_params, _convert_to_wrapped_module_name, _param_name_infos, _shared_param_name_infos, _enter_unshard_params_ctx, _exit_unshard_params_ctx | imports: torch, _fsdp_extensions, _unshard_param_utils | [torch distributed fsdp _state_dict_utils.py]",
    "role": "src",
    "loc": 771
  },
  {
    "id": "torch\\distributed\\fsdp\\_trace_utils.py",
    "summary": "This represents a symbolic tracing configuration. | classes: TracingConfig, _ParamUsageInfo, _ExecutionInfo, _ExecOrderTracer | imports: functools, dataclasses, torch | [torch distributed fsdp _trace_utils.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "torch\\distributed\\fsdp\\_traversal_utils.py",
    "summary": "NOTE: This file must be imported like | functions: _composable, _get_fsdp_states_with_modules, _get_fsdp_states, _get_fsdp_handles | imports: torch | [torch distributed fsdp _traversal_utils.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\distributed\\fsdp\\_unshard_param_utils.py",
    "summary": "For the handle, writes back the this rank's shard of the unsharded | functions: _writeback_to_local_shard, _get_shard, _deregister_flat_param, _register_flat_param, _unflatten_as_params, _validate_unshard_params_args | imports: torch, _flat_param | [torch distributed fsdp _unshard_param_utils.py]",
    "role": "src",
    "loc": 284
  },
  {
    "id": "torch\\distributed\\fsdp\\_wrap_utils.py",
    "summary": "Auto wraps modules in ``root_module`` 's tree according to ``policy`` | functions: _auto_wrap, _check_nested_wrapping, _warn_on_overridden_mixed_precision, _validate_frozen_params, _get_post_order_named_modules, _get_managed_param_to_fqn | imports: functools, inspect, torch | [torch distributed fsdp",
    "role": "src",
    "loc": 237
  },
  {
    "id": "torch\\distributed\\fsdp\\__init__.py",
    "summary": "Package initializer | imports: _flat_param, _fully_shard, fully_sharded_data_parallel | [torch distributed fsdp __init__.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_api.py",
    "summary": "This configures FSDP's mixed precision. Unlike autocast, this applies mixed | classes: MixedPrecisionPolicy, OffloadPolicy, CPUOffloadPolicy | imports: dataclasses, torch | [torch distributed fsdp _fully_shard _fsdp_api.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_collectives.py",
    "summary": "No description | classes: AllGatherResult | functions: all_gather_copy_in_meta, all_gather_copy_in_cuda, split_with_sizes_copy, chunk_cat, foreach_all_gather, _get_param_all_gather_inputs | imports: torch, _fsdp_common, _fsdp_param | [torch distributed fsdp _fully_shard _fsdp_collectives.py]",
    "role": "src",
    "loc": 483
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_common.py",
    "summary": "No description | classes: DataParallelMeshInfo, FSDPMeshInfo, DDPMeshInfo, HSDPMeshInfo, TrainingState | functions: detect_compiled_autograd, compiled_autograd_enabled, _raise_assert_with_print, _is_composable_with_fsdp, _get_dim0_padded_size, _chunk_with_empty | imports: traceback, dataclasses, tor",
    "role": "src",
    "loc": 132
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_init.py",
    "summary": "No description | functions: _get_post_forward_mesh_info, _init_default_fully_shard_mesh, _get_device_from_mesh, _ignore_module, _adjust_managed_modules, _get_managed_modules | imports: torch, _fsdp_common, _fsdp_state | [torch distributed fsdp _fully_shard _fsdp_init.py]",
    "role": "src",
    "loc": 190
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_param.py",
    "summary": "- ``SHARDED``: The sharded parameter is registered to the module. It is the | classes: ShardedState, ParamModuleInfo, ExtensionsData, FSDPParam | functions: copy_, copy__functionalize, alloc_storage, free_storage, unsafe_setattr_param, set_requires_grad_if_needed | imports: inspect, dataclasses, tor",
    "role": "src",
    "loc": 774
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_param_group.py",
    "summary": "This has the communication state shared across FSDP states/parameter groups. | classes: FSDPCommContext, AllGatherState, ReduceScatterState, AllReduceState, FSDPParamGroup, RegisterPostBackwardFunction | functions: _get_param_module_infos | imports: torch, _fsdp_api, _fsdp_collectives, _fsdp_common ",
    "role": "src",
    "loc": 581
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fsdp_state.py",
    "summary": "This has state shared across FSDP states. | classes: FSDPStateContext, FSDPState | functions: disable_if_config_true, fsdp_hook_wrapper, _get_module_fsdp_state, _register_group_forward_hooks, wrapped_pre_hook, get_wrapped_post_hook | imports: functools, torch, _fsdp_api, _fsdp_common | [torch distri",
    "role": "src",
    "loc": 326
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\_fully_shard.py",
    "summary": "No description | classes: FSDPModule, UnshardHandle, _UnshardHandleImpl | functions: fully_shard, _unimplemented_deepcopy, register_fsdp_forward_method, wrapped_method, _assert_all_fsdp_modules | imports: functools, torch, _fsdp_api, _fsdp_common | [torch distributed fsdp _fully_shard _fully_shard.p",
    "role": "src",
    "loc": 502
  },
  {
    "id": "torch\\distributed\\fsdp\\_fully_shard\\__init__.py",
    "summary": "Package initializer | imports: _fsdp_api, _fully_shard | [torch distributed fsdp _fully_shard __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\distributed\\launcher\\api.py",
    "summary": "Creates a rendezvous config. | classes: LaunchConfig, elastic_launch | functions: _get_entrypoint_name, _get_addr_and_port, launch_agent | imports: uuid, dataclasses, torch | [torch distributed launcher api.py]",
    "role": "src",
    "loc": 231
  },
  {
    "id": "torch\\distributed\\launcher\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed launcher __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\nn\\functional.py",
    "summary": "Broadcasts the tensor to the whole group. | classes: _Broadcast, _Gather, _Scatter, _Reduce, _Reduce_Scatter, _AllGather | functions: broadcast, gather, scatter, reduce, reduce_scatter, all_gather | imports: torch | [torch distributed nn functional.py]",
    "role": "src",
    "loc": 348
  },
  {
    "id": "torch\\distributed\\nn\\__init__.py",
    "summary": "Package initializer | imports: torch, functional, api | [torch distributed nn __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\distributed\\nn\\api\\remote_module.py",
    "summary": "No description | classes: _RemoteModule, RemoteModule | functions: _instantiate_template, _create_module, _create_module_with_interface, _param_rrefs, _raise_not_supported, _remote_module_receiver | imports: io, types, torch | [torch distributed nn api remote_module.py]",
    "role": "src",
    "loc": 580
  },
  {
    "id": "torch\\distributed\\nn\\api\\__init__.py",
    "summary": "Package initializer | [torch distributed nn api __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\nn\\jit\\instantiator.py",
    "summary": "No description | functions: get_arg_return_types_from_interface, _write, _do_instantiate_remote_module_template, instantiate_scriptable_remote_module_template, instantiate_non_scriptable_remote_module_template | imports: importlib, tempfile, torch | [torch distributed nn jit instantiator.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "torch\\distributed\\nn\\jit\\__init__.py",
    "summary": "Package initializer | [torch distributed nn jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\nn\\jit\\templates\\remote_module_template.py",
    "summary": "No description | functions: get_remote_module_template | [torch distributed nn jit templates remote_module_template.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\distributed\\nn\\jit\\templates\\__init__.py",
    "summary": "Package initializer | [torch distributed nn jit templates __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\optim\\apply_optimizer_in_backward.py",
    "summary": "Upon ``backward()``, the optimizer specified for each parameter will fire after | functions: _apply_optimizer_in_backward, _apply_optimizer_in_backward_to_param, optimizer_hook, _get_in_backward_optimizers | imports: torch | [torch distributed optim apply_optimizer_in_backward.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\distributed\\optim\\functional_adadelta.py",
    "summary": "No description | classes: _FunctionalAdadelta | imports: torch | [torch distributed optim functional_adadelta.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\distributed\\optim\\functional_adagrad.py",
    "summary": "No description | classes: _FunctionalAdagrad | imports: torch | [torch distributed optim functional_adagrad.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\distributed\\optim\\functional_adam.py",
    "summary": "No description | classes: _FunctionalAdam | imports: torch | [torch distributed optim functional_adam.py]",
    "role": "src",
    "loc": 166
  },
  {
    "id": "torch\\distributed\\optim\\functional_adamax.py",
    "summary": "No description | classes: _FunctionalAdamax | imports: torch | [torch distributed optim functional_adamax.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\distributed\\optim\\functional_adamw.py",
    "summary": "No description | classes: _FunctionalAdamW | imports: torch | [torch distributed optim functional_adamw.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\distributed\\optim\\functional_rmsprop.py",
    "summary": "No description | classes: _FunctionalRMSprop | imports: torch | [torch distributed optim functional_rmsprop.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\distributed\\optim\\functional_rprop.py",
    "summary": "No description | classes: _FunctionalRprop | imports: torch | [torch distributed optim functional_rprop.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\distributed\\optim\\functional_sgd.py",
    "summary": "No description | classes: _FunctionalSGD | imports: torch | [torch distributed optim functional_sgd.py]",
    "role": "src",
    "loc": 133
  },
  {
    "id": "torch\\distributed\\optim\\named_optimizer.py",
    "summary": "``_NamedOptimizer`` takes a dict of parameters and exposes ``state_dict`` by parameter key. | classes: _NamedOptimizer | functions: _gen_param_group_key | imports: copy, torch | [torch distributed optim named_optimizer.py]",
    "role": "src",
    "loc": 273
  },
  {
    "id": "torch\\distributed\\optim\\optimizer.py",
    "summary": "No description | classes: _ScriptLocalOptimizerInterface, _ScriptLocalOptimizer, _LocalOptimizer, DistributedOptimizer | functions: _new_local_optimizer, _local_optimizer_step, _new_script_local_optimizer, _script_local_optimizer_step, _wait_for_all | imports: threading, torch, utils | [torch distri",
    "role": "src",
    "loc": 183
  },
  {
    "id": "torch\\distributed\\optim\\post_localSGD_optimizer.py",
    "summary": "Wraps an arbitrary :class:`torch.optim.Optimizer` and runs `post-local SGD <https://arxiv.org/abs/1808.07217>`_, | classes: PostLocalSGDOptimizer | imports: torch | [torch distributed optim post_localSGD_optimizer.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\distributed\\optim\\utils.py",
    "summary": "Interface to insert a new functional optimizer to functional_optim_map | functions: register_functional_optim, as_functional_optim, _create_functional_optim | imports: torch, functional_adadelta, functional_adagrad, functional_adam | [torch distributed optim utils.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\distributed\\optim\\zero_redundancy_optimizer.py",
    "summary": "Zero Redundancy Optimizer. | classes: _ZeROJoinHook, _DDPBucketAssignment, _OverlapStatus, _OverlapInfo, ZeroRedundancyOptimizer | functions: _recursive_copy_to_device, _is_trainable, _broadcast_object | imports: copy, inspect, io, torch | [torch distributed optim zero_redundancy_optimizer.py]",
    "role": "src",
    "loc": 1336
  },
  {
    "id": "torch\\distributed\\optim\\_deprecation_warning.py",
    "summary": "No description | functions: _scripted_functional_optimizer_deprecation_warning | imports: torch | [torch distributed optim _deprecation_warning.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\distributed\\optim\\__init__.py",
    "summary": ":mod:`torch.distributed.optim` exposes DistributedOptimizer, which takes a list | imports: torch, apply_optimizer_in_backward, functional_adadelta, functional_adagrad | [torch distributed optim __init__.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\distributed\\pipelining\\microbatch.py",
    "summary": "Custom reducer class that can be used to specify a custom operation that | classes: _CustomReducer, _LossReducer, TensorChunkSpec, _Replicate | functions: _shard_dict_of_args, split_args_kwargs_into_chunks, merge_chunks | imports: torch | [torch distributed pipelining microbatch.py]",
    "role": "src",
    "loc": 298
  },
  {
    "id": "torch\\distributed\\pipelining\\schedules.py",
    "summary": "Formats the pipeline order in a timestep (row) x rank (column) grid of actions | classes: _ComputationType, _Action, _PipelineSchedule, PipelineScheduleSingle, _ScheduleForwardOnly, ScheduleGPipe | functions: _format_pipeline_order, _batch_p2p, _sorted_batch_p2p, _add_unshard_reshard, next_stage_ind",
    "role": "src",
    "loc": 2068
  },
  {
    "id": "torch\\distributed\\pipelining\\stage.py",
    "summary": "Placeholder for model-level inputs. | classes: _RootArgPlaceholder, _RecvInfo, _PipelineStageBase, _PipelineStage, PipelineStage | functions: _normalize_model_output_as_tuple, _make_tensor_from_meta, build_stage | imports: operator, abc, torch, _backward | [torch distributed pipelining stage.py]",
    "role": "src",
    "loc": 1136
  },
  {
    "id": "torch\\distributed\\pipelining\\_backward.py",
    "summary": "Get the grad function or grad accumulator for a tensor. | functions: _get_grad_fn_or_grad_acc, reverse_closure, construct_reverse_graph, get_param_groups, stage_backward_input, get_hook | imports: torch, _debug | [torch distributed pipelining _backward.py]",
    "role": "src",
    "loc": 303
  },
  {
    "id": "torch\\distributed\\pipelining\\_debug.py",
    "summary": "Helper function to print out debug info in a friendly way. | functions: friendly_debug_info, map_debug_info | imports: torch | [torch distributed pipelining _debug.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\distributed\\pipelining\\_IR.py",
    "summary": "No description | classes: PipeSequential, LossWrapper, TrivialLossWrapper, MultiUseParameterConfig, DetachExecutor, _NodeReference | functions: _find_loss_from_output_and_spec, _find_loss_output, _insert_stage_symbolic_backward, assign_or_accumulate_grad, add_to_live_nodes, _pipe_split | imports: co",
    "role": "src",
    "loc": 911
  },
  {
    "id": "torch\\distributed\\pipelining\\_unflatten.py",
    "summary": "No description | functions: _outline_submodules | imports: torch | [torch distributed pipelining _unflatten.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\distributed\\pipelining\\_utils.py",
    "summary": "Shape mismatch between configured and runtime values. | classes: PipeliningShapeError, PipeInfo | functions: flatten_args_detach, extract_tensor_args, flatten_args, validate_tensor_metadata, validate_tensors_metadata, generate_stage_to_rank_mapping | imports: dataclasses, torch | [torch distributed ",
    "role": "src",
    "loc": 104
  },
  {
    "id": "torch\\distributed\\pipelining\\__init__.py",
    "summary": "Package initializer | imports: _IR, schedules, stage | [torch distributed pipelining __init__.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\distributed\\rpc\\api.py",
    "summary": "rpc_pickler: (.internal._InternalRPCPickler) Overrides the default RPC pickler | classes: AllGatherStates, RRef, RRefMeta | functions: _use_rpc_pickler, _require_initialized, wrapper, _init_rpc_states, _gather_to_leader, _broadcast_to_followers | imports: functools, inspect, threading, torch | [torc",
    "role": "src",
    "loc": 725
  },
  {
    "id": "torch\\distributed\\rpc\\backend_registry.py",
    "summary": "No description | functions: _backend_type_repr, backend_registered, register_backend, construct_rpc_backend_options, init_backend, _init_process_group | imports: torch, _utils | [torch distributed rpc backend_registry.py]",
    "role": "src",
    "loc": 321
  },
  {
    "id": "torch\\distributed\\rpc\\constants.py",
    "summary": "No description | imports: datetime, torch | [torch distributed rpc constants.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\distributed\\rpc\\functions.py",
    "summary": "A decorator for a function indicating that the return value of the function | functions: async_execution, wrapper | imports: functools | [torch distributed rpc functions.py]",
    "role": "src",
    "loc": 153
  },
  {
    "id": "torch\\distributed\\rpc\\internal.py",
    "summary": "No description | classes: RPCExecMode, _InternalRPCPickler | functions: serialize, deserialize, _run_function, _handle_exception, _build_rpc_profiling_key, _start_record_function | imports: copyreg, io, pickle, threading | [torch distributed rpc internal.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "torch\\distributed\\rpc\\options.py",
    "summary": "The backend options for | classes: TensorPipeRpcBackendOptions | functions: _to_device, _to_device_map, _to_device_list | imports: torch | [torch distributed rpc options.py]",
    "role": "src",
    "loc": 152
  },
  {
    "id": "torch\\distributed\\rpc\\rref_proxy.py",
    "summary": "No description | classes: RRefProxy | functions: _local_invoke, _local_invoke_async_execution, _invoke_rpc, _rref_type_cont, _wrap_rref_type_cont, _complete_op | imports: functools, torch, constants | [torch distributed rpc rref_proxy.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torch\\distributed\\rpc\\server_process_global_profiler.py",
    "summary": "It has the same API as ``torch.autograd.profiler.profile`` class, | classes: _server_process_global_profile | imports: torch | [torch distributed rpc server_process_global_profiler.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "torch\\distributed\\rpc\\_utils.py",
    "summary": "No description | functions: _group_membership_management, _update_group_membership | [torch distributed rpc _utils.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\distributed\\rpc\\__init__.py",
    "summary": "Package initializer | functions: is_available, init_rpc, _validate_rpc_args, _init_rpc_backend, _get_debug_info | imports: threading, datetime, urllib, torch | [torch distributed rpc __init__.py]",
    "role": "src",
    "loc": 195
  },
  {
    "id": "torch\\distributed\\rpc\\_testing\\faulty_agent_backend_registry.py",
    "summary": "No description | functions: _faulty_tensorpipe_construct_rpc_backend_options_handler, _faulty_tensorpipe_init_backend_handler | imports: torch | [torch distributed rpc _testing faulty_agent_backend_registry.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\distributed\\rpc\\_testing\\__init__.py",
    "summary": "Package initializer | functions: is_available | imports: torch | [torch distributed rpc _testing __init__.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\distributed\\tensor\\device_mesh.py",
    "summary": "No description | imports: torch | [torch distributed tensor device_mesh.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\distributed\\tensor\\placement_types.py",
    "summary": "The base class for the Placement type, where it describes how a DTensor is placed onto the | classes: Placement, Shard, _StridedShard, Replicate, Partial | imports: dataclasses, torch | [torch distributed tensor placement_types.py]",
    "role": "src",
    "loc": 522
  },
  {
    "id": "torch\\distributed\\tensor\\_api.py",
    "summary": "Distribute a leaf ``torch.Tensor`` (i.e. nn.Parameter/buffers) to the ``device_mesh`` according | classes: _ToTorchTensor, _FromTorchTensor, DTensor | functions: distribute_tensor, _shard_tensor, distribute_module, replicate_module_params_buffers, _dtensor_init_helper, ones | imports: inspect, typin",
    "role": "src",
    "loc": 1015
  },
  {
    "id": "torch\\distributed\\tensor\\_collective_utils.py",
    "summary": "Mesh information for collective cost estimation | classes: MeshTopoInfo | functions: _shard_dim_alltoall_meta, shard_dim_alltoall, mesh_scatter, mesh_broadcast, pad_tensor, unpad_tensor | imports: dataclasses, functools, torch | [torch distributed tensor _collective_utils.py]",
    "role": "src",
    "loc": 277
  },
  {
    "id": "torch\\distributed\\tensor\\_dispatch.py",
    "summary": "Op dispatching class instance to handle args/kwargs pre-processing (un-wrapping), sharding | classes: OpDispatcher | functions: decompose_handler, is_same_size_handler, found_inf_reduce_handler | imports: functools, operator, torch | [torch distributed tensor _dispatch.py]",
    "role": "src",
    "loc": 418
  },
  {
    "id": "torch\\distributed\\tensor\\_dtensor_spec.py",
    "summary": "No description | classes: TensorMeta, DTensorSpec | imports: dataclasses, torch | [torch distributed tensor _dtensor_spec.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "torch\\distributed\\tensor\\_op_schema.py",
    "summary": "A placement strategy describes acceptable sharding placements of the output | classes: PlacementStrategy, StrategyType, OpStrategy, TupleStrategy, RuntimeSchemaInfo, OpSchema | functions: _rebuild_tensor_from_dtensor_meta, _is_inplace_op, _is_out_variant_op, _pretty_print_spec | imports: dataclasses",
    "role": "src",
    "loc": 347
  },
  {
    "id": "torch\\distributed\\tensor\\_random.py",
    "summary": "_RNGStateTracker stores Random Number Generator (RNG) state (a ByteTensor object) | classes: _RNGStateTracker, OffsetBasedRNGTracker | functions: is_rng_supported_mesh, manual_seed, _resolve_device | imports: torch | [torch distributed tensor _random.py]",
    "role": "src",
    "loc": 297
  },
  {
    "id": "torch\\distributed\\tensor\\_redistribute.py",
    "summary": "Generate the transform infos from the source placements to the target placements. | classes: _TransformInfo, Redistribute | functions: _gen_transform_infos_non_cached, _gen_transform_infos, redistribute_local_tensor | imports: functools, torch | [torch distributed tensor _redistribute.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "torch\\distributed\\tensor\\_sharding_prop.py",
    "summary": "No description | classes: LocalLRUCache, ShardingPropagator | functions: _length | imports: threading, functools, torch | [torch distributed tensor _sharding_prop.py]",
    "role": "src",
    "loc": 406
  },
  {
    "id": "torch\\distributed\\tensor\\_shards_wrapper.py",
    "summary": "A wrapper class to hold local shards of a DTensor. | classes: LocalShardsWrapper | imports: torch | [torch distributed tensor _shards_wrapper.py]",
    "role": "src",
    "loc": 245
  },
  {
    "id": "torch\\distributed\\tensor\\_tp_conv.py",
    "summary": "No description | functions: _requires_data_exchange, _is_supported, _ring_send_recv_construct, _ring_send_recv_aggregate, tp_convolution, tp_convolution_backward | imports: torch | [torch distributed tensor _tp_conv.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "torch\\distributed\\tensor\\_utils.py",
    "summary": "Compute the local tensor shape and the global offsets into the original tensor | functions: compute_local_shape_and_global_offset, compute_global_tensor_info, try_find_mesh_from_args, compute_local_stride, normalize_to_torch_size | imports: torch | [torch distributed tensor _utils.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\distributed\\tensor\\__init__.py",
    "summary": "Package initializer | imports: torch, _dtensor_spec | [torch distributed tensor __init__.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "torch\\distributed\\tensor\\debug\\_comm_mode.py",
    "summary": "Inherits ModuleTracker and expands on its functionality to track the | classes: _CommModeModuleTracker, CommDebugMode | imports: copy, json, weakref, torch | [torch distributed tensor debug _comm_mode.py]",
    "role": "src",
    "loc": 563
  },
  {
    "id": "torch\\distributed\\tensor\\debug\\_op_coverage.py",
    "summary": "No description | functions: fwd_bwd_compiler, get_inductor_decomp_graphs, print_op_coverage_summary | imports: operator, torch, functorch, csv | [torch distributed tensor debug _op_coverage.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "torch\\distributed\\tensor\\debug\\_visualize_sharding.py",
    "summary": "Given a n-dimensional list of device mesh, this function creates a map of | functions: _mesh_to_coordinate, _convert_offset_to_ranges, _create_table, _compute_local_shape_and_global_offset, visualize_sharding | imports: numpy, torch, tabulate | [torch distributed tensor debug _visualize_sharding.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torch\\distributed\\tensor\\debug\\__init__.py",
    "summary": "Get the cache info for the sharding propagation cache, used for debugging purpose only. | functions: _get_sharding_prop_cache_info | imports: torch | [torch distributed tensor debug __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\distributed\\tensor\\examples\\comm_mode_features_example.py",
    "summary": "To run the example, use the following command: | classes: Foo, CommDebugModeExample | functions: get_device_type, run_example | imports: argparse, torch | [torch distributed tensor examples comm_mode_features_example.py]",
    "role": "examples",
    "loc": 667
  },
  {
    "id": "torch\\distributed\\tensor\\examples\\convnext_example.py",
    "summary": "The following example demonstrates how to train a ConvNeXt model | classes: LayerNorm, Block, DownSampling, ConvNeXt | functions: init_weights, _conv_fn, train_convnext_example | imports: torch | [torch distributed tensor examples convnext_example.py]",
    "role": "examples",
    "loc": 219
  },
  {
    "id": "torch\\distributed\\tensor\\examples\\flex_attention_cp.py",
    "summary": "To run the example, use the following command: | functions: get_device_type, create_block_mask_cached, flex_attn_example, causal_mask, rewrite_mask_mod_for_cp | imports: functools, torch | [torch distributed tensor examples flex_attention_cp.py]",
    "role": "examples",
    "loc": 139
  },
  {
    "id": "torch\\distributed\\tensor\\examples\\torchrec_sharding_example.py",
    "summary": "The following example demonstrates how to represent torchrec's embedding | classes: LocalShardsWrapper | functions: get_device_type, run_torchrec_row_wise_even_sharding_example, run_torchrec_row_wise_uneven_sharding_example, run_torchrec_table_wise_sharding_example, run_example | imports: argparse, ",
    "role": "examples",
    "loc": 243
  },
  {
    "id": "torch\\distributed\\tensor\\examples\\visualize_sharding_example.py",
    "summary": "To run the example, use the following command: | imports: torch | [torch distributed tensor examples visualize_sharding_example.py]",
    "role": "examples",
    "loc": 73
  },
  {
    "id": "torch\\distributed\\tensor\\experimental\\_attention.py",
    "summary": "Calculate is_causal behavior for each KV block. The attention can either be | classes: _CausalBehavior, _RotateMethod, _ContextParallelOptions, _SDPAMerger, _AttentionOp, _RingRotater | functions: _is_causal_behavior, _maybe_wait, _partial_update, _scaled_dot_product_ring_flash_attention, _scaled_do",
    "role": "src",
    "loc": 1049
  },
  {
    "id": "torch\\distributed\\tensor\\experimental\\_func_map.py",
    "summary": ":meth:`local_map` is an experimental API that allows users to pass :class:`DTensor` s | functions: local_map, wrapped | imports: torch | [torch distributed tensor experimental _func_map.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\distributed\\tensor\\experimental\\_register_sharding.py",
    "summary": ":meth:`register_sharding` is an experimental API that allows users to register sharding | functions: register_sharding, custom_strategy, strategy_to_spec, wrapper, derive_schema_info | imports: functools, torch | [torch distributed tensor experimental _register_sharding.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "torch\\distributed\\tensor\\experimental\\_tp_transform.py",
    "summary": "This pass is responsible for transforming a single-device graph into a tensor parallel | classes: _TensorParallelTransformPass | functions: tensor_parallel_transformation, _generate_parameter_and_buffer_placements, _mark_tensor_parallel_shardings, _get_input_node_fqn, _mark_sharding, _get_output_spe",
    "role": "src",
    "loc": 485
  },
  {
    "id": "torch\\distributed\\tensor\\experimental\\__init__.py",
    "summary": "This context manager allows :class:`DTensor` to implicitly treat all non-DTensors (``torch.Tensor``) | functions: implicit_replication | imports: torch | [torch distributed tensor experimental __init__.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\api.py",
    "summary": "Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan. | functions: parallelize_module | imports: fnmatch, torch | [torch distributed tensor parallel api.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\ddp.py",
    "summary": "Get submodule and the direct path of parameter from the module | functions: _get_submodule_n_params, _update_module_param, _reconstruct_dtensor, _localize_dtensor, _pre_dp_module_transform | imports: torch | [torch distributed tensor parallel ddp.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\fsdp.py",
    "summary": "DTensorExtension is the TensorFlattener extension needed for 2D FSDP + TP. | classes: DTensorExtensions | functions: _get_box, _get_box_for, _get_local_box, _create_shard_md_from_dt, _create_sharded_tensor_md_from_dt, _get_dt_pg | imports: copy, torch | [torch distributed tensor parallel fsdp.py]",
    "role": "src",
    "loc": 295
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\input_reshard.py",
    "summary": "Register hooks to an nn.Module for input resharding, enabling sharding and restoration during backward computation. | functions: input_reshard, input_reshard_forward_pre_hook, input_reshard_backward_hook, _pack_hook_tp, _unpack_hook_tp | imports: functools, torch | [torch distributed tensor parallel",
    "role": "src",
    "loc": 93
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\loss.py",
    "summary": "A context manager that enables loss parallelism, where efficient parallelized loss computation | functions: loss_parallel, _find_all_reduce_mesh_dim, _cast_to_dtensor, _propagate_tensor_meta, _log_softmax, _log_softmax_handler | imports: torch | [torch distributed tensor parallel loss.py]",
    "role": "src",
    "loc": 377
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\style.py",
    "summary": "The parallel style contract defines how the module or submodule should be parallelized. | classes: ParallelStyle, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput, PrepareModuleOutput | imports: abc, functools, torch | [torch distributed tensor parallel style.py]",
    "role": "src",
    "loc": 551
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\_data_parallel_utils.py",
    "summary": "No description | functions: sync_grad_hook, _flatten_tensor, _unflatten_tensor | imports: functools, torch | [torch distributed tensor parallel _data_parallel_utils.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\_utils.py",
    "summary": "No description | functions: is_torchdynamo_compiling, _deprecate_warnings, _validate_tp_mesh_dim | imports: torch | [torch distributed tensor parallel _utils.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\distributed\\tensor\\parallel\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed tensor parallel __init__.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\utils.py",
    "summary": "No description | functions: register_prop_rule, wrapper, register_op_strategy, as_list, normalize_dim, normalize_dims | imports: functools, operator, typing_extensions, torch | [torch distributed tensor _ops utils.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_common_rules.py",
    "summary": "No description | functions: _replace_char_in_str, _gen_reshard_suggestions, einop_rule, merge_sharding, pointwise_rule | imports: string, torch | [torch distributed tensor _ops _common_rules.py]",
    "role": "src",
    "loc": 215
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_conv_ops.py",
    "summary": "No description | functions: convolution_rules, convolution_backward_rules | imports: torch | [torch distributed tensor _ops _conv_ops.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_einsum_strategy.py",
    "summary": "Generate a strategy list for the ops that follow einsum style notation. | classes: EinsumDims | functions: gen_einsum_strategies | imports: dataclasses, torch | [torch distributed tensor _ops _einsum_strategy.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_embedding_ops.py",
    "summary": "This strategy handles embedding op. We have two possible embedding shardings: | classes: MaskBuffer, _MaskPartial | functions: embedding_strategy, embedding_dense_backward_strategy | imports: dataclasses, torch | [torch distributed tensor _ops _embedding_ops.py]",
    "role": "src",
    "loc": 185
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_experimental_ops.py",
    "summary": "slice_backward is a new_zeros + slice_scatter, we only allow replication | functions: slice_backward_rules | imports: torch | [torch distributed tensor _ops _experimental_ops.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_math_ops.py",
    "summary": "No description | classes: Reduction, NormReduction, _NormPartial | functions: _infer_reduction_dims, _infer_reduce_dims_map, _replicate_dims_start_at, _skip_dim, replicate_reduction_dims, map_placements_after_reduction | imports: dataclasses, torch | [torch distributed tensor _ops _math_ops.py]",
    "role": "src",
    "loc": 862
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_matrix_ops.py",
    "summary": "No description | functions: transpose_strategy, _mm_like_strategy, _addmm_like_strategy, _scaled_mm_like_strategy, mm_strategy, addmm_strategy | imports: torch | [torch distributed tensor _ops _matrix_ops.py]",
    "role": "src",
    "loc": 419
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_pointwise_ops.py",
    "summary": "No description | functions: pointwise_strategy, common_pointwise_strategy, linear_pointwise_strategy, list_pointwise_strategy, args_tuple_strategies, list_linear_pointwise_strategy | imports: torch | [torch distributed tensor _ops _pointwise_ops.py]",
    "role": "src",
    "loc": 622
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_random_ops.py",
    "summary": "No description | functions: random_op_strategy | imports: torch | [torch distributed tensor _ops _random_ops.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_tensor_ops.py",
    "summary": "No description | functions: default_strategy, equal_strategy, create_like_strategy, new_factory_strategy, gen_bucketize_strategy, gen_slice_strategy | imports: torch | [torch distributed tensor _ops _tensor_ops.py]",
    "role": "src",
    "loc": 595
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\_view_ops.py",
    "summary": "Specifies how an output dimension maps to an input dimension. | classes: DimSpec, Singleton, InputDim, Broadcast, NewDim, Repeat | functions: dim_pad_left, dim_atleast_3d, expand, normalize_sizes, dim_flatten, dim_movedim | imports: dataclasses, torch | [torch distributed tensor _ops _view_ops.py]",
    "role": "src",
    "loc": 496
  },
  {
    "id": "torch\\distributed\\tensor\\_ops\\__init__.py",
    "summary": "Package initializer | imports: _conv_ops, _embedding_ops, _experimental_ops, _math_ops | [torch distributed tensor _ops __init__.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\distributed\\_composable\\checkpoint_activation.py",
    "summary": "Disable hooks installed by checkpoint to avoid unintentional recursion | classes: _CheckpointState | functions: _no_hook, checkpoint, forward_pre_hook, context_fns, forward_hook | imports: torch, contract | [torch distributed _composable checkpoint_activation.py]",
    "role": "src",
    "loc": 109
  },
  {
    "id": "torch\\distributed\\_composable\\contract.py",
    "summary": "No description | classes: RegistryItem, _ContractFn | functions: generate_state_key, contract, inner, wrapper, check_fqn, get_state | imports: uuid, functools, typing_extensions, torch | [torch distributed _composable contract.py]",
    "role": "src",
    "loc": 201
  },
  {
    "id": "torch\\distributed\\_composable\\replicate.py",
    "summary": "No description | classes: _ReplicateState, DDP | functions: unimplemented_deepcopy, replicate, _is_fully_sharded | imports: weakref, torch, contract | [torch distributed _composable replicate.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "torch\\distributed\\_composable\\__init__.py",
    "summary": "Package initializer | imports: checkpoint_activation, contract, replicate | [torch distributed _composable __init__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\distributed\\_composable\\fsdp\\fully_shard.py",
    "summary": "No description | imports: torch | [torch distributed _composable fsdp fully_shard.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\distributed\\_composable\\fsdp\\__init__.py",
    "summary": "Package initializer | imports: torch, fully_shard | [torch distributed _composable fsdp __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\distributed\\_shard\\api.py",
    "summary": "Given a :class:`torch.Tensor`, it shards that tensor according to the provided | functions: _shard_tensor, shard_parameter, load_with_process_group, _get_current_process_group, _reshard_output, hook_func | imports: torch, sharder, sharding_plan, sharding_spec | [torch distributed _shard api.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "torch\\distributed\\_shard\\common_op_utils.py",
    "summary": "Common validation across all ops go in here. | functions: _basic_validation, is_distributed_tensor, validate_pg, _register_default_op, tensor_default_op | imports: torch | [torch distributed _shard common_op_utils.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torch\\distributed\\_shard\\metadata.py",
    "summary": "Represents a shard of the overall Tensor including its | classes: ShardMetadata | imports: dataclasses, functools, torch | [torch distributed _shard metadata.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "torch\\distributed\\_shard\\op_registry_utils.py",
    "summary": "Performs basic validation and registers the provided op in the given | functions: _register_op, _decorator_func, wrapper | imports: functools, inspect, common_op_utils | [torch distributed _shard op_registry_utils.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\distributed\\_shard\\sharder.py",
    "summary": "This is an interface which allows user to create more advanced | classes: Sharder | imports: abc, torch | [torch distributed _shard sharder.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\distributed\\_shard\\_utils.py",
    "summary": "Narrow the tensor according to ``offsets`` and ``sizes``. | functions: narrow_tensor_by_index, narrow_tensor | imports: torch | [torch distributed _shard _utils.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\distributed\\_shard\\__init__.py",
    "summary": "Package initializer | imports: api | [torch distributed _shard __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\distributed\\_shard\\checkpoint\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed _shard checkpoint __init__.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_optim\\api.py",
    "summary": "No description | classes: ShardedOptimizer | imports: torch | [torch distributed _shard sharded_optim api.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_optim\\__init__.py",
    "summary": "Returns an iterator over module parameters (together with the | functions: named_params_with_sharded_tensor | imports: torch, api | [torch distributed _shard sharded_optim __init__.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\api.py",
    "summary": "No description | classes: ShardedTensorBase, ProcessGroupState, ShardedTensor | functions: _register_remote_shards, _create_tensor_from_params | imports: copy, operator, threading, weakref | [torch distributed _shard sharded_tensor api.py]",
    "role": "src",
    "loc": 1073
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\logger.py",
    "summary": "No description | functions: _get_or_create_logger, _get_logging_handler | imports: torch | [torch distributed _shard sharded_tensor logger.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\logging_handlers.py",
    "summary": "No description | [torch distributed _shard sharded_tensor logging_handlers.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\metadata.py",
    "summary": "No description | classes: MEM_FORMAT_ENCODING, TensorProperties, ShardedTensorMetadata | imports: dataclasses, torch | [torch distributed _shard sharded_tensor metadata.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\reshard.py",
    "summary": "Return the position of the current rank in the given placements. | functions: get_idx_from_placements, build_reshard_metadata, reshuffle_local_shard, reshard_local_shard | imports: copy, torch, shard | [torch distributed _shard sharded_tensor reshard.py]",
    "role": "src",
    "loc": 206
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\shard.py",
    "summary": "Container which holds the data for a shard as a Tensor and also | classes: Shard | imports: dataclasses, torch | [torch distributed _shard sharded_tensor shard.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\utils.py",
    "summary": "No description | functions: _parse_and_validate_remote_device, _validate_output_tensor_for_gather, _flatten_tensor_size, _raise_if_mismatch, build_metadata_from_local_shards, build_global_metadata | imports: copy, torch, metadata, shard | [torch distributed _shard sharded_tensor utils.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\__init__.py",
    "summary": "Returns a :class:`ShardedTensor` filled with uninitialized data. | functions: empty, ones, zeros, full, rand, randn | imports: functools, torch, api, metadata | [torch distributed _shard sharded_tensor __init__.py]",
    "role": "src",
    "loc": 435
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\binary_cmp.py",
    "summary": "No description | functions: _communicate_result, binary_cmp, equal, allclose | imports: torch | [torch distributed _shard sharded_tensor _ops binary_cmp.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\init.py",
    "summary": "No description | functions: validate_param, uniform_, normal_, kaiming_uniform_, constant_, register_tensor_creation_op | imports: torch | [torch distributed _shard sharded_tensor _ops init.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\misc_ops.py",
    "summary": "No description | functions: tensor_has_compatible_shallow_copy_type | imports: torch | [torch distributed _shard sharded_tensor _ops misc_ops.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\tensor_ops.py",
    "summary": "No description | functions: tensor_device, st_is_meta, sharded_type_as_check, same_dtype, sharded_type_as, sharded_deepcopy | imports: copy, torch, _common | [torch distributed _shard sharded_tensor _ops tensor_ops.py]",
    "role": "src",
    "loc": 155
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\_common.py",
    "summary": "Inject sharded tensor op registration with common logics executed before | functions: _sharded_op_common, decorator_sharded_func, wrapper, _register_sharded_op_on_local_shards, sharded_tensor_op_on_local_shards | imports: functools, torch | [torch distributed _shard sharded_tensor _ops _common.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\distributed\\_shard\\sharded_tensor\\_ops\\__init__.py",
    "summary": "Package initializer | imports: torch, binary_cmp, init | [torch distributed _shard sharded_tensor _ops __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_plan\\api.py",
    "summary": "Representation of a sharding plan, describes how to shard a module | classes: ShardingPlan, ShardingPlanner | imports: abc, dataclasses, torch | [torch distributed _shard sharding_plan api.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_plan\\__init__.py",
    "summary": "Package initializer | imports: api | [torch distributed _shard sharding_plan __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\api.py",
    "summary": "Base class representing the placement of an entity. Subclasses of this | classes: PlacementSpec, DevicePlacementSpec, ShardingSpec, EnumerableShardingSpec | functions: _has_custom_op, _dispatch_custom_op, custom_sharding_spec_op, _infer_sharding_spec_from_shards_metadata | imports: functools, operat",
    "role": "src",
    "loc": 206
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\chunk_sharding_spec.py",
    "summary": "This is a type of PlacementSpec that defines the placement as being sharded | classes: ChunkShardingSpec | imports: dataclasses, torch, _internals, api | [torch distributed _shard sharding_spec chunk_sharding_spec.py]",
    "role": "src",
    "loc": 171
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\_internals.py",
    "summary": "Checks if two shards overlap. | functions: _check_shard_metadata_pair_overlap, _find_nd_overlapping_shards, _find_1d_overlapping_shards, validate_non_overlapping_shards_metadata, check_tensor, get_split_size | imports: torch | [torch distributed _shard sharding_spec _internals.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\__init__.py",
    "summary": "Package initializer | imports: torch, api, chunk_sharding_spec | [torch distributed _shard sharding_spec __init__.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\chunk_sharding_spec_ops\\embedding.py",
    "summary": "Handles ``__torch_function__`` dispatch for ``torch.nn.functional.embedding``. | functions: sharded_embedding, _validate_embedding_param, _handle_col_wise_sharding, _handle_row_wise_sharding | imports: torch, _common | [torch distributed _shard sharding_spec chunk_sharding_spec_ops embedding.py]",
    "role": "src",
    "loc": 248
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\chunk_sharding_spec_ops\\embedding_bag.py",
    "summary": "Handles ``__torch_function__`` dispatch for ``torch.nn.functional.embedding_bag``. | functions: sharded_embedding_bag, _validate_embedding_bag_param, _handle_col_wise_sharding, _handle_row_wise_sharding, _all_gather_embedding_bag_input | imports: torch, _common | [torch distributed _shard sharding_s",
    "role": "src",
    "loc": 419
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\chunk_sharding_spec_ops\\_common.py",
    "summary": "For the given op implementation check if the sharding spec is ChunkShardingSpec. | functions: _chunk_sharding_spec_check, _register_sharded_op_on_local_tensor, sharded_tensor_op_on_local_tensor, _handle_col_wise_sharding_base, _result_distribute_with_col_rearrange, _handle_max_norm_col_wise | import",
    "role": "src",
    "loc": 296
  },
  {
    "id": "torch\\distributed\\_shard\\sharding_spec\\chunk_sharding_spec_ops\\__init__.py",
    "summary": "Package initializer | [torch distributed _shard sharding_spec chunk_sharding_spec_ops __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\distributed\\_sharded_tensor\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed _sharded_tensor __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\distributed\\_sharding_spec\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch distributed _sharding_spec __init__.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\distributed\\_symmetric_memory\\__init__.py",
    "summary": "Enables symmetric memory for a process group. | classes: _ScaleMode, Work | functions: enable_symm_mem_for_group, _test_mode, is_symm_mem_enabled_for_group, get_symm_mem_workspace, _get_backend_stream, _pipelined_multi_all_gather_and_consume | imports: socket, uuid, datetime, functools | [torch dist",
    "role": "src",
    "loc": 1271
  },
  {
    "id": "torch\\distributed\\_tensor\\api.py",
    "summary": "NOTE: torch.distributed._tensor has been moved to torch.distributed.tensor. | imports: torch | [torch distributed _tensor api.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\distributed\\_tensor\\placement_types.py",
    "summary": "NOTE: torch.distributed._tensor has been moved to torch.distributed.tensor. | imports: torch | [torch distributed _tensor placement_types.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\distributed\\_tensor\\__init__.py",
    "summary": "NOTICE: DTensor has moved to torch.distributed.tensor | imports: importlib, torch | [torch distributed _tensor __init__.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\distributed\\_tools\\fsdp2_mem_tracker.py",
    "summary": "Enumerates categories of memory usage in FSDP modules, including parameters, gradients, activations, | classes: _FSDPRefType, _SavedFSDPMethods, _SavedCollectives, _FSDPModState, _FSDPModMemStats, FakeWork | imports: copy, datetime, functools, typing_extensions | [torch distributed _tools fsdp2_mem_",
    "role": "src",
    "loc": 501
  },
  {
    "id": "torch\\distributed\\_tools\\ilp_utils.py",
    "summary": "Collect modulewise stats for a given model, including memory, runtime, and AC tradeoff stats. | classes: ModOrder, ModRuntime, ModStats, ModuleInfo, Node, Graph | functions: aggregate_stats, parse_module_info, is_self_or_submodule, is_submodule, display_bytes, get_peak_memory_runtime_baseline | impo",
    "role": "src",
    "loc": 223
  },
  {
    "id": "torch\\distributed\\_tools\\memory_tracker.py",
    "summary": "Run in ``TorchDispatchMode`` to get memory stats at operator level. | classes: MemoryProfileDispatchMode, MemoryTracker | imports: operator, pickle, torch, matplotlib | [torch distributed _tools memory_tracker.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\distributed\\_tools\\mem_tracker.py",
    "summary": "Base Class for defining memory reference types, categorizing tensors based on their usage within a model. | classes: _RefType, _State, _MemRefType, _ModState, _ModMemStats, _WeakRefInfo | functions: _get_mem_divisor, _rounding_fn, _print_snapshot, _print_snapshot_tabular, _print_state_snapshots, _pr",
    "role": "src",
    "loc": 746
  },
  {
    "id": "torch\\distributed\\_tools\\mod_tracker.py",
    "summary": "``ModTracker`` is a context manager that tracks the nn.Module hierarchy during execution | classes: ModTracker | imports: weakref, torch | [torch distributed _tools mod_tracker.py]",
    "role": "src",
    "loc": 197
  },
  {
    "id": "torch\\distributed\\_tools\\runtime_estimator.py",
    "summary": "Estimates the GPU runtime in milliseconds using various estimation methods under the ``FakeTensorMode``. | classes: RuntimeEstimator | imports: typing_extensions, torch | [torch distributed _tools runtime_estimator.py]",
    "role": "src",
    "loc": 417
  },
  {
    "id": "torch\\distributed\\_tools\\sac_estimator.py",
    "summary": "Stores metadata for a single operator for SAC. | classes: _SACMetadata, _SACModMetadata, SACStats, MSPS, SACTradeOffStats, SACGreedyOrderMeta | functions: _get_untyped_storages, _display_stats_tabular | imports: dataclasses, typing_extensions, torch, tabulate | [torch distributed _tools sac_estimato",
    "role": "src",
    "loc": 804
  },
  {
    "id": "torch\\distributed\\_tools\\sac_ilp.py",
    "summary": "MILP to decide which modules to AC and how much memory to discard. | classes: SACDecision | functions: sac_milp, get_optimal_checkpointing_policy_per_module | imports: torch, pulp | [torch distributed _tools sac_ilp.py]",
    "role": "src",
    "loc": 193
  },
  {
    "id": "torch\\distributed\\_tools\\__init__.py",
    "summary": "Package initializer | imports: fsdp2_mem_tracker, mem_tracker, memory_tracker, mod_tracker | [torch distributed _tools __init__.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\distributions\\bernoulli.py",
    "summary": "Creates a Bernoulli distribution parameterized by :attr:`probs` | classes: Bernoulli | imports: torch | [torch distributions bernoulli.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\distributions\\beta.py",
    "summary": "Beta distribution parameterized by :attr:`concentration1` and :attr:`concentration0`. | classes: Beta | imports: torch | [torch distributions beta.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\distributions\\binomial.py",
    "summary": "Creates a Binomial distribution parameterized by :attr:`total_count` and | classes: Binomial | functions: _clamp_by_zero | imports: torch | [torch distributions binomial.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "torch\\distributions\\categorical.py",
    "summary": "Creates a categorical distribution parameterized by either :attr:`probs` or | classes: Categorical | imports: torch | [torch distributions categorical.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "torch\\distributions\\cauchy.py",
    "summary": "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of | classes: Cauchy | imports: torch | [torch distributions cauchy.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "torch\\distributions\\chi2.py",
    "summary": "Creates a Chi-squared distribution parameterized by shape parameter :attr:`df`. | classes: Chi2 | imports: torch | [torch distributions chi2.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\distributions\\constraints.py",
    "summary": "Abstract base class for constraints. | classes: Constraint, _Dependent, _DependentProperty, _IndependentConstraint, _Boolean, _OneHot | functions: is_dependent | imports: torch | [torch distributions constraints.py]",
    "role": "src",
    "loc": 538
  },
  {
    "id": "torch\\distributions\\constraint_registry.py",
    "summary": "PyTorch provides two global :class:`ConstraintRegistry` objects that link | classes: ConstraintRegistry | functions: _transform_to_real, _biject_to_independent, _transform_to_independent, _transform_to_positive, _transform_to_greater_than, _transform_to_less_than | imports: torch | [torch distributi",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\distributions\\continuous_bernoulli.py",
    "summary": "Creates a continuous Bernoulli distribution parameterized by :attr:`probs` | classes: ContinuousBernoulli | imports: torch | [torch distributions continuous_bernoulli.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "torch\\distributions\\dirichlet.py",
    "summary": "No description | classes: _Dirichlet, Dirichlet | functions: _Dirichlet_backward | imports: torch | [torch distributions dirichlet.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\distributions\\distribution.py",
    "summary": "Distribution is the abstract base class for probability distributions. | classes: Distribution | imports: typing_extensions, torch | [torch distributions distribution.py]",
    "role": "src",
    "loc": 293
  },
  {
    "id": "torch\\distributions\\exponential.py",
    "summary": "Creates a Exponential distribution parameterized by :attr:`rate`. | classes: Exponential | imports: torch | [torch distributions exponential.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\distributions\\exp_family.py",
    "summary": "ExponentialFamily is the abstract base class for probability distributions belonging to an | classes: ExponentialFamily | imports: torch | [torch distributions exp_family.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\distributions\\fishersnedecor.py",
    "summary": "Creates a Fisher-Snedecor distribution parameterized by :attr:`df1` and :attr:`df2`. | classes: FisherSnedecor | imports: torch | [torch distributions fishersnedecor.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\distributions\\gamma.py",
    "summary": "Creates a Gamma distribution parameterized by shape :attr:`concentration` and :attr:`rate`. | classes: Gamma | functions: _standard_gamma | imports: torch | [torch distributions gamma.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\distributions\\geometric.py",
    "summary": "Creates a Geometric distribution parameterized by :attr:`probs`, | classes: Geometric | imports: torch | [torch distributions geometric.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\distributions\\gumbel.py",
    "summary": "Samples from a Gumbel Distribution. | classes: Gumbel | imports: torch | [torch distributions gumbel.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "torch\\distributions\\half_cauchy.py",
    "summary": "Creates a half-Cauchy distribution parameterized by `scale` where:: | classes: HalfCauchy | imports: torch | [torch distributions half_cauchy.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\distributions\\half_normal.py",
    "summary": "Creates a half-normal distribution parameterized by `scale` where:: | classes: HalfNormal | imports: torch | [torch distributions half_normal.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\distributions\\independent.py",
    "summary": "Reinterprets some of the batch dims of a distribution as event dims. | classes: Independent | imports: torch | [torch distributions independent.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "torch\\distributions\\inverse_gamma.py",
    "summary": "Creates an inverse gamma distribution parameterized by :attr:`concentration` and :attr:`rate` | classes: InverseGamma | imports: torch | [torch distributions inverse_gamma.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\distributions\\kl.py",
    "summary": "Decorator to register a pairwise function with :meth:`kl_divergence`. | classes: _Match | functions: register_kl, decorator, _dispatch_kl, _infinite_like, _x_log_x, _batch_trace_XXT | imports: functools, torch, bernoulli, beta | [torch distributions kl.py]",
    "role": "src",
    "loc": 752
  },
  {
    "id": "torch\\distributions\\kumaraswamy.py",
    "summary": "Samples from a Kumaraswamy distribution. | classes: Kumaraswamy | functions: _moments | imports: torch | [torch distributions kumaraswamy.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\distributions\\laplace.py",
    "summary": "Creates a Laplace distribution parameterized by :attr:`loc` and :attr:`scale`. | classes: Laplace | imports: torch | [torch distributions laplace.py]",
    "role": "src",
    "loc": 74
  },
  {
    "id": "torch\\distributions\\lkj_cholesky.py",
    "summary": "This closely follows the implementation in NumPyro (https://github.com/pyro-ppl/numpyro). | classes: LKJCholesky | imports: torch | [torch distributions lkj_cholesky.py]",
    "role": "src",
    "loc": 100
  },
  {
    "id": "torch\\distributions\\logistic_normal.py",
    "summary": "Creates a logistic-normal distribution parameterized by :attr:`loc` and :attr:`scale` | classes: LogisticNormal | imports: torch | [torch distributions logistic_normal.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\distributions\\log_normal.py",
    "summary": "Creates a log-normal distribution parameterized by | classes: LogNormal | imports: torch | [torch distributions log_normal.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torch\\distributions\\lowrank_multivariate_normal.py",
    "summary": "Creates a multivariate normal distribution with covariance matrix having a low-rank form | classes: LowRankMultivariateNormal | functions: _batch_capacitance_tril, _batch_lowrank_logdet, _batch_lowrank_mahalanobis | imports: torch | [torch distributions lowrank_multivariate_normal.py]",
    "role": "src",
    "loc": 203
  },
  {
    "id": "torch\\distributions\\mixture_same_family.py",
    "summary": "The `MixtureSameFamily` distribution implements a (batch of) mixture | classes: MixtureSameFamily | imports: torch | [torch distributions mixture_same_family.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "torch\\distributions\\multinomial.py",
    "summary": "Creates a Multinomial distribution parameterized by :attr:`total_count` and | classes: Multinomial | imports: torch | [torch distributions multinomial.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torch\\distributions\\multivariate_normal.py",
    "summary": "Creates a multivariate normal (also called Gaussian) distribution | classes: MultivariateNormal | functions: _batch_mv, _batch_mahalanobis, _precision_to_scale_tril | imports: torch | [torch distributions multivariate_normal.py]",
    "role": "src",
    "loc": 222
  },
  {
    "id": "torch\\distributions\\negative_binomial.py",
    "summary": "Creates a Negative Binomial distribution, i.e. distribution | classes: NegativeBinomial | imports: torch | [torch distributions negative_binomial.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "torch\\distributions\\normal.py",
    "summary": "Creates a normal (also called Gaussian) distribution parameterized by | classes: Normal | imports: torch | [torch distributions normal.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torch\\distributions\\one_hot_categorical.py",
    "summary": "Creates a one-hot categorical distribution parameterized by :attr:`probs` or | classes: OneHotCategorical, OneHotCategoricalStraightThrough | imports: torch | [torch distributions one_hot_categorical.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "torch\\distributions\\pareto.py",
    "summary": "Samples from a Pareto Type 1 distribution. | classes: Pareto | imports: torch | [torch distributions pareto.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\distributions\\poisson.py",
    "summary": "Creates a Poisson distribution parameterized by :attr:`rate`, the rate parameter. | classes: Poisson | imports: torch | [torch distributions poisson.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\distributions\\relaxed_bernoulli.py",
    "summary": "Creates a LogitRelaxedBernoulli distribution parameterized by :attr:`probs` | classes: LogitRelaxedBernoulli, RelaxedBernoulli | imports: torch | [torch distributions relaxed_bernoulli.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "torch\\distributions\\relaxed_categorical.py",
    "summary": "Creates a ExpRelaxedCategorical parameterized by | classes: ExpRelaxedCategorical, RelaxedOneHotCategorical | imports: torch | [torch distributions relaxed_categorical.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\distributions\\studentT.py",
    "summary": "Creates a Student's t-distribution parameterized by degree of | classes: StudentT | imports: torch | [torch distributions studentT.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\distributions\\transformed_distribution.py",
    "summary": "Extension of the Distribution class, which applies a sequence of Transforms | classes: TransformedDistribution | imports: torch | [torch distributions transformed_distribution.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\distributions\\transforms.py",
    "summary": "Abstract class for invertable transformations with computable log | classes: Transform, _InverseTransform, ComposeTransform, IndependentTransform, ReshapeTransform, ExpTransform | functions: _clipped_sigmoid | imports: functools, operator, weakref, torch | [torch distributions transforms.py]",
    "role": "src",
    "loc": 978
  },
  {
    "id": "torch\\distributions\\uniform.py",
    "summary": "Generates uniformly distributed random samples from the half-open interval | classes: Uniform | imports: torch | [torch distributions uniform.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\distributions\\utils.py",
    "summary": "Used as a decorator for lazy loading of class attributes. This uses a | classes: lazy_property, _lazy_property_and_property | functions: broadcast_all, _standard_normal, _sum_rightmost, logits_to_probs, clamp_probs, probs_to_logits | imports: functools, typing_extensions, torch | [torch distribution",
    "role": "src",
    "loc": 174
  },
  {
    "id": "torch\\distributions\\von_mises.py",
    "summary": "A circular von Mises distribution. | classes: VonMises | functions: _eval_poly, _log_modified_bessel_fn, _rejection_sample | imports: torch | [torch distributions von_mises.py]",
    "role": "src",
    "loc": 176
  },
  {
    "id": "torch\\distributions\\weibull.py",
    "summary": "Samples from a two-parameter Weibull distribution. | classes: Weibull | imports: torch | [torch distributions weibull.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "torch\\distributions\\wishart.py",
    "summary": "Creates a Wishart distribution parameterized by a symmetric positive definite matrix :math:`\\Sigma`, | classes: Wishart | functions: _mvdigamma, _clamp_above_eps | imports: torch | [torch distributions wishart.py]",
    "role": "src",
    "loc": 276
  },
  {
    "id": "torch\\distributions\\__init__.py",
    "summary": "The ``distributions`` package contains parameterizable probability distributions | imports: bernoulli, beta, binomial, categorical | [torch distributions __init__.py]",
    "role": "src",
    "loc": 153
  },
  {
    "id": "torch\\export\\custom_obj.py",
    "summary": "Metadata which is stored on nodes representing ScriptObjects. | classes: ScriptObjectMeta | imports: dataclasses | [torch export custom_obj.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\export\\custom_ops.py",
    "summary": "No description | functions: _access_subclass_inner_tensor | imports: torch | [torch export custom_ops.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\export\\decomp_utils.py",
    "summary": "This is a custom dictionary that is specifically used for handling decomp_table in export. | classes: CustomDecompTable | imports: torch | [torch export decomp_utils.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "torch\\export\\dynamic_shapes.py",
    "summary": "Enum for dynamic shape hints. | classes: _DimHint, _Dim, _StaticDim, _DerivedDim, _ConstraintTarget, _Constraint | functions: Dim, dims, _process_equalities, _tree_map_with_path, is_leaf, f | imports: dataclasses, inspect, torch, exported_program | [torch export dynamic_shapes.py]",
    "role": "src",
    "loc": 876
  },
  {
    "id": "torch\\export\\exported_program.py",
    "summary": "No description | classes: ModuleCallSignature, ModuleCallEntry, ExportedProgram | functions: _disable_prexisiting_fake_mode, wrapper, _fx_collection_equivalence_fn, _override_composite_implicit_decomp, _force_dispatch_to_orig_cia_callable, _override_decomp_aten_to_variants | imports: copy, dataclass",
    "role": "src",
    "loc": 1213
  },
  {
    "id": "torch\\export\\graph_signature.py",
    "summary": "Creates a mapping where items cannot be added, deleted, or updated. | classes: TensorArgument, TokenArgument, SymIntArgument, SymFloatArgument, SymBoolArgument, CustomObjArgument | functions: _immutable_dict, _make_argument_spec, _convert_to_export_graph_signature, to_input_spec, to_output_spec | im",
    "role": "src",
    "loc": 525
  },
  {
    "id": "torch\\export\\unflatten.py",
    "summary": "No description | classes: _AttrKind, _SubmoduleBase, InterpreterModule, InterpreterModuleDispatcher, FlatArgsAdapter, UnflattenedModule | functions: _disable_interpreter, _assign_attr, unflatten, _inplace_buffer_mutations, _is_prefix, _compute_accessor | imports: abc, copy, operator, dataclasses | [",
    "role": "src",
    "loc": 1260
  },
  {
    "id": "torch\\export\\_draft_export.py",
    "summary": "No description | classes: FailureType, FailureReport, DraftExportReport, ExpressionCreatedNode, LogRecord, CaptureStructuredTrace | functions: prettify_stack, prettify_frame_locals, get_loc, draft_export | imports: getpass, json, tempfile, dataclasses | [torch export _draft_export.py]",
    "role": "src",
    "loc": 375
  },
  {
    "id": "torch\\export\\_remove_auto_functionalized_pass.py",
    "summary": "No description | functions: remove_self_clone, unsafe_remove_auto_functionalized_pass | imports: torch | [torch export _remove_auto_functionalized_pass.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\export\\_remove_effect_tokens_pass.py",
    "summary": "No description | functions: _remove_effect_tokens_from_graph_helper, _remove_effect_tokens | imports: operator, torch, exported_program, graph_signature | [torch export _remove_effect_tokens_pass.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\export\\_safeguard.py",
    "summary": "Detect grad state ops during exporting the graph and fail the process by | classes: AutogradStateOpsFailSafeguard | imports: torch | [torch export _safeguard.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\export\\_swap.py",
    "summary": "No description | functions: _get_getitem_users, _try_remove_connecting_pytrees, _remove_extraneous_pytrees, _construct_inputs, _insert_call_module, _deconstruct_outputs | imports: operator, types, torch, unflatten | [torch export _swap.py]",
    "role": "src",
    "loc": 347
  },
  {
    "id": "torch\\export\\_trace.py",
    "summary": "Manage Export-specific configurations of Dynamo. | classes: ExportDynamoConfig, ATenExportArtifact, ExportArtifact, Path, _WrapperModule, Wrapper | functions: _ignore_backend_decomps, _disable_custom_triton_op_functional_decomposition, custom_triton_ops_decomposition_disabled, _fixup_key, _strip_roo",
    "role": "src",
    "loc": 1732
  },
  {
    "id": "torch\\export\\_tree_utils.py",
    "summary": "Reorder user-provided kwargs to match the order in `spec`. `spec` is | functions: reorder_kwargs, is_equivalent | imports: torch | [torch export _tree_utils.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\export\\_unlift.py",
    "summary": "Metaclass that ensures a private constructor for _StatefulGraphModule | classes: _StatefulGraphModuleFactory, _StatefulGraphModule | functions: _check_inputs_match, _check_input_constraints_pre_hook, _unlift_inputs_as_getattr, _insert_copy_for_mutations, _get_codegen, _unlift | imports: copy, torch,",
    "role": "src",
    "loc": 352
  },
  {
    "id": "torch\\export\\__init__.py",
    "summary": ":func:`export_for_training` takes any nn.Module along with example inputs, and produces a traced graph representing | functions: export_for_training, export_for_inference, export, save, load, register_dataclass | imports: builtins, copy, dataclasses, inspect | [torch export __init__.py]",
    "role": "src",
    "loc": 476
  },
  {
    "id": "torch\\export\\experimental\\__init__.py",
    "summary": "Package initializer | functions: _copy_graph_module_and_signature, _remove_detach_pass, _export_forward_backward | imports: copy, torch | [torch export experimental __init__.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torch\\export\\passes\\__init__.py",
    "summary": "Move the exported program to the given device. | functions: move_to_device_pass, _get_new_device | imports: torch | [torch export passes __init__.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torch\\fft\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch fft __init__.py]",
    "role": "src",
    "loc": 1149
  },
  {
    "id": "torch\\func\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch func __init__.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\futures\\__init__.py",
    "summary": "Collects the provided :class:`~torch.futures.Future` objects into a single | classes: _PyFutureMeta, Future | functions: collect_all, wait_all | imports: torch | [torch futures __init__.py]",
    "role": "src",
    "loc": 277
  },
  {
    "id": "torch\\fx\\annotate.py",
    "summary": "Annotates a Proxy object with a given type. | functions: annotate | imports: torch, _compatibility | [torch fx annotate.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\fx\\config.py",
    "summary": "No description | [torch fx config.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\fx\\graph.py",
    "summary": "Additional objs that we add to every graph's globals. | classes: _CustomBuiltin, _Namespace, PythonCode, _InsertPoint, _node_list, _PyTreeInfo | functions: _register_custom_builtin, _is_magic, _snake_case, _is_from_torch, _format_target, _parse_stack_trace | imports: builtins, copy, functools, inspe",
    "role": "src",
    "loc": 1518
  },
  {
    "id": "torch\\fx\\graph_module.py",
    "summary": "No description | classes: _EvalCacheLoader, _CodeOnlyModule, KeepModules, _WrappedCall, GraphModuleImpl, GraphModule | functions: _exec_with_source, _forward_from_src, _method_from_src, _format_import_statement, _format_import_block, reduce_graph_module | imports: copy, linecache, traceback, torch |",
    "role": "src",
    "loc": 749
  },
  {
    "id": "torch\\fx\\immutable_collections.py",
    "summary": "An immutable version of :class:`list`. | classes: immutable_list, immutable_dict | functions: _no_mutation, _immutable_list_flatten, _immutable_list_unflatten, _immutable_dict_flatten, _immutable_dict_unflatten | imports: typing_extensions, torch, _compatibility | [torch fx immutable_collections.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torch\\fx\\interpreter.py",
    "summary": "An Interpreter executes an FX graph Node-by-Node. This pattern | classes: Interpreter, TransformerTracer, Transformer | imports: inspect, torch, _compatibility, _lazy_graph_module | [torch fx interpreter.py]",
    "role": "src",
    "loc": 472
  },
  {
    "id": "torch\\fx\\node.py",
    "summary": "``Node`` is the data structure that represents individual operations within | classes: Node | functions: has_side_effect, _find_module_of_method, _type_repr, _get_qualified_name, _format_arg, map_arg | imports: builtins, inspect, operator, types | [torch fx node.py]",
    "role": "src",
    "loc": 741
  },
  {
    "id": "torch\\fx\\operator_schemas.py",
    "summary": "Simple named tuple for wrapping args/kwargs pairs. | classes: ArgsKwargsPair, _FakeGlobalNamespace | functions: _nonzero_schemas, nonzero, _torchscript_type_to_python_type, _torchscript_schema_to_signature_impl, _torchscript_schema_to_signature, check_for_mutable_operation | imports: inspect, number",
    "role": "src",
    "loc": 420
  },
  {
    "id": "torch\\fx\\proxy.py",
    "summary": "Scope object that records the module path and the module type | classes: Scope, ScopeContextManager, TracerBase, GraphAppendingTracer, TraceError, Proxy | functions: assert_fn, _scope, impl, _define_reflectable | imports: copy, dis, inspect, operator | [torch fx proxy.py]",
    "role": "src",
    "loc": 549
  },
  {
    "id": "torch\\fx\\subgraph_rewriter.py",
    "summary": "No description | classes: Match, ReplacedPatterns | functions: _replace_attributes, try_get_attr, replace_pattern, replace_pattern_with_filters, _replace_pattern | imports: copy, dataclasses, torch, _compatibility | [torch fx subgraph_rewriter.py]",
    "role": "src",
    "loc": 315
  },
  {
    "id": "torch\\fx\\tensor_type.py",
    "summary": "TensorType defines a type for tensors, which consists of a list of dimensions. | classes: TensorType, _DynType | functions: is_consistent, is_more_precise | imports: torch, _compatibility | [torch fx tensor_type.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "torch\\fx\\traceback.py",
    "summary": "No description | classes: NodeSourceAction, NodeInfo, NodeSource | functions: preserve_node_meta, set_stack_trace, set_grad_fn_seq_nr, reset_grad_fn_seq_nr, format_stack, has_preserved_node_meta | imports: copy, traceback, _compatibility, graph | [torch fx traceback.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\fx\\_compatibility.py",
    "summary": "No description | functions: compatibility, mark_back_compat, mark_not_back_compat | imports: textwrap | [torch fx _compatibility.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\fx\\_graph_pickler.py",
    "summary": "GraphPickler is a Pickler which helps pickling fx graph - in particular | classes: GraphPickler, _UnpickleState, _GraphUnpickler, _ShapeEnvPickleData, _SymNodePickleData, _TensorPickleData | imports: dataclasses, importlib, io, pickle | [torch fx _graph_pickler.py]",
    "role": "src",
    "loc": 421
  },
  {
    "id": "torch\\fx\\_lazy_graph_module.py",
    "summary": "The main difference between _LazyGraphModule and GraphModule is how recompile happens. | classes: _LazyGraphModule | functions: _force_skip_lazy_graph_module, _use_lazy_graph_module, _get_graph_module_cls, _make_graph_module | imports: torch, _compatibility | [torch fx _lazy_graph_module.py]",
    "role": "src",
    "loc": 139
  },
  {
    "id": "torch\\fx\\_pytree.py",
    "summary": "No description | functions: register_pytree_flatten_spec, _deregister_pytree_flatten_spec, tree_flatten_spec, _dict_flatten_spec, _list_flatten_spec, _tuple_flatten_spec | imports: typing_extensions, torch | [torch fx _pytree.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\fx\\_symbolic_trace.py",
    "summary": "ProxyableClassMeta allows you to make construction of a given Python class | classes: ProxyableClassMeta, PHBase, PHWithMeta, Tracer, _PatchedFn, _PatchedFnSetItem | functions: is_fx_tracing, _patch_function, _transfer_attrs, _find_proxy, find_proxy, _create_wrapped_func | imports: builtins, copy, f",
    "role": "src",
    "loc": 979
  },
  {
    "id": "torch\\fx\\_utils.py",
    "summary": "Returns a LazyString that formats the graph code. | functions: lazy_format_graph_code, format_name, _format_graph_code, first_call_function_nn_module_stack, get_node_context | imports: torch | [torch fx _utils.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "torch\\fx\\__init__.py",
    "summary": "FX is a toolkit for developers to use to transform ``nn.Module`` | imports: torch | [torch fx __init__.py]",
    "role": "src",
    "loc": 93
  },
  {
    "id": "torch\\fx\\experimental\\accelerator_partitioner.py",
    "summary": "DAGNode class maintains useful information for a partition (submodule), | classes: DAGNode, DAG, PartitionResult, Partitioner | functions: reset_partition_device, combine_two_partitions, set_parents_and_children, reorganize_partitions, get_bfs_level_partition, get_node_to_partition_mapping | imports",
    "role": "src",
    "loc": 880
  },
  {
    "id": "torch\\fx\\experimental\\const_fold.py",
    "summary": "FoldedGraphModule is a GraphModule which also contains another | classes: FoldedGraphModule | functions: _inline_module, replacement_fn, get_unique_attr_name_in_module, split_const_subgraphs, mod_partition | imports: torch | [torch fx experimental const_fold.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "torch\\fx\\experimental\\debug.py",
    "summary": "Sets a breakpoint in `gm`'s generated python code. It drops into pdb when | functions: set_trace, insert_pdb | imports: torch | [torch fx experimental debug.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\fx\\experimental\\graph_gradual_typechecker.py",
    "summary": "Expand a type to the desired tensor dimension if possible | classes: GraphTypeChecker, Refine | functions: expand_to_tensor_dim, broadcast_types, register_inference_rule, register, register_refinement_rule, register_algebraic_expressions_inference_rule | imports: operator, functools, typing_extensio",
    "role": "src",
    "loc": 815
  },
  {
    "id": "torch\\fx\\experimental\\merge_matmul.py",
    "summary": "A free function for use in the merge_matmul graph transformation below that | functions: split_result_tensors, may_depend_on, are_nodes_independent, merge_matmul | imports: operator, torch | [torch fx experimental merge_matmul.py]",
    "role": "src",
    "loc": 113
  },
  {
    "id": "torch\\fx\\experimental\\meta_tracer.py",
    "summary": "No description | classes: MetaProxy, MetaAttribute, MetaDeviceAttribute, MetaTracer | functions: embedding_override, nn_layernorm_override, torch_relu_override, torch_nn_relu_override, functional_relu_override, torch_where_override | imports: builtins, functools, torch | [torch fx experimental meta_",
    "role": "src",
    "loc": 235
  },
  {
    "id": "torch\\fx\\experimental\\normalize.py",
    "summary": "Normalize arguments to Python targets. This means that | classes: NormalizeArgs, NormalizeOperators | imports: operator, torch, schema_type_annotation | [torch fx experimental normalize.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "torch\\fx\\experimental\\optimization.py",
    "summary": "Splits a qualname into parent path and last atom. | classes: DropoutRemover, MklSubgraph, UnionFind, MklSupport | functions: _parent_name, matches_module_pattern, replace_node_module, fuse, remove_dropout, extract_subgraph | imports: copy, operator, torch | [torch fx experimental optimization.py]",
    "role": "src",
    "loc": 398
  },
  {
    "id": "torch\\fx\\experimental\\partitioner_utils.py",
    "summary": "Partition class contains all the information about an individual partition. | classes: Partition, Device, NodeLatency, PartitionLatency, PartitionMode, PartitionerConfig | functions: get_extra_size_of, get_latency_of_one_partition, get_top_nodes, dfs_helper, get_partition_to_latency_mapping, get_com",
    "role": "src",
    "loc": 241
  },
  {
    "id": "torch\\fx\\experimental\\proxy_tensor.py",
    "summary": "FX gets confused by varargs, de-confuse it | classes: _NoDefault, _HasMeta, _ProxyTensor, _SymNodeDict, PythonKeyTracer, TorchFunctionMetadataMode | functions: fake_signature, decompose, is_sym_node, set_proxy_slot, has_proxy_slot, get_proxy_slot | imports: functools, inspect, operator, traceback | ",
    "role": "src",
    "loc": 1688
  },
  {
    "id": "torch\\fx\\experimental\\recording.py",
    "summary": "No description | classes: ShapeEnvEvent, FakeTensorMeta, NotEqualError | functions: _extract_shape_env_and_assert_equal, assert_equal, record_shapeenv_event, decorator, wrapper, retlog | imports: functools, inspect, dataclasses, torch | [torch fx experimental recording.py]",
    "role": "src",
    "loc": 247
  },
  {
    "id": "torch\\fx\\experimental\\refinement_types.py",
    "summary": "No description | classes: Equality | [torch fx experimental refinement_types.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\fx\\experimental\\rewriter.py",
    "summary": "Take a FunctionType object representing a `forward` method, then | classes: AST_Rewriter, RewritingTracer, RewrittenModule | functions: _rewrite, rewrite_module | imports: ast, copy, functools, inspect | [torch fx experimental rewriter.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\fx\\experimental\\schema_type_annotation.py",
    "summary": "Use Python function signatures to annotate types for `Nodes` within an FX graph. | classes: AnnotateTypesWithSchema | imports: inspect, torch | [torch fx experimental schema_type_annotation.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\fx\\experimental\\symbolic_shapes.py",
    "summary": "No description | classes: GuardOnDataDependentSymNode, PendingUnbackedSymbolNotFound, SymIntEqByExpr, ConstraintViolationError, ConvertIntKey, CallMethodKey | functions: log_lru_cache_stats, _nested_int_aware_sort, lru_cache, inner, cumulative_cache_info, new_cache_clear | imports: abc, atexit, dis,",
    "role": "src",
    "loc": 5108
  },
  {
    "id": "torch\\fx\\experimental\\sym_node.py",
    "summary": "This is a type erased SymInt/SymFloat which we use to do actual operations. | classes: SymNode | functions: _to_symtype, _get_sym_node_fn, fn, _sympy_float_truediv, _sympy_int_truediv, _sympy_floordiv | imports: builtins, functools, inspect, operator | [torch fx experimental sym_node.py]",
    "role": "src",
    "loc": 1230
  },
  {
    "id": "torch\\fx\\experimental\\unify_refinements.py",
    "summary": "Calls our symbolic inferencer once. | functions: infer_symbolic_types_single_pass, infer_symbolic_types, convert_eq, unify_eq, substitute_solution_one_type, substitute_all_types | imports: torch | [torch fx experimental unify_refinements.py]",
    "role": "src",
    "loc": 102
  },
  {
    "id": "torch\\fx\\experimental\\validator.py",
    "summary": "No description | classes: _Z3Ops, PopulateValidator, SympyToZ3, TranslationValidator, ValidationException, BisectValidationException | functions: z3str, get_args_str, collect_str_args, _bitwise_op, wrapper, z3op | imports: builtins, functools, operator, dataclasses | [torch fx experimental validator",
    "role": "src",
    "loc": 512
  },
  {
    "id": "torch\\fx\\experimental\\_backward_state.py",
    "summary": "BackwardState is used to pass Python hooks from the forwards pass | classes: BackwardState | imports: torch | [torch fx experimental _backward_state.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\fx\\experimental\\_config.py",
    "summary": "No description | imports: torch | [torch fx experimental _config.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\fx\\experimental\\_constant_symnode.py",
    "summary": "No description | classes: ConstantIntNode | [torch fx experimental _constant_symnode.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\fx\\experimental\\__init__.py",
    "summary": "Package initializer | [torch fx experimental __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\constraint.py",
    "summary": "No description | classes: Constraint, Conj, Disj, Prod, T, F | functions: is_algebraic_expression, is_bool_expr, is_dim | imports: torch | [torch fx experimental migrate_gradual_types constraint.py]",
    "role": "src",
    "loc": 528
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\constraint_generator.py",
    "summary": "No description | classes: ConstraintGenerator | functions: register_inference_rule, register, generate_flatten_constraints, get_attr_inference_rule, bmm_inference_rule, index_select_inference_rule | imports: operator, typing_extensions, torch | [torch fx experimental migrate_gradual_types constraint",
    "role": "src",
    "loc": 1180
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\constraint_transformation.py",
    "summary": "No description | functions: register_transformation_rule, register, valid_index, transform_transpose, transform_index_select, transform_get_item | imports: copy, torch | [torch fx experimental migrate_gradual_types constraint_transformation.py]",
    "role": "src",
    "loc": 1035
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\operation.py",
    "summary": "No description | [torch fx experimental migrate_gradual_types operation.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\transform_to_z3.py",
    "summary": "No description | functions: transform_to_z3, transform_var, transform_dimension, transform_algebraic_expression, transform_all_constraints, iterate_till_fixed_point | imports: torch, z3 | [torch fx experimental migrate_gradual_types transform_to_z3.py]",
    "role": "src",
    "loc": 346
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\util.py",
    "summary": "Generate a tensor variable | functions: gen_tvar, gen_dvar, gen_bvar, gen_tensor_dims, gen_nat_constraints | imports: torch | [torch fx experimental migrate_gradual_types util.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\z3_types.py",
    "summary": "No description | imports: z3 | [torch fx experimental migrate_gradual_types z3_types.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\fx\\experimental\\migrate_gradual_types\\__init__.py",
    "summary": "Package initializer | [torch fx experimental migrate_gradual_types __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\fx\\experimental\\shape_inference\\infer_shape.py",
    "summary": "No description | functions: infer_shape, mksym | imports: copy, torch | [torch fx experimental shape_inference infer_shape.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "torch\\fx\\experimental\\shape_inference\\infer_symbol_values.py",
    "summary": "No description | functions: infer_symbol_values, calculate_value, solve_equation, update_equation | imports: numpy, sympy, torch | [torch fx experimental shape_inference infer_symbol_values.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\fx\\experimental\\unification\\core.py",
    "summary": "No description | functions: _reify, reify, _unify, unify | imports: functools, dispatch, unification_tools, utils | [torch fx experimental unification core.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "torch\\fx\\experimental\\unification\\dispatch.py",
    "summary": "No description | imports: functools, multipledispatch | [torch fx experimental unification dispatch.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\fx\\experimental\\unification\\match.py",
    "summary": "No description | classes: Dispatcher, VarDispatcher | functions: match, _, supercedes, edge, ordering | imports: core, unification_tools, utils, variable | [torch fx experimental unification match.py]",
    "role": "src",
    "loc": 101
  },
  {
    "id": "torch\\fx\\experimental\\unification\\more.py",
    "summary": "Register standard unify and reify operations on class | functions: unifiable, reify_object, _reify_object_dict, _reify_object_slots, _reify, unify_object | imports: core, dispatch | [torch fx experimental unification more.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "torch\\fx\\experimental\\unification\\unification_tools.py",
    "summary": "No description | functions: _get_factory, merge, merge_with, valmap, keymap, itemmap | imports: operator, functools | [torch fx experimental unification unification_tools.py]",
    "role": "src",
    "loc": 332
  },
  {
    "id": "torch\\fx\\experimental\\unification\\utils.py",
    "summary": "No description | functions: hashable, transitive_get, raises, _toposort, reverse_dict, xfail | [torch fx experimental unification utils.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torch\\fx\\experimental\\unification\\variable.py",
    "summary": "Logic Variable | classes: Var | functions: var, vars, isvar, variables | imports: dispatch, utils | [torch fx experimental unification variable.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\fx\\experimental\\unification\\__init__.py",
    "summary": "Package initializer | imports: core, more, variable | [torch fx experimental unification __init__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\conflict.py",
    "summary": "A is consistent and strictly more specific than B | classes: AmbiguityWarning | functions: supercedes, consistent, ambiguous, ambiguities, super_signature, edge | imports: operator, utils, variadic | [torch fx experimental unification multipledispatch conflict.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\core.py",
    "summary": "Dispatch function on the types of the inputs | functions: dispatch, _df, ismethod | imports: inspect, dispatcher | [torch fx experimental unification multipledispatch core.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\dispatcher.py",
    "summary": "A NotImplementedError for multiple dispatch | classes: MDNotImplementedError, Dispatcher, MethodDispatcher | functions: ambiguity_warn, halt_ordering, restart_ordering, variadic_signature_matches_iter, variadic_signature_matches, source | imports: inspect, typing_extensions, conflict, utils | [torch",
    "role": "src",
    "loc": 373
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\utils.py",
    "summary": "No description | functions: raises, expand_tuples, _toposort, reverse_dict, groupby, typename | [torch fx experimental unification multipledispatch utils.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\variadic.py",
    "summary": "Check whether the type `obj` is variadic. | classes: VariadicSignatureType, VariadicSignatureMeta, Variadic | functions: isvariadic | imports: utils | [torch fx experimental unification multipledispatch variadic.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\fx\\experimental\\unification\\multipledispatch\\__init__.py",
    "summary": "Package initializer | imports: core, dispatcher | [torch fx experimental unification multipledispatch __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\fx\\passes\\annotate_getitem_nodes.py",
    "summary": "Annotate the type of getitem nodes, inferred from the type of sequence node. | functions: annotate_getitem_nodes | imports: operator, torch | [torch fx passes annotate_getitem_nodes.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\fx\\passes\\fake_tensor_prop.py",
    "summary": "Execute an FX graph Node-by-Node and record a fake tensor representing | classes: FakeTensorProp | imports: torch | [torch fx passes fake_tensor_prop.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\fx\\passes\\graph_drawer.py",
    "summary": "Visualize a torch.fx.Graph with graphviz | classes: FxGraphDrawer | imports: hashlib, torch, pydot | [torch fx passes graph_drawer.py]",
    "role": "src",
    "loc": 421
  },
  {
    "id": "torch\\fx\\passes\\graph_manipulation.py",
    "summary": "Modifies all nodes in fx_module.graph.nodes which match the specified op code and target, | classes: size_bytes | functions: replace_target_nodes_with, get_size_of_all_nodes, get_tensor_meta, get_size_of_node | imports: torch | [torch fx passes graph_manipulation.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\fx\\passes\\graph_transform_observer.py",
    "summary": "No description | classes: GraphTransformObserver | imports: torch, graph_drawer | [torch fx passes graph_transform_observer.py]",
    "role": "src",
    "loc": 163
  },
  {
    "id": "torch\\fx\\passes\\net_min_base.py",
    "summary": "Raised if failed to split out a minimize module | classes: FxNetMinimizerBadModuleError, FxNetMinimizerRunFuncError, FxNetMinimizerResultMismatchError, _MinimizerSettingBase, _MinimizerBase | imports: dataclasses, torch, shape_prop, split_utils | [torch fx passes net_min_base.py]",
    "role": "src",
    "loc": 767
  },
  {
    "id": "torch\\fx\\passes\\operator_support.py",
    "summary": "Interface for determining if a fx.Node is supported by a backend | classes: OperatorSupportBase, OperatorSupport, FunctionalOperatorSupport, OpSupports | functions: create_op_support, chain, _chain, any_chain, _any_chain, _get_arg_dtype | imports: abc, torch, shape_prop, tools_common | [torch fx pas",
    "role": "src",
    "loc": 156
  },
  {
    "id": "torch\\fx\\passes\\param_fetch.py",
    "summary": "Default matching method | functions: default_matching, extract_attrs_for_lowering, lift_lowering_attrs_to_nodes | imports: torch | [torch fx passes param_fetch.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\fx\\passes\\pass_manager.py",
    "summary": "Construct a PassManager. | classes: PassManager | functions: inplace_wrapper, wrapped_fn, log_hook, loop_pass, new_pass, _validate_pass_schedule_constraint | imports: functools, inspect | [torch fx passes pass_manager.py]",
    "role": "src",
    "loc": 191
  },
  {
    "id": "torch\\fx\\passes\\reinplace.py",
    "summary": "No description | classes: _ViewType, _FunctionalizationMetadataProp | functions: _is_view_op, _get_view_type, _schemas_match, _maybe_get_inplace_op, _get_all_later_node_usages, _add_if_tensor | imports: _operator, torch | [torch fx passes reinplace.py]",
    "role": "src",
    "loc": 490
  },
  {
    "id": "torch\\fx\\passes\\runtime_assert.py",
    "summary": "Get the example value key for a node, since dynamo uses \"example_value\" | functions: _get_example_value, _get_sym_val, insert_deferred_runtime_asserts, _is_intermediate_tensor_sym_call, _node_metadata_hook, _sympy_interp | imports: functools, operator, sympy, torch | [torch fx passes runtime_assert.",
    "role": "src",
    "loc": 461
  },
  {
    "id": "torch\\fx\\passes\\shape_prop.py",
    "summary": "Extract a TensorMetadata NamedTuple describing `result`. | classes: TensorMetadata, ShapeProp | functions: _extract_tensor_metadata | imports: traceback, torch | [torch fx passes shape_prop.py]",
    "role": "src",
    "loc": 168
  },
  {
    "id": "torch\\fx\\passes\\splitter_base.py",
    "summary": "Generate inputs for targeting submdoules in the given model. Note that if two submodules refer to the same obj, this | classes: _SplitterSettingBase, FxNetAccNodesFinder, FxNetSplitterInternalError, Subgraph, SplitResult, CustomDrawer | functions: generate_inputs_for_submodules, pre_forward, clean_u",
    "role": "src",
    "loc": 719
  },
  {
    "id": "torch\\fx\\passes\\split_module.py",
    "summary": "No description | classes: Partition | functions: _get_attr_from_qualname, split_module, construct_graph, record_cross_partition_use, instantiate_node_partition_mapping | imports: inspect, torch, sympy | [torch fx passes split_module.py]",
    "role": "src",
    "loc": 463
  },
  {
    "id": "torch\\fx\\passes\\split_utils.py",
    "summary": "A component serves as a container for a subgraph we want to create afterwards. | classes: Component | functions: getattr_recursive, setattr_recursive, split_by_tags, flatten, remap_func | imports: copy, dataclasses, torch, tools_common | [torch fx passes split_utils.py]",
    "role": "src",
    "loc": 197
  },
  {
    "id": "torch\\fx\\passes\\tools_common.py",
    "summary": "No description | classes: FusionGroup, FxNetAccFusionsFinder | functions: get_acc_ops_name, get_node_target, is_node_output_tensor, legalize_graph | imports: operator, dataclasses, torch | [torch fx passes tools_common.py]",
    "role": "src",
    "loc": 247
  },
  {
    "id": "torch\\fx\\passes\\_tensorify_python_scalars.py",
    "summary": "Converts Python scalar operations into Tensor operations within the graph. This pass looks for | functions: tensorify_python_scalars, _sympy_interp | imports: sympy, torch | [torch fx passes _tensorify_python_scalars.py]",
    "role": "src",
    "loc": 238
  },
  {
    "id": "torch\\fx\\passes\\__init__.py",
    "summary": "Package initializer | [torch fx passes __init__.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\fx\\passes\\backends\\cudagraphs.py",
    "summary": "Partition an FX graph into sub-GraphModules that can be validly run under | classes: CudaGraphsSupport | functions: partition_cudagraphs | imports: operator, torch | [torch fx passes backends cudagraphs.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\fx\\passes\\backends\\__init__.py",
    "summary": "Package initializer | [torch fx passes backends __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\fx\\passes\\dialect\\__init__.py",
    "summary": "Package initializer | [torch fx passes dialect __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\fx\\passes\\dialect\\common\\cse_pass.py",
    "summary": "No description | classes: CSEPass | functions: get_CSE_banned_ops | imports: torch | [torch fx passes dialect common cse_pass.py]",
    "role": "src",
    "loc": 120
  },
  {
    "id": "torch\\fx\\passes\\dialect\\common\\__init__.py",
    "summary": "Package initializer | [torch fx passes dialect common __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\fx\\passes\\infra\\partitioner.py",
    "summary": "No description | classes: Partition, _DependencyViewer, CapabilityBasedPartitioner | imports: copy, torch | [torch fx passes infra partitioner.py]",
    "role": "src",
    "loc": 282
  },
  {
    "id": "torch\\fx\\passes\\infra\\pass_base.py",
    "summary": "Result of a pass: | classes: PassResult, PassBase | imports: abc, torch | [torch fx passes infra pass_base.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\fx\\passes\\infra\\pass_manager.py",
    "summary": "Construct a PassManager. | classes: PassManager | functions: pass_result_wrapper, wrapped_fn, _validate_pass_schedule_constraint, _topological_sort_passes, this_before_that_pass_constraint, depends_on | imports: inspect, functools, queue, torch | [torch fx passes infra pass_manager.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\fx\\passes\\infra\\__init__.py",
    "summary": "Package initializer | [torch fx passes infra __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\fx\\passes\\tests\\test_pass_manager.py",
    "summary": "No description | classes: TestPassManager | imports: unittest, pass_manager | [torch fx passes tests test_pass_manager.py]",
    "role": "tests",
    "loc": 42
  },
  {
    "id": "torch\\fx\\passes\\tests\\__init__.py",
    "summary": "Package initializer | [torch fx passes tests __init__.py]",
    "role": "tests",
    "loc": 0
  },
  {
    "id": "torch\\fx\\passes\\utils\\common.py",
    "summary": "HolderModule is used to copy all the attributes from original module to submodules | classes: HolderModule | functions: lift_subgraph_as_module, compare_graphs | imports: torch | [torch fx passes utils common.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "torch\\fx\\passes\\utils\\fuser_utils.py",
    "summary": "No description | functions: topo_sort, validate_partition, bfs_find_cycle, fuse_as_graphmodule, remap_inputs, insert_subgm | imports: copy, queue, torch | [torch fx passes utils fuser_utils.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torch\\fx\\passes\\utils\\matcher_utils.py",
    "summary": "No description | classes: InternalMatch, SubgraphMatcher | functions: _init_logger | imports: copy, dataclasses, torch | [torch fx passes utils matcher_utils.py]",
    "role": "src",
    "loc": 320
  },
  {
    "id": "torch\\fx\\passes\\utils\\matcher_with_name_node_map_utils.py",
    "summary": "Extends SubgraphMatcher to support querying the matched subgraph nodes through node name, | classes: SubgraphMatcherWithNameNodeMap | functions: _split_to_graph_and_name_node_map | imports: torch, matcher_utils | [torch fx passes utils matcher_with_name_node_map_utils.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "torch\\fx\\passes\\utils\\source_matcher_utils.py",
    "summary": "No description | classes: SourcePartition | functions: _init_logger, get_source_partitions, make_partition, check_subgraphs_connected | imports: dataclasses, torch | [torch fx passes utils source_matcher_utils.py]",
    "role": "src",
    "loc": 111
  },
  {
    "id": "torch\\fx\\passes\\utils\\__init__.py",
    "summary": "Package initializer | imports: common | [torch fx passes utils __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\jit\\annotations.py",
    "summary": "No description | classes: Module, EvalEnv | functions: get_signature, is_function_or_method, is_vararg, get_param_names, check_fn, _eval_no_call | imports: ast, builtins, dis, inspect | [torch jit annotations.py]",
    "role": "src",
    "loc": 428
  },
  {
    "id": "torch\\jit\\frontend.py",
    "summary": "No description | classes: FrontendError, NotSupportedError, UnsupportedNodeError, FrontendTypeError, Builder, WithItemBuilder | functions: is_reserved_name, build_withitems, build_stmts, get_class_properties, get_class_assigns, maybe_build_assign | imports: ast, copy, dataclasses, inspect | [torch j",
    "role": "src",
    "loc": 1041
  },
  {
    "id": "torch\\jit\\generate_bytecode.py",
    "summary": "No description | functions: format_bytecode, listify, generate_upgraders_bytecode | imports: torch | [torch jit generate_bytecode.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\jit\\quantized.py",
    "summary": "No description | classes: QuantizedLinear, QuantizedLinearFP16, QuantizedRNNCellBase, QuantizedRNNCell, QuantizedLSTMCell, QuantizedGRUCell | functions: quantize_rnn_cell_modules, quantize_linear_modules, quantize_rnn_modules | imports: torch | [torch jit quantized.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "torch\\jit\\supported_ops.py",
    "summary": "No description | functions: _hidden, _emit_type, _emit_arg, _emit_args, _emit_ret, _emit_rets | imports: inspect, textwrap, torch | [torch jit supported_ops.py]",
    "role": "src",
    "loc": 257
  },
  {
    "id": "torch\\jit\\unsupported_tensor_ops.py",
    "summary": "No description | functions: execWrapper, _gen_unsupported_methods_properties, _list_unsupported_tensor_ops | imports: textwrap, torch | [torch jit unsupported_tensor_ops.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\jit\\_async.py",
    "summary": "Async API. | functions: fork, wait | imports: torch | [torch jit _async.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "torch\\jit\\_await.py",
    "summary": "Create Await object that will call specified functioni with specified args, when it is requested for the result. | functions: _awaitable, _awaitable_wait, _awaitable_nowait | imports: torch | [torch jit _await.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\jit\\_builtins.py",
    "summary": "No description | functions: _gen_torch_functional_registered_ops, _is_special_functional_bound_op, _get_builtin_table, register_all, _register_builtin, _find_builtin | imports: cmath, torch | [torch jit _builtins.py]",
    "role": "src",
    "loc": 156
  },
  {
    "id": "torch\\jit\\_check.py",
    "summary": "Check the ``__init__`` method of a given ``nn.Module``. | classes: AttributeTypeIsSupportedChecker | imports: ast, inspect, textwrap, torch | [torch jit _check.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\jit\\_dataclass_impls.py",
    "summary": "No description | functions: _get_fake_filename, compose_fn, synthesize__init__, synthesize__repr__, synthesize__hash__, synthesize_equality | imports: ast, dataclasses, inspect, functools | [torch jit _dataclass_impls.py]",
    "role": "src",
    "loc": 142
  },
  {
    "id": "torch\\jit\\_decompositions.py",
    "summary": "No description | functions: check_decomposition_has_type_annotations, signatures_match, register_decomposition, decomposition_decorator, var_decomposition, var | imports: torch, inspect, typing_extensions | [torch jit _decompositions.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\jit\\_decomposition_utils.py",
    "summary": "No description | functions: _register_decomposition | imports: torch | [torch jit _decomposition_utils.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\jit\\_freeze.py",
    "summary": "Freezing. | functions: freeze, run_frozen_optimizations, optimize_for_inference | imports: torch | [torch jit _freeze.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "torch\\jit\\_fuser.py",
    "summary": "Context manager that controls whether the JIT's executor will run optimizations before executing a function. | functions: optimized_execution, fuser, _get_differentiable_graph_node, _graph_for, _script_method_graph_for, set_fusion_strategy | imports: torch | [torch jit _fuser.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "torch\\jit\\_ir_utils.py",
    "summary": "No description | classes: _InsertPoint | functions: insert_point_guard | imports: types, torch | [torch jit _ir_utils.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\jit\\_logging.py",
    "summary": "No description | imports: torch | [torch jit _logging.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\jit\\_monkeytype_config.py",
    "summary": "A JitTypeCallTraceLogger that stores logged traces in a CallTraceStore. | classes: JitTypeTraceStoreLogger, JitTypeTraceStore, JitTypeTraceConfig | functions: is_torch_native_class, get_type, get_optional_of_element_type, get_qualified_name, jit_code_filter | imports: inspect, types, torch, monkeyty",
    "role": "src",
    "loc": 133
  },
  {
    "id": "torch\\jit\\_pickle.py",
    "summary": "No description | functions: build_intlist, build_tensorlist, build_doublelist, build_boollist, build_tensor_from_id, restore_type_tag | [torch jit _pickle.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\jit\\_recursive.py",
    "summary": "No description | classes: SourceContext, ConcreteTypeStore | functions: _compile_and_register_class, make_stub, make_stub_from_method, make_stubs_from_exported_methods, jit_ignored_properties, get_properties_names | imports: functools, inspect, textwrap, types | [torch jit _recursive.py]",
    "role": "src",
    "loc": 754
  },
  {
    "id": "torch\\jit\\_script.py",
    "summary": "TorchScript. | classes: OrderedDictWrapper, OrderedModuleDict, ScriptMeta, _CachedForward, ScriptWarning, ConstMap | functions: _reduce, Attribute, _get_type_trace_db, _get_function_from_type, _is_new_style_class, script_method | imports: copy, functools, inspect, pickle | [torch jit _script.py]",
    "role": "src",
    "loc": 1186
  },
  {
    "id": "torch\\jit\\_serialization.py",
    "summary": "Serialization. | functions: save, load, validate_map_location, jit_module_from_flatbuffer, save_jit_module_to_flatbuffer, get_flatbuffer_module_info | imports: torch | [torch jit _serialization.py]",
    "role": "src",
    "loc": 206
  },
  {
    "id": "torch\\jit\\_shape_functions.py",
    "summary": "No description | functions: broadcast, broadcast_three, broadcast_one_three, adaptive_avg_pool2d, _copy, unary | imports: torch | [torch jit _shape_functions.py]",
    "role": "src",
    "loc": 1214
  },
  {
    "id": "torch\\jit\\_state.py",
    "summary": "JIT-related state. | classes: EnabledProxy | functions: disable, enable, _add_script_class, _get_script_class, _get_python_class, _clear_class_state | imports: weakref, torch | [torch jit _state.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "torch\\jit\\_trace.py",
    "summary": "Tracing. | classes: ONNXTracedModule, TracingCheckError, TracerWarning, _ExportType, _ExportOutcome, QualnameWrapper | functions: _create_interpreter_name_lookup_fn, _get_interpreter_name_for_var, _unique_state_dict, _clone_inputs, clone_input, _time | imports: copy, functools, inspect, typing_exten",
    "role": "src",
    "loc": 1223
  },
  {
    "id": "torch\\jit\\__init__.py",
    "summary": "Give errors if not all nodes have been fused in inference, or symbolically differentiated in training. | classes: strict_fusion | functions: export_opnames, annotate, script_if_tracing, isinstance, _hide_source_ranges, enable_onednn_fusion | imports: torch | [torch jit __init__.py]",
    "role": "src",
    "loc": 222
  },
  {
    "id": "torch\\jit\\mobile\\__init__.py",
    "summary": "Load a :class:`LiteScriptModule` saved with :func:`torch.jit._save_for_lite_interpreter`. | classes: LiteScriptModule | functions: _load_for_lite_interpreter, _export_operator_list, _get_model_bytecode_version, _get_mobile_model_contained_types, _backport_for_mobile, _backport_for_mobile_to_buffer |",
    "role": "src",
    "loc": 165
  },
  {
    "id": "torch\\jit\\_passes\\_property_propagation.py",
    "summary": "Tools to help with tensor property propagation. | functions: apply_input_props_using_example | imports: torch | [torch jit _passes _property_propagation.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\jit\\_passes\\__init__.py",
    "summary": "Package initializer | [torch jit _passes __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\linalg\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch linalg __init__.py]",
    "role": "src",
    "loc": 2302
  },
  {
    "id": "torch\\masked\\_docs.py",
    "summary": "No description | [torch masked _docs.py]",
    "role": "src",
    "loc": 978
  },
  {
    "id": "torch\\masked\\_ops.py",
    "summary": "Decorator that applies docstring templates to function docstring | classes: Combine | functions: _apply_docstring_templates, _generate_docstring, _reduction_identity, _canonical_dim, _sparse_coo_flatten_indices, _any | imports: typing_extensions, torch | [torch masked _ops.py]",
    "role": "src",
    "loc": 1513
  },
  {
    "id": "torch\\masked\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch masked __init__.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "torch\\masked\\maskedtensor\\binary.py",
    "summary": "No description | functions: _get_at_least_one_mask, _binary_helper, _torch_binary, binary_fn, _torch_inplace_binary, _is_native_binary | imports: torch, core | [torch masked maskedtensor binary.py]",
    "role": "src",
    "loc": 161
  },
  {
    "id": "torch\\masked\\maskedtensor\\core.py",
    "summary": "Returns True if the input is a MaskedTensor, else False | classes: Constructor, GetData, MaskedTensor | functions: is_masked_tensor, _tensors_match, _masks_match, _map_mt_args_kwargs, _helper, _wrap_result | imports: typing_extensions, torch, _ops, _ops_refs | [torch masked maskedtensor core.py]",
    "role": "src",
    "loc": 296
  },
  {
    "id": "torch\\masked\\maskedtensor\\creation.py",
    "summary": "No description | functions: masked_tensor, as_masked_tensor | imports: core | [torch masked maskedtensor creation.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\masked\\maskedtensor\\passthrough.py",
    "summary": "These are functions that should simply be applied to both mask and data. | functions: _is_pass_through_fn, _apply_pass_through_fn | imports: torch, core | [torch masked maskedtensor passthrough.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\masked\\maskedtensor\\reductions.py",
    "summary": "No description | functions: _masked_all_all, _masked_all_dim, _masked_all, _multidim_any, _get_masked_fn, _torch_reduce_all | imports: torch, core, creation | [torch masked maskedtensor reductions.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torch\\masked\\maskedtensor\\unary.py",
    "summary": "No description | functions: _unary_helper, _torch_unary, unary_fn, _torch_inplace_unary, _is_native_unary, _apply_native_unary | imports: torch, core | [torch masked maskedtensor unary.py]",
    "role": "src",
    "loc": 160
  },
  {
    "id": "torch\\masked\\maskedtensor\\_ops_refs.py",
    "summary": "No description | classes: _MaskedContiguous, _MaskedToDense, _MaskedToSparse, _MaskedToSparseCsr, _MaskedWhere | functions: _check_args_kwargs_length, register_function_func, wrapper, _general_function_reductions, _function_where, _function_contiguous | imports: functools, torch, binary, core | [tor",
    "role": "src",
    "loc": 398
  },
  {
    "id": "torch\\masked\\maskedtensor\\__init__.py",
    "summary": "Package initializer | imports: binary, core, passthrough, reductions | [torch masked maskedtensor __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\monitor\\__init__.py",
    "summary": "TensorboardEventHandler is an event handler that will write known events to | classes: TensorboardEventHandler | imports: torch | [torch monitor __init__.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\mps\\event.py",
    "summary": "Wrapper around an MPS event. | classes: Event | imports: torch | [torch mps event.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\mps\\profiler.py",
    "summary": "Start OS Signpost tracing from MPS backend. | functions: start, stop, profile, is_metal_capture_enabled, is_capturing_metal, metal_capture | imports: torch | [torch mps profiler.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\mps\\__init__.py",
    "summary": "This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. | functions: _get_default_mps_generator, device_count, synchronize, get_rng_state, set_rng_state, manual_seed | imports: torch, event | [torch mps __init__.py]",
    "role": "src",
    "loc": 138
  },
  {
    "id": "torch\\mtia\\memory.py",
    "summary": "This package adds support for device memory management implemented in MTIA. | functions: memory_stats, max_memory_allocated, reset_peak_memory_stats | imports: torch, _utils | [torch mtia memory.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\mtia\\_utils.py",
    "summary": "Get the device index from :attr:`device`, which can be a torch.device object, a Python integer, or ``None``. | functions: _get_device_index | imports: torch | [torch mtia _utils.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\mtia\\__init__.py",
    "summary": "This package enables an interface for accessing MTIA backend in python | classes: DeferredMtiaCallError, device, StreamContext | functions: init, is_initialized, _is_in_bad_fork, _lazy_init, _is_compiled, is_available | imports: threading, torch, _utils, memory | [torch mtia __init__.py]",
    "role": "src",
    "loc": 260
  },
  {
    "id": "torch\\multiprocessing\\pool.py",
    "summary": "Pool implementation which uses our version of SimpleQueue. | classes: Pool | functions: clean_worker | imports: multiprocessing, queue, gc | [torch multiprocessing pool.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\multiprocessing\\queue.py",
    "summary": "Proxy class for _multiprocessing.Connection which uses ForkingPickler for object serialization. | classes: ConnectionWrapper, Queue, SimpleQueue | imports: io, multiprocessing, pickle | [torch multiprocessing queue.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "torch\\multiprocessing\\reductions.py",
    "summary": "A weak reference to a Storage. | classes: StorageWeakRef, SharedCache | functions: rebuild_event, reduce_event, rebuild_tensor, rebuild_meta_tensor, rebuild_cuda_tensor, reduce_tensor | imports: multiprocessing, threading, torch | [torch multiprocessing reductions.py]",
    "role": "src",
    "loc": 430
  },
  {
    "id": "torch\\multiprocessing\\spawn.py",
    "summary": "No description | classes: ProcessException, ProcessRaisedException, ProcessExitedException, ProcessContext, SpawnContext | functions: _wrap, start_processes, start_process, spawn | imports: multiprocessing, pickle, signal, tempfile | [torch multiprocessing spawn.py]",
    "role": "src",
    "loc": 249
  },
  {
    "id": "torch\\multiprocessing\\_atfork.py",
    "summary": "No description | functions: _register, wrapper, register_after_fork | imports: multiprocessing | [torch multiprocessing _atfork.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\multiprocessing\\__init__.py",
    "summary": "torch.multiprocessing is a wrapper around the native :mod:`multiprocessing` module. | functions: set_sharing_strategy, get_sharing_strategy, get_all_sharing_strategies, _set_thread_name, _get_thread_name | imports: multiprocessing, torch, reductions, spawn | [torch multiprocessing __init__.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\nested\\__init__.py",
    "summary": "Constructs a nested tensor preserving autograd history from a tensor or a list / tuple of | functions: as_nested_tensor, nested_tensor, narrow, nested_tensor_from_jagged, masked_select | imports: torch, _internal | [torch nested __init__.py]",
    "role": "src",
    "loc": 420
  },
  {
    "id": "torch\\nested\\_internal\\nested_int.py",
    "summary": "No description | classes: NestedIntNode | functions: _eq, _ge | imports: torch | [torch nested _internal nested_int.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "torch\\nested\\_internal\\nested_tensor.py",
    "summary": "No description | classes: NestedTensor, ViewBufferFromNested, ViewNestedFromBuffer | functions: get_tensor_symint, _get_sdpa_extreme_seqlen, _store_val_in_tensor, _load_val_from_tensor, _rebuild_njt, buffer_from_jagged | imports: torch, ops | [torch nested _internal nested_tensor.py]",
    "role": "src",
    "loc": 466
  },
  {
    "id": "torch\\nested\\_internal\\ops.py",
    "summary": "No description | functions: _outer_to_inner_dim, _wrap_jagged_dim, _wrap_jagged_dims, check_schema, check_fn, check_ragged_dim_same | imports: functools, operator, torch, nested_tensor | [torch nested _internal ops.py]",
    "role": "src",
    "loc": 1997
  },
  {
    "id": "torch\\nested\\_internal\\sdpa.py",
    "summary": "No description | functions: _validate_sdpa_input, _check_batch_size_nested, _check_head_dim_size_flash_nested, _check_for_seq_len_0_and_consistent_head_dim_nested_helper, _try_broadcast_param_size, _check_for_seq_len_0_nested | imports: torch, nested_tensor | [torch nested _internal sdpa.py]",
    "role": "src",
    "loc": 560
  },
  {
    "id": "torch\\nested\\_internal\\__init__.py",
    "summary": "Package initializer | [torch nested _internal __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\nn\\common_types.py",
    "summary": "No description | imports: torch | [torch nn common_types.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\nn\\cpp.py",
    "summary": "Functionality for Python <-> C++ frontend inter-op. | classes: OrderedDictWrapper, ModuleWrapper | imports: torch | [torch nn cpp.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\nn\\functional.py",
    "summary": "Functional interface. | functions: fractional_max_pool2d_with_indices, _fractional_max_pool2d, fractional_max_pool3d_with_indices, _fractional_max_pool3d, max_pool1d_with_indices, _max_pool1d | imports: importlib, torch, numpy | [torch nn functional.py]",
    "role": "src",
    "loc": 5345
  },
  {
    "id": "torch\\nn\\grad.py",
    "summary": "Gradient interface. | functions: conv1d_input, conv1d_weight, conv2d_input, conv2d_weight, conv3d_input, conv3d_weight | imports: torch | [torch nn grad.py]",
    "role": "src",
    "loc": 251
  },
  {
    "id": "torch\\nn\\init.py",
    "summary": "This file contains utilities for initializing neural network parameters. | functions: _no_grad_uniform_, _no_grad_normal_, _no_grad_trunc_normal_, norm_cdf, _no_grad_fill_, _no_grad_zero_ | imports: torch | [torch nn init.py]",
    "role": "src",
    "loc": 540
  },
  {
    "id": "torch\\nn\\parameter.py",
    "summary": "No description | classes: _ParameterMeta, Parameter, UninitializedTensorMixin, UninitializedParameter, _BufferMeta, Buffer | functions: is_lazy | imports: torch | [torch nn parameter.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\nn\\_reduction.py",
    "summary": "No description | functions: get_enum, legacy_get_string, legacy_get_enum | [torch nn _reduction.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\nn\\__init__.py",
    "summary": "Return a canonicalized dict of factory kwargs. | functions: factory_kwargs | imports: torch | [torch nn __init__.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\nn\\attention\\bias.py",
    "summary": "Defines bias subclasses that work with scaled_dot_product_attention | classes: CausalVariant, CausalBias | functions: causal_upper_left, causal_lower_right | imports: torch | [torch nn attention bias.py]",
    "role": "src",
    "loc": 284
  },
  {
    "id": "torch\\nn\\attention\\flex_attention.py",
    "summary": "This module implements the user facing API for flex_attention in PyTorch. | classes: _ModificationType, BlockMask | functions: _get_mod_type, _vmap_for_bhqkv, _identity, noop_mask, _ordered_to_dense, create_dense_one | imports: functools, inspect, operator, torch | [torch nn attention flex_attention",
    "role": "src",
    "loc": 1125
  },
  {
    "id": "torch\\nn\\attention\\_utils.py",
    "summary": "Defines utilities for interacting with scaled_dot_product_attention | functions: _input_requires_grad, _postprocess_flash_output, _calculate_scale, _supported_head_dim, _validate_sdpa_input | imports: torch | [torch nn attention _utils.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\nn\\attention\\__init__.py",
    "summary": "This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention | functions: _raise_kernel_warnings, _backend_from_string, _cur_sdpa_kernel_backends, _sdpa_kernel, sdpa_kernel, _sdpa_kernel_variadic | imports: torch | [torch nn attention __init_",
    "role": "src",
    "loc": 112
  },
  {
    "id": "torch\\nn\\attention\\experimental\\_paged_attention.py",
    "summary": "This module implements Paged Attention on top of flex_attention. | classes: PagedAttention | functions: _cdiv | imports: torch | [torch nn attention experimental _paged_attention.py]",
    "role": "src",
    "loc": 267
  },
  {
    "id": "torch\\nn\\attention\\experimental\\__init__.py",
    "summary": "Package initializer | [torch nn attention experimental __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\nn\\backends\\thnn.py",
    "summary": "No description | functions: _get_thnn_function_backend | [torch nn backends thnn.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\nn\\backends\\__init__.py",
    "summary": "Package initializer | [torch nn backends __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\nn\\intrinsic\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic __init__.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "torch\\nn\\intrinsic\\modules\\fused.py",
    "summary": "No description | imports: torch | [torch nn intrinsic modules fused.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\nn\\intrinsic\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic modules __init__.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\nn\\intrinsic\\qat\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic qat __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\nn\\intrinsic\\qat\\modules\\conv_fused.py",
    "summary": "Intrinsic QAT Modules. | imports: torch | [torch nn intrinsic qat modules conv_fused.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\nn\\intrinsic\\qat\\modules\\linear_fused.py",
    "summary": "Intrinsic QAT Modules. | imports: torch | [torch nn intrinsic qat modules linear_fused.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\nn\\intrinsic\\qat\\modules\\linear_relu.py",
    "summary": "Intrinsic QAT Modules. | imports: torch | [torch nn intrinsic qat modules linear_relu.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\nn\\intrinsic\\qat\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic qat modules __init__.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic quantized __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic quantized dynamic __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\dynamic\\modules\\linear_relu.py",
    "summary": "No description | imports: torch | [torch nn intrinsic quantized dynamic modules linear_relu.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\dynamic\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic quantized dynamic modules __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\modules\\bn_relu.py",
    "summary": "No description | imports: torch | [torch nn intrinsic quantized modules bn_relu.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\modules\\conv_relu.py",
    "summary": "No description | imports: torch | [torch nn intrinsic quantized modules conv_relu.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\modules\\linear_relu.py",
    "summary": "No description | imports: torch | [torch nn intrinsic quantized modules linear_relu.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\nn\\intrinsic\\quantized\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn intrinsic quantized modules __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\nn\\modules\\activation.py",
    "summary": "Thresholds each element of the input Tensor. | classes: Threshold, ReLU, RReLU, Hardtanh, ReLU6, Sigmoid | functions: _check_arg_device, _arg_requires_grad, _is_make_fx_tracing | imports: torch, linear, module | [torch nn modules activation.py]",
    "role": "src",
    "loc": 1313
  },
  {
    "id": "torch\\nn\\modules\\adaptive.py",
    "summary": "Efficient softmax approximation. | classes: AdaptiveLogSoftmaxWithLoss | imports: torch, container, linear, module | [torch nn modules adaptive.py]",
    "role": "src",
    "loc": 260
  },
  {
    "id": "torch\\nn\\modules\\batchnorm.py",
    "summary": "Common base of _InstanceNorm and _BatchNorm. | classes: _NormBase, _BatchNorm, _LazyNormBase, BatchNorm1d, LazyBatchNorm1d, BatchNorm2d | imports: torch, _functions, lazy, module | [torch nn modules batchnorm.py]",
    "role": "src",
    "loc": 736
  },
  {
    "id": "torch\\nn\\modules\\channelshuffle.py",
    "summary": "Divides and rearranges the channels in a tensor. | classes: ChannelShuffle | imports: torch, module | [torch nn modules channelshuffle.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\nn\\modules\\container.py",
    "summary": "No description | classes: Container, Sequential, ModuleList, ModuleDict, ParameterList, ParameterDict | functions: _addindent | imports: operator, typing_extensions, torch, module | [torch nn modules container.py]",
    "role": "src",
    "loc": 754
  },
  {
    "id": "torch\\nn\\modules\\conv.py",
    "summary": "No description | classes: _ConvNd, Conv1d, Conv2d, Conv3d, _ConvTransposeNd, ConvTranspose1d | imports: typing_extensions, torch, lazy, module | [torch nn modules conv.py]",
    "role": "src",
    "loc": 1550
  },
  {
    "id": "torch\\nn\\modules\\distance.py",
    "summary": "Computes the pairwise distance between input vectors, or between columns of input matrices. | classes: PairwiseDistance, CosineSimilarity | imports: torch, module | [torch nn modules distance.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "torch\\nn\\modules\\dropout.py",
    "summary": "No description | classes: _DropoutNd, Dropout, Dropout1d, Dropout2d, Dropout3d, AlphaDropout | imports: torch, module | [torch nn modules dropout.py]",
    "role": "src",
    "loc": 221
  },
  {
    "id": "torch\\nn\\modules\\flatten.py",
    "summary": "Flattens a contiguous range of dims into a tensor. | classes: Flatten, Unflatten | imports: torch, module | [torch nn modules flatten.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torch\\nn\\modules\\fold.py",
    "summary": "Combines an array of sliding local blocks into a large containing tensor. | classes: Fold, Unfold | imports: torch, module | [torch nn modules fold.py]",
    "role": "src",
    "loc": 251
  },
  {
    "id": "torch\\nn\\modules\\instancenorm.py",
    "summary": "No description | classes: _InstanceNorm, InstanceNorm1d, LazyInstanceNorm1d, InstanceNorm2d, LazyInstanceNorm2d, InstanceNorm3d | imports: torch, batchnorm | [torch nn modules instancenorm.py]",
    "role": "src",
    "loc": 376
  },
  {
    "id": "torch\\nn\\modules\\lazy.py",
    "summary": "This class is used to avoid errors with mypy checks for the attributes in a mixin. | classes: _LazyProtocol, LazyModuleMixin | imports: torch | [torch nn modules lazy.py]",
    "role": "src",
    "loc": 234
  },
  {
    "id": "torch\\nn\\modules\\linear.py",
    "summary": "A placeholder identity operator that is argument-insensitive. | classes: Identity, Linear, NonDynamicallyQuantizableLinear, Bilinear, LazyLinear | imports: torch, lazy, module | [torch nn modules linear.py]",
    "role": "src",
    "loc": 225
  },
  {
    "id": "torch\\nn\\modules\\loss.py",
    "summary": "No description | classes: _Loss, _WeightedLoss, L1Loss, NLLLoss, NLLLoss2d, PoissonNLLLoss | imports: typing_extensions, torch, distance, module | [torch nn modules loss.py]",
    "role": "src",
    "loc": 1673
  },
  {
    "id": "torch\\nn\\modules\\module.py",
    "summary": "No description | classes: _IncompatibleKeys, _WrappedHook, Module | functions: _addindent, register_module_buffer_registration_hook, register_module_module_registration_hook, register_module_parameter_registration_hook, register_module_forward_pre_hook, register_module_forward_hook | imports: functo",
    "role": "src",
    "loc": 2408
  },
  {
    "id": "torch\\nn\\modules\\normalization.py",
    "summary": "Applies local response normalization over an input signal. | classes: LocalResponseNorm, CrossMapLRN2d, LayerNorm, GroupNorm, RMSNorm | imports: numbers, torch, _functions, module | [torch nn modules normalization.py]",
    "role": "src",
    "loc": 328
  },
  {
    "id": "torch\\nn\\modules\\padding.py",
    "summary": "No description | classes: _CircularPadNd, CircularPad1d, CircularPad2d, CircularPad3d, _ConstantPadNd, ConstantPad1d | imports: torch, module, utils | [torch nn modules padding.py]",
    "role": "src",
    "loc": 614
  },
  {
    "id": "torch\\nn\\modules\\pixelshuffle.py",
    "summary": "Rearrange elements in a tensor according to an upscaling factor. | classes: PixelShuffle, PixelUnshuffle | imports: torch, module | [torch nn modules pixelshuffle.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\nn\\modules\\pooling.py",
    "summary": "No description | classes: _MaxPoolNd, MaxPool1d, MaxPool2d, MaxPool3d, _MaxUnpoolNd, MaxUnpool1d | imports: torch, module, utils | [torch nn modules pooling.py]",
    "role": "src",
    "loc": 1173
  },
  {
    "id": "torch\\nn\\modules\\rnn.py",
    "summary": "Base class for RNN modules (RNN, LSTM, GRU). | classes: RNNBase, RNN, LSTM, GRU, RNNCellBase, RNNCell | functions: _apply_permutation, apply_permutation | imports: numbers, weakref, typing_extensions, torch | [torch nn modules rnn.py]",
    "role": "src",
    "loc": 1541
  },
  {
    "id": "torch\\nn\\modules\\sparse.py",
    "summary": "A simple lookup table that stores embeddings of a fixed dictionary and size. | classes: Embedding, EmbeddingBag | imports: torch, module | [torch nn modules sparse.py]",
    "role": "src",
    "loc": 484
  },
  {
    "id": "torch\\nn\\modules\\transformer.py",
    "summary": "A transformer model. | classes: Transformer, TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer | functions: _generate_square_subsequent_mask, _get_seq_len, _get_clones, _get_activation_fn, _detect_is_causal_mask | imports: copy, torch, activation, container | [",
    "role": "src",
    "loc": 1048
  },
  {
    "id": "torch\\nn\\modules\\upsampling.py",
    "summary": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. | classes: Upsample, UpsamplingNearest2d, UpsamplingBilinear2d | imports: torch, module | [torch nn modules upsampling.py]",
    "role": "src",
    "loc": 236
  },
  {
    "id": "torch\\nn\\modules\\utils.py",
    "summary": "No description | functions: _ntuple, parse, _reverse_repeat_tuple, _list_with_default, consume_prefix_in_state_dict_if_present | imports: torch | [torch nn modules utils.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\nn\\modules\\_functions.py",
    "summary": "No description | classes: SyncBatchNorm, CrossMapLRN2d, BackwardHookFunction | imports: torch | [torch nn modules _functions.py]",
    "role": "src",
    "loc": 230
  },
  {
    "id": "torch\\nn\\modules\\__init__.py",
    "summary": "Package initializer | imports: module, linear, activation, adaptive | [torch nn modules __init__.py]",
    "role": "src",
    "loc": 330
  },
  {
    "id": "torch\\nn\\parallel\\comm.py",
    "summary": "Broadcasts a tensor to specified GPU devices. | functions: broadcast, broadcast_coalesced, reduce_add, reduce_add_coalesced, scatter, gather | imports: torch | [torch nn parallel comm.py]",
    "role": "src",
    "loc": 217
  },
  {
    "id": "torch\\nn\\parallel\\data_parallel.py",
    "summary": "Implements data parallelism at the module level. | classes: DataParallel | functions: _check_balance, warn_imbalance, data_parallel | imports: operator, torch | [torch nn parallel data_parallel.py]",
    "role": "src",
    "loc": 225
  },
  {
    "id": "torch\\nn\\parallel\\distributed.py",
    "summary": "This configures DDP-native mixed precision training. | classes: _MixedPrecision, _BufferCommHookLocation, _BufferCommHook, _DDPSink, _DDPJoinHook, DistributedDataParallel | functions: _cast_buffers, _setup_mixed_precision_params, _tree_flatten_with_rref, _tree_unflatten_with_rref, _find_tensors, _du",
    "role": "src",
    "loc": 1843
  },
  {
    "id": "torch\\nn\\parallel\\parallel_apply.py",
    "summary": "No description | functions: get_a_var, parallel_apply, _worker | imports: threading, torch | [torch nn parallel parallel_apply.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\nn\\parallel\\replicate.py",
    "summary": "No description | functions: _is_script_module, _is_script_method, _init_script_module, _is_jit_enabled, _replicatable_module, descendant_modules | imports: typing_extensions, torch | [torch nn parallel replicate.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "torch\\nn\\parallel\\scatter_gather.py",
    "summary": "No description | functions: is_namedtuple, _is_namedtuple, scatter, scatter_map, scatter_kwargs, gather | imports: typing_extensions, torch | [torch nn parallel scatter_gather.py]",
    "role": "src",
    "loc": 101
  },
  {
    "id": "torch\\nn\\parallel\\_functions.py",
    "summary": "Get a background stream for copying between CPU and target device. | classes: Broadcast, ReduceAddCoalesced, Gather, Scatter | functions: _get_stream | imports: torch | [torch nn parallel _functions.py]",
    "role": "src",
    "loc": 113
  },
  {
    "id": "torch\\nn\\parallel\\__init__.py",
    "summary": "Package initializer | classes: DistributedDataParallelCPU | imports: typing_extensions, torch | [torch nn parallel __init__.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\nn\\qat\\__init__.py",
    "summary": "QAT Dynamic Modules. | imports: torch | [torch nn qat __init__.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\nn\\qat\\dynamic\\__init__.py",
    "summary": "QAT Dynamic Modules. | imports: torch | [torch nn qat dynamic __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\nn\\qat\\dynamic\\modules\\linear.py",
    "summary": "QAT Modules. | imports: torch | [torch nn qat dynamic modules linear.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\qat\\dynamic\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn qat dynamic modules __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\nn\\qat\\modules\\conv.py",
    "summary": "QAT Modules. | imports: torch | [torch nn qat modules conv.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\qat\\modules\\embedding_ops.py",
    "summary": "QAT Modules. | imports: torch | [torch nn qat modules embedding_ops.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\nn\\qat\\modules\\linear.py",
    "summary": "QAT Modules. | imports: torch | [torch nn qat modules linear.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\qat\\modules\\__init__.py",
    "summary": "QAT Modules. | imports: torch | [torch nn qat modules __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\nn\\quantizable\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn quantizable __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\nn\\quantizable\\modules\\activation.py",
    "summary": "Quantizable Modules. | imports: torch | [torch nn quantizable modules activation.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\quantizable\\modules\\rnn.py",
    "summary": "Quantizable Modules. | imports: torch | [torch nn quantizable modules rnn.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\quantizable\\modules\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn quantizable modules __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\nn\\quantized\\functional.py",
    "summary": "nn.quantized.functional. | imports: torch | [torch nn quantized functional.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\nn\\quantized\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn quantized __init__.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torch\\nn\\quantized\\dynamic\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn quantized dynamic __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\nn\\quantized\\dynamic\\modules\\conv.py",
    "summary": "Quantized Dynamic Modules. | imports: torch | [torch nn quantized dynamic modules conv.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\nn\\quantized\\dynamic\\modules\\linear.py",
    "summary": "Quantized Dynamic Modules. | imports: torch | [torch nn quantized dynamic modules linear.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\quantized\\dynamic\\modules\\rnn.py",
    "summary": "Quantized Dynamic Modules. | imports: torch | [torch nn quantized dynamic modules rnn.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\nn\\quantized\\dynamic\\modules\\__init__.py",
    "summary": "Quantized Dynamic Modules. | imports: torch | [torch nn quantized dynamic modules __init__.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\nn\\quantized\\modules\\activation.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules activation.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\nn\\quantized\\modules\\batchnorm.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules batchnorm.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\quantized\\modules\\conv.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules conv.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\nn\\quantized\\modules\\dropout.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules dropout.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\nn\\quantized\\modules\\embedding_ops.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules embedding_ops.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\nn\\quantized\\modules\\functional_modules.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules functional_modules.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\nn\\quantized\\modules\\linear.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules linear.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\nn\\quantized\\modules\\normalization.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules normalization.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\nn\\quantized\\modules\\rnn.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules rnn.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\nn\\quantized\\modules\\utils.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules utils.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\nn\\quantized\\modules\\__init__.py",
    "summary": "Quantized Modules. | imports: torch | [torch nn quantized modules __init__.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch nn quantized _reference __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\conv.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules conv.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\linear.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules linear.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\rnn.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules rnn.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\sparse.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules sparse.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\utils.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules utils.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\nn\\quantized\\_reference\\modules\\__init__.py",
    "summary": "Quantized Reference Modules. | imports: torch | [torch nn quantized _reference modules __init__.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\nn\\utils\\clip_grad.py",
    "summary": "This wrapper is needed to avoid a circular import when using @torch.no_grad on the exposed functions | functions: _no_grad, _no_grad_wrapper, _get_total_norm, _clip_grads_with_norm_, clip_grad_norm_, clip_grad_norm | imports: functools, typing_extensions, torch | [torch nn utils clip_grad.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\nn\\utils\\convert_parameters.py",
    "summary": "Flatten an iterable of parameters into a single vector. | functions: parameters_to_vector, vector_to_parameters, _check_param_device | imports: torch | [torch nn utils convert_parameters.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\nn\\utils\\fusion.py",
    "summary": "Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. | functions: fuse_conv_bn_eval, fuse_conv_bn_weights, fuse_linear_bn_eval, fuse_linear_bn_weights | imports: copy, torch | [torch nn utils fusion.py]",
    "role": "src",
    "loc": 157
  },
  {
    "id": "torch\\nn\\utils\\init.py",
    "summary": "Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. | functions: skip_init | imports: inspect, torch | [torch nn utils init.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\nn\\utils\\memory_format.py",
    "summary": "Convert ``memory_format`` of ``nn.Conv2d.weight`` to ``memory_format``. | functions: convert_conv2d_weight_memory_format, convert_conv3d_weight_memory_format | imports: torch | [torch nn utils memory_format.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\nn\\utils\\parametrizations.py",
    "summary": "No description | classes: _OrthMaps, _Orthogonal, _WeightNorm, _SpectralNorm | functions: _is_orthogonal, _make_orthogonal, orthogonal, weight_norm, _weight_norm_compat_hook, spectral_norm | imports: torch | [torch nn utils parametrizations.py]",
    "role": "src",
    "loc": 438
  },
  {
    "id": "torch\\nn\\utils\\parametrize.py",
    "summary": "A sequential container that holds and manages the original parameters or buffers of a parametrized :class:`torch.nn.Modu | classes: ParametrizationList | functions: cached, _register_parameter_or_buffer, _maybe_set, _inject_new_class, default_deepcopy, getstate | imports: copyreg, copy, torch | [tor",
    "role": "src",
    "loc": 612
  },
  {
    "id": "torch\\nn\\utils\\prune.py",
    "summary": "Pruning methods. | classes: BasePruningMethod, PruningContainer, Identity, RandomUnstructured, L1Unstructured, RandomStructured | functions: identity, random_unstructured, l1_unstructured, random_structured, ln_structured, global_unstructured | imports: numbers, abc, torch | [torch nn utils prune.py",
    "role": "src",
    "loc": 989
  },
  {
    "id": "torch\\nn\\utils\\rnn.py",
    "summary": "No description | classes: PackedSequence_, PackedSequence | functions: bind, _packed_sequence_init_args, _packed_sequence_init, invert_permutation, pack_padded_sequence, pad_packed_sequence | imports: typing_extensions, torch | [torch nn utils rnn.py]",
    "role": "src",
    "loc": 472
  },
  {
    "id": "torch\\nn\\utils\\spectral_norm.py",
    "summary": "Spectral Normalization from https://arxiv.org/abs/1802.05957. | classes: SpectralNorm, SpectralNormLoadStateDictPreHook, SpectralNormStateDictHook | functions: spectral_norm, remove_spectral_norm | imports: torch | [torch nn utils spectral_norm.py]",
    "role": "src",
    "loc": 251
  },
  {
    "id": "torch\\nn\\utils\\stateless.py",
    "summary": "Unties all tied tensors in the module to parameters_and_buffers. | functions: _untie_named_tensors_map, _reparametrize_module, functional_call, _functional_call | imports: typing_extensions, torch | [torch nn utils stateless.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "torch\\nn\\utils\\weight_norm.py",
    "summary": "Weight Normalization from https://arxiv.org/abs/1602.07868. | classes: WeightNorm | functions: weight_norm, remove_weight_norm | imports: typing_extensions, torch | [torch nn utils weight_norm.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\nn\\utils\\_deprecation_utils.py",
    "summary": "Import utility to lazily import deprecated packages / modules / functional. | functions: lazy_deprecated_import, getattr_dunder | imports: importlib | [torch nn utils _deprecation_utils.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\nn\\utils\\_named_member_accessor.py",
    "summary": "A class that provides a way to access the submodules and parameters/buffers of a module. | classes: NamedMemberAccessor | functions: set_tensor, swap_tensor, swap_submodule | imports: torch | [torch nn utils _named_member_accessor.py]",
    "role": "src",
    "loc": 312
  },
  {
    "id": "torch\\nn\\utils\\_per_sample_grad.py",
    "summary": "Return a forward function for a module, populating grad_sample with per sample gradients on backward invocation. | functions: call_for_per_sample_grads, maybe_build_expanded_weight, compute_batch_size, wrapper | imports: functools, torch | [torch nn utils _per_sample_grad.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "torch\\nn\\utils\\__init__.py",
    "summary": "Package initializer | imports: clip_grad, convert_parameters, fusion, init | [torch nn utils __init__.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\conv_expanded_weights.py",
    "summary": "No description | classes: ConvPerSampleGrad | imports: torch, conv_utils, expanded_weights_impl, expanded_weights_utils | [torch nn utils _expanded_weights conv_expanded_weights.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\conv_utils.py",
    "summary": "No description | functions: conv_picker, conv_args_and_kwargs, conv_normalizer, conv_input_for_string_padding, int_padding_for_string_padding, get_dilation | imports: numpy, torch, expanded_weights_utils | [torch nn utils _expanded_weights conv_utils.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\embedding_expanded_weights.py",
    "summary": "No description | classes: EmbeddingPerSampleGrad | imports: torch, expanded_weights_impl, expanded_weights_utils | [torch nn utils _expanded_weights embedding_expanded_weights.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\expanded_weights_impl.py",
    "summary": "No description | classes: ExpandedWeight | functions: batch_second, set_batch_second, reset_batch_first, allow_smaller_batches, allow, reset | imports: functools, torch | [torch nn utils _expanded_weights expanded_weights_impl.py]",
    "role": "src",
    "loc": 128
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\expanded_weights_utils.py",
    "summary": "No description | functions: is_batch_first, standard_kwargs, forward_helper, _check_and_unexpand_args, maybe_scale_by_batch_size, set_grad_sample_if_exists | imports: torch, expanded_weights_impl | [torch nn utils _expanded_weights expanded_weights_utils.py]",
    "role": "src",
    "loc": 156
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\group_norm_expanded_weights.py",
    "summary": "No description | classes: GroupNormPerSampleGrad | imports: operator, functools, torch, expanded_weights_impl | [torch nn utils _expanded_weights group_norm_expanded_weights.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\instance_norm_expanded_weights.py",
    "summary": "No description | classes: InstanceNormPerSampleGrad | imports: functools, torch, expanded_weights_impl, expanded_weights_utils | [torch nn utils _expanded_weights instance_norm_expanded_weights.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\layer_norm_expanded_weights.py",
    "summary": "No description | classes: LayerNormPerSampleGrad | imports: torch, expanded_weights_impl, expanded_weights_utils | [torch nn utils _expanded_weights layer_norm_expanded_weights.py]",
    "role": "src",
    "loc": 74
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\linear_expanded_weights.py",
    "summary": "No description | classes: LinearPerSampleGrad | imports: torch, expanded_weights_impl, expanded_weights_utils | [torch nn utils _expanded_weights linear_expanded_weights.py]",
    "role": "src",
    "loc": 52
  },
  {
    "id": "torch\\nn\\utils\\_expanded_weights\\__init__.py",
    "summary": "Package initializer | imports: conv_expanded_weights, embedding_expanded_weights, expanded_weights_impl, group_norm_expanded_weights | [torch nn utils _expanded_weights __init__.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\onnx\\errors.py",
    "summary": "ONNX exporter exceptions. | classes: OnnxExporterWarning, OnnxExporterError, UnsupportedOperatorError, SymbolicValueError | imports: textwrap, torch | [torch onnx errors.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "torch\\onnx\\operators.py",
    "summary": "This file provides a location for operators that help exporting models via onnx. | imports: torch | [torch onnx operators.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\onnx\\symbolic_caffe2.py",
    "summary": "No description | functions: register_quantized_ops, _permute_helper, nchw2nhwc, nhwc2nchw, linear_prepack, linear | imports: importlib, inspect, torch | [torch onnx symbolic_caffe2.py]",
    "role": "src",
    "loc": 301
  },
  {
    "id": "torch\\onnx\\symbolic_helper.py",
    "summary": "No description | functions: _parse_arg, _node_get, _is_onnx_constant, _maybe_get_const, _maybe_get_scalar, _get_const | imports: functools, inspect, typing_extensions, torch | [torch onnx symbolic_helper.py]",
    "role": "src",
    "loc": 1848
  },
  {
    "id": "torch\\onnx\\symbolic_opset10.py",
    "summary": "No description | functions: div, _div_rounding_mode, _floor_divide, sort, topk, _aten_max_pool_onnx | imports: functools, torch | [torch onnx symbolic_opset10.py]",
    "role": "src",
    "loc": 973
  },
  {
    "id": "torch\\onnx\\symbolic_opset11.py",
    "summary": "This file exports ONNX ops for opset 11. | functions: hardtanh, clamp, _cast_if_not_none, clamp_min, clamp_max, relu6 | imports: functools, torch | [torch onnx symbolic_opset11.py]",
    "role": "src",
    "loc": 1135
  },
  {
    "id": "torch\\onnx\\symbolic_opset12.py",
    "summary": "No description | functions: _einsum_helper, einsum, outer, _dropout_returns_masked_input_and_mask, dropout, native_dropout | imports: functools, torch | [torch onnx symbolic_opset12.py]",
    "role": "src",
    "loc": 368
  },
  {
    "id": "torch\\onnx\\symbolic_opset13.py",
    "summary": "No description | functions: softmax, log_softmax, frobenius_norm, split, split_with_sizes, unsafe_split | imports: functools, torch | [torch onnx symbolic_opset13.py]",
    "role": "src",
    "loc": 873
  },
  {
    "id": "torch\\onnx\\symbolic_opset14.py",
    "summary": "This file exports ONNX ops for opset 14. | functions: hardswish, tril, triu, reshape, batch_norm, quantized_hardswish | imports: functools, torch | [torch onnx symbolic_opset14.py]",
    "role": "src",
    "loc": 221
  },
  {
    "id": "torch\\onnx\\symbolic_opset15.py",
    "summary": "This file exports ONNX ops for opset 15. | functions: aten__is_, aten__isnot_, bernoulli, prim_unchecked_cast | imports: functools, torch | [torch onnx symbolic_opset15.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\onnx\\symbolic_opset16.py",
    "summary": "This file exports ONNX ops for opset 16. | functions: grid_sampler, scatter_add, scatter_reduce | imports: functools, torch | [torch onnx symbolic_opset16.py]",
    "role": "src",
    "loc": 145
  },
  {
    "id": "torch\\onnx\\symbolic_opset17.py",
    "summary": "This file exports ONNX ops for opset 17. | functions: layer_norm, quantized_layer_norm, _compute_edge_sizes, stft | imports: functools, torch | [torch onnx symbolic_opset17.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\onnx\\symbolic_opset18.py",
    "summary": "This file exports ONNX ops for opset 18. | functions: __and_, col2im, _reduce_with_dtype, _native_layer_norm, _glu, max | imports: functools, torch | [torch onnx symbolic_opset18.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "torch\\onnx\\symbolic_opset19.py",
    "summary": "This file exports ONNX ops for opset 19. | [torch onnx symbolic_opset19.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\onnx\\symbolic_opset20.py",
    "summary": "This file exports ONNX ops for opset 20. | functions: convert_grid_sample_mode, _grid_sampler, _affine_grid_generator, gelu | imports: functools, torch | [torch onnx symbolic_opset20.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "torch\\onnx\\symbolic_opset7.py",
    "summary": "Note [ONNX operators that are added/updated from opset 7 to opset 8] | functions: max, min | imports: functools, torch | [torch onnx symbolic_opset7.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\onnx\\symbolic_opset8.py",
    "summary": "Note [ONNX operators that are added/updated from opset 8 to opset 9] | functions: _interpolate, symbolic_fn, __interpolate, _try_cast_integer_to_float, _cast_to_type, _comparison_operator | imports: functools, torch | [torch onnx symbolic_opset8.py]",
    "role": "src",
    "loc": 390
  },
  {
    "id": "torch\\onnx\\symbolic_opset9.py",
    "summary": "This file exports ONNX ops for opset 9. | functions: _export, wrapper, unused, _shape_as_tensor, _reshape_from_tensor, reshape | imports: builtins, functools, typing_extensions, torch | [torch onnx symbolic_opset9.py]",
    "role": "src",
    "loc": 5368
  },
  {
    "id": "torch\\onnx\\utils.py",
    "summary": "Functions to export models into the ONNX IR format. | functions: is_in_onnx_export, select_model_mode_for_export, disable_apex_o2_state_dict_hook, setup_onnx_logging, exporter_context, _get_torch_export_args | imports: copy, inspect, typing_extensions, torch | [torch onnx utils.py]",
    "role": "src",
    "loc": 1470
  },
  {
    "id": "torch\\onnx\\verification.py",
    "summary": "Functions to verify exported ONNX model is functionally equivalent to original PyTorch model. | classes: OnnxBackend, VerificationOptions, _GraphDiff, GraphInfoPrettyPrinter, OnnxTestCaseRepro, GraphInfo | functions: _flatten_tuples, _to_numpy, _inline_flatten_list, _unpack_to_numpy, _run_onnx, _ort",
    "role": "src",
    "loc": 1527
  },
  {
    "id": "torch\\onnx\\_constants.py",
    "summary": "Constant values used in ONNX. | [torch onnx _constants.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\onnx\\_experimental.py",
    "summary": "Experimental classes and functions used by ONNX export. | classes: ExportOptions | imports: dataclasses, torch | [torch onnx _experimental.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\onnx\\_flags.py",
    "summary": "Internal feature flags for torch.onnx. | functions: _load_boolean_flag | [torch onnx _flags.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\onnx\\_globals.py",
    "summary": "Globals used internally by the ONNX exporter. | classes: _InternalGlobals | imports: torch | [torch onnx _globals.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\onnx\\_onnx_supported_ops.py",
    "summary": "No description | classes: _TorchSchema | functions: _symbolic_argument_count, all_forward_schemas, all_symbolics_schemas | imports: inspect, torch | [torch onnx _onnx_supported_ops.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\onnx\\_type_utils.py",
    "summary": "Utilities for converting and operating on ONNX, JIT and torch types. | classes: JitScalarType | functions: valid_scalar_name, valid_torch_name | imports: torch | [torch onnx _type_utils.py]",
    "role": "src",
    "loc": 324
  },
  {
    "id": "torch\\onnx\\__init__.py",
    "summary": "Exports a model into ONNX format. | functions: export, dynamo_export, _to_dynamic_shape | imports: typing_extensions, torch, _internal, _type_utils | [torch onnx __init__.py]",
    "role": "src",
    "loc": 435
  },
  {
    "id": "torch\\onnx\\_internal\\io_adapter.py",
    "summary": "A protocol that defines a step in the input adapting process. | classes: InputAdaptStep, InputAdapter, OutputAdaptStep, OutputAdapter, _DummyLeaf, BindInputStep | functions: _replace_list_with_tuple, replace_list_with_tuple, _open_top_level_sequence_if_single_element, _assert_identical_pytree_spec |",
    "role": "src",
    "loc": 492
  },
  {
    "id": "torch\\onnx\\_internal\\jit_utils.py",
    "summary": "Utilities for manipulating the torch.Graph object and the torchscript. | classes: GraphContext | functions: add_op_with_blocks, _add_op, _const_if_tensor, _create_node, _is_onnx_list, _scalar | imports: dataclasses, torch | [torch onnx _internal jit_utils.py]",
    "role": "src",
    "loc": 289
  },
  {
    "id": "torch\\onnx\\_internal\\onnxruntime.py",
    "summary": "Operator support for ONNXRuntime backend. | classes: OrtOperatorSupport, OrtExecutionInfoPerSession, OrtExecutionInfoForAllGraphModules, OrtBackendOptions, OrtBackend | functions: is_onnxrt_backend_supported, _dump_onnx_model, _infer_default_eps, _nvtx_range_push, _nvtx_range_pop, _get_ort_device_ty",
    "role": "src",
    "loc": 837
  },
  {
    "id": "torch\\onnx\\_internal\\onnx_proto_utils.py",
    "summary": "Utilities for manipulating the onnx and onnx-script dependencies and ONNX proto. | functions: export_as_test_case, load_test_case, export_data, _export_file, _add_onnxscript_fn, _find_onnxscript_op | imports: glob, shutil, torch, io | [torch onnx _internal onnx_proto_utils.py]",
    "role": "src",
    "loc": 199
  },
  {
    "id": "torch\\onnx\\_internal\\registration.py",
    "summary": "Module for handling symbolic function registration. | classes: OverrideDict, _SymbolicFunctionGroup, SymbolicRegistry | functions: _dispatch_opset_version, onnx_symbolic, wrapper, custom_onnx_symbolic | imports: torch | [torch onnx _internal registration.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "torch\\onnx\\_internal\\_exporter_legacy.py",
    "summary": "A dataclass used to store context for model export using FakeTensor. | classes: ONNXFakeContext, OnnxRegistry, ExportOptions, ResolvedExportOptions, ONNXRuntimeOptions, FXGraphExtractor | functions: enable_fake_mode, _assert_dependencies, missing_package, missing_opset, dynamo_export, common_pre_exp",
    "role": "src",
    "loc": 690
  },
  {
    "id": "torch\\onnx\\_internal\\_lazy_import.py",
    "summary": "Utility to lazily import modules. | classes: _LazyModule | imports: importlib, onnx, onnxscript | [torch onnx _internal _lazy_import.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\onnx\\_internal\\__init__.py",
    "summary": "Package initializer | [torch onnx _internal __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\_diagnostic.py",
    "summary": "Diagnostic components for TorchScript based ONNX export, i.e. `torch.onnx.export`. | classes: TorchScriptOnnxExportDiagnostic, ExportDiagnosticEngine | functions: _cpp_call_stack, create_export_diagnostic_context, diagnose, export_context | imports: gzip, torch | [torch onnx _internal diagnostics _d",
    "role": "src",
    "loc": 170
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\_rules.py",
    "summary": "GENERATED CODE - DO NOT EDIT DIRECTLY | classes: _NodeMissingOnnxShapeInference, _MissingCustomSymbolicFunction, _MissingStandardSymbolicFunction, _OperatorSupportedInNewerOpsetVersion, _FxGraphToOnnx, _FxNodeToOnnx | imports: dataclasses, torch | [torch onnx _internal diagnostics _rules.py]",
    "role": "src",
    "loc": 535
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\__init__.py",
    "summary": "Package initializer | imports: _diagnostic, _rules, infra | [torch onnx _internal diagnostics __init__.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\context.py",
    "summary": "A diagnostic context based on SARIF. | classes: Diagnostic, RuntimeErrorWithDiagnostic, DiagnosticContext | imports: dataclasses, gzip, typing_extensions, torch | [torch onnx _internal diagnostics infra context.py]",
    "role": "src",
    "loc": 335
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\decorator.py",
    "summary": "No description | functions: format_message_in_text, format_exception_in_markdown, format_function_signature_in_markdown, format_return_values_in_markdown, diagnose_call, decorator | imports: functools, traceback, torch | [torch onnx _internal diagnostics infra decorator.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\formatter.py",
    "summary": "No description | functions: lazy_format_exception, snake_case_to_camel_case, camel_case_to_snake_case, kebab_case_to_snake_case, _convert_key, sarif_to_json | imports: dataclasses, json, traceback, torch | [torch onnx _internal diagnostics infra formatter.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\utils.py",
    "summary": "Returns a StackFrame for the given traceback.FrameSummary. | functions: python_frame, python_call_stack, _function_source_info, function_location, function_state | imports: functools, inspect, traceback, torch | [torch onnx _internal diagnostics infra utils.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\_infra.py",
    "summary": "This file defines an additional layer of abstraction on top of the SARIF OM. | classes: Level, Tag, PatchedPropertyBag, Rule, Location, StackFrame | imports: dataclasses, torch | [torch onnx _internal diagnostics infra _infra.py]",
    "role": "src",
    "loc": 226
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\__init__.py",
    "summary": "Package initializer | imports: _infra, context | [torch onnx _internal diagnostics infra __init__.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\version.py",
    "summary": "No description | [torch onnx _internal diagnostics infra sarif version.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_address.py",
    "summary": "A physical or virtual address, or a range of addresses, in an 'addressable region' (memory or a binary file). | classes: Address | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _address.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_artifact.py",
    "summary": "A single artifact. In some cases, this artifact might be nested within another artifact. | classes: Artifact | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _artifact.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_artifact_change.py",
    "summary": "A change to a single artifact. | classes: ArtifactChange | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _artifact_change.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_artifact_content.py",
    "summary": "Represents the contents of an artifact. | classes: ArtifactContent | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _artifact_content.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_artifact_location.py",
    "summary": "Specifies the location of an artifact. | classes: ArtifactLocation | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _artifact_location.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_attachment.py",
    "summary": "An artifact relevant to a result. | classes: Attachment | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _attachment.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_code_flow.py",
    "summary": "A set of threadFlows which together describe a pattern of code execution relevant to detecting a result. | classes: CodeFlow | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _code_flow.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_configuration_override.py",
    "summary": "Information about how a specific rule or notification was reconfigured at runtime. | classes: ConfigurationOverride | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _configuration_override.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_conversion.py",
    "summary": "Describes how a converter transformed the output of a static analysis tool from the analysis tool's native output format | classes: Conversion | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _conversion.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_edge.py",
    "summary": "Represents a directed edge in a graph. | classes: Edge | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _edge.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_edge_traversal.py",
    "summary": "Represents the traversal of a single edge during a graph traversal. | classes: EdgeTraversal | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _edge_traversal.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_exception.py",
    "summary": "Describes a runtime exception encountered during the execution of an analysis tool. | classes: Exception | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _exception.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_external_properties.py",
    "summary": "The top-level element of an external property file. | classes: ExternalProperties | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _external_properties.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_external_property_file_reference.py",
    "summary": "Contains information that enables a SARIF consumer to locate the external property file that contains the value of an ex | classes: ExternalPropertyFileReference | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _external_property_file_reference.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_external_property_file_references.py",
    "summary": "References to external property files that should be inlined with the content of a root log file. | classes: ExternalPropertyFileReferences | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _external_property_file_references.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_fix.py",
    "summary": "A proposed fix for the problem represented by a result object. A fix specifies a set of artifacts to modify. For each ar | classes: Fix | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _fix.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_graph.py",
    "summary": "A network of nodes and directed edges that describes some aspect of the structure of the code (for example, a call graph | classes: Graph | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _graph.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_graph_traversal.py",
    "summary": "Represents a path through a graph. | classes: GraphTraversal | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _graph_traversal.py]",
    "role": "src",
    "loc": 32
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_invocation.py",
    "summary": "The runtime environment of the analysis tool run. | classes: Invocation | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _invocation.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_location.py",
    "summary": "A location within a programming artifact. | classes: Location | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _location.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_location_relationship.py",
    "summary": "Information about the relation of one location to another. | classes: LocationRelationship | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _location_relationship.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_logical_location.py",
    "summary": "A logical location of a construct that produced a result. | classes: LogicalLocation | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _logical_location.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_message.py",
    "summary": "Encapsulates a message intended to be read by the end user. | classes: Message | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _message.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_multiformat_message_string.py",
    "summary": "A message string or message format string rendered in multiple formats. | classes: MultiformatMessageString | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _multiformat_message_string.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_node.py",
    "summary": "Represents a node in a graph. | classes: Node | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _node.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_notification.py",
    "summary": "Describes a condition relevant to the tool itself, as opposed to being relevant to a target being analyzed by the tool. | classes: Notification | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _notification.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_physical_location.py",
    "summary": "A physical location relevant to a result. Specifies a reference to a programming artifact together with a range of bytes | classes: PhysicalLocation | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _physical_location.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_property_bag.py",
    "summary": "Key/value pairs that provide additional information about the object. | classes: PropertyBag | imports: dataclasses | [torch onnx _internal diagnostics infra sarif _property_bag.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_rectangle.py",
    "summary": "An area within an image. | classes: Rectangle | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _rectangle.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_region.py",
    "summary": "A region within an artifact where a result was detected. | classes: Region | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _region.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_replacement.py",
    "summary": "The replacement of a single region of an artifact. | classes: Replacement | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _replacement.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_reporting_configuration.py",
    "summary": "Information about a rule or notification that can be configured at runtime. | classes: ReportingConfiguration | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _reporting_configuration.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_reporting_descriptor.py",
    "summary": "Metadata that describes a specific report produced by the tool, as part of the analysis it provides or its runtime repor | classes: ReportingDescriptor | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _reporting_descriptor.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_reporting_descriptor_reference.py",
    "summary": "Information about how to locate a relevant reporting descriptor. | classes: ReportingDescriptorReference | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _reporting_descriptor_reference.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_reporting_descriptor_relationship.py",
    "summary": "Information about the relation of one reporting descriptor to another. | classes: ReportingDescriptorRelationship | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _reporting_descriptor_relationship.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_result.py",
    "summary": "A result produced by an analysis tool. | classes: Result | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _result.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_result_provenance.py",
    "summary": "Contains information about how and when a result was detected. | classes: ResultProvenance | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _result_provenance.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_run.py",
    "summary": "Describes a single run of an analysis tool, and contains the reported output of that run. | classes: Run | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _run.py]",
    "role": "src",
    "loc": 123
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_run_automation_details.py",
    "summary": "Information that describes a run's identity and role within an engineering system process. | classes: RunAutomationDetails | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _run_automation_details.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_sarif_log.py",
    "summary": "Static Analysis Results Format (SARIF) Version 2.1.0 JSON Schema: a standard format for the output of static analysis to | classes: SarifLog | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _sarif_log.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_special_locations.py",
    "summary": "Defines locations of special significance to SARIF consumers. | classes: SpecialLocations | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _special_locations.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_stack.py",
    "summary": "A call stack that is relevant to a result. | classes: Stack | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _stack.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_stack_frame.py",
    "summary": "A function call within a stack trace. | classes: StackFrame | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _stack_frame.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_suppression.py",
    "summary": "A suppression that is relevant to a result. | classes: Suppression | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _suppression.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_thread_flow.py",
    "summary": "Describes a sequence of code locations that specify a path through a single thread of execution such as an operating sys | classes: ThreadFlow | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _thread_flow.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_thread_flow_location.py",
    "summary": "A location visited by an analysis tool while simulating or monitoring the execution of a program. | classes: ThreadFlowLocation | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _thread_flow_location.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_tool.py",
    "summary": "The analysis tool that was run. | classes: Tool | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _tool.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_tool_component.py",
    "summary": "A component, such as a plug-in or the driver, of the analysis tool that was run. | classes: ToolComponent | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _tool_component.py]",
    "role": "src",
    "loc": 112
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_tool_component_reference.py",
    "summary": "Identifies a particular toolComponent object, either the driver or an extension. | classes: ToolComponentReference | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _tool_component_reference.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_translation_metadata.py",
    "summary": "Provides additional metadata related to translation. | classes: TranslationMetadata | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _translation_metadata.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_version_control_details.py",
    "summary": "Specifies the information necessary to retrieve a desired revision from a version control system. | classes: VersionControlDetails | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _version_control_details.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_web_request.py",
    "summary": "Describes an HTTP request. | classes: WebRequest | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _web_request.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_web_response.py",
    "summary": "Describes the response to an HTTP request. | classes: WebResponse | imports: dataclasses, torch | [torch onnx _internal diagnostics infra sarif _web_response.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch onnx _internal diagnostics infra sarif __init__.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_analysis.py",
    "summary": "Compatibility analyzer for PyTorch models. | classes: ModelInfo | functions: _count_weights, _format_model_info, _get_io_specs, _count_fx_targets, analyze, compare_ops | imports: dataclasses, textwrap, traceback, torch | [torch onnx _internal exporter _analysis.py]",
    "role": "src",
    "loc": 198
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_building.py",
    "summary": "NOTES: | classes: OpRecorder | functions: _construct_named_inputs_and_attrs, _resolve_parameter_dtypes, _determine_input_dtype, _allowed_types_are_sequence_types, _get_or_create_constant, _process_python_constants | imports: copy, inspect, onnxscript, torch | [torch onnx _internal exporter _building",
    "role": "src",
    "loc": 577
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_capture_strategies.py",
    "summary": "Strategies for capturing ExportedPrograms. | classes: Result, CaptureStrategy, TorchExportStrategy, TorchExportNonStrictStrategy, WrappedModel, JitTraceConvertStrategy | functions: _verbose_printer, _take_first_line, _patch_dynamo_unsupported_functions | imports: abc, dataclasses, datetime, torch | ",
    "role": "src",
    "loc": 257
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_compat.py",
    "summary": "Compatibility functions for the torch.onnx.export API. | functions: _get_torch_export_args, export_compat | imports: torch | [torch onnx _internal exporter _compat.py]",
    "role": "src",
    "loc": 163
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_constants.py",
    "summary": "No description | [torch onnx _internal exporter _constants.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_core.py",
    "summary": "No description | classes: TorchTensor | functions: _torch_dtype_to_onnx_dtype, _set_shape_types, _set_shape_type, _get_qualified_module_name, _get_node_namespace, _set_node_metadata | imports: ctypes, datetime, inspect, operator | [torch onnx _internal exporter _core.py]",
    "role": "src",
    "loc": 1238
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_decomp.py",
    "summary": "Creates a set of OperatorBase and Callable objects that represent ONNX-supported PyTorch operations. | functions: get_onnx_implemented_overloads, create_onnx_friendly_decomposition_table | imports: torch | [torch onnx _internal exporter _decomp.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_dispatching.py",
    "summary": "No description | functions: _torch_dtype_to_onnx_compatible_dtype, _attribute_type_compatible_with_arg, _param_type_compatible_with_arg, _get_type_from_tensor, _get_first_tensor_in_node_list, _get_named_fx_node_args | imports: onnxscript, torch | [torch onnx _internal exporter _dispatching.py]",
    "role": "src",
    "loc": 297
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_dynamic_shapes.py",
    "summary": "Compatibility functions for the torch.onnx.export API. | functions: from_dynamic_axes_to_dynamic_shapes, from_dynamic_shapes_to_dynamic_axes, _any_str_or_dim_in_dynamic_shapes, convert_str_to_export_dim, create_rename_mapping, _get_custom_axis_name | imports: inspect, torch | [torch onnx _internal e",
    "role": "src",
    "loc": 259
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_errors.py",
    "summary": "Error classes for the ONNX exporter. | classes: TorchExportError, ConversionError, DispatchError, GraphConstructionError | imports: torch | [torch onnx _internal exporter _errors.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_fx_passes.py",
    "summary": "Decompose the exported program with the given registry. | functions: decompose_with_registry, insert_type_promotion_nodes, remove_assertion_nodes | imports: torch | [torch onnx _internal exporter _fx_passes.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_ir_passes.py",
    "summary": "No description | functions: rename_inputs, rename_outputs, _all_values, _replace_names, rename_axis, add_torchlib_common_imports | imports: torch, onnxscript | [torch onnx _internal exporter _ir_passes.py]",
    "role": "src",
    "loc": 99
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_isolated.py",
    "summary": "Isolated calls to methods that may segfault. | functions: _call_function_and_return_exception, safe_call | imports: multiprocessing | [torch onnx _internal exporter _isolated.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_onnx_program.py",
    "summary": "A class to represent an ONNX program that is callable with torch tensors. | classes: ONNXProgram | functions: _ort_session_initializer, _count_initializer_size, _process_args, _flatten_inputs, _remove_none_from_inputs, _remove_non_tensor | imports: copy, gc, tempfile, textwrap | [torch onnx _interna",
    "role": "src",
    "loc": 254
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_registration.py",
    "summary": "Module for handling ATen to ONNX functions registration. | classes: OnnxDecompMeta, ONNXRegistry | functions: _get_overload | imports: dataclasses, importlib, operator, types | [torch onnx _internal exporter _registration.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_reporting.py",
    "summary": "No description | classes: ExportStatus | functions: _status_emoji, _format_export_status, _strip_color_from_string, _format_exported_program, construct_report_file_name, format_decomp_comparison | imports: dataclasses, torch, onnxscript | [torch onnx _internal exporter _reporting.py]",
    "role": "src",
    "loc": 149
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_schemas.py",
    "summary": "Converter a type_str from ONNX Opschema to ir.TypeProtocol. | classes: _Empty, TypeConstraintParam, Parameter, AttributeParameter, OpSignature | functions: _get_type_from_str, _convert_formal_parameter, _is_optional, _get_attr_type, _get_type_constraint_name, _get_allowed_types_from_type_annotation ",
    "role": "src",
    "loc": 443
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_tensors.py",
    "summary": "Subclass of ir.Value that supports Python operators. | classes: SymbolicTensor | imports: onnxscript | [torch onnx _internal exporter _tensors.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_testing.py",
    "summary": "Test utilities for ONNX export. | functions: assert_onnx_program | imports: torch | [torch onnx _internal exporter _testing.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_verification.py",
    "summary": "No description | classes: VerificationInfo | functions: _compare_tensors, verify_onnx_program | imports: dataclasses, torch | [torch onnx _internal exporter _verification.py]",
    "role": "src",
    "loc": 92
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\__init__.py",
    "summary": "Package initializer | [torch onnx _internal exporter __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\_tensor_typing.py",
    "summary": "Typings for function definitions. | imports: onnxscript | [torch onnx _internal exporter _torchlib _tensor_typing.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\_torchlib_registry.py",
    "summary": "Registry for aten functions. | functions: onnx_impl, wrapper, get_torchlib_ops | imports: onnxscript, torch | [torch onnx _internal exporter _torchlib _torchlib_registry.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\__init__.py",
    "summary": "Package initializer | [torch onnx _internal exporter _torchlib __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\ops\\core.py",
    "summary": "torch.ops.aten operators under the `core` module. | functions: aten_abs, aten_abs_complex, aten_add, aten_add_complex | imports: operator, onnxscript, torch | [torch onnx _internal exporter _torchlib ops core.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\ops\\hop.py",
    "summary": "Implementation for higher-order operators. | functions: call_op, higher_order_cond | imports: torch, onnxscript | [torch onnx _internal exporter _torchlib ops hop.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "torch\\onnx\\_internal\\exporter\\_torchlib\\ops\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch onnx _internal exporter _torchlib ops __init__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\decomposition_skip.py",
    "summary": "A context manager that disables the decomposition of certain ops during dynamo tracing. | classes: DecompSkip, UpsampleBilinear2DDecompSkip, UpsampleTrilinear3DDecompSkip, InstanceNormDecompSkip | functions: enable_decomposition_skips | imports: abc, onnxscript, torch | [torch onnx _internal fx deco",
    "role": "src",
    "loc": 197
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\decomposition_table.py",
    "summary": "Dispatcher for AtenLib functions from onnx-script. | functions: _create_onnx_supports_op_overload_table, create_onnx_friendly_decomposition_table | imports: torch | [torch onnx _internal fx decomposition_table.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\diagnostics.py",
    "summary": "No description | classes: Diagnostic, DiagnosticContext, UnsupportedFxNodeDiagnostic | functions: is_onnx_diagnostics_log_artifact_enabled, _format_argument, format_argument, _torch_nn_module, _torch_fx_graph_module, _torch_fx_node | imports: dataclasses, functools, onnxscript, torch | [torch onnx _",
    "role": "src",
    "loc": 155
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\dynamo_graph_extractor.py",
    "summary": "Context manager to register PyTree extension. | classes: _PyTreeExtensionContext, DynamoFlattenOutputStep, DynamoExport | functions: _wrap_model_with_output_adapter, wrapped | imports: functools, inspect, torch, transformers | [torch onnx _internal fx dynamo_graph_extractor.py]",
    "role": "src",
    "loc": 173
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\fx_onnx_interpreter.py",
    "summary": "Stateless class to process FX graph Nodes and translate them into their ONNX counterparts. | classes: FxOnnxInterpreter | functions: _fx_node_to_onnx_message_formatter, _fx_graph_to_onnx_message_formatter, _location_from_fx_stack_trace, _retrieve_or_adapt_input_to_graph_set, filter_incompatible_and_",
    "role": "src",
    "loc": 602
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\fx_symbolic_graph_extractor.py",
    "summary": "Tracer to create ONNX-exporting friendly FX graph. | classes: ModuleExpansionTracer, FXSymbolicTracer | functions: _wrap_for_symbolic_trace, wrapper, check_has_proxy, _module_expansion_symbolic_trace | imports: functools, torch | [torch onnx _internal fx fx_symbolic_graph_extractor.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\onnxfunction_dispatcher.py",
    "summary": "Dispatcher for AtenLib functions from onnx-script. | classes: OnnxFunctionDispatcher, _OnnxSchemaChecker | functions: _find_opschema_matched_symbolic_function_disagnostic_message_formatter, _find_operator_overloads_in_onnx_registry_disagnostic_message_formatter, _is_arg_with_complex_dtype, _find_onn",
    "role": "src",
    "loc": 693
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\patcher.py",
    "summary": "Context manager to temporarily patch PyTorch during FX-to-ONNX export. | classes: ONNXTorchPatcher | functions: has_safetensors_and_transformers | imports: copy, functools, torch, io | [torch onnx _internal fx patcher.py]",
    "role": "src",
    "loc": 102
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\registration.py",
    "summary": "Module for handling ATen to ONNX functions registration. | classes: ONNXFunction, OpName | imports: dataclasses, types, onnxscript, torch | [torch onnx _internal fx registration.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\serialization.py",
    "summary": "Create a TensorProto with external data from a PyTorch tensor. | functions: _create_tensor_proto_with_external_data, _convert_safetensors_to_torch_format, save_model_with_external_data | imports: io, torch, onnx, safetensors | [torch onnx _internal fx serialization.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\type_utils.py",
    "summary": "Utilities for converting and operating on ONNX, JIT and torch types. | classes: TensorLike | functions: is_torch_complex_dtype, from_complex_to_float, from_sym_value_to_torch_dtype, is_optional_onnx_dtype_str, from_torch_dtype_to_onnx_dtype_str, from_python_type_to_onnx_attribute_type | imports: num",
    "role": "src",
    "loc": 193
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\_pass.py",
    "summary": "Context patching `difflib.SequenceMatcher` for fx readable graph. | classes: PackageInfo, GraphModuleOnnxMeta, Transform, AnalysisResult, Analysis | functions: _patch_difflib_sequence_matcher_init, patched_init, _unified_diff, _transform_diagnose_call_message_formatter, maybe_fx_graph_tabular | impo",
    "role": "src",
    "loc": 248
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\__init__.py",
    "summary": "Package initializer | imports: patcher, serialization | [torch onnx _internal fx __init__.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\analysis\\unsupported_nodes.py",
    "summary": "No description | classes: UnsupportedFxNodesAnalysisResult, UnsupportedFxNodesAnalysis | imports: dataclasses, torch | [torch onnx _internal fx analysis unsupported_nodes.py]",
    "role": "src",
    "loc": 68
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\analysis\\__init__.py",
    "summary": "Package initializer | imports: unsupported_nodes | [torch onnx _internal fx analysis __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\decomp.py",
    "summary": "No description | classes: Decompose | imports: torch | [torch onnx _internal fx passes decomp.py]",
    "role": "src",
    "loc": 51
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\functionalization.py",
    "summary": "Functionalize a GraphModule. | classes: Functionalize, RemoveInputMutation | imports: torch | [torch onnx _internal fx passes functionalization.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\modularization.py",
    "summary": "Meta information about a module. | classes: _ModuleMeta, _ModuleStackMeta, _IRNode, _ModuleNode, _LeafNode, Modularize | functions: _module_stack_meta_from_node, _get_unique_module_name | imports: abc, copy, operator, torch | [torch onnx _internal fx passes modularization.py]",
    "role": "src",
    "loc": 649
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\readability.py",
    "summary": "Restore parameter and buffer names from original nn.module. | classes: RestoreParameterAndBufferNames | imports: torch | [torch onnx _internal fx passes readability.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\type_promotion.py",
    "summary": "Type promotion snapshot for a fx node and its inputs. | classes: TypePromotionSnapshot, TypePromotionRule, ElementwiseTypePromotionRule, DivElementwiseTypePromotionRule, ReductionTypePromotionRule, AllOrAnyReductionTypePromotionRule | functions: _try_getclosurevars, get_type_promotion_rule, find_com",
    "role": "src",
    "loc": 1519
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\virtualization.py",
    "summary": "This pass move all placeholder nodes to the front of the graph node list. | classes: MovePlaceholderToFront, ReplaceGetAttrWithPlaceholder | imports: torch | [torch onnx _internal fx passes virtualization.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\_utils.py",
    "summary": "Common utility functions for FX passes. | functions: wrap_graph_module_for_node_meta_preservation, wrapped, _get_node_base_name, set_node_name, replace_placeholder_name_and_target | imports: torch | [torch onnx _internal fx passes _utils.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\onnx\\_internal\\fx\\passes\\__init__.py",
    "summary": "Package initializer | imports: decomp, functionalization, modularization, readability | [torch onnx _internal fx passes __init__.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\optim\\adadelta.py",
    "summary": "No description | classes: Adadelta | functions: _single_tensor_adadelta, _multi_tensor_adadelta, adadelta | imports: torch, optimizer | [torch optim adadelta.py]",
    "role": "src",
    "loc": 389
  },
  {
    "id": "torch\\optim\\adagrad.py",
    "summary": "Functional API that performs Adagrad algorithm computation. | classes: Adagrad | functions: adagrad, _make_sparse, _single_tensor_adagrad, _multi_tensor_adagrad, _fused_adagrad | imports: torch, optimizer | [torch optim adagrad.py]",
    "role": "src",
    "loc": 483
  },
  {
    "id": "torch\\optim\\adam.py",
    "summary": "No description | classes: Adam | functions: _single_tensor_adam, _multi_tensor_adam, _fused_adam, adam | imports: torch, optimizer | [torch optim adam.py]",
    "role": "src",
    "loc": 761
  },
  {
    "id": "torch\\optim\\adamax.py",
    "summary": "No description | classes: Adamax | functions: _single_tensor_adamax, _multi_tensor_adamax, adamax | imports: torch, optimizer | [torch optim adamax.py]",
    "role": "src",
    "loc": 392
  },
  {
    "id": "torch\\optim\\adamw.py",
    "summary": "Functional API that performs AdamW algorithm computation. | classes: AdamW | functions: adamw | imports: torch, adam, optimizer | [torch optim adamw.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\optim\\asgd.py",
    "summary": "No description | classes: ASGD | functions: _single_tensor_asgd, _multi_tensor_asgd, asgd | imports: torch, optimizer | [torch optim asgd.py]",
    "role": "src",
    "loc": 384
  },
  {
    "id": "torch\\optim\\lbfgs.py",
    "summary": "Implements L-BFGS algorithm. | classes: LBFGS | functions: _cubic_interpolate, _strong_wolfe | imports: torch, optimizer | [torch optim lbfgs.py]",
    "role": "src",
    "loc": 346
  },
  {
    "id": "torch\\optim\\lr_scheduler.py",
    "summary": "Learning Rate Scheduler. | classes: LRScheduler, _LRScheduler, _enable_get_lr_call, LambdaLR, MultiplicativeLR, StepLR | functions: _check_verbose_deprecated_warning, _format_param, _copy, _warn_get_lr_called_within_step | imports: types, bisect, functools, weakref | [torch optim lr_scheduler.py]",
    "role": "src",
    "loc": 1823
  },
  {
    "id": "torch\\optim\\nadam.py",
    "summary": "Implementation for the NAdam algorithm. | classes: NAdam | functions: _single_tensor_nadam, _multi_tensor_nadam, nadam | imports: torch, optimizer | [torch optim nadam.py]",
    "role": "src",
    "loc": 537
  },
  {
    "id": "torch\\optim\\optimizer.py",
    "summary": "Base optimizer. | classes: _RequiredParameter, Optimizer | functions: _use_grad_for_differentiable, _use_grad, _get_value, _stack_if_compiling, _disable_dynamo_if_unsupported, wrapper | imports: functools, copy, typing_extensions, torch | [torch optim optimizer.py]",
    "role": "src",
    "loc": 855
  },
  {
    "id": "torch\\optim\\radam.py",
    "summary": "Implementation for the RAdam algorithm. | classes: RAdam | functions: _single_tensor_radam, _compute_rect, _compute_adaptive_lr, _multi_tensor_radam, radam | imports: torch, optimizer | [torch optim radam.py]",
    "role": "src",
    "loc": 514
  },
  {
    "id": "torch\\optim\\rmsprop.py",
    "summary": "Implementation for the RMSprop algorithm. | classes: RMSprop | functions: _single_tensor_rmsprop, _multi_tensor_rmsprop, rmsprop | imports: torch, optimizer | [torch optim rmsprop.py]",
    "role": "src",
    "loc": 455
  },
  {
    "id": "torch\\optim\\rprop.py",
    "summary": "Implementation for the Resilient backpropagation. | classes: Rprop | functions: _single_tensor_rprop, _multi_tensor_rprop, rprop | imports: torch, optimizer | [torch optim rprop.py]",
    "role": "src",
    "loc": 375
  },
  {
    "id": "torch\\optim\\sgd.py",
    "summary": "Implementation for Stochastic Gradient Descent optimizer. | classes: SGD | functions: sgd, _single_tensor_sgd, _multi_tensor_sgd, _fused_sgd | imports: torch, optimizer | [torch optim sgd.py]",
    "role": "src",
    "loc": 446
  },
  {
    "id": "torch\\optim\\sparse_adam.py",
    "summary": "No description | classes: SparseAdam | imports: torch, optimizer | [torch optim sparse_adam.py]",
    "role": "src",
    "loc": 146
  },
  {
    "id": "torch\\optim\\swa_utils.py",
    "summary": "Implementation for Stochastic Weight Averaging implementation. | classes: AveragedModel, SWALR | functions: get_ema_multi_avg_fn, ema_update, get_swa_multi_avg_fn, swa_update, get_ema_avg_fn, get_swa_avg_fn | imports: copy, torch, optimizer | [torch optim swa_utils.py]",
    "role": "src",
    "loc": 403
  },
  {
    "id": "torch\\optim\\_adafactor.py",
    "summary": "No description | classes: Adafactor | functions: _single_tensor_adafactor, _group_tensors_by_device_dtype_and_is_multidim, _multi_tensor_adafactor, adafactor | imports: torch, optimizer | [torch optim _adafactor.py]",
    "role": "src",
    "loc": 538
  },
  {
    "id": "torch\\optim\\_functional.py",
    "summary": "Functional interface. | functions: sparse_adam, make_sparse | imports: torch, adadelta, adagrad, adam | [torch optim _functional.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\optim\\__init__.py",
    "summary": ":mod:`torch.optim` is a package implementing various optimization algorithms. | imports: torch | [torch optim __init__.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\optim\\_multi_tensor\\__init__.py",
    "summary": ":mod:`torch.optim._multi_tensor` is a package implementing various optimization algorithms. | classes: NewCls | functions: partialclass | imports: functools, torch | [torch optim _multi_tensor __init__.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\package\\file_structure_representation.py",
    "summary": "A file structure representation. Organized as Directory nodes that have lists of | classes: Directory | functions: _create_directory_from_file_list | imports: glob_group | [torch package file_structure_representation.py]",
    "role": "src",
    "loc": 107
  },
  {
    "id": "torch\\package\\find_file_dependencies.py",
    "summary": "Extract the list of global variables a block of code will read and write | classes: _ExtractModuleReferences | imports: ast, _importlib | [torch package find_file_dependencies.py]",
    "role": "src",
    "loc": 71
  },
  {
    "id": "torch\\package\\glob_group.py",
    "summary": "A set of patterns that candidate strings will be matched against. | classes: GlobGroup | [torch package glob_group.py]",
    "role": "src",
    "loc": 66
  },
  {
    "id": "torch\\package\\importer.py",
    "summary": "Raised when an importer cannot find an object by searching for its name. | classes: ObjNotFoundError, ObjMismatchError, Importer, _SysImporter, OrderedImporter | imports: importlib, abc, pickle, types | [torch package importer.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "torch\\package\\package_exporter.py",
    "summary": "Represents one of the actions that :class:`PackageExporter` can take on a module. | classes: _ModuleProviderAction, PackagingErrorReason, _PatternInfo, EmptyMatchError, PackagingError, PackageExporter | functions: _read_file | imports: importlib, io, linecache, pickletools | [torch package package_e",
    "role": "src",
    "loc": 940
  },
  {
    "id": "torch\\package\\package_importer.py",
    "summary": "Importers allow you to load code written to packages by :class:`PackageExporter`. | classes: PackageImporter, _PathNode, _PackageNode, _ModuleNode, _ExternNode, _PackageResourceReader | functions: _patched_getfile | imports: builtins, importlib, inspect, io | [torch package package_importer.py]",
    "role": "src",
    "loc": 587
  },
  {
    "id": "torch\\package\\_digraph.py",
    "summary": "Really simple unweighted directed graph data structure to track dependencies. | classes: DiGraph | [torch package _digraph.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\package\\_directory_reader.py",
    "summary": "No description | classes: _HasStorage, DirectoryReader | imports: glob, torch | [torch package _directory_reader.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\package\\_importlib.py",
    "summary": "No description | functions: _normalize_line_endings, _resolve_name, _sanity_check, _calc___package__, _normalize_path | imports: _warnings | [torch package _importlib.py]",
    "role": "src",
    "loc": 67
  },
  {
    "id": "torch\\package\\_mangling.py",
    "summary": "Import mangling. | classes: PackageMangler | functions: is_mangled, demangle, get_mangle_prefix | [torch package _mangling.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\package\\_mock.py",
    "summary": "No description | classes: MockedObject | functions: install_method, _not_implemented | [torch package _mock.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "torch\\package\\_package_pickler.py",
    "summary": "No description | classes: _PyTorchLegacyPickler, PackagePickler | functions: create_pickler | imports: pickle, struct, types, importer | [torch package _package_pickler.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "torch\\package\\_package_unpickler.py",
    "summary": "Package-aware unpickler. | classes: PackageUnpickler | imports: _compat_pickle, pickle, importer | [torch package _package_unpickler.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\package\\_stdlib.py",
    "summary": "List of Python standard library modules. | functions: is_stdlib_module, _get_stdlib_modules | [torch package _stdlib.py]",
    "role": "src",
    "loc": 235
  },
  {
    "id": "torch\\package\\__init__.py",
    "summary": "Package initializer | imports: analyze, file_structure_representation, glob_group, importer | [torch package __init__.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\package\\analyze\\find_first_use_of_broken_modules.py",
    "summary": "Find all broken modules in a PackagingError, and for each one, return the | functions: find_first_use_of_broken_modules | imports: torch | [torch package analyze find_first_use_of_broken_modules.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\package\\analyze\\is_from_package.py",
    "summary": "Return whether an object was loaded from a package. | functions: is_from_package | imports: types, _mangling | [torch package analyze is_from_package.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\package\\analyze\\trace_dependencies.py",
    "summary": "Trace the execution of a callable in order to determine which modules it uses. | functions: trace_dependencies, record_used_modules | [torch package analyze trace_dependencies.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torch\\package\\analyze\\__init__.py",
    "summary": "Package initializer | imports: find_first_use_of_broken_modules, trace_dependencies | [torch package analyze __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\profiler\\itt.py",
    "summary": "Check if ITT feature is available or not | classes: _ITTStub | functions: is_available, range_push, range_pop, mark, range | imports: torch | [torch profiler itt.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\profiler\\profiler.py",
    "summary": "Json encoder for numpy types (np.int, np.float, np.array etc.) | classes: _NumpyEncoder, _ITraceObserver, _KinetoProfile, ProfilerAction, profile, ExecutionTraceObserver | functions: supported_activities, schedule, schedule_fn, _default_schedule_fn, tensorboard_trace_handler, handler_fn | imports: g",
    "role": "src",
    "loc": 930
  },
  {
    "id": "torch\\profiler\\python_tracer.py",
    "summary": "No description | functions: _prefix_regex | imports: site, torch | [torch profiler python_tracer.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\profiler\\_memory_profiler.py",
    "summary": "No description | classes: Category, Action, Key, _Storage, TensorKey, SchemaMatcher | functions: _extract_parameters_and_gradients, extract_parameters, extract_gradients, get_scopes | imports: dataclasses, typing_extensions, torch, json | [torch profiler _memory_profiler.py]",
    "role": "src",
    "loc": 883
  },
  {
    "id": "torch\\profiler\\_pattern_matcher.py",
    "summary": "Base class for all patterns, subclass this class and implement match() | classes: Pattern, NamePattern, ExtraCUDACopyPattern, ForLoopIndexingPattern, FP32MatMulPattern, OptimizerSingleTensorPattern | functions: source_code_location, input_shapes, input_dtypes, report_all_anti_patterns | imports: jso",
    "role": "src",
    "loc": 550
  },
  {
    "id": "torch\\profiler\\_utils.py",
    "summary": "No description | classes: EventMetrics, Interval, EventKey, BasicEvaluation | functions: _traverse, index_of_first_match, argmax, source_code_location, _init_for_cuda_graphs | imports: functools, operator, dataclasses, torch | [torch profiler _utils.py]",
    "role": "src",
    "loc": 305
  },
  {
    "id": "torch\\profiler\\__init__.py",
    "summary": "PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. | functions: _optimizer_post_hook | imports: torch, profiler | [torch profiler __init__.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\quantization\\fake_quantize.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fake_quantize.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\quantization\\fuser_method_mappings.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fuser_method_mappings.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\quantization\\fuse_modules.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fuse_modules.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\quantization\\observer.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization observer.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\quantization\\qconfig.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization qconfig.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\quantization\\quantization_mappings.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization quantization_mappings.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\quantization\\quantize.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization quantize.py]",
    "role": "src",
    "loc": 28
  },
  {
    "id": "torch\\quantization\\quantize_fx.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization quantize_fx.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\quantization\\quantize_jit.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization quantize_jit.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\quantization\\quant_type.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization quant_type.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\stubs.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization stubs.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\utils.py",
    "summary": "Utils shared by different modes of quantization (eager/graph) | imports: torch | [torch quantization utils.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\quantization\\_numeric_suite.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization _numeric_suite.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\quantization\\_numeric_suite_fx.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization _numeric_suite_fx.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\quantization\\_quantized_conversions.py",
    "summary": "No description | functions: pack_int4_to_int8, unpack_int8_to_int4, quantized_weight_reorder_for_mixed_dtypes_linear_cutlass | imports: torch | [torch quantization _quantized_conversions.py]",
    "role": "src",
    "loc": 100
  },
  {
    "id": "torch\\quantization\\__init__.py",
    "summary": "Default evaluation function takes a torch.utils.data.Dataset or a list of | functions: default_eval_fn | imports: fake_quantize, fuse_modules, fuser_method_mappings, observer | [torch quantization __init__.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\quantization\\fx\\convert.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx convert.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\fx\\fuse.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx fuse.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\fx\\fusion_patterns.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx fusion_patterns.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\fx\\graph_module.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx graph_module.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\quantization\\fx\\match_utils.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx match_utils.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\quantization\\fx\\pattern_utils.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx pattern_utils.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\quantization\\fx\\prepare.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx prepare.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\fx\\quantization_patterns.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx quantization_patterns.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\quantization\\fx\\quantization_types.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx quantization_types.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\quantization\\fx\\utils.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx utils.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\quantization\\fx\\_equalize.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx _equalize.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\quantization\\fx\\__init__.py",
    "summary": "This file is in the process of migration to `torch/ao/quantization`, and | imports: torch | [torch quantization fx __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\signal\\__init__.py",
    "summary": "Package initializer | [torch signal __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\signal\\windows\\windows.py",
    "summary": "Adds docstrings to a given decorated function. | functions: _add_docstr, decorator, _window_function_checks, exponential, cosine, gaussian | imports: torch | [torch signal windows windows.py]",
    "role": "src",
    "loc": 714
  },
  {
    "id": "torch\\signal\\windows\\__init__.py",
    "summary": "Package initializer | imports: windows | [torch signal windows __init__.py]",
    "role": "src",
    "loc": 26
  },
  {
    "id": "torch\\sparse\\semi_structured.py",
    "summary": "This class implementes semi-structured sparsity as a Tensor subclass. | classes: SparseSemiStructuredTensor, SparseSemiStructuredTensorCUTLASS, SparseSemiStructuredTensorCUSPARSELT | functions: to_sparse_semi_structured | imports: torch | [torch sparse semi_structured.py]",
    "role": "src",
    "loc": 556
  },
  {
    "id": "torch\\sparse\\_semi_structured_conversions.py",
    "summary": "This is PyTorch implementation of main part of reorder_meta() | functions: _calculate_meta_reordering_scatter_offsets, sparse_semi_structured_from_dense_cutlass, sparse_semi_structured_to_dense_cutlass, _sparse_semi_structured_tile, greedy_prune_tile, _compute_compressed_swizzled_bitmask | imports: ",
    "role": "src",
    "loc": 242
  },
  {
    "id": "torch\\sparse\\_semi_structured_ops.py",
    "summary": "No description | functions: no_dispatch, fallback_dispatcher, semi_sparse_values, semi_sparse_indices, semi_sparse_t, semi_sparse_view | imports: torch | [torch sparse _semi_structured_ops.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\sparse\\_triton_ops.py",
    "summary": "A light-weight wrapper of a tensor that enables storing tensors as | classes: TensorAsKey | functions: check, check_bsr_layout, check_device, check_mm_compatible_shapes, check_dtype, check_blocksize | imports: weakref, functools, torch, _triton_ops_meta | [torch sparse _triton_ops.py]",
    "role": "src",
    "loc": 2085
  },
  {
    "id": "torch\\sparse\\_triton_ops_meta.py",
    "summary": "Provides optimal triton kernel parameters. | functions: get_meta, update, dump, sort_key, minimize, to_key | imports: inspect, torch, triton | [torch sparse _triton_ops_meta.py]",
    "role": "src",
    "loc": 7608
  },
  {
    "id": "torch\\sparse\\__init__.py",
    "summary": "A tool to control checking sparse tensor invariants. | classes: check_sparse_tensor_invariants | functions: sum, as_sparse_gradcheck, gradcheck_with_sparse_support, convert_to_strided_representation, restore_from_strided_representation, func_wrapper | imports: torch, semi_structured | [torch sparse ",
    "role": "src",
    "loc": 557
  },
  {
    "id": "torch\\special\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch special __init__.py]",
    "role": "src",
    "loc": 1218
  },
  {
    "id": "torch\\testing\\_comparison.py",
    "summary": "Internal testing exception that makes that carries error metadata. | classes: ErrorMeta, UnsupportedInputs, Pair, ObjectPair, NonePair, BooleanPair | functions: default_tolerances, get_tolerances, _make_mismatch_msg, make_diff_msg, make_scalar_mismatch_msg, make_tensor_mismatch_msg | imports: abc, c",
    "role": "src",
    "loc": 1312
  },
  {
    "id": "torch\\testing\\_creation.py",
    "summary": "This module contains tensor creation utilities. | functions: _uniform_random_, make_tensor, modify_low_high, clamp | imports: functools, torch | [torch testing _creation.py]",
    "role": "src",
    "loc": 231
  },
  {
    "id": "torch\\testing\\_utils.py",
    "summary": "Wrapper to set seed manually for some functions like dropout | functions: wrapper_set_seed, freeze_rng_state | imports: torch | [torch testing _utils.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\testing\\__init__.py",
    "summary": "Package initializer | imports: torch, _comparison, _creation | [torch testing __init__.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\testing\\_internal\\autocast_test_lists.py",
    "summary": "No description | classes: AutocastTestLists, AutocastCPUTestLists, TestAutocast | imports: torch | [torch testing _internal autocast_test_lists.py]",
    "role": "src",
    "loc": 401
  },
  {
    "id": "torch\\testing\\_internal\\autograd_function_db.py",
    "summary": "No description | classes: NumpyCube, CubeGenVmap, NumpyCubeNotComposable, NumpyMul, MulGenVmap, NumpyExp_ | functions: to_numpy, sample_inputs_numpy_cube, sample_inputs_numpy_mul, sample_inputs_numpy_mul_scalar, sample_inputs_numpy_sort, sample_inputs_numpy_take | imports: torch, functools, numpy | ",
    "role": "src",
    "loc": 511
  },
  {
    "id": "torch\\testing\\_internal\\check_kernel_launches.py",
    "summary": "Given a string \"prefix (unknown number of characters) suffix\" | functions: find_matching_paren, should_exclude_file, check_code_for_cuda_kernel_launches, check_file, check_cuda_kernel_launches | [torch testing _internal check_kernel_launches.py]",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\testing\\_internal\\common_cuda.py",
    "summary": "This file is allowed to initialize CUDA context when imported. | functions: CDNA2OrLater, evaluate_gfx_arch_exact, evaluate_platform_supports_flash_attention, evaluate_platform_supports_efficient_attention, evaluate_platform_supports_cudnn_attention, evaluate_platform_supports_fp8 | imports: functoo",
    "role": "src",
    "loc": 222
  },
  {
    "id": "torch\\testing\\_internal\\common_device_type.py",
    "summary": "Returns the test suffix for a dtype, sequence of dtypes, or None. | classes: DeviceTypeTestBase, CPUTestBase, CUDATestBase, LazyTestBase, MPSTestBase, XPUTestBase | functions: _dtype_test_suffix, _update_param_kwargs, get_device_type_test_bases, filter_desired_device_types, get_desired_device_type_t",
    "role": "src",
    "loc": 1101
  },
  {
    "id": "torch\\testing\\_internal\\common_distributed.py",
    "summary": "Skips if the world size exceeds the number of GPUs, ensuring that if the | classes: TestSkip, DistTestCases, Event, MultiProcessTestCase, DistributedTestBase, MultiThreadedTestCase | functions: skip_if_no_gpu, wrapper, skip_if_small_worldsize, skip_if_odd_worldsize, require_n_gpus_for_nccl_backend, ",
    "role": "src",
    "loc": 1172
  },
  {
    "id": "torch\\testing\\_internal\\common_dist_composable.py",
    "summary": "No description | classes: UnitModule, CompositeModel, UnitParamModule, CompositeParamModel, FakeSequential, NestedSequentialModel | imports: torch | [torch testing _internal common_dist_composable.py]",
    "role": "src",
    "loc": 83
  },
  {
    "id": "torch\\testing\\_internal\\common_dtype.py",
    "summary": "No description | classes: _dispatch_dtypes | functions: _validate_dtypes, empty_types, floating_types, floating_types_and_half, floating_types_and, floating_and_complex_types | imports: torch | [torch testing _internal common_dtype.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\testing\\_internal\\common_fsdp.py",
    "summary": "All-gathers module states across ranks and calls ``assert_fn`` on each pair | classes: FSDPInitMode, DEVICEInitMode, FSDPTestModel, DummyProcessGroup, TransformerWithSharedParams, NestedWrappedModule | functions: _assert_module_states, get_devtype, _zero_model, _get_state_dict, subtest_name, _broadc",
    "role": "src",
    "loc": 1295
  },
  {
    "id": "torch\\testing\\_internal\\common_jit.py",
    "summary": "No description | classes: JitCommonTestCase | functions: check_output_types, check_against_reference, allSum, clone_tensor, clone_inputs, get_recording_tensors | imports: torch, io | [torch testing _internal common_jit.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "torch\\testing\\_internal\\common_mkldnn.py",
    "summary": "No description | functions: bf32_is_not_fp32, bf32_off, bf32_on, bf32_on_and_off, with_bf32_disabled, with_bf32_enabled | imports: functools, inspect, torch | [torch testing _internal common_mkldnn.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "torch\\testing\\_internal\\common_modules.py",
    "summary": "PROTOTYPE: Decorator for specifying a list of modules over which to run a test. | classes: modules, FunctionInput, ModuleInput, ModuleErrorEnum, ErrorModuleInput, ModuleInfo | functions: get_module_common_name, module_inputs_torch_nn_Linear, module_inputs_torch_nn_Bilinear, bilinear_reference_fn, mo",
    "role": "src",
    "loc": 3733
  },
  {
    "id": "torch\\testing\\_internal\\common_nn.py",
    "summary": "No description | classes: FunctionalModule, NNTestCase, TestBase, ModuleTest, InputVariableMixin, NewModuleTest | functions: get_reduction, get_weight, _rand_tensor_non_equal, wrap_functional, poissonnllloss_no_reduce_test, bceloss_no_reduce_test | imports: abc, tempfile, unittest, copy | [torch tes",
    "role": "src",
    "loc": 3494
  },
  {
    "id": "torch\\testing\\_internal\\common_optimizers.py",
    "summary": "Contains args / kwargs to be passed to an optimizer constructor. | classes: OptimizerInput, OptimizerErrorEnum, ErrorOptimizerInput, OptimizerInfo, optims, TensorTracker | functions: get_error_inputs_for_all_optims, optim_inputs_func_adadelta, optim_error_inputs_func_adadelta, optim_inputs_func_adaf",
    "role": "src",
    "loc": 2049
  },
  {
    "id": "torch\\testing\\_internal\\common_pruning.py",
    "summary": "Checks to see if all rows in subset tensor are present in the superset tensor | classes: ImplementedSparsifier, MockSparseLinear, SimpleLinear, LinearBias, LinearActivation, LinearActivationFunctional | functions: rows_are_subset | imports: torch | [torch testing _internal common_pruning.py]",
    "role": "src",
    "loc": 325
  },
  {
    "id": "torch\\testing\\_internal\\common_quantization.py",
    "summary": "Importing this file includes common utility methods and base clases for | classes: NodeSpec, AverageMeter, QuantizationTestCase, QuantizationLiteTestCase, M, PT2EQuantizationTestCase | functions: get_supported_device_types, test_only_eval_fn, test_only_train_fn, accuracy, train_one_epoch, ddp_setup ",
    "role": "src",
    "loc": 2533
  },
  {
    "id": "torch\\testing\\_internal\\common_quantized.py",
    "summary": "Importing this file includes common utility methods for checking quantized | functions: _conv_output_shape, _quantize, _dequantize, _requantize, _calculate_dynamic_qparams, _calculate_dynamic_per_channel_qparams | imports: numpy, torch | [torch testing _internal common_quantized.py]",
    "role": "src",
    "loc": 188
  },
  {
    "id": "torch\\testing\\_internal\\common_subclass.py",
    "summary": "No description | classes: WrapperTensor, WrapperTensorWithCustomSizes, WrapperTensorWithCustomStrides, DiagTensorBelow, SparseTensor, NonWrapperTensor | functions: _create_and_access_shape | imports: torch, copy | [torch testing _internal common_subclass.py]",
    "role": "src",
    "loc": 240
  },
  {
    "id": "torch\\testing\\_internal\\common_utils.py",
    "summary": "Importing this file must **not** initialize CUDA context. test_distributed | classes: TestEnvironment, TrackedInput, TrackedInputIter, _TestParametrizer, subtest, parametrize | functions: freeze_rng_state, maybe_load_json, gcIfJetson, wrapper, extract_test_fn, get_tracked_input | imports: argparse, ",
    "role": "src",
    "loc": 4163
  },
  {
    "id": "torch\\testing\\_internal\\composite_compliance.py",
    "summary": "No description | classes: CompositeCompliantTensor, CompositeCompliantTensorMode | functions: check_attr_consistency, check_metadata_consistency, is_view_fn, is_inplace_view_fn, is_inplace, generate_cct_and_mode | imports: torch, functools | [torch testing _internal composite_compliance.py]",
    "role": "src",
    "loc": 387
  },
  {
    "id": "torch\\testing\\_internal\\custom_op_db.py",
    "summary": "No description | functions: to_numpy, numpy_cube, _, numpy_cube_setup_context, numpy_cube_backward, numpy_cube_vmap | imports: torch, functools, numpy | [torch testing _internal custom_op_db.py]",
    "role": "src",
    "loc": 458
  },
  {
    "id": "torch\\testing\\_internal\\custom_tensor.py",
    "summary": "No description | classes: ConstantExtraMetadataTensor, CustomTensorPlainOut | imports: torch | [torch testing _internal custom_tensor.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\testing\\_internal\\dist_utils.py",
    "summary": "We use this decorator for setting up and tearing down state since | functions: dist_init, new_test_method, noop, wait_until_node_failure, wait_until_pending_futures_and_users_flushed, get_num_owners_and_forks | imports: functools, torch | [torch testing _internal dist_utils.py]",
    "role": "src",
    "loc": 155
  },
  {
    "id": "torch\\testing\\_internal\\dynamo_test_failures.py",
    "summary": "No description | functions: find_test_dir | [torch testing _internal dynamo_test_failures.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\testing\\_internal\\fake_config_module.py",
    "summary": "No description | classes: nested | imports: torch | [torch testing _internal fake_config_module.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\testing\\_internal\\fake_config_module2.py",
    "summary": "No description | imports: torch | [torch testing _internal fake_config_module2.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\testing\\_internal\\fake_config_module3.py",
    "summary": "No description | imports: torch | [torch testing _internal fake_config_module3.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\testing\\_internal\\hop_db.py",
    "summary": "No description | functions: sample_inputs_map, inner_f, simple_map, f, nested_map, f1 | imports: functools, unittest, torch, functorch | [torch testing _internal hop_db.py]",
    "role": "src",
    "loc": 342
  },
  {
    "id": "torch\\testing\\_internal\\hypothesis_utils.py",
    "summary": "No description | functions: _get_valid_min_max, _floats_wrapper, floats, assume_not_overflowing, qparams, array_shapes | imports: numpy, torch, hypothesis, functools | [torch testing _internal hypothesis_utils.py]",
    "role": "src",
    "loc": 300
  },
  {
    "id": "torch\\testing\\_internal\\inductor_utils.py",
    "summary": "No description | functions: test_cpu, _check_has_dynamic_shape, skipDeviceIf, decorate_fn, inner, skip_windows_ci | imports: torch, unittest, functools, subprocess | [torch testing _internal inductor_utils.py]",
    "role": "src",
    "loc": 179
  },
  {
    "id": "torch\\testing\\_internal\\jit_metaprogramming_utils.py",
    "summary": "No description | classes: dont_convert, SplitInputs, TheModule | functions: unpack_variables, create_input, map_arg, maybe_non_contig, conjugate, get_nn_functional_tests | imports: torch, copy | [torch testing _internal jit_metaprogramming_utils.py]",
    "role": "src",
    "loc": 592
  },
  {
    "id": "torch\\testing\\_internal\\jit_utils.py",
    "summary": "A context manager that is useful for checking that error messages highlight | classes: _AssertRaisesRegexWithHighlightContext, capture_stdout, capture_stderr, JitTestCase, NoTracerWarnContextManager, TensorExprTestOptions | functions: execWrapper, do_input_map, clear_class_registry, get_execution_pl",
    "role": "src",
    "loc": 683
  },
  {
    "id": "torch\\testing\\_internal\\logging_tensor.py",
    "summary": "No description | classes: LoggingTensor, LoggingTensorMode, LoggingTensorReentrant, LoggingTensorHandler, GatherTraceback | functions: log_input, capture_logs, capture_logs_with_logging_tensor_mode | imports: torch, functools | [torch testing _internal logging_tensor.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "torch\\testing\\_internal\\logging_utils.py",
    "summary": "No description | classes: LoggingTestCase | functions: preserve_log_state, log_settings, log_api, kwargs_to_settings, append_setting, make_logging_test | imports: torch, unittest, io | [torch testing _internal logging_utils.py]",
    "role": "src",
    "loc": 181
  },
  {
    "id": "torch\\testing\\_internal\\quantization_torch_package_models.py",
    "summary": "No description | classes: LinearReluFunctionalChild, LinearReluFunctional | imports: torch | [torch testing _internal quantization_torch_package_models.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\testing\\_internal\\static_module.py",
    "summary": "No description | classes: StaticModule | imports: torch | [torch testing _internal static_module.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\testing\\_internal\\subclasses.py",
    "summary": "No description | classes: WrapperSubclass | imports: torch | [torch testing _internal subclasses.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "torch\\testing\\_internal\\torchbind_impls.py",
    "summary": "No description | classes: FakeFoo, FakeContainsTensor | functions: init_torchbind_implementations, _empty_tensor_queue, register_fake_operators, fake_takes_foo, fake_queue_pop, fake_queue_push | imports: torch, unittest | [torch testing _internal torchbind_impls.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\testing\\_internal\\triton_utils.py",
    "summary": "No description | functions: add_kernel, sub_kernel, add_kernel_with_optional_param, add_kernel_autotuned, add_kernel_autotuned_weird_param_order, add_kernel_2d_autotuned | imports: unittest, torch, triton | [torch testing _internal triton_utils.py]",
    "role": "src",
    "loc": 497
  },
  {
    "id": "torch\\testing\\_internal\\two_tensor.py",
    "summary": "No description | classes: TwoTensor, TwoTensorMode | imports: torch | [torch testing _internal two_tensor.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "torch\\testing\\_internal\\__init__.py",
    "summary": "Package initializer | [torch testing _internal __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\codegen\\__init__.py",
    "summary": "Package initializer | [torch testing _internal codegen __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\data\\network1.py",
    "summary": "No description | classes: Net | imports: torch | [torch testing _internal data network1.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\testing\\_internal\\data\\network2.py",
    "summary": "No description | classes: Net | imports: torch | [torch testing _internal data network2.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\testing\\_internal\\data\\__init__.py",
    "summary": "Package initializer | [torch testing _internal data __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\checkpoint_utils.py",
    "summary": "No description | classes: Writer, Reader, Rot13Example | functions: get_test_extension_registry, with_temp_dir, wrapper | imports: io, shutil, tempfile, functools | [torch testing _internal distributed checkpoint_utils.py]",
    "role": "src",
    "loc": 118
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\common_state_dict.py",
    "summary": "No description | classes: VerifyStateDictMixin, FusionEmbedding, FusionEmbeddingWithHook, FusionEmbeddingWithModifier | imports: copy, torch | [torch testing _internal distributed common_state_dict.py]",
    "role": "src",
    "loc": 139
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\ddp_under_dist_autograd_test.py",
    "summary": "No description | classes: DdpMode, FeatureSet, RemoteEM, RemoteNet, HybridModel, Trainer | functions: init_logger, _call_method, _remote_method, _remote_method_async, getLinear, get_training_examples | imports: threading, torch | [torch testing _internal distributed ddp_under_dist_autograd_test.py]",
    "role": "src",
    "loc": 574
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\distributed_test.py",
    "summary": "No description | classes: NetWithBuffers, Foo, TestNamedTupleInput_1, DDPUnevenTestInput, _FC2, Net | functions: get_profiling_event, get_profiler_nccl_meta, get_timeout, require_backend_is_available, check, require_world_size | imports: copy, json, random, tempfile | [torch testing _internal distri",
    "role": "src",
    "loc": 8480
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\distributed_utils.py",
    "summary": "No description | classes: MockProcessGroup | functions: create_mock_pg, mock_init_dist, with_dist, with_fake_comms, wrapper | imports: datetime, functools, torch | [torch testing _internal distributed distributed_utils.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\fake_pg.py",
    "summary": "A fake store is a fake Key-Value store simply for initialization usage | classes: FakeStore | functions: _create_fake_pg | imports: torch | [torch testing _internal distributed fake_pg.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\multi_threaded_pg.py",
    "summary": "No description | classes: AllToAll, AllToAllBase, AllReduce, AllGather, Scatter, Gather | functions: flatten_list, ret_work, binop_reduce, bitwise_reduce, _create_threaded_pg, _install_threaded_pg | imports: threading, dataclasses, functools, torch | [torch testing _internal distributed multi_thread",
    "role": "src",
    "loc": 403
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc_utils.py",
    "summary": "No description | classes: SpawnHelper | functions: _check_and_set_tcp_init, _check_and_unset_tcp_init, generate_tests | imports: unittest, torch | [torch testing _internal distributed rpc_utils.py]",
    "role": "src",
    "loc": 136
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\nn\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed nn __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\nn\\api\\remote_module_test.py",
    "summary": "No description | classes: ModuleCreationMode, MyModuleInterface, RemoteMyModuleInterface, MyModule, BadModule, CommonRemoteModuleTest | functions: remote_device, remote_module_attributes, remote_forward, remote_forward_async, get_remote_training_arg, create_scripted_module | imports: torch | [torch ",
    "role": "src",
    "loc": 595
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\nn\\api\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed nn api __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\dist_autograd_test.py",
    "summary": "No description | classes: SimulateBackwardError, ExecMode, CommonDistAutogradTest, TensorPipeAgentDistAutogradTest, MyBackwardFunc, TestDebugInfoFunc | functions: _set_rpc_done, _check_rpc_done, _torch_ones, _compare_owner_value, create_tensor, build_sparse_tensor | imports: threading, random, torch",
    "role": "src",
    "loc": 2083
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\dist_optimizer_test.py",
    "summary": "No description | classes: MyModule, FailingOptimizer, OptimizerFailingOnConstructor, DistOptimizerTest | functions: _call_method, remote_method, rpc_async_method | imports: threading, torch | [torch testing _internal distributed rpc dist_optimizer_test.py]",
    "role": "src",
    "loc": 201
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\faulty_agent_rpc_test.py",
    "summary": "No description | classes: FaultyAgentRpcTest | functions: my_sleep_func, my_script_func, add_rref_to_value | imports: torch | [torch testing _internal distributed rpc faulty_agent_rpc_test.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\faulty_rpc_agent_test_fixture.py",
    "summary": "No description | classes: FaultyRpcAgentTestFixture | imports: torch | [torch testing _internal distributed rpc faulty_rpc_agent_test_fixture.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\rpc_agent_test_fixture.py",
    "summary": "No description | classes: RpcAgentTestFixture | imports: abc, torch | [torch testing _internal distributed rpc rpc_agent_test_fixture.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\rpc_test.py",
    "summary": "No description | classes: StubRpcAgent, MyPickleClass, SlowPickleClass, MyClass, CustomException, TensorWrapper | functions: foo_add, udf_with_torch_ops, _increment_count, _reset_count, _stub_construct_rpc_backend_options_handler, _stub_init_rpc_backend_handler | imports: concurrent, json, threading",
    "role": "src",
    "loc": 5049
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\tensorpipe_rpc_agent_test_fixture.py",
    "summary": "No description | classes: TensorPipeRpcAgentTestFixture | imports: torch | [torch testing _internal distributed rpc tensorpipe_rpc_agent_test_fixture.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed rpc __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\examples\\parameter_server_test.py",
    "summary": "No description | classes: BatchUpdateParameterServer, Trainer, ParameterServerTest | functions: timed_log, run_trainer, run_ps | imports: threading, datetime, torch | [torch testing _internal distributed rpc examples parameter_server_test.py]",
    "role": "examples",
    "loc": 108
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\examples\\reinforcement_learning_rpc_test.py",
    "summary": "Borrowing the ``Policy`` class from the Reinforcement Learning example. | classes: Policy, DummyEnv, Observer, Agent, ReinforcementLearningRpcTest | functions: _call_method, _remote_method, run_agent | imports: numpy, torch | [torch testing _internal distributed rpc examples reinforcement_learning_r",
    "role": "examples",
    "loc": 201
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\examples\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed rpc examples __init__.py]",
    "role": "examples",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\jit\\dist_autograd_test.py",
    "summary": "No description | classes: JitDistAutogradTest | functions: local_add, remote_add, fork_add | imports: torch | [torch testing _internal distributed rpc jit dist_autograd_test.py]",
    "role": "src",
    "loc": 88
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\jit\\rpc_test.py",
    "summary": "No description | classes: RRefAPITest, MyScriptModuleWithRRefs, RRefTypingTest, FutureTypingTest, MyScriptClass, MyModuleInterface | functions: rref_isinstance, sleep, rpc_return_rref, rref_local_value, list_create, rref_list_mutate | imports: io, torch | [torch testing _internal distributed rpc jit",
    "role": "src",
    "loc": 1019
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\jit\\rpc_test_faulty.py",
    "summary": "Run tests for rpc_async in JIT under the faulty agent test fixture to test | classes: JitFaultyAgentRpcTest | functions: two_args_two_kwargs, script_rpc_async_call, rpc_async_call_with_timeout, rpc_async_call_with_timeout_future_ret, rpc_async_call_future_ret, rref_to_here | imports: torch | [torch ",
    "role": "src",
    "loc": 160
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\rpc\\jit\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed rpc jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_shard\\test_common.py",
    "summary": "No description | classes: SimpleMegatronLM | imports: torch | [torch testing _internal distributed _shard test_common.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_shard\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed _shard __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_shard\\sharded_tensor\\_test_ops_common.py",
    "summary": "No description | functions: generate_chunk_sharding_specs_for_test, generate_enumerable_sharding_specs_for_test, generate_local_weight_sharding_params_for_test, clone_module_parameter, gen_binary_op_func | imports: builtins, torch | [torch testing _internal distributed _shard sharded_tensor _test_op",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_shard\\sharded_tensor\\_test_st_common.py",
    "summary": "No description | classes: MyShardedModel2, MyShardedModel1 | functions: _chunk_sharding_specs_list_for_test | imports: copy, random, torch | [torch testing _internal distributed _shard sharded_tensor _test_st_common.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_shard\\sharded_tensor\\__init__.py",
    "summary": "Package initializer | classes: ShardedTensorTestBase | functions: with_comms, wrapper | imports: functools, torch | [torch testing _internal distributed _shard sharded_tensor __init__.py]",
    "role": "src",
    "loc": 76
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_tensor\\common_dtensor.py",
    "summary": "Test decorator which skips the test unless there's a GPU available to torch. | classes: RMSNormPython, MLPModule, MLPStacked, ModelArgs, Attention, FeedForward | functions: skip_unless_torch_gpu, with_comms, decorator, wrapper | imports: dataclasses, functools, torch | [torch testing _internal distr",
    "role": "src",
    "loc": 432
  },
  {
    "id": "torch\\testing\\_internal\\distributed\\_tensor\\__init__.py",
    "summary": "Package initializer | [torch testing _internal distributed _tensor __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\generated\\__init__.py",
    "summary": "Package initializer | [torch testing _internal generated __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\core.py",
    "summary": "Describes which test, or type of tests, should be wrapped in the given | classes: DecorateInfo, SampleInput, ErrorInput, AliasInfo, OpInfo, SampleRule | functions: _getattr_qual, _generate_reduction_inputs, _generate_reduction_kwargs, sample_inputs_reduction, _reference_inputs_elementwise_binary, re",
    "role": "src",
    "loc": 2088
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\refs.py",
    "summary": "An OpInfo for a Python reference of an OpInfo base class operation. | classes: PythonRefInfo, ReductionPythonRefInfo, ElementwiseUnaryPythonRefInfo, ElementwiseBinaryPythonRefInfo | functions: _find_referenced_opinfo, _inherit_constructor_args | imports: torch | [torch testing _internal opinfo refs.",
    "role": "src",
    "loc": 141
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\utils.py",
    "summary": "No description | classes: _dynamic_dispatch_dtypes | functions: get_supported_dtypes, dtypes_dispatch_hint, is_dynamic_dtype_set, str_format_dynamic_dtype, np_unary_ufunc_integer_promotion_wrapper, is_integral | imports: functools, numpy, torch | [torch testing _internal opinfo utils.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch testing _internal opinfo __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\fft.py",
    "summary": "An OpInfo for a Python reference of an elementwise unary operation. | classes: SpectralFuncPythonRefInfo | functions: error_inputs_fft, error_inputs_fftn, sample_inputs_fft_with_min, sample_inputs_fftshift, mt | imports: unittest, functools, numpy, torch | [torch testing _internal opinfo definitions",
    "role": "src",
    "loc": 713
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\linalg.py",
    "summary": "No description | functions: sample_kwargs_vector_norm, ords, sample_inputs_svd, uniformize, fn_U, fn_S | imports: random, unittest, functools, numpy | [torch testing _internal opinfo definitions linalg.py]",
    "role": "src",
    "loc": 2056
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\nested.py",
    "summary": "Contains info on top of the typical OpInfo data that is useful for NJT test generation. | classes: ExtraOpData | functions: _rnd, _raggedness_matches, _clone, _update_sample, random_nt_from_dims, _describe_njt | imports: copy, dataclasses, functools, torch | [torch testing _internal opinfo definitio",
    "role": "src",
    "loc": 1206
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\signal.py",
    "summary": "Base function used to create sample inputs for windows. | functions: sample_inputs_window, reference_inputs_window, reference_inputs_exponential_window, reference_inputs_gaussian_window, reference_inputs_kaiser_window, reference_inputs_general_cosine_window | imports: unittest, functools, numpy, tor",
    "role": "src",
    "loc": 372
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\sparse.py",
    "summary": "No description | functions: _check_validate, _check_fail, _check_success, _sample_inputs_sparse, _error_inputs_sparse, _apply_requires_grad_to_samples | imports: torch | [torch testing _internal opinfo definitions sparse.py]",
    "role": "src",
    "loc": 798
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\special.py",
    "summary": "No description | functions: sample_inputs_i0_i1, sample_inputs_polygamma, reference_polygamma, sample_inputs_entr, sample_inputs_erfcx | imports: unittest, functools, numpy, torch | [torch testing _internal opinfo definitions special.py]",
    "role": "src",
    "loc": 763
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\_masked.py",
    "summary": "No description | functions: sample_inputs_softmax_variant, _generate_masked_op_mask, sample_inputs_masked_reduction, sample_inputs_sparse_coo_masked_reduction, sample_inputs_sparse_csr_masked_reduction, sample_inputs_masked_norm | imports: unittest, functools, numpy, torch | [torch testing _internal",
    "role": "src",
    "loc": 1076
  },
  {
    "id": "torch\\testing\\_internal\\opinfo\\definitions\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch testing _internal opinfo definitions __init__.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\testing\\_internal\\optests\\aot_autograd.py",
    "summary": "Compares func(*args, **kwargs) in eager-mode to under AOTAutograd. | classes: assert_raises_regex | functions: aot_autograd_check, func_no_tensors, _test_aot_autograd_forwards_backwards_helper, call_forwards_backwards, check | imports: torch, functorch, make_fx | [torch testing _internal optests aot",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\testing\\_internal\\optests\\autograd_registration.py",
    "summary": "No description | functions: set_autograd_fallback_mode, autograd_registration_check, not_an_input_and_requires_grad | imports: torch | [torch testing _internal optests autograd_registration.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\testing\\_internal\\optests\\fake_tensor.py",
    "summary": "No description | functions: is_builtin, fake_check | imports: torch | [torch testing _internal optests fake_tensor.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\testing\\_internal\\optests\\generate_tests.py",
    "summary": "For a given test, OpCheckMode intercepts calls to operators and runs | classes: OpCheckMode, OpCheckError, FailuresDict | functions: dontGenerateOpCheckTests, inner, is_abstract, safe_schema_check, safe_autograd_registration_check, safe_fake_check | imports: datetime, difflib, functools, inspect | [",
    "role": "src",
    "loc": 694
  },
  {
    "id": "torch\\testing\\_internal\\optests\\make_fx.py",
    "summary": "No description | functions: make_fx_check, run, handle_sizes_for_dynamic_shapes, f, randomize, transform | imports: torch | [torch testing _internal optests make_fx.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\testing\\_internal\\optests\\__init__.py",
    "summary": "Package initializer | imports: make_fx, aot_autograd, fake_tensor, autograd_registration | [torch testing _internal optests __init__.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\testing\\_internal\\test_module\\future_div.py",
    "summary": "No description | functions: div_int_future, div_float_future | [torch testing _internal test_module future_div.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\testing\\_internal\\test_module\\no_future_div.py",
    "summary": "No description | functions: div_int_nofuture, div_float_nofuture | imports: torch | [torch testing _internal test_module no_future_div.py]",
    "role": "src",
    "loc": 5
  },
  {
    "id": "torch\\testing\\_internal\\test_module\\__init__.py",
    "summary": "Package initializer | [torch testing _internal test_module __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\backend_registration.py",
    "summary": "Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs. | functions: rename_privateuse1_backend, _check_register_once, _normalization_device, _get_current_device_index, _generate_tensor_methods_for_privateuse1_backend, wrap_tensor_backend | impor",
    "role": "src",
    "loc": 281
  },
  {
    "id": "torch\\utils\\bundled_inputs.py",
    "summary": "Helper type for bundled inputs. | classes: InflatableArg | functions: bundle_inputs, augment_model_with_bundled_inputs, augment_many_model_functions_with_bundled_inputs, _inflate_expr, _get_bundled_inputs_attributes_and_methods, _get_inflate_helper_fn_name | imports: textwrap, torch | [torch utils b",
    "role": "src",
    "loc": 359
  },
  {
    "id": "torch\\utils\\checkpoint.py",
    "summary": "A class that manages the default device type for checkpointing. | classes: DefaultDeviceType, CheckpointFunction, _Handle, _Holder, _NoopSaveInputs, _CheckpointFrame | functions: set_checkpoint_debug_enabled, detach_variable, check_backward_validity, _get_device_module, _infer_device_type, add_devic",
    "role": "src",
    "loc": 1059
  },
  {
    "id": "torch\\utils\\collect_env.py",
    "summary": "Return (return-code, stdout, stderr). | functions: run, run_and_read_all, run_and_parse_first_match, run_and_return_first_line, get_conda_packages, get_gcc_version | imports: datetime, json, locale, subprocess | [torch utils collect_env.py]",
    "role": "src",
    "loc": 488
  },
  {
    "id": "torch\\utils\\cpp_backtrace.py",
    "summary": "Return a string containing the C++ stack trace of the current thread. | functions: get_cpp_backtrace | imports: torch | [torch utils cpp_backtrace.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\utils\\cpp_extension.py",
    "summary": "Quote command-line arguments for DOS/Windows conventions. | classes: cls_with_options, BuildExtension | functions: _nt_quote_args, _find_cuda_home, _find_rocm_home, _find_sycl_home, _join_rocm_home, _join_sycl_home | imports: copy, glob, importlib, shlex | [torch utils cpp_extension.py]",
    "role": "src",
    "loc": 2244
  },
  {
    "id": "torch\\utils\\deterministic.py",
    "summary": "No description | classes: _Deterministic | imports: types, torch | [torch utils deterministic.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\utils\\dlpack.py",
    "summary": "from_dlpack(ext_tensor) -> Tensor | classes: DLDeviceType | functions: from_dlpack | imports: torch | [torch utils dlpack.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\utils\\file_baton.py",
    "summary": "A primitive, file-based synchronization utility. | classes: FileBaton | [torch utils file_baton.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\utils\\flop_counter.py",
    "summary": "``FlopCounterMode`` is a context manager that counts the number of flops within its context. | classes: FlopCounterMode, _FlopCounterMode | functions: get_shape, shape_wrapper, nf, register_flop_formula, register_fun, register | imports: torch, module_tracker, typing_extensions, functools | [torch u",
    "role": "src",
    "loc": 612
  },
  {
    "id": "torch\\utils\\hooks.py",
    "summary": "A handle which provides the capability to remove a hook. | classes: RemovableHandle, BackwardHook | functions: unserializable_hook, warn_if_has_hooks | imports: torch, weakref | [torch utils hooks.py]",
    "role": "src",
    "loc": 187
  },
  {
    "id": "torch\\utils\\mkldnn.py",
    "summary": "No description | classes: MkldnnLinear, _MkldnnConvNd, MkldnnConv1d, MkldnnConv2d, MkldnnConv3d, MkldnnBatchNorm | functions: to_mkldnn, m_fn, m_fn_rec | imports: torch | [torch utils mkldnn.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "torch\\utils\\mobile_optimizer.py",
    "summary": "This module contains utility method for mobile model optimization and lint. | classes: LintCode | functions: optimize_for_mobile, generate_mobile_module_lints, _get_bundled_inputs_preserved_attributes | imports: torch | [torch utils mobile_optimizer.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "torch\\utils\\model_zoo.py",
    "summary": "No description | imports: torch | [torch utils model_zoo.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\utils\\module_tracker.py",
    "summary": "``ModuleTracker`` is a context manager that tracks the nn.Module hierarchy during execution | classes: ModuleTracker | imports: weakref, torch | [torch utils module_tracker.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "torch\\utils\\show_pickle.py",
    "summary": "No description | classes: FakeObject, FakeClass, DumpUnpickler | functions: main | imports: pickle, struct, pprint, zipfile | [torch utils show_pickle.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\utils\\throughput_benchmark.py",
    "summary": "Define time formatting. | classes: ExecutionStats, ThroughputBenchmark | functions: format_time | imports: torch | [torch utils throughput_benchmark.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\utils\\weak.py",
    "summary": "No description | classes: WeakIdRef, _WeakHashRef, WeakIdKeyDictionary, TensorWeakRef | imports: weakref, _weakrefset, torch, copy | [torch utils weak.py]",
    "role": "src",
    "loc": 202
  },
  {
    "id": "torch\\utils\\_backport_slots.py",
    "summary": "No description | functions: dataclass_slots, _get_slots, _add_slots, _dataclass_getstate, _dataclass_setstate | imports: dataclasses, _typeshed | [torch utils _backport_slots.py]",
    "role": "src",
    "loc": 69
  },
  {
    "id": "torch\\utils\\_config_module.py",
    "summary": "Represents a config with richer behaviour than just a default value. | classes: _Config, ConfigModuleInstance, _ConfigEntry, ConfigPatch, ConfigModule, _TestCase | functions: Config, _read_env_variable, install_config_module, visit, get_assignments_with_compile_ignored_comments, patch_object | impor",
    "role": "src",
    "loc": 615
  },
  {
    "id": "torch\\utils\\_content_store.py",
    "summary": "Lazily wrap a function with torch.compile on the first call | classes: ContentStoreWriter, ContentStoreReader | functions: lazy_compile, decorate_fn, compile_hook, hash_storage_kernel, hash_storage | imports: ctypes, functools, hashlib, struct | [torch utils _content_store.py]",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\utils\\_contextlib.py",
    "summary": "Allow a context manager to be used as a decorator. | classes: _DecoratorContextManager, _NoParamDecoratorContextManager | functions: _wrap_generator, generator_context, context_decorator, ctx_factory, decorate_context | imports: functools, inspect | [torch utils _contextlib.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "torch\\utils\\_cpp_embed_headers.py",
    "summary": "No description | functions: read_file, _embed_headers, embed_headers | [torch utils _cpp_embed_headers.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\utils\\_cpp_extension_versioner.py",
    "summary": "No description | classes: ExtensionVersioner | functions: update_hash, hash_source_files, hash_build_arguments | [torch utils _cpp_extension_versioner.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\utils\\_cxx_pytree.py",
    "summary": "Contains utility functions for working with nested python data structures. | classes: _DummyLeaf, LeafSpecMeta, LeafSpec | functions: _reverse_args, wrapped, register_pytree_node, _register_pytree_node, _private_register_pytree_node, _is_pytreespec_instance | imports: functools, types, typing_extens",
    "role": "src",
    "loc": 877
  },
  {
    "id": "torch\\utils\\_device.py",
    "summary": "No description | classes: DeviceContext | functions: _device_constructors, device_decorator, set_device | imports: torch, functools | [torch utils _device.py]",
    "role": "src",
    "loc": 89
  },
  {
    "id": "torch\\utils\\_exposed_in.py",
    "summary": "No description | functions: exposed_in, wrapper | [torch utils _exposed_in.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\utils\\_filelock.py",
    "summary": "This behaves like a normal file lock. | classes: FileLock | imports: types, typing_extensions, filelock, torch | [torch utils _filelock.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\utils\\_foreach_utils.py",
    "summary": "Return the device type list that supports foreach kernels. | functions: _get_foreach_kernels_supported_devices, _get_fused_kernels_supported_devices, _group_tensors_by_device_and_dtype, _device_has_foreach_support, _has_foreach_support | imports: torch, typing_extensions | [torch utils _foreach_util",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\utils\\_freeze.py",
    "summary": "Freeze Python packages. | classes: FrozenModule, Freezer | functions: indent_msg, wrapper, main | imports: argparse, functools, marshal, types | [torch utils _freeze.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "torch\\utils\\_functools.py",
    "summary": "Like `@functools.cache` but for methods. | functions: cache_method, wrap | imports: functools, typing_extensions | [torch utils _functools.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\utils\\_get_clean_triton.py",
    "summary": "No description | functions: remove_triton_function_declaration, remove_async_compile, rename_kernels, merge_params, add_launch_params, replace | imports: argparse | [torch utils _get_clean_triton.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "torch\\utils\\_import_utils.py",
    "summary": "Returns if a top-level module with :attr:`name` exists *without** | functions: _check_module_exists, dill_available, import_dill | imports: functools, importlib, types, torch | [torch utils _import_utils.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\utils\\_mode_utils.py",
    "summary": "No description | functions: all_same_mode | imports: torch | [torch utils _mode_utils.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\utils\\_ordered_set.py",
    "summary": "Insertion ordered set, similar to OrderedDict. | classes: OrderedSet | [torch utils _ordered_set.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "torch\\utils\\_python_dispatch.py",
    "summary": "A ``TorchDispatchMode`` allows you to override the meaning of all | classes: TorchDispatchMode, BaseTorchDispatchMode, TensorWithFlatten, AliasInfo, SchemaInfo | functions: is_in_torch_dispatch_mode, _get_current_dispatch_mode, _detect_infra_mode, _unset_infra_mode, _disable_infra_mode, _get_current",
    "role": "src",
    "loc": 521
  },
  {
    "id": "torch\\utils\\_pytree.py",
    "summary": "Contains utility functions for working with nested python data structures. | classes: KeyEntry, EnumEncoder, NodeDef, _SerializeNodeDef, ConstantNode, SequenceKey | functions: register_pytree_node, register_dataclass, register_constant, _flatten, _unflatten, _flatten_with_keys | imports: dataclasses",
    "role": "src",
    "loc": 1397
  },
  {
    "id": "torch\\utils\\_stats.py",
    "summary": "No description | functions: count_label, count, wrapper | imports: functools, typing_extensions | [torch utils _stats.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\utils\\_thunk.py",
    "summary": "A simple lazy evaluation implementation that lets you delay | classes: Thunk | [torch utils _thunk.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\utils\\_traceback.py",
    "summary": "No description | classes: CapturedTraceback | functions: report_compile_source_on_error, shorten_filename, format_frame, format_traceback_short, _extract_symbolized_tb | imports: types, tempfile, traceback, inspect | [torch utils _traceback.py]",
    "role": "src",
    "loc": 144
  },
  {
    "id": "torch\\utils\\_triton.py",
    "summary": "No description | functions: has_triton_package, has_triton_tma, has_triton_tma_device, has_triton, cuda_extra_check, cpu_extra_check | imports: functools, hashlib, triton, torch | [torch utils _triton.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\utils\\_typing_utils.py",
    "summary": "Miscellaneous utilities to aid with typing. | functions: not_none | [torch utils _typing_utils.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\utils\\_zip.py",
    "summary": "No description | functions: remove_prefix, write_to_zip, main | imports: argparse, glob, zipfile | [torch utils _zip.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torch\\utils\\__init__.py",
    "summary": "Set the module attribute on a python object for a given object for nicer printing | functions: set_module, swap_tensors, swap_attr, error_pre_hook, check_use_count | imports: copyreg, weakref, torch | [torch utils __init__.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\utils\\backcompat\\__init__.py",
    "summary": "Package initializer | classes: Warning | imports: torch | [torch utils backcompat __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\utils\\benchmark\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils benchmark __init__.py]",
    "role": "benchmarks",
    "loc": 6
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\blas_compare_setup.py",
    "summary": "Convenience method. | functions: conda_run, main | imports: shutil, subprocess, conda | [torch utils benchmark examples blas_compare_setup.py]",
    "role": "examples",
    "loc": 172
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\compare.py",
    "summary": "Example of Timer and Compare APIs: | classes: FauxTorch | functions: main | imports: pickle, torch | [torch utils benchmark examples compare.py]",
    "role": "examples",
    "loc": 75
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\fuzzer.py",
    "summary": "Example of the Timer and Fuzzer APIs: | functions: main, time_fn | imports: torch | [torch utils benchmark examples fuzzer.py]",
    "role": "examples",
    "loc": 69
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\op_benchmark.py",
    "summary": "Example use of Timer and op fuzzers to measure kernel performance. | functions: assert_dicts_equal, run, main | imports: numpy, torch, operator | [torch utils benchmark examples op_benchmark.py]",
    "role": "examples",
    "loc": 82
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\simple_timeit.py",
    "summary": "Trivial use of Timer API: | functions: main | imports: torch | [torch utils benchmark examples simple_timeit.py]",
    "role": "examples",
    "loc": 17
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\spectral_ops_fuzz_test.py",
    "summary": "Microbenchmarks for the torch.fft module | functions: _dim_options, run_benchmark, _output_csv | imports: argparse, torch | [torch utils benchmark examples spectral_ops_fuzz_test.py]",
    "role": "examples",
    "loc": 98
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\__init__.py",
    "summary": "Package initializer | [torch utils benchmark examples __init__.py]",
    "role": "examples",
    "loc": 0
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\sparse\\compare.py",
    "summary": "Example of Timer and Compare APIs: | classes: Sparse, FauxTorch | functions: generate_coo_data, gen_sparse, main | imports: pickle, torch | [torch utils benchmark examples sparse compare.py]",
    "role": "examples",
    "loc": 103
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\sparse\\fuzzer.py",
    "summary": "Example of the Timer and Sparse Fuzzer APIs: | functions: main, time_fn | imports: torch | [torch utils benchmark examples sparse fuzzer.py]",
    "role": "examples",
    "loc": 87
  },
  {
    "id": "torch\\utils\\benchmark\\examples\\sparse\\op_benchmark.py",
    "summary": "Example use of Timer and sparse op fuzzers to measure kernel performance. | functions: assert_dicts_equal, run, main | imports: numpy, torch, operator | [torch utils benchmark examples sparse op_benchmark.py]",
    "role": "examples",
    "loc": 82
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\binary.py",
    "summary": "No description | classes: BinaryOpFuzzer | imports: numpy, torch | [torch utils benchmark op_fuzzers binary.py]",
    "role": "benchmarks",
    "loc": 84
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\sparse_binary.py",
    "summary": "No description | classes: BinaryOpSparseFuzzer | imports: numpy, torch | [torch utils benchmark op_fuzzers sparse_binary.py]",
    "role": "benchmarks",
    "loc": 89
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\sparse_unary.py",
    "summary": "No description | classes: UnaryOpSparseFuzzer | imports: numpy, torch | [torch utils benchmark op_fuzzers sparse_unary.py]",
    "role": "benchmarks",
    "loc": 69
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\spectral.py",
    "summary": "No description | classes: SpectralOpFuzzer | functions: power_range | imports: torch | [torch utils benchmark op_fuzzers spectral.py]",
    "role": "benchmarks",
    "loc": 71
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\unary.py",
    "summary": "No description | classes: UnaryOpFuzzer | imports: numpy, torch | [torch utils benchmark op_fuzzers unary.py]",
    "role": "benchmarks",
    "loc": 62
  },
  {
    "id": "torch\\utils\\benchmark\\op_fuzzers\\__init__.py",
    "summary": "Package initializer | [torch utils benchmark op_fuzzers __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\common.py",
    "summary": "Base shared classes and utilities. | classes: TaskSpec, Measurement | functions: select_unit, unit_to_english, trim_sigfig, ordered_unique, set_torch_threads, _make_temp_dir | imports: dataclasses, shutil, tempfile, textwrap | [torch utils benchmark utils common.py]",
    "role": "benchmarks",
    "loc": 266
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\compare.py",
    "summary": "Display class to aggregate and print the results of many measurements. | classes: Colorize, _Column, _Row, Table, Compare | functions: optional_min | imports: torch, operator | [torch utils benchmark utils compare.py]",
    "role": "benchmarks",
    "loc": 288
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\compile.py",
    "summary": "No description | functions: _enable_tensor_cores, _disable_tensor_cores, bench_loop, benchmark_compile, bench_all | imports: torch, tabulate | [torch utils benchmark utils compile.py]",
    "role": "benchmarks",
    "loc": 147
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\cpp_jit.py",
    "summary": "JIT C++ strings into executables. | functions: _get_build_root, get_compat_bindings, _compile_template, compile_timeit_template, compile_callgrind_template | imports: atexit, shutil, textwrap, threading | [torch utils benchmark utils cpp_jit.py]",
    "role": "benchmarks",
    "loc": 101
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\fuzzer.py",
    "summary": "Specification for a parameter to be generated during fuzzing. | classes: FuzzedParameter, ParameterAlias, FuzzedTensor, Fuzzer | functions: dtype_size, prod | imports: functools, torch, numpy | [torch utils benchmark utils fuzzer.py]",
    "role": "benchmarks",
    "loc": 393
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\sparse_fuzzer.py",
    "summary": "No description | classes: FuzzedSparseTensor | imports: numbers, torch | [torch utils benchmark utils sparse_fuzzer.py]",
    "role": "benchmarks",
    "loc": 109
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\timer.py",
    "summary": "Timer class based on the timeit.Timer class, but torch aware. | classes: Language, CPPTimer, Timer | functions: timer | imports: timeit, textwrap, torch | [torch utils benchmark utils timer.py]",
    "role": "benchmarks",
    "loc": 420
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\_stubs.py",
    "summary": "This is the portion of the `timeit.Timer` API used by benchmark utils. | classes: TimerClass, TimeitModuleType, CallgrindModuleType | [torch utils benchmark utils _stubs.py]",
    "role": "benchmarks",
    "loc": 30
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\__init__.py",
    "summary": "Package initializer | [torch utils benchmark utils __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\valgrind_wrapper\\timer_interface.py",
    "summary": "Intermediate layer between `Timer` and `valgrind`. | classes: FunctionCount, FunctionCounts, CallgrindStats, Serialization, CopyIfCallgrind, GlobalsBridge | functions: wrapper_singleton | imports: dataclasses, pickle, shutil, subprocess | [torch utils benchmark utils valgrind_wrapper timer_interface",
    "role": "benchmarks",
    "loc": 714
  },
  {
    "id": "torch\\utils\\benchmark\\utils\\valgrind_wrapper\\__init__.py",
    "summary": "Package initializer | [torch utils benchmark utils valgrind_wrapper __init__.py]",
    "role": "benchmarks",
    "loc": 0
  },
  {
    "id": "torch\\utils\\bottleneck\\__init__.py",
    "summary": "Package initializer | [torch utils bottleneck __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\bottleneck\\__main__.py",
    "summary": "No description | functions: redirect_argv, compiled_with_cuda, run_env_analysis, run_cprofile, print_cprofile_summary, run_autograd_prof | imports: argparse, cProfile, pstats, torch | [torch utils bottleneck __main__.py]",
    "role": "src",
    "loc": 170
  },
  {
    "id": "torch\\utils\\data\\backward_compatibility.py",
    "summary": "No description | functions: worker_init_fn | imports: typing_extensions | [torch utils data backward_compatibility.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\utils\\data\\dataloader.py",
    "summary": "Definition of the DataLoader and associated iterators that subclass _BaseDataLoaderIter. | classes: _DatasetKind, _InfiniteConstantSampler, DataLoader, _BaseDataLoaderIter, _SingleProcessDataLoaderIter, _MultiProcessingDataLoaderIter | functions: _get_distributed_settings, _sharding_worker_init_fn, ",
    "role": "src",
    "loc": 861
  },
  {
    "id": "torch\\utils\\data\\dataset.py",
    "summary": "An abstract class representing a :class:`Dataset`. | classes: Dataset, IterableDataset, TensorDataset, StackDataset, ConcatDataset, ChainDataset | functions: random_split | imports: bisect, typing_extensions, torch | [torch utils data dataset.py]",
    "role": "src",
    "loc": 379
  },
  {
    "id": "torch\\utils\\data\\distributed.py",
    "summary": "Sampler that restricts data loading to a subset of the dataset. | classes: DistributedSampler | imports: torch | [torch utils data distributed.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\utils\\data\\graph.py",
    "summary": "No description | functions: _stub_unpickler, _list_connected_datapipes, getstate_hook, reduce_hook, traverse_dps, traverse | imports: io, pickle, torch, dill | [torch utils data graph.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torch\\utils\\data\\graph_settings.py",
    "summary": "No description | functions: get_all_graph_pipes, _get_all_graph_pipes_helper, _is_sharding_datapipe, apply_sharding, _helper, _is_shuffle_datapipe | imports: inspect, typing_extensions, torch | [torch utils data graph_settings.py]",
    "role": "src",
    "loc": 136
  },
  {
    "id": "torch\\utils\\data\\sampler.py",
    "summary": "Base class for all Samplers. | classes: Sampler, SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler | imports: torch | [torch utils data sampler.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "torch\\utils\\data\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils data __init__.py]",
    "role": "src",
    "loc": 73
  },
  {
    "id": "torch\\utils\\data\\datapipes\\datapipe.py",
    "summary": "No description | classes: DataChunk, IterDataPipe, DFIterDataPipe, MapDataPipe, _DataPipeSerializationWrapper, _IterDataPipeSerializationWrapper | imports: functools, pickle, torch | [torch utils data datapipes datapipe.py]",
    "role": "src",
    "loc": 338
  },
  {
    "id": "torch\\utils\\data\\datapipes\\gen_pyi.py",
    "summary": "No description | functions: materialize_lines, gen_from_template, find_file_paths, extract_method_name, extract_class_name, parse_datapipe_file | [torch utils data datapipes gen_pyi.py]",
    "role": "src",
    "loc": 260
  },
  {
    "id": "torch\\utils\\data\\datapipes\\_decorator.py",
    "summary": "No description | classes: functional_datapipe, guaranteed_datapipes_determinism, non_deterministic, runtime_validation_disabled | functions: argument_validation, wrapper, runtime_validation | imports: inspect, functools, torch | [torch utils data datapipes _decorator.py]",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\utils\\data\\datapipes\\_hook_iterator.py",
    "summary": "These are the snapshotting-related states that IterDataPipes can be in. | classes: _SnapshotState, IteratorDecorator | functions: _simplify_obj_name, _strip_datapipe_from_name, _generate_input_args_string, _generate_iterdatapipe_msg, _gen_invalid_iterdatapipe_msg, _check_iterator_valid | imports: fu",
    "role": "src",
    "loc": 218
  },
  {
    "id": "torch\\utils\\data\\datapipes\\_typing.py",
    "summary": "Check if the left-side type is a subtype of the right-side type. | classes: GenericMeta, Integer, Boolean, _DataPipeType, _DataPipeMeta, _IterDataPipeMeta | functions: issubtype, _decompose_type, _issubtype_with_constraints, issubinstance, _dp_init_subclass, reinforce_type | imports: functools, numb",
    "role": "src",
    "loc": 331
  },
  {
    "id": "torch\\utils\\data\\datapipes\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils data datapipes __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\utils\\data\\datapipes\\dataframe\\dataframes.py",
    "summary": "No description | classes: CaptureControl, DataFrameTracedOps, Capture, CaptureF, CaptureA, CaptureLikeMock | functions: disable_capture, get_val | imports: torch, unittest | [torch utils data datapipes dataframe dataframes.py]",
    "role": "src",
    "loc": 340
  },
  {
    "id": "torch\\utils\\data\\datapipes\\dataframe\\dataframe_wrapper.py",
    "summary": "No description | classes: PandasWrapper | functions: _try_import_pandas, _with_pandas, get_df_wrapper, set_df_wrapper, create_dataframe, is_dataframe | imports: pandas | [torch utils data datapipes dataframe dataframe_wrapper.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\utils\\data\\datapipes\\dataframe\\datapipes.py",
    "summary": "No description | classes: DataFramesAsTuplesPipe, PerRowDataFramesPipe, ConcatDataFramesPipe, ShuffleDataFramesPipe, FilterDataFramesPipe, ExampleAggregateAsDataFrames | imports: random, torch | [torch utils data datapipes dataframe datapipes.py]",
    "role": "src",
    "loc": 110
  },
  {
    "id": "torch\\utils\\data\\datapipes\\dataframe\\structures.py",
    "summary": "DataChunkDF iterating over individual items inside of DataFrame containers, to access DataFrames user `raw_iterator`. | classes: DataChunkDF | imports: torch | [torch utils data datapipes dataframe structures.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\utils\\data\\datapipes\\dataframe\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils data datapipes dataframe __init__.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\callable.py",
    "summary": "Applies a function over each item from the source DataPipe (functional name: ``map``). | classes: MapperIterDataPipe, CollatorIterDataPipe | functions: _collate_helper | imports: functools, torch, torcharrow | [torch utils data datapipes iter callable.py]",
    "role": "src",
    "loc": 193
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\combinatorics.py",
    "summary": "Generate sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`). | classes: SamplerIterDataPipe, ShufflerIterDataPipe | imports: random, torch | [torch utils data datapipes iter combinatorics.py]",
    "role": "src",
    "loc": 155
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\combining.py",
    "summary": "Concatenates multiple Iterable DataPipes (functional name: ``concat``). | classes: ConcaterIterDataPipe, ForkerIterDataPipe, _ContainerTemplate, _ForkerIterDataPipe, _ChildDataPipe, DemultiplexerIterDataPipe | functions: _no_op | imports: copy, abc, torch | [torch utils data datapipes iter combining",
    "role": "src",
    "loc": 579
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\filelister.py",
    "summary": "Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory. | classes: FileListerIterDataPipe | imports: torch | [torch utils data datapipes iter filelister.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\fileopener.py",
    "summary": "Given pathnames, opens files and yield pathname and file stream in a tuple (functional name: ``open_files``). | classes: FileOpenerIterDataPipe | imports: io, torch | [torch utils data datapipes iter fileopener.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\grouping.py",
    "summary": "Creates mini-batches of data (functional name: ``batch``). | classes: BatcherIterDataPipe, UnBatcherIterDataPipe, GrouperIterDataPipe | functions: __getattr__ | imports: torch | [torch utils data datapipes iter grouping.py]",
    "role": "src",
    "loc": 272
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\routeddecoder.py",
    "summary": "Decodes binary streams from input DataPipe, yields pathname and decoded data in a tuple. | classes: RoutedDecoderIterDataPipe | imports: io, torch | [torch utils data datapipes iter routeddecoder.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\selecting.py",
    "summary": "Filters out elements from the source datapipe according to input ``filter_fn`` (functional name: ``filter``). | classes: FilterIterDataPipe | imports: torch | [torch utils data datapipes iter selecting.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\sharding.py",
    "summary": "No description | classes: SHARDING_PRIORITIES, _ShardingIterDataPipe, ShardingFilterIterDataPipe | imports: torch | [torch utils data datapipes iter sharding.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\streamreader.py",
    "summary": "Given IO streams and their label names, yield bytes with label name as tuple. | classes: StreamReaderIterDataPipe | imports: io, torch | [torch utils data datapipes iter streamreader.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\utils.py",
    "summary": "Wraps an iterable object to create an IterDataPipe. | classes: IterableWrapperIterDataPipe | imports: copy, torch | [torch utils data datapipes iter utils.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\utils\\data\\datapipes\\iter\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils data datapipes iter __init__.py]",
    "role": "src",
    "loc": 61
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\callable.py",
    "summary": "Apply the input function over each item from the source DataPipe (functional name: ``map``). | classes: MapperMapDataPipe | functions: default_fn | imports: torch | [torch utils data datapipes map callable.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\combinatorics.py",
    "summary": "Shuffle the input MapDataPipe via its indices (functional name: ``shuffle``). | classes: ShufflerIterDataPipe | imports: random, torch | [torch utils data datapipes map combinatorics.py]",
    "role": "src",
    "loc": 105
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\combining.py",
    "summary": "Concatenate multiple Map DataPipes (functional name: ``concat``). | classes: ConcaterMapDataPipe, ZipperMapDataPipe | imports: torch | [torch utils data datapipes map combining.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\grouping.py",
    "summary": "Create mini-batches of data (functional name: ``batch``). | classes: BatcherMapDataPipe | imports: torch | [torch utils data datapipes map grouping.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\utils.py",
    "summary": "Wraps a sequence object into a MapDataPipe. | classes: SequenceWrapperMapDataPipe | imports: copy, torch | [torch utils data datapipes map utils.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\utils\\data\\datapipes\\map\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch utils data datapipes map __init__.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\utils\\data\\datapipes\\utils\\common.py",
    "summary": "StreamWrapper is introduced to wrap file handler generated by DataPipe operation like `FileOpener`. | classes: StreamWrapper | functions: validate_input_col, _is_local_fn, _check_unpickable_fn, match_masks, get_file_pathnames_from_root, onerror | imports: fnmatch, functools, inspect, io | [torch uti",
    "role": "src",
    "loc": 334
  },
  {
    "id": "torch\\utils\\data\\datapipes\\utils\\decoder.py",
    "summary": "Decode image data using the given `imagespec`. | classes: ImageHandler, MatHandler, Decoder | functions: basichandlers, handle_extension, g, imagehandler, videohandler, audiohandler | imports: io, json, pickle, tempfile | [torch utils data datapipes utils decoder.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "torch\\utils\\data\\datapipes\\utils\\snapshot.py",
    "summary": "Fast-forward the given DataPipe and its parents by ``n_iterations``, re-doing computations to restore a snapshot. | functions: _simple_graph_snapshot_restoration | imports: torch | [torch utils data datapipes utils snapshot.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\utils\\data\\datapipes\\utils\\__init__.py",
    "summary": "Package initializer | [torch utils data datapipes utils __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\data\\_utils\\collate.py",
    "summary": "Contains definitions of the methods used by the _BaseDataLoaderIter workers. | functions: default_convert, collate, collate_tensor_fn, collate_numpy_array_fn, collate_numpy_scalar_fn, collate_float_fn | imports: copy, torch, numpy | [torch utils data _utils collate.py]",
    "role": "src",
    "loc": 321
  },
  {
    "id": "torch\\utils\\data\\_utils\\fetch.py",
    "summary": "Contains definitions of the methods used by the _BaseDataLoaderIter to fetch data from an iterable-style or map-style da | classes: _BaseDatasetFetcher, _IterableDatasetFetcher, _MapDatasetFetcher | [torch utils data _utils fetch.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\utils\\data\\_utils\\pin_memory.py",
    "summary": "Contains definitions of the methods used by the _BaseDataLoaderIter to put fetched tensors into pinned memory. | functions: _pin_memory_loop, do_one_step, pin_memory | imports: copy, queue, torch | [torch utils data _utils pin_memory.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\utils\\data\\_utils\\signal_handling.py",
    "summary": "Signal handling for multiprocessing data loading. | functions: _set_SIGCHLD_handler, handler | imports: signal, threading, torch | [torch utils data _utils signal_handling.py]",
    "role": "src",
    "loc": 55
  },
  {
    "id": "torch\\utils\\data\\_utils\\worker.py",
    "summary": "\"Contains definitions of the methods used by the _BaseDataLoaderIter workers. | classes: ManagerWatchdog, WorkerInfo, _IterableDatasetStopIteration, _ResumeIteration | functions: get_worker_info, _generate_state, hash, mix, _worker_loop | imports: queue, random, dataclasses, torch | [torch utils dat",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\utils\\data\\_utils\\__init__.py",
    "summary": "Utility classes & functions for data loading. Code in this folder is mostly used by ../dataloder.py. | functions: _set_python_exit_flag | imports: atexit, torch, numpy | [torch utils data _utils __init__.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\utils\\hipify\\constants.py",
    "summary": "Constants for annotations in the mapping. | [torch utils hipify constants.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\utils\\hipify\\cuda_to_hip_mappings.py",
    "summary": "No description | imports: constants | [torch utils hipify cuda_to_hip_mappings.py]",
    "role": "src",
    "loc": 8680
  },
  {
    "id": "torch\\utils\\hipify\\hipify_python.py",
    "summary": "The Python Hipify script. | classes: CurrentState, HipifyResult, InputError, bcolors, GeneratedFileCleaner, TrieNode | functions: openf, _to_unix_path, match_extensions, _fnmatch, matched_files_iter, preprocess_file_and_save_result | imports: argparse, fnmatch, shutil, cuda_to_hip_mappings | [torch ",
    "role": "src",
    "loc": 812
  },
  {
    "id": "torch\\utils\\hipify\\version.py",
    "summary": "No description | [torch utils hipify version.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\utils\\hipify\\__init__.py",
    "summary": "Package initializer | imports: version | [torch utils hipify __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\utils\\jit\\log_extract.py",
    "summary": "No description | functions: extract_ir, make_tensor_from_type, load_graph_and_inputs, time_cuda, time_cpu, run_test | imports: random, torch | [torch utils jit log_extract.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "torch\\utils\\jit\\__init__.py",
    "summary": "Package initializer | [torch utils jit __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\model_dump\\__init__.py",
    "summary": "model_dump: a one-stop shop for TorchScript model inspection. | functions: get_storage_info, hierarchical_pickle, get_model_info, get_pickle, ist, parse_new_format | imports: argparse, io, json, pickle | [torch utils model_dump __init__.py]",
    "role": "src",
    "loc": 330
  },
  {
    "id": "torch\\utils\\model_dump\\__main__.py",
    "summary": "No description | [torch utils model_dump __main__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\utils\\serialization\\config.py",
    "summary": "No description | classes: load, save | imports: torch | [torch utils serialization config.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\utils\\serialization\\__init__.py",
    "summary": "Package initializer | [torch utils serialization __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\utils\\tensorboard\\summary.py",
    "summary": "Casts a half-precision float value into an integer. | functions: half_to_int, int_to_half, _tensor_to_half_val, _tensor_to_complex_val, _tensor_to_list, _calc_scale_factor | imports: json, struct, torch, numpy | [torch utils tensorboard summary.py]",
    "role": "src",
    "loc": 828
  },
  {
    "id": "torch\\utils\\tensorboard\\writer.py",
    "summary": "Provide an API for writing protocol buffers to event files to be consumed by TensorBoard for visualization. | classes: FileWriter, SummaryWriter | imports: torch, matplotlib, tensorboard, _convert_np | [torch utils tensorboard writer.py]",
    "role": "src",
    "loc": 998
  },
  {
    "id": "torch\\utils\\tensorboard\\_convert_np.py",
    "summary": "This module converts objects into numpy array. | functions: make_np, _prepare_pytorch | imports: numpy, torch | [torch utils tensorboard _convert_np.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\utils\\tensorboard\\_embedding.py",
    "summary": "No description | functions: _gfile_join, make_tsv, make_sprite, get_embedding_info, write_pbtxt, make_mat | imports: numpy, _convert_np, _utils, tensorboard | [torch utils tensorboard _embedding.py]",
    "role": "src",
    "loc": 59
  },
  {
    "id": "torch\\utils\\tensorboard\\_onnx_graph.py",
    "summary": "No description | functions: load_onnx_graph, parse | imports: tensorboard, onnx | [torch utils tensorboard _onnx_graph.py]",
    "role": "src",
    "loc": 49
  },
  {
    "id": "torch\\utils\\tensorboard\\_proto_graph.py",
    "summary": "Create a dict of objects matching a NodeDef's attr field. | functions: attr_value_proto, tensor_shape_proto, node_proto | imports: tensorboard | [torch utils tensorboard _proto_graph.py]",
    "role": "src",
    "loc": 44
  },
  {
    "id": "torch\\utils\\tensorboard\\_pytorch_graph.py",
    "summary": "Parse an optimized PyTorch model graph and produces a list of nodes and node stats. | classes: NodeBase, NodePy, NodePyIO, NodePyOP, GraphPy | functions: parse, parse_traced_name, graph, _set_model_to_eval, _node_get | imports: tensorboard, torch, _proto_graph | [torch utils tensorboard _pytorch_gra",
    "role": "src",
    "loc": 293
  },
  {
    "id": "torch\\utils\\tensorboard\\_utils.py",
    "summary": "Render matplotlib figure to numpy format. | functions: figure_to_image, render_to_rgb, _prepare_video, is_power2, make_grid, convert_to_HWC | imports: numpy, matplotlib | [torch utils tensorboard _utils.py]",
    "role": "src",
    "loc": 95
  },
  {
    "id": "torch\\utils\\tensorboard\\__init__.py",
    "summary": "Package initializer | imports: tensorboard, torch, writer | [torch utils tensorboard __init__.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\utils\\viz\\_cycles.py",
    "summary": "No description | classes: Node | functions: observe_garbage, disable, gc_callback, do_collect, remove, _get_cell_type | imports: gc, types, weakref, json | [torch utils viz _cycles.py]",
    "role": "src",
    "loc": 397
  },
  {
    "id": "torch\\utils\\viz\\__init__.py",
    "summary": "Package initializer | [torch utils viz __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\_strobelight\\cli_function_profiler.py",
    "summary": "Raised when an error happens during strobelight profiling | classes: StrobelightCLIProfilerError, StrobelightCLIFunctionProfiler | functions: _pid_namespace_link, _pid_namespace, _command_to_string, strobelight, strobelight_inner, wrapper_function | imports: functools, subprocess, threading, typing_",
    "role": "src",
    "loc": 246
  },
  {
    "id": "torch\\utils\\_strobelight\\__init__.py",
    "summary": "Package initializer | [torch utils _strobelight __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\utils\\_strobelight\\examples\\cli_function_profiler_example.py",
    "summary": "No description | functions: fn, work, work2 | imports: torch | [torch utils _strobelight examples cli_function_profiler_example.py]",
    "role": "examples",
    "loc": 24
  },
  {
    "id": "torch\\utils\\_sympy\\functions.py",
    "summary": "We maintain this so that: | classes: FloorDiv, ModularIndexing, Where, PythonMod, Mod, CleanDiv | functions: _is_symbols_binary_summation, _keep_float, inner, fuzzy_eq, simple_floordiv_gcd, integer_coefficient | imports: functools, operator, typing_extensions, sympy | [torch utils _sympy functions.p",
    "role": "src",
    "loc": 927
  },
  {
    "id": "torch\\utils\\_sympy\\interp.py",
    "summary": "This is a simple interpreter for Sympy expressions that dispatches to | functions: handlers, _run_sympy_handler, sympy_interp | imports: functools, sympy, torch, functions | [torch utils _sympy interp.py]",
    "role": "src",
    "loc": 171
  },
  {
    "id": "torch\\utils\\_sympy\\numbers.py",
    "summary": "Positive integer infinite quantity. | classes: IntInfinity, NegativeIntInfinity | imports: mpmath, sympy | [torch utils _sympy numbers.py]",
    "role": "src",
    "loc": 313
  },
  {
    "id": "torch\\utils\\_sympy\\printers.py",
    "summary": "No description | classes: ExprPrinter, PythonPrinter, CppPrinter | imports: sympy | [torch utils _sympy printers.py]",
    "role": "src",
    "loc": 324
  },
  {
    "id": "torch\\utils\\_sympy\\reference.py",
    "summary": "No description | classes: ReferenceAnalysis, PythonReferenceAnalysis, OptimizedPythonReferenceAnalysis, TensorReferenceAnalysis | functions: _to_dtype | imports: operator, sympy, torch | [torch utils _sympy reference.py]",
    "role": "src",
    "loc": 418
  },
  {
    "id": "torch\\utils\\_sympy\\singleton_int.py",
    "summary": "No description | classes: SingletonInt | functions: _eval_is_ge | imports: sympy | [torch utils _sympy singleton_int.py]",
    "role": "src",
    "loc": 64
  },
  {
    "id": "torch\\utils\\_sympy\\solve.py",
    "summary": "No description | functions: mirror_rel_op, try_solve, _try_isolate_lhs | imports: sympy, torch | [torch utils _sympy solve.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\utils\\_sympy\\symbol.py",
    "summary": "This file contains canonical definitions for our symbol naming conventions, | classes: SymT | functions: make_symbol, symbol_is_type, free_symbol_is_type | imports: sympy | [torch utils _sympy symbol.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torch\\utils\\_sympy\\value_ranges.py",
    "summary": "No description | classes: ValueRangeError, ValueRanges, SymPyValueRangeAnalysis | functions: simple_sympify, sympy_generic_le, vr_is_bool, vr_is_expr, bound_sympy, missing_handler | imports: dataclasses, functools, operator, typing_extensions | [torch utils _sympy value_ranges.py]",
    "role": "src",
    "loc": 794
  },
  {
    "id": "torch\\utils\\_sympy\\__init__.py",
    "summary": "Package initializer | [torch utils _sympy __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\xpu\\memory.py",
    "summary": "Release all unoccupied cached memory currently held by the caching | functions: empty_cache, reset_peak_memory_stats, reset_accumulated_memory_stats, memory_stats_as_nested_dict, memory_stats, _recurse_add_to_result | imports: torch | [torch xpu memory.py]",
    "role": "src",
    "loc": 156
  },
  {
    "id": "torch\\xpu\\random.py",
    "summary": "Return the random number generator state of the specified GPU as a ByteTensor. | functions: get_rng_state, get_rng_state_all, set_rng_state, cb, set_rng_state_all, manual_seed | imports: torch | [torch xpu random.py]",
    "role": "src",
    "loc": 131
  },
  {
    "id": "torch\\xpu\\streams.py",
    "summary": "Wrapper around a XPU stream. | classes: Stream, Event | imports: ctypes, torch | [torch xpu streams.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\xpu\\_gpu_trace.py",
    "summary": "No description | functions: register_callback_for_event_creation, register_callback_for_event_deletion, register_callback_for_event_record, register_callback_for_event_wait, register_callback_for_memory_allocation, register_callback_for_memory_deallocation | imports: torch | [torch xpu _gpu_trace.py",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\xpu\\_utils.py",
    "summary": "Get the device index from :attr:`device`, which can be a torch.device | functions: _get_device_index | imports: torch | [torch xpu _utils.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\xpu\\__init__.py",
    "summary": "This package introduces support for the XPU backend, specifically tailored for | classes: _DeviceGuard, device, device_of, StreamContext | functions: _is_compiled, _exchange_device, _maybe_exchange_device, device_count, is_available, is_bf16_supported | imports: threading, traceback, functools, torc",
    "role": "src",
    "loc": 427
  },
  {
    "id": "torch\\_awaits\\__init__.py",
    "summary": "Package initializer | classes: _PyAwaitMeta, _Await | imports: torch | [torch _awaits __init__.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\_custom_op\\autograd.py",
    "summary": "No description | functions: autograd_kernel_indirection, inner, autograd_not_implemented, kernel, mark_non_differentiable, construct_autograd_kernel | imports: torch, functools | [torch _custom_op autograd.py]",
    "role": "src",
    "loc": 204
  },
  {
    "id": "torch\\_custom_op\\impl.py",
    "summary": "This API is deprecated, please use torch.library.custom_op instead | classes: CustomOp, FuncAndLocation | functions: warn_deprecated, custom_op, inner, find_ophandle_or_throw, validate_namespace, validate_schema | imports: dataclasses, functools, inspect, weakref | [torch _custom_op impl.py]",
    "role": "src",
    "loc": 519
  },
  {
    "id": "torch\\_custom_op\\__init__.py",
    "summary": "Package initializer | [torch _custom_op __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_decomp\\decompositions.py",
    "summary": "No description | classes: Reduction | functions: type_casts, inner, increase_prec, decrease_prec, _unsqueeze_to_dim, tanh_backward | imports: functools, numbers, operator, torch | [torch _decomp decompositions.py]",
    "role": "src",
    "loc": 4161
  },
  {
    "id": "torch\\_decomp\\decompositions_for_jvp.py",
    "summary": "No description | functions: maybe_register_decomposition, decorator, register_decomposition_for_jvp, _register_jit_decomposition_for_jvp, get_function_def, trace | imports: inspect, torch | [torch _decomp decompositions_for_jvp.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\_decomp\\decompositions_for_rng.py",
    "summary": "Represents a PhiloxRngState - (seed, offset) where offset = base_offset + | classes: PhiloxState, PhiloxStateTracker | functions: register_rng_decomposition, throw_on_non_cuda, rand, rand_like, bernoulli_, bernoulli_p | imports: functools, torch | [torch _decomp decompositions_for_rng.py]",
    "role": "src",
    "loc": 182
  },
  {
    "id": "torch\\_decomp\\__init__.py",
    "summary": "Returns True if the op must always decompose in export/compile tracing system | functions: _should_decompose_because_unsafe_op, _add_op_to_registry, _convert_out_params, _fn, register_decomposition, decomposition_decorator | imports: inspect, functools, typing_extensions, torch | [torch _decomp __in",
    "role": "src",
    "loc": 459
  },
  {
    "id": "torch\\_dispatch\\python.py",
    "summary": "Warning: the set of overloads this will report is very subtle.  It is precisely | classes: Lit | functions: all_py_loaded_overloads, suspend_functionalization, check_tensor_metadata_matches, check_metadata_matches, _fmt, make_crossref_functionalize | imports: unittest, torch | [torch _dispatch pytho",
    "role": "src",
    "loc": 135
  },
  {
    "id": "torch\\_dispatch\\__init__.py",
    "summary": "Package initializer | [torch _dispatch __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_dynamo\\bytecode_analysis.py",
    "summary": "This module provides utilities for analyzing and optimizing Python bytecode. | classes: ReadsWrites, FixedPointBox, StackSize | functions: get_indexof, remove_dead_code, find_live_code, remove_pointless_jumps, propagate_line_nums, populate_line_num | imports: bisect, dataclasses, dis | [torch _dynam",
    "role": "src",
    "loc": 203
  },
  {
    "id": "torch\\_dynamo\\bytecode_transformation.py",
    "summary": "This module provides utilities for analyzing, transforming and manipulating Python bytecode. | classes: InstructionExnTabEntry, Instruction, _NotProvided, ExceptionTableEntry | functions: convert_instruction, inst_has_op_bits, create_instruction, create_jump_absolute, create_load_const, create_dup_t",
    "role": "src",
    "loc": 1305
  },
  {
    "id": "torch\\_dynamo\\cache_size.py",
    "summary": "We track the number of cache entries that have same id_match objects as the | classes: CacheSizeRelevantForFrame | functions: _get_weakref_from_f_locals, _has_same_id_matched_objs, compute_cache_size, is_recompilation, exceeds_recompile_limit | imports: weakref, dataclasses, torch, types | [torch _d",
    "role": "src",
    "loc": 132
  },
  {
    "id": "torch\\_dynamo\\callback.py",
    "summary": "This module provides callback management functionality for TorchDynamo's compilation process. | classes: CompilationCallbackHandler | functions: on_compile_start, on_compile_end | imports: threading, dataclasses, torch | [torch _dynamo callback.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "torch\\_dynamo\\codegen.py",
    "summary": "This module provides utilities for generating Python bytecode in PyTorch's Dynamo system. | classes: GraphOutputEntry, PyCodegen | imports: dataclasses, types, torch, bytecode_transformation | [torch _dynamo codegen.py]",
    "role": "src",
    "loc": 526
  },
  {
    "id": "torch\\_dynamo\\code_context.py",
    "summary": "This module provides thread-safe code context management for TorchDynamo using weak references. | classes: CodeContextDict | imports: types, utils | [torch _dynamo code_context.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\_dynamo\\compiled_autograd.py",
    "summary": "Provides functionality for compiling PyTorch's autograd (automatic differentiation) system. | classes: OpNamespace, Op, AutogradCompilerInstance | functions: snapshot_verbose_logging_enabled, snapshot_cudagraph_enabled, maybe_clone, make_compile_context, _enable, _disable | imports: functools, opera",
    "role": "src",
    "loc": 1103
  },
  {
    "id": "torch\\_dynamo\\comptime.py",
    "summary": "This module provides the public comptime interface to TorchDynamo, enabling users to execute | classes: ComptimeVar, ComptimeContext, _Comptime | imports: builtins, dis, traceback, torch | [torch _dynamo comptime.py]",
    "role": "src",
    "loc": 339
  },
  {
    "id": "torch\\_dynamo\\config.py",
    "summary": "Configuration module for TorchDynamo compiler and optimization settings. | functions: default_debug_dir_root, _make_closure_patcher | imports: getpass, tempfile, torch | [torch _dynamo config.py]",
    "role": "src",
    "loc": 186
  },
  {
    "id": "torch\\_dynamo\\convert_frame.py",
    "summary": "This module implements TorchDynamo's core frame conversion functionality, transforming Python | classes: TODO_UNKNOWN, Tracker, ConvertFrameAssert, ConvertFrame, ConvertFrameProtocol, CatchErrorsWrapper | functions: fx_forward_from_src_skip_result, preserve_global_state, _fn, has_tensor_in_frame, ha",
    "role": "src",
    "loc": 1146
  },
  {
    "id": "torch\\_dynamo\\create_parameter_op.py",
    "summary": "No description | classes: TracableCreateParameter | functions: tracable_create_parameter, new_parameter_placeholder, do_not_convert_to_tracable_parameter, can_convert_to_tracable_parameter | imports: threading, torch | [torch _dynamo create_parameter_op.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torch\\_dynamo\\current_scope_id.py",
    "summary": "Provides thread-local scope identification for SubgraphTracer instances. | functions: current_scope_id, enter_new_scope | imports: threading | [torch _dynamo current_scope_id.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\_dynamo\\debug_utils.py",
    "summary": "Debug utilities for TorchDynamo compilation and execution. | classes: BuckTargetWriter, NNModuleToString, AccuracyError, NopInputReader, InputReader, InputWriter | functions: minifier_dir, _cuda_system_info_comment, generate_env_vars_string, filter, generate_config_string, get_minifier_repro_path | ",
    "role": "src",
    "loc": 675
  },
  {
    "id": "torch\\_dynamo\\decorators.py",
    "summary": "This module provides decorators and utilities for controlling TorchDynamo's behavior during compilation. | classes: set_stance, _DimRange | functions: run, disable, skip, assume_constant_result, allow_in_graph, nonstrict_trace | imports: functools, inspect, dataclasses, typing_extensions | [torch _d",
    "role": "src",
    "loc": 516
  },
  {
    "id": "torch\\_dynamo\\device_interface.py",
    "summary": "Device abstraction layer for TorchDynamo and Inductor backends. | classes: device, Event, Stream, Worker, DeviceInterface, DeviceGuard | functions: register_interface_for_device, get_interface_for_device, get_registered_device_interfaces, init_device_reg | imports: dataclasses, torch, multiprocessin",
    "role": "src",
    "loc": 342
  },
  {
    "id": "torch\\_dynamo\\distributed.py",
    "summary": "Manages process groups for distributed compilation in TorchDynamo. | functions: get_compile_pg | imports: torch | [torch _dynamo distributed.py]",
    "role": "src",
    "loc": 29
  },
  {
    "id": "torch\\_dynamo\\eval_frame.py",
    "summary": "This module implements the core frame evaluation handler for TorchDynamo's compilation system. | classes: Unset, DynamoStance, OptimizedModule, DynamoTLS, _TorchDynamoContext, OptimizeContext | functions: _maybe_set_eval_frame, _set_stance, _callback_from_stance, fail_callback, _is_skip_guard_eval_u",
    "role": "src",
    "loc": 1449
  },
  {
    "id": "torch\\_dynamo\\exc.py",
    "summary": "No description | classes: TorchDynamoException, InternalTorchDynamoError, RestartAnalysis, SpeculationRestartAnalysis, UnspecializeRestartAnalysis, CompileCollectiveRestartAnalysis | functions: exportdb_error_message, get_dynamo_observed_exception, raise_observed_exception, handle_observed_exception",
    "role": "src",
    "loc": 433
  },
  {
    "id": "torch\\_dynamo\\external_utils.py",
    "summary": "This module contains utility functions that are explicitly allowed to be called during | classes: FakeBackwardCFunction, FakeCompiledAutogradEngine | functions: is_compiling, wrap_inline, inner, call_hook, wrap_numpy, wrap | imports: functools, typing_extensions, torch, numpy | [torch _dynamo extern",
    "role": "src",
    "loc": 140
  },
  {
    "id": "torch\\_dynamo\\funcname_cache.py",
    "summary": "This module provides functionality for caching and looking up fully qualified function | functions: clearcache, _add_file, get_funcname | imports: tokenize | [torch _dynamo funcname_cache.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "torch\\_dynamo\\graph_break_hints.py",
    "summary": "No description | [torch _dynamo graph_break_hints.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\_dynamo\\graph_deduplication.py",
    "summary": "This module implements graph deduplication functionality for TorchDynamo's optimization pipeline. | functions: apply_graph_deduplication, _flatten_args_kwargs, flatten, _replace_region_with_subgraph, _get_external_inputs, _get_all_output_indices | imports: operator, torch, graph_region_tracker | [to",
    "role": "src",
    "loc": 182
  },
  {
    "id": "torch\\_dynamo\\graph_region_tracker.py",
    "summary": "This module provides functionality for tracking and managing regions in computational graphs. | classes: NodeHashException, InputPickler, BackwardBfsArgIter, GraphRegionTracker | functions: debug_log, _extract_tensor_metadata_for_node_hash, _extract_tensor_arg, _normalize_args, get_global_state_key,",
    "role": "src",
    "loc": 275
  },
  {
    "id": "torch\\_dynamo\\guards.py",
    "summary": "Core guard system for Dynamo that detects when compiled code needs to be recompiled due to | classes: IndentedBufferWithPrefix, GuardManagerWrapper, NNModuleAttrAccessorInfo, GuardCodeList, GuardManagerType, GuardBuilder | functions: from_numpy, uninteresting_files, _get_closure_vars, _ast_unparse, ",
    "role": "src",
    "loc": 2301
  },
  {
    "id": "torch\\_dynamo\\hooks.py",
    "summary": "Hook system for Dynamo's guard functionality. | classes: Hooks | imports: dataclasses, torch, types | [torch _dynamo hooks.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\_dynamo\\logging.py",
    "summary": "Logging utilities for Dynamo and Inductor. | functions: get_loggers, get_step_logger, log | imports: torch, triton | [torch _dynamo logging.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\_dynamo\\metrics_context.py",
    "summary": "Metrics collection and management system for Dynamo. | classes: MetricsContext, RuntimeMetricsContext | imports: typing_extensions | [torch _dynamo metrics_context.py]",
    "role": "src",
    "loc": 157
  },
  {
    "id": "torch\\_dynamo\\mutation_guard.py",
    "summary": "Mutation tracking and dynamic module detection system for Dynamo. | classes: MutationTracker, GenerationTracker | functions: watch, ensure_patched, custom_setattr, is_dynamic_nn_module, install_generation_tagging_init, patched_init | imports: functools, weakref, torch, utils | [torch _dynamo mutatio",
    "role": "src",
    "loc": 118
  },
  {
    "id": "torch\\_dynamo\\output_graph.py",
    "summary": "Core graph building functionality for PyTorch's Dynamo system. This module contains | classes: VariableTrackerCacheKey, VariableTrackerCache, GraphCompileReason, FakeRootModule, WrapperBackend, OutputGraph | functions: _step_logger, _get_gen_rand_values_fn, _gen_rand_values, check_pt2_compliant_op, ",
    "role": "src",
    "loc": 1914
  },
  {
    "id": "torch\\_dynamo\\pgo.py",
    "summary": "Profile Guided Optimization (PGO) implementation for Dynamo. | classes: ReservedWorkflowIdUserError, CodeId, CodeState, InferStride, AutoUnset, AutoDynamic | functions: update_automatic_dynamic, log_tup, process_automatic_dynamic, get_cache_key, code_state_path, should_use_remote_dynamo_pgo_cache | ",
    "role": "src",
    "loc": 524
  },
  {
    "id": "torch\\_dynamo\\profiler.py",
    "summary": "Dynamo profiling implementation. | classes: ProfileMetrics, ProfileResult, Profiler | functions: should_print_missing, print_missing, fx_insert_profiling, _wrapped | imports: dataclasses, typing_extensions, torch, utils | [torch _dynamo profiler.py]",
    "role": "src",
    "loc": 138
  },
  {
    "id": "torch\\_dynamo\\replay_record.py",
    "summary": "Python execution state recording and replay functionality. | classes: ModuleRecord, DummyModule, ExecutionRecord, ExecutionRecorder | imports: dataclasses, types, typing_extensions, torch | [torch _dynamo replay_record.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\_dynamo\\resume_execution.py",
    "summary": "This module provides functionality for resuming Python execution at specific points in code, | classes: ReenterWith, ResumeFunctionMetadata, ContinueExecutionCache | functions: _initial_push_null, _bytecode_from_template_with_split, _try_except_tf_mode_template, _filter_iter, _load_tuple_and_call | ",
    "role": "src",
    "loc": 514
  },
  {
    "id": "torch\\_dynamo\\side_effects.py",
    "summary": "Maintain records of mutations and provide methods to apply them during code generation. | classes: SideEffects | functions: _manual_dict_setitem, _manual_list_update, allow_side_effects_under_checkpoint, disallow_side_effects_in_generator | imports: inspect, weakref, types, torch | [torch _dynamo si",
    "role": "src",
    "loc": 801
  },
  {
    "id": "torch\\_dynamo\\source.py",
    "summary": "This module provides Source classes that track the origins of values in PyTorch Dynamo. | classes: LocalSource, SyntheticLocalSource, RandomValueSource, GlobalSource, GlobalWeakRefSource, WeakRefCallSource | functions: is_constant_source, is_from_local_source, is_from_unspecialized_param_buffer_sour",
    "role": "src",
    "loc": 603
  },
  {
    "id": "torch\\_dynamo\\symbolic_convert.py",
    "summary": "Core module responsible for converting Python bytecode into TorchDynamo's symbolic execution format. | classes: SpeculationEntry, SpeculationLog, LocalState, DistributedState, TensorifyState, BlockStackEntry | functions: _import_module, _step_logger, save_and_restart_speculation_log, temporarely_all",
    "role": "src",
    "loc": 3088
  },
  {
    "id": "torch\\_dynamo\\tensor_version_op.py",
    "summary": "This module implements tensor version operations for Dynamo tracing. | functions: _tensor_version_fake, _tensor_version_functional, _unsafe_set_version_counter_functional | imports: torch | [torch _dynamo tensor_version_op.py]",
    "role": "src",
    "loc": 46
  },
  {
    "id": "torch\\_dynamo\\testing.py",
    "summary": "Testing utilities and infrastructure for Dynamo. | classes: CompileCounter, CompileCounterWithBackend, EagerAndRecordGraphs, AotEagerAndRecordGraphs | functions: clone_me, remove_optimized_module_prefix, extract_graph_and_tracker, extract_graph_backend, collect_results, requires_bwd_pass | imports: ",
    "role": "src",
    "loc": 402
  },
  {
    "id": "torch\\_dynamo\\test_case.py",
    "summary": "Testing utilities for Dynamo, providing a specialized TestCase class and test running functionality. | classes: TestCase | functions: run_tests | imports: importlib, torch | [torch _dynamo test_case.py]",
    "role": "src",
    "loc": 75
  },
  {
    "id": "torch\\_dynamo\\test_minifier_common.py",
    "summary": "Common utilities for testing Dynamo's minifier functionality. | classes: MinifierTestResult, MinifierTestBase | imports: dataclasses, io, shutil, subprocess | [torch _dynamo test_minifier_common.py]",
    "role": "src",
    "loc": 224
  },
  {
    "id": "torch\\_dynamo\\trace_rules.py",
    "summary": "Track a set of `id()`s of objects which are either allowed or not | classes: FunctionIdSet, SkipResult, FunctionInfo | functions: get_torch_obj_rule_map, _load_obj_from_str, load_object, get_tensor_method, is_aten_op_or_tensor_method, _allowed_callable_ids | imports: abc, builtins, copy, dataclasses",
    "role": "src",
    "loc": 3519
  },
  {
    "id": "torch\\_dynamo\\types.py",
    "summary": "This module contains the core type definitions and protocols used throughout Dynamo. | classes: GuardFail, GuardFn, GuardedCode, ConvertFrameReturn, DynamoCallbackFn, DynamoGuardHook | functions: wrap_guarded_code | imports: dataclasses, types, torch | [torch _dynamo types.py]",
    "role": "src",
    "loc": 87
  },
  {
    "id": "torch\\_dynamo\\utils.py",
    "summary": "Utility functions and classes used throughout the TorchDynamo system. | classes: ReInplaceTrigger, ReinplaceCounters, CompileEventLogLevel, CompileEventLogger, DuplicateWarningChecker, ExactWeakKeyDictionary | functions: tabulate, increment_frame, reset_frame_count, increment_op_count, calculate_tim",
    "role": "src",
    "loc": 3363
  },
  {
    "id": "torch\\_dynamo\\_trace_wrapped_higher_order_op.py",
    "summary": "trace_wrapped(*args, fn) is equivalent to fn(*args), but with a twist: | classes: ModIndex, TransformGetItemToIndex, TraceWrapped | functions: zeros_and_scatter, _, trace_wrapped, _assert_meta, inner_trace, self_invoke | imports: torch | [torch _dynamo _trace_wrapped_higher_order_op.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "torch\\_dynamo\\__init__.py",
    "summary": "TorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. | functions: reset, reset_code_caches | imports: torch, backends, callback, code_context | [torch _dynamo __init__.py]",
    "role": "src",
    "loc": 131
  },
  {
    "id": "torch\\_dynamo\\backends\\common.py",
    "summary": "This module provides common utilities and base classes for TorchDynamo backends. | classes: AotAutograd | functions: aot_autograd, mem_efficient_fusion_kwargs, fake_tensor_unsupported, wrapper, device_from_inputs, dtype_from_inputs | imports: functools, unittest, torch, functorch | [torch _dynamo ba",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\_dynamo\\backends\\cudagraphs.py",
    "summary": "This module implements CUDA graphs support for TorchDynamo backends. | classes: CudagraphsBackend | functions: find_input_mutations, meta_fk, get_device_node_mapping, check_for_mutation_ignore_cuda_graph_managed_tensor, check_for_skip, get_device_index | imports: functools, torch, registry | [torch ",
    "role": "src",
    "loc": 218
  },
  {
    "id": "torch\\_dynamo\\backends\\debugging.py",
    "summary": "This module provides debugging backends for TorchDynamo to help diagnose and troubleshoot | classes: ReluCompileError, TestingOnlyCompileError, ExplainOutput, ExplainWithBackend | functions: eager, make_eager_backend_with_torch_function_mode, make_eager_backend_with_torch_function_modes, fn, eager_n",
    "role": "src",
    "loc": 334
  },
  {
    "id": "torch\\_dynamo\\backends\\distributed.py",
    "summary": "This module implements distributed training optimizations for TorchDynamo backends. | classes: Bucket, WrapperModule, FakeifyFirstAOTInvocationGuard, SubmodCompiler, DDPOptimizer | functions: args_str, bucket_has_external_output, pretty_print_buckets, has_higher_order_op | imports: traceback, datacl",
    "role": "src",
    "loc": 394
  },
  {
    "id": "torch\\_dynamo\\backends\\inductor.py",
    "summary": "This module provides the TorchInductor backend integration for TorchDynamo. | functions: inductor | imports: torch | [torch _dynamo backends inductor.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torch\\_dynamo\\backends\\onnxrt.py",
    "summary": "No description | functions: has_onnxruntime, information_displaying_backend | imports: torch, registry | [torch _dynamo backends onnxrt.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\_dynamo\\backends\\registry.py",
    "summary": "This module implements TorchDynamo's backend registry system for managing compiler backends. | classes: CompiledFn | functions: register_backend, lookup_backend, list_backends, _lazy_import, _discover_entrypoint_backends | imports: functools, importlib, torch, exc | [torch _dynamo backends registry.",
    "role": "src",
    "loc": 139
  },
  {
    "id": "torch\\_dynamo\\backends\\tensorrt.py",
    "summary": "Placeholder for TensorRT backend for dynamo via torch-tensorrt | [torch _dynamo backends tensorrt.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torch\\_dynamo\\backends\\torchxla.py",
    "summary": "No description | functions: openxla_eval, openxla_eval_boxed, xla_backend_helper, fwd | imports: functorch, backends, registry, torch_xla | [torch _dynamo backends torchxla.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\_dynamo\\backends\\tvm.py",
    "summary": "This module provides TVM backend integration for TorchDynamo. | functions: tvm, to_torch_tensor, to_tvm_tensor, exec_tvm, has_tvm, llvm_target | imports: functools, importlib, tempfile, types | [torch _dynamo backends tvm.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torch\\_dynamo\\backends\\__init__.py",
    "summary": "Package initializer | [torch _dynamo backends __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_dynamo\\polyfills\\builtins.py",
    "summary": "Python polyfills for builtins | functions: all, any, enumerate, sum | imports: builtins, functools, operator, decorators | [torch _dynamo polyfills builtins.py]",
    "role": "src",
    "loc": 42
  },
  {
    "id": "torch\\_dynamo\\polyfills\\functools.py",
    "summary": "Python polyfills for functools | classes: _INITIAL_MISSING | functions: reduce | imports: functools, decorators | [torch _dynamo polyfills functools.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\_dynamo\\polyfills\\itertools.py",
    "summary": "Python polyfills for itertools | functions: chain, chain_from_iterable, compress, dropwhile, islice, pairwise | imports: typing_extensions, decorators | [torch _dynamo polyfills itertools.py]",
    "role": "src",
    "loc": 159
  },
  {
    "id": "torch\\_dynamo\\polyfills\\loader.py",
    "summary": "No description | imports: importlib, types | [torch _dynamo polyfills loader.py]",
    "role": "src",
    "loc": 23
  },
  {
    "id": "torch\\_dynamo\\polyfills\\operator.py",
    "summary": "Python polyfills for operator | functions: attrgetter, resolve_attr, getter, itemgetter, methodcaller, caller | imports: operator, typing_extensions, decorators | [torch _dynamo polyfills operator.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torch\\_dynamo\\polyfills\\os.py",
    "summary": "Python polyfills for os | functions: fspath | imports: decorators | [torch _dynamo polyfills os.py]",
    "role": "src",
    "loc": 27
  },
  {
    "id": "torch\\_dynamo\\polyfills\\pytree.py",
    "summary": "Python polyfills for torch.utils.pytree | classes: _Asterisk, PyTreeSpec | functions: _, tree_is_leaf, tree_iter, tree_leaves, _is_pytreespec_instance, tree_flatten | imports: dataclasses, typing_extensions, torch, decorators | [torch _dynamo polyfills pytree.py]",
    "role": "src",
    "loc": 339
  },
  {
    "id": "torch\\_dynamo\\polyfills\\sys.py",
    "summary": "Python polyfills for sys | functions: intern, getrecursionlimit | imports: decorators | [torch _dynamo polyfills sys.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\_dynamo\\polyfills\\__init__.py",
    "summary": "Python polyfills for common builtins. | classes: NoEnterTorchFunctionMode | functions: index, repeat, radians, accumulate_grad, list_cmp, set_isdisjoint | imports: types, torch, utils | [torch _dynamo polyfills __init__.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\_dynamo\\repro\\after_aot.py",
    "summary": "Utilities for reproducing and debugging issues in PyTorch's Dynamo AOT compilation. | classes: WriterInterp, ExactReaderInterp, ReaderInterp | functions: wrap_compiler_debug, debug_wrapper, deferred_for_real_inputs, inner_debug_fn, maybe_fbcode_instructions, generate_compiler_repro_string | imports:",
    "role": "src",
    "loc": 852
  },
  {
    "id": "torch\\_dynamo\\repro\\after_dynamo.py",
    "summary": "Utilities for reproducing and debugging issues in Dynamo after graph capture. | classes: WrapBackendDebug | functions: _accuracy_fails, wrap_backend_debug, generate_dynamo_fx_repro_string, dump_backend_repro_as_file, dump_backend_state, dump_to_minify_after_dynamo | imports: argparse, copy, functool",
    "role": "src",
    "loc": 485
  },
  {
    "id": "torch\\_dynamo\\repro\\aoti.py",
    "summary": "Utilities for debugging and reproducing issues in Ahead of Time with Inductor (AOTI) compilation. | classes: AOTIMinifierError | functions: dump_to_minify, get_module_string, _convert_to_comment, save_graph_repro_ep, dump_compiler_graph_state, generate_compiler_repro_exported_program | imports: argp",
    "role": "src",
    "loc": 528
  },
  {
    "id": "torch\\_dynamo\\repro\\__init__.py",
    "summary": "Package initializer | [torch _dynamo repro __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_dynamo\\variables\\base.py",
    "summary": "Core variable tracking functionality for Dynamo. This module defines the fundamental | classes: SourceType, MutationType, ValueMutationNew, ValueMutationExisting, AttributeMutation, AttributeMutationExisting | functions: _is_top_level_scope, is_side_effect_safe, typestr | imports: current_scope_id, ",
    "role": "src",
    "loc": 418
  },
  {
    "id": "torch\\_dynamo\\variables\\builder.py",
    "summary": "This module contains classes and utilities for building variable trackers in Dynamo. | classes: _missing, GraphArg, BackwardStateGraphArg, JITFunction, Autotuner, VariableBuilder | functions: safe_has_grad, _dataclasses_fields_lambda, _clone_input, wrap_fx_proxy, cache_real_value_when_export, wrap_f",
    "role": "src",
    "loc": 2405
  },
  {
    "id": "torch\\_dynamo\\variables\\builtin.py",
    "summary": "A VariableTracker that represents a built-in value (functions and operators). | classes: BuiltinVariable | functions: dynamo_disable_grad | imports: functools, inspect, operator, types | [torch _dynamo variables builtin.py]",
    "role": "src",
    "loc": 1841
  },
  {
    "id": "torch\\_dynamo\\variables\\constant.py",
    "summary": "Constant and enum variable tracking in Dynamo. | classes: ConstantVariable, EnumVariable | imports: operator, torch, exc, utils | [torch _dynamo variables constant.py]",
    "role": "src",
    "loc": 211
  },
  {
    "id": "torch\\_dynamo\\variables\\ctx_manager.py",
    "summary": "This file contains a collection of context manager classes used by Dynamo for tracking | classes: ContextManagerState, ContextWrappingVariable, GenericContextWrappingVariable, GradInplaceRequiresGradCtxManagerVariable, TemporarilyPopInterpreterStackCtxManagerVariable, JvpIncrementNestingCtxManagerVa",
    "role": "src",
    "loc": 1090
  },
  {
    "id": "torch\\_dynamo\\variables\\dicts.py",
    "summary": "Dictionary-related variable tracking classes for PyTorch Dynamo. | classes: _HashableTracker, ConstDictVariable, MappingProxyVariable, NNModuleHooksDictVariable, DefaultDictVariable, SetVariable | functions: is_hashable | imports: functools, types, torch, bytecode_transformation | [torch _dynamo var",
    "role": "src",
    "loc": 743
  },
  {
    "id": "torch\\_dynamo\\variables\\distributed.py",
    "summary": "Distributed computing variable tracking classes for PyTorch Dynamo. | classes: DistributedVariable, WorldMetaClassVariable, PlacementClassVariable, PlacementVariable, DeviceMeshVariable, ProcessGroupVariable | functions: is_from_local, is_constant_pg_functions | imports: functools, inspect, torch, _",
    "role": "src",
    "loc": 335
  },
  {
    "id": "torch\\_dynamo\\variables\\functions.py",
    "summary": "Function-related variable tracking classes for Dynamo's symbolic execution. | classes: BaseUserFunctionVariable, UserFunctionVariable, BuiltinMethodVariable, LocalGeneratorObjectVariable, ContextlibContextManagerLocalGeneratorObjectVariable, LocalGeneratorFunctionVariable | functions: wrap_bound_arg",
    "role": "src",
    "loc": 1503
  },
  {
    "id": "torch\\_dynamo\\variables\\higher_order_ops.py",
    "summary": "This module contains classes and utilities for handling higher-order operators in Dynamo. | classes: TorchHigherOrderOperatorVariable, CustomFunctionHigherOrderOperatorVariable, CondHigherOrderVariable, CallTorchbindHigherOrderVariable, WhileLoopHigherOrderVariable, AssociativeScanHigherOrderVariabl",
    "role": "src",
    "loc": 2386
  },
  {
    "id": "torch\\_dynamo\\variables\\iter.py",
    "summary": "This module provides iterator-related variable tracking functionality for Dynamo. | classes: ItertoolsVariable, IteratorVariable, RepeatIteratorVariable, CountIteratorVariable, CycleIteratorVariable, ZipVariable | imports: operator, bytecode_transformation, exc, base | [torch _dynamo variables iter.",
    "role": "src",
    "loc": 479
  },
  {
    "id": "torch\\_dynamo\\variables\\lazy.py",
    "summary": "Container to cache the real VariableTracker | classes: LazyCache, LazyVariableTracker, LazySymNodeFormatString | functions: _create_realize_and_forward, realize_and_forward, _populate | imports: functools, inspect, typing_extensions, utils | [torch _dynamo variables lazy.py]",
    "role": "src",
    "loc": 161
  },
  {
    "id": "torch\\_dynamo\\variables\\lists.py",
    "summary": "Variable tracking implementations for list-like data structures in Dynamo. | classes: BaseListVariable, RangeVariable, CommonListMethodsVariable, ListVariable, DequeVariable, TupleVariable | imports: inspect, operator, torch, bytecode_transformation | [torch _dynamo variables lists.py]",
    "role": "src",
    "loc": 862
  },
  {
    "id": "torch\\_dynamo\\variables\\misc.py",
    "summary": "This module contains miscellaneous variable tracker implementations for various Python types | classes: NO_SUCH_SUBOBJ, SuperVariable, ExceptionVariable, UnknownVariable, DelayGraphBreakVariable, ComptimeVariable | functions: produce_trampoline_autograd_apply, trampoline_autograd_apply, get_np_to_tn",
    "role": "src",
    "loc": 1249
  },
  {
    "id": "torch\\_dynamo\\variables\\nn_module.py",
    "summary": "This module implements variable tracking for PyTorch nn.Module instances during Dynamo tracing. | classes: NNModuleVariable, UnspecializedNNModuleVariable, UnspecializedBuiltinNNModuleVariable, FSDPManagedNNModuleVariable | functions: initialize_lazy_module, convert_to_fake, record_nn_module_stack, ",
    "role": "src",
    "loc": 924
  },
  {
    "id": "torch\\_dynamo\\variables\\optimizer.py",
    "summary": "This module implements variable tracking for PyTorch optimizers during Dynamo tracing. | classes: ArgMappingException, GuardInstallException, OptimizerVariable | functions: _is_static_for_cudagraphs | imports: weakref, torch, guards, source | [torch _dynamo variables optimizer.py]",
    "role": "src",
    "loc": 290
  },
  {
    "id": "torch\\_dynamo\\variables\\script_object.py",
    "summary": "This module implements variable tracking for TorchScript objects during Dynamo tracing. | classes: TorchScriptObjectVariable | functions: _raise_hard_error_if_graph_break, deco, graph_break_as_hard_error | imports: functools, torch, exc, base | [torch _dynamo variables script_object.py]",
    "role": "src",
    "loc": 74
  },
  {
    "id": "torch\\_dynamo\\variables\\sdpa.py",
    "summary": "Represents the c++ params struct for scaled dot product attention. | classes: SDPAParamsVariable | imports: inspect, bytecode_transformation, exc, source | [torch _dynamo variables sdpa.py]",
    "role": "src",
    "loc": 56
  },
  {
    "id": "torch\\_dynamo\\variables\\tensor.py",
    "summary": "This module contains variable tracker classes for handling tensors and tensor-related operations in Dynamo. | classes: TensorVariable, SymNodeVariable, NumpyNdarrayVariable, UnspecializedPythonVariable, FakeItemVariable, TensorSubclassVariable | functions: is_bound_tensor_method | imports: functools",
    "role": "src",
    "loc": 1138
  },
  {
    "id": "torch\\_dynamo\\variables\\torch.py",
    "summary": "This module implements variable tracking for torch functions and operations during Dynamo tracing. | classes: BaseTorchVariable, TorchCtxManagerClassVariable, TorchInGraphFunctionVariable, DispatchKeySetVariable, FuncTorchInterpreterVariable | functions: get_overridable_functions | imports: functool",
    "role": "src",
    "loc": 1152
  },
  {
    "id": "torch\\_dynamo\\variables\\torch_function.py",
    "summary": "TorchDynamo support for __torch_function__ tensor subclasses. | classes: GetMethodMode, TorchFunctionModeStackStateManager, SymbolicTorchFunctionState, TorchFunctionModeStackVariable, TorchFunctionModeVariable, TensorWithTFOverrideVariable | functions: populate_builtin_to_tensor_fn_map, get_prev_sta",
    "role": "src",
    "loc": 506
  },
  {
    "id": "torch\\_dynamo\\variables\\user_defined.py",
    "summary": "This module contains variable classes for handling user-defined objects in Dynamo's tracing system. | classes: UserDefinedVariable, UserDefinedClassVariable, NO_SUCH_SUBOBJ, UserDefinedObjectVariable, FrozenDataClassVariable, SourcelessGraphModuleVariable | functions: is_standard_setattr, is_forbidd",
    "role": "src",
    "loc": 1320
  },
  {
    "id": "torch\\_dynamo\\variables\\__init__.py",
    "summary": "This package implements variable tracking and symbolic execution capabilities for Dynamo, | imports: base, builtin, constant, ctx_manager | [torch _dynamo variables __init__.py]",
    "role": "src",
    "loc": 209
  },
  {
    "id": "torch\\_export\\converter.py",
    "summary": "No description | classes: TS2FXGraphConverter, _DictMock, ExplainTS2FXGraphConverter, TS2EPConverter | functions: _get_param_count_list, _trace_and_get_graph_from_model, _create_jit_graph, list_add, list_append, execute_subgraph_from_prim_loop | imports: builtins, operator, torch | [torch _export co",
    "role": "src",
    "loc": 1195
  },
  {
    "id": "torch\\_export\\error.py",
    "summary": "This is exir's custom assert method. It internally just throws InternalError. | classes: ExportErrorType, InternalError, ExportError | functions: internal_assert | [torch _export error.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\_export\\non_strict_utils.py",
    "summary": "1. Handles data-dependent errors raised by torch function calls in non-strict. | classes: _NonStrictTorchFunctionHandler | functions: key_path_to_source, _is_constant_argument, fakify, make_fake_inputs, _flatten_dynamic_shapes, _tree_map_helper | imports: inspect, torch, sympy | [torch _export non_s",
    "role": "src",
    "loc": 549
  },
  {
    "id": "torch\\_export\\pass_base.py",
    "summary": "No description | classes: ExportPassBaseError, ExportTracer, ExportInterpreter, _ExportPassBaseDeprecatedDoNotUse | imports: operator, traceback, torch, functorch | [torch _export pass_base.py]",
    "role": "src",
    "loc": 373
  },
  {
    "id": "torch\\_export\\tools.py",
    "summary": "Generate inputs for targeting submdoules in the given model. Note that if two submodules refer to the same obj, this | functions: _generate_inputs_for_submodules, pre_forward, report_exportability, try_export | imports: torch | [torch _export tools.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\_export\\utils.py",
    "summary": "No description | classes: PrototypeModule, _WrappedMethod | functions: _collect_and_set_constant_attrs, _register_constants_as_buffers, _override_graph_signature_for_temp_registered_constants, _overwrite_signature_for_non_persistent_buffers, _collect_param_buffer_metadata, _getattr | imports: ast, c",
    "role": "src",
    "loc": 1080
  },
  {
    "id": "torch\\_export\\verifier.py",
    "summary": "No description | classes: SpecViolationError, _VerifierMeta, Verifier, TrainingIRVerifier | functions: is_functional, _check_has_fake_tensor, _check_val, _check_correct_val, _no_returns, _check_torch_fn | imports: inspect, operator, torch | [torch _export verifier.py]",
    "role": "src",
    "loc": 388
  },
  {
    "id": "torch\\_export\\wrappers.py",
    "summary": "No description | classes: ExportTracepoint | functions: export_tracepoint_dispatch_mode, export_tracepoint_fake_tensor_mode, export_tracepoint_functional, export_tracepoint_cpu, _wrap_submodule, update_module_call_signatures | imports: torch | [torch _export wrappers.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\_export\\__init__.py",
    "summary": "Manage Export-specific configurations of Dynamo. | classes: ExportDynamoConfig | functions: aot_compile_warning, aot_compile, aot_load, optimized | imports: copy, dataclasses, functools, io | [torch _export __init__.py]",
    "role": "src",
    "loc": 143
  },
  {
    "id": "torch\\_export\\db\\case.py",
    "summary": "Indicates at what stage the feature | classes: SupportLevel, ExportCase | functions: check_inputs_type, _validate_tag, register_db_case, to_snake_case, _make_export_case, export_case | imports: inspect, string, dataclasses, types | [torch _export db case.py]",
    "role": "src",
    "loc": 136
  },
  {
    "id": "torch\\_export\\db\\gen_example.py",
    "summary": "No description | imports: torch | [torch _export db gen_example.py]",
    "role": "src",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\logging.py",
    "summary": "No description | functions: exportdb_error_message, get_class_if_classified_error | imports: examples, torch | [torch _export db logging.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\_export\\db\\__init__.py",
    "summary": "Package initializer | [torch _export db __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_export\\db\\examples\\assume_constant_result.py",
    "summary": "Applying `assume_constant_result` decorator to burn make non-tracable code as constant. | classes: AssumeConstantResult | imports: torch | [torch _export db examples assume_constant_result.py]",
    "role": "examples",
    "loc": 14
  },
  {
    "id": "torch\\_export\\db\\examples\\autograd_function.py",
    "summary": "No description | classes: MyAutogradFunction, AutogradFunction | imports: torch | [torch _export db examples autograd_function.py]",
    "role": "examples",
    "loc": 17
  },
  {
    "id": "torch\\_export\\db\\examples\\class_method.py",
    "summary": "Class methods are inlined during tracing. | classes: ClassMethod | imports: torch | [torch _export db examples class_method.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_branch_class_method.py",
    "summary": "No description | classes: MySubModule, CondBranchClassMethod | imports: torch, functorch | [torch _export db examples cond_branch_class_method.py]",
    "role": "examples",
    "loc": 32
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_branch_nested_function.py",
    "summary": "The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: | classes: CondBranchNestedFunction | imports: torch, functorch | [torch _export db examples cond_branch_nested_function.py]",
    "role": "examples",
    "loc": 30
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_branch_nonlocal_variables.py",
    "summary": "The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: | classes: CondBranchNonlocalVariables | imports: torch, functorch | [torch _export db examples cond_branch_nonlocal_variables.py]",
    "role": "examples",
    "loc": 45
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_closed_over_variable.py",
    "summary": "torch.cond() supports branches closed over arbitrary variables. | classes: CondClosedOverVariable | imports: torch, functorch | [torch _export db examples cond_closed_over_variable.py]",
    "role": "examples",
    "loc": 15
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_operands.py",
    "summary": "The operands passed to cond() must be: | classes: CondOperands | imports: torch | [torch _export db examples cond_operands.py]",
    "role": "examples",
    "loc": 26
  },
  {
    "id": "torch\\_export\\db\\examples\\cond_predicate.py",
    "summary": "The conditional statement (aka predicate) passed to cond() must be one of the following: | classes: CondPredicate | imports: torch, functorch | [torch _export db examples cond_predicate.py]",
    "role": "examples",
    "loc": 18
  },
  {
    "id": "torch\\_export\\db\\examples\\constrain_as_size_example.py",
    "summary": "If the value is not known at tracing time, you can provide hint so that we | classes: ConstrainAsSizeExample | imports: torch | [torch _export db examples constrain_as_size_example.py]",
    "role": "examples",
    "loc": 19
  },
  {
    "id": "torch\\_export\\db\\examples\\constrain_as_value_example.py",
    "summary": "If the value is not known at tracing time, you can provide hint so that we | classes: ConstrainAsValueExample | imports: torch | [torch _export db examples constrain_as_value_example.py]",
    "role": "examples",
    "loc": 21
  },
  {
    "id": "torch\\_export\\db\\examples\\decorator.py",
    "summary": "Decorators calls are inlined into the exported function during tracing. | classes: Decorator | functions: test_decorator, wrapper | imports: functools, torch | [torch _export db examples decorator.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\dictionary.py",
    "summary": "Dictionary structures are inlined and flattened along tracing. | classes: Dictionary | imports: torch | [torch _export db examples dictionary.py]",
    "role": "examples",
    "loc": 13
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_assert.py",
    "summary": "A basic usage of python assertion. | classes: DynamicShapeAssert | imports: torch | [torch _export db examples dynamic_shape_assert.py]",
    "role": "examples",
    "loc": 12
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_constructor.py",
    "summary": "Tensor constructors should be captured with dynamic shape inputs rather | classes: DynamicShapeConstructor | imports: torch | [torch _export db examples dynamic_shape_constructor.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_if_guard.py",
    "summary": "`if` statement with backed dynamic shape predicate will be specialized into | classes: DynamicShapeIfGuard | imports: torch | [torch _export db examples dynamic_shape_if_guard.py]",
    "role": "examples",
    "loc": 14
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_map.py",
    "summary": "functorch map() maps a function over the first tensor dimension. | classes: DynamicShapeMap | imports: torch, functorch | [torch _export db examples dynamic_shape_map.py]",
    "role": "examples",
    "loc": 13
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_round.py",
    "summary": "Calling round on dynamic shapes is not supported. | classes: DynamicShapeRound | imports: torch | [torch _export db examples dynamic_shape_round.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_slicing.py",
    "summary": "Slices with dynamic shape arguments should be captured into the graph | classes: DynamicShapeSlicing | imports: torch | [torch _export db examples dynamic_shape_slicing.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\dynamic_shape_view.py",
    "summary": "Dynamic shapes should be propagated to view arguments instead of being | classes: DynamicShapeView | imports: torch | [torch _export db examples dynamic_shape_view.py]",
    "role": "examples",
    "loc": 13
  },
  {
    "id": "torch\\_export\\db\\examples\\fn_with_kwargs.py",
    "summary": "Keyword arguments are not supported at the moment. | classes: FnWithKwargs | imports: torch | [torch _export db examples fn_with_kwargs.py]",
    "role": "examples",
    "loc": 26
  },
  {
    "id": "torch\\_export\\db\\examples\\list_contains.py",
    "summary": "List containment relation can be checked on a dynamic shape or constants. | classes: ListContains | imports: torch | [torch _export db examples list_contains.py]",
    "role": "examples",
    "loc": 13
  },
  {
    "id": "torch\\_export\\db\\examples\\list_unpack.py",
    "summary": "Lists are treated as static construct, therefore unpacking should be | classes: ListUnpack | imports: torch | [torch _export db examples list_unpack.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\model_attr_mutation.py",
    "summary": "Attribute mutation is not supported. | classes: ModelAttrMutation | imports: torch | [torch _export db examples model_attr_mutation.py]",
    "role": "examples",
    "loc": 18
  },
  {
    "id": "torch\\_export\\db\\examples\\nested_function.py",
    "summary": "Nested functions are traced through. Side effects on global captures | classes: NestedFunction | imports: torch | [torch _export db examples nested_function.py]",
    "role": "examples",
    "loc": 17
  },
  {
    "id": "torch\\_export\\db\\examples\\null_context_manager.py",
    "summary": "Null context manager in Python will be traced out. | classes: NullContextManager | imports: torch | [torch _export db examples null_context_manager.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\optional_input.py",
    "summary": "Tracing through optional input is not supported yet | classes: OptionalInput | imports: torch | [torch _export db examples optional_input.py]",
    "role": "examples",
    "loc": 14
  },
  {
    "id": "torch\\_export\\db\\examples\\pytree_flatten.py",
    "summary": "Pytree from PyTorch can be captured by TorchDynamo. | classes: PytreeFlatten | imports: torch | [torch _export db examples pytree_flatten.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\scalar_output.py",
    "summary": "Returning scalar values from the graph is supported, in addition to Tensor | classes: ScalarOutput | imports: torch | [torch _export db examples scalar_output.py]",
    "role": "examples",
    "loc": 17
  },
  {
    "id": "torch\\_export\\db\\examples\\specialized_attribute.py",
    "summary": "No description | classes: Animal, SpecializedAttribute | imports: torch | [torch _export db examples specialized_attribute.py]",
    "role": "examples",
    "loc": 19
  },
  {
    "id": "torch\\_export\\db\\examples\\static_for_loop.py",
    "summary": "A for loop with constant number of iterations should be unrolled in the exported graph. | classes: StaticForLoop | imports: torch | [torch _export db examples static_for_loop.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\static_if.py",
    "summary": "`if` statement with static predicate value should be traced through with the | classes: StaticIf | imports: torch | [torch _export db examples static_if.py]",
    "role": "examples",
    "loc": 13
  },
  {
    "id": "torch\\_export\\db\\examples\\tensor_setattr.py",
    "summary": "setattr() call onto tensors is not supported. | classes: TensorSetattr | imports: torch | [torch _export db examples tensor_setattr.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\type_reflection_method.py",
    "summary": "No description | classes: A, TypeReflectionMethod | imports: torch | [torch _export db examples type_reflection_method.py]",
    "role": "examples",
    "loc": 16
  },
  {
    "id": "torch\\_export\\db\\examples\\unsupported_operator.py",
    "summary": "torch.sym_min operator is not supported in export. | classes: TorchSymMin | imports: torch | [torch _export db examples unsupported_operator.py]",
    "role": "examples",
    "loc": 12
  },
  {
    "id": "torch\\_export\\db\\examples\\user_input_mutation.py",
    "summary": "Directly mutate user input in forward | classes: UserInputMutation | imports: torch | [torch _export db examples user_input_mutation.py]",
    "role": "examples",
    "loc": 11
  },
  {
    "id": "torch\\_export\\db\\examples\\__init__.py",
    "summary": "Package initializer | functions: _collect_examples, all_examples, get_name, filter_examples_by_support_level, get_rewrite_cases | imports: dataclasses, glob, inspect, torch | [torch _export db examples __init__.py]",
    "role": "examples",
    "loc": 45
  },
  {
    "id": "torch\\_export\\passes\\add_runtime_assertions_for_constraints_pass.py",
    "summary": "No description | classes: InputDim, _AddRuntimeAssertionsForInlineConstraintsPass | functions: _convert_to_int, _convert_range_to_int, _get_existing_inline_assertions, maybe_get_symint | imports: operator, traceback, functools, sympy | [torch _export passes add_runtime_assertions_for_constraints_pas",
    "role": "src",
    "loc": 170
  },
  {
    "id": "torch\\_export\\passes\\collect_tracepoints_pass.py",
    "summary": "Performs constant folding and constant propagation. | classes: CollectTracepointsPass | imports: operator, torch | [torch _export passes collect_tracepoints_pass.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torch\\_export\\passes\\constant_folding.py",
    "summary": "No description | classes: ConstantFolder | functions: replace_node_with_constant, constant_fold, constant_graph_tag, run_and_get_constant_graph | imports: torch | [torch _export passes constant_folding.py]",
    "role": "src",
    "loc": 204
  },
  {
    "id": "torch\\_export\\passes\\functionalize_side_effectful_ops_pass.py",
    "summary": "Functionalize ops with side effect in graph module by replacing the op with | classes: _FunctionalizeSideEffectfulOpsPass | imports: copy, torch | [torch _export passes functionalize_side_effectful_ops_pass.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "torch\\_export\\passes\\insert_custom_op_guards.py",
    "summary": "This is used by draft_export to insert guards in front of calls to custom | functions: insert_custom_op_guards | imports: functools, torch | [torch _export passes insert_custom_op_guards.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\_export\\passes\\lift_constants_pass.py",
    "summary": "A mapping class that understands how to use module constants (tensors, | classes: ConstantAttrMap | functions: get_constant_fqn, _get_first_fqn, lift_constants_pass, rewrite_script_object_meta, _materialize_and_lift_constants | imports: torch | [torch _export passes lift_constants_pass.py]",
    "role": "src",
    "loc": 266
  },
  {
    "id": "torch\\_export\\passes\\remove_runtime_assertions.py",
    "summary": "Remove runtime assertions inserted by the | classes: _RemoveRuntimeAssertionsPass | imports: torch | [torch _export passes remove_runtime_assertions.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\_export\\passes\\replace_autocast_with_hop_pass.py",
    "summary": "No description | functions: _is_autocast_node, _is_enter_autocast_node, _is_exit_autocast_node, _is_autocast_sub_mod, _check_valid_autocast_block, _replace_with_hop | imports: torch, utils, replace_with_hop_pass_util | [torch _export passes replace_autocast_with_hop_pass.py]",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\_export\\passes\\replace_quantized_ops_with_standard_ops_pass.py",
    "summary": "No description | functions: int_to_valid_dtype, fx_enum_to_dtype, insert_quantized_node, get_dequantized, insert_dequantized_node, get_qmin_qmax | imports: operator, torch | [torch _export passes replace_quantized_ops_with_standard_ops_pass.py]",
    "role": "src",
    "loc": 567
  },
  {
    "id": "torch\\_export\\passes\\replace_set_grad_with_hop_pass.py",
    "summary": "No description | functions: _is_set_grad_enabled_node, _is_set_grad_enabled_sub_mod, _replace_with_hop, _remove_set_grad_and_inline, _sequential_split_and_maybe_inline_subgraphs, _maybe_inline_or_replace_with_hop | imports: torch, utils, replace_with_hop_pass_util | [torch _export passes replace_set",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\_export\\passes\\replace_view_ops_with_view_copy_ops_pass.py",
    "summary": "Our backend expects pure functional operators. For efficiency | classes: ReplaceViewOpsWithViewCopyOpsPass | functions: is_view_op, get_view_copy_of_view_op | imports: torch | [torch _export passes replace_view_ops_with_view_copy_ops_pass.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\_export\\passes\\replace_with_hop_pass_util.py",
    "summary": "No description | functions: _replace_with_hop_helper, set_hoo_node_meta, _sequential_split_and_maybe_inline_subgraphs_helper, _replace_with_hop_pass_helper | imports: copy, operator, torch, utils | [torch _export passes replace_with_hop_pass_util.py]",
    "role": "src",
    "loc": 151
  },
  {
    "id": "torch\\_export\\passes\\_node_metadata_hook.py",
    "summary": "Hook for adding the appropriate metadata to nodes that are created during a | functions: _node_metadata_hook, _set_node_metadata_hook | imports: torch | [torch _export passes _node_metadata_hook.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\_export\\passes\\__init__.py",
    "summary": "Package initializer | imports: replace_view_ops_with_view_copy_ops_pass | [torch _export passes __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\_export\\pass_infra\\node_metadata.py",
    "summary": "No description | classes: NodeMetadata | [torch _export pass_infra node_metadata.py]",
    "role": "src",
    "loc": 22
  },
  {
    "id": "torch\\_export\\pass_infra\\proxy_value.py",
    "summary": "No description | classes: ProxyValue | imports: torch | [torch _export pass_infra proxy_value.py]",
    "role": "src",
    "loc": 31
  },
  {
    "id": "torch\\_export\\pass_infra\\__init__.py",
    "summary": "Package initializer | [torch _export pass_infra __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_export\\serde\\aoti_schema.py",
    "summary": "No description | classes: ExternKernelNode, ExternKernelNodes | imports: dataclasses, torch | [torch _export serde aoti_schema.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\_export\\serde\\dynamic_shapes.py",
    "summary": "This represents a _Dim object. | classes: RootDim, DynamicShapesSpec | functions: _postprocess_serialized_shapes, _dump_dynamic_shapes, _standardize_shapes, _track_dim_from_dims, _load_dynamic_shapes, deserialize_shape | imports: dataclasses, torch, serialize, sympy | [torch _export serde dynamic_sh",
    "role": "src",
    "loc": 281
  },
  {
    "id": "torch\\_export\\serde\\schema.py",
    "summary": "No description | classes: ScalarType, Layout, MemoryFormat, Device, SymExprHint, SymExpr | imports: dataclasses, torch | [torch _export serde schema.py]",
    "role": "src",
    "loc": 301
  },
  {
    "id": "torch\\_export\\serde\\schema_check.py",
    "summary": "No description | classes: SchemaUpdateError, _Commit | functions: _check, _staged_schema, _handle_aggregate, dump_type, dump_cpp_value, dump_field | imports: dataclasses, hashlib, inspect, torch | [torch _export serde schema_check.py]",
    "role": "src",
    "loc": 624
  },
  {
    "id": "torch\\_export\\serde\\serialize.py",
    "summary": "No description | classes: SerializeError, SerializedArtifact, _SerializedProgram, GraphState, Final, GraphModuleSerializer | functions: _reverse_map, deserialize_device, _print_sympy, serialize_sym_int, serialize_sym_float, serialize_sym_bool | imports: base64, copy, copyreg, dataclasses | [torch _e",
    "role": "src",
    "loc": 2849
  },
  {
    "id": "torch\\_export\\serde\\union.py",
    "summary": "No description | classes: _UnionTag, _Union | functions: _get_field_names | imports: functools, dataclasses | [torch _export serde union.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torch\\_export\\serde\\__init__.py",
    "summary": "Package initializer | [torch _export serde __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_functorch\\aot_autograd.py",
    "summary": "Represents a fw or bw_compiler passed to AOTAutograd. | classes: AOTDispatchCompiler, SerializableAOTDispatchCompiler, AOTModule | functions: process_inputs, convert, construct_fake_mode, create_aot_dispatcher_function, _create_aot_dispatcher_function, _dup_fake_script_obj | imports: functools, unit",
    "role": "src",
    "loc": 1078
  },
  {
    "id": "torch\\_functorch\\apis.py",
    "summary": "vmap is the vectorizing map; ``vmap(func)`` returns a new function that | functions: vmap, wrapped, chunk_vmap, _get_chunk_flat_args, wrapped_with_chunks, grad | imports: functools, torch | [torch _functorch apis.py]",
    "role": "src",
    "loc": 359
  },
  {
    "id": "torch\\_functorch\\autograd_function.py",
    "summary": "No description | classes: CustomFunctionHigherOrderOperator, VmapInfo, WrappedCtx, CtxWithSavedTensors, CtxCustomSave, ApplyTemplate | functions: custom_function_call_grad, generate_single_level_function, forward, wrap_fn, setup_context, backward | imports: torch | [torch _functorch autograd_functio",
    "role": "src",
    "loc": 447
  },
  {
    "id": "torch\\_functorch\\batch_norm_replacement.py",
    "summary": "No description | functions: batch_norm_without_running_stats, replace_all_batch_norm_modules_ | imports: torch | [torch _functorch batch_norm_replacement.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\_functorch\\benchmark_utils.py",
    "summary": "No description | functions: synchronize, dump_chrome_trace, get_chrome_trace_events, is_gpu_compute_event, get_sorted_gpu_events, get_duration | imports: json, operator, torch | [torch _functorch benchmark_utils.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "torch\\_functorch\\compilers.py",
    "summary": "No description | classes: DebugInterpreter | functions: _canonicalize, _disable_jit_autocast, ts_compile, _draw_graph_compile, draw_graph_compile, nop | imports: copy, pickle, random, functools | [torch _functorch compilers.py]",
    "role": "src",
    "loc": 352
  },
  {
    "id": "torch\\_functorch\\compile_utils.py",
    "summary": "No description | functions: get_aten_target, fx_graph_cse, checkable_node, substitute, strip_overloads, get_placeholders | imports: torch | [torch _functorch compile_utils.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\_functorch\\config.py",
    "summary": "Global flags for aot autograd | functions: remote_autograd_cache_default | imports: torch | [torch _functorch config.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torch\\_functorch\\deprecated.py",
    "summary": "The APIs in this file are exposed as `functorch.*`. They are thin wrappers | functions: get_warning, warn_deprecated, setup_docs, vmap, grad, grad_and_value | imports: textwrap, torch | [torch _functorch deprecated.py]",
    "role": "src",
    "loc": 134
  },
  {
    "id": "torch\\_functorch\\eager_transforms.py",
    "summary": "No description | functions: lazy_dynamo_disallow, enable_inplace_requires_grad, _set_tensor_requires_grad, _create_differentiable, create_differentiable, _undo_create_differentiable | imports: functools, torch, apis, vmap | [torch _functorch eager_transforms.py]",
    "role": "src",
    "loc": 1315
  },
  {
    "id": "torch\\_functorch\\functional_call.py",
    "summary": "Performs a functional call on the module by replacing the module parameters | functions: functional_call, stack_module_state, construct_stacked_leaf | imports: torch | [torch _functorch functional_call.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "torch\\_functorch\\fx_minifier.py",
    "summary": "No description | classes: LoadTensorMeta, ConcreteProp, ReproState | functions: is_load_tensor_node, _convert_node_to_placeholder, create_minified_hlo_graph, dump_state, is_power_of_two, minifier | imports: copy, dataclasses, functools, torch | [torch _functorch fx_minifier.py]",
    "role": "src",
    "loc": 402
  },
  {
    "id": "torch\\_functorch\\make_functional.py",
    "summary": "This is the callable object returned by :func:`make_functional_with_buffers`. | classes: FunctionalModuleWithBuffers, FunctionalModule | functions: raise_parameter_tying_error, create_names_map, _extract_members, extract_weights, extract_buffers, load_weights | imports: copy, torch | [torch _functor",
    "role": "src",
    "loc": 483
  },
  {
    "id": "torch\\_functorch\\partitioners.py",
    "summary": "Class for keeping track of different operator categories | classes: OpTypes, NodeInfo, MinCutOptions, InvalidNodeBase | functions: must_recompute, has_recomputable_ops, has_recomputable_rng_ops, sym_node_size, _extract_graph_with_inputs_outputs, _is_primal | imports: copy, functools, heapq, operator",
    "role": "src",
    "loc": 1472
  },
  {
    "id": "torch\\_functorch\\pyfunctorch.py",
    "summary": "No description | classes: FuncTorchInterpreter, VmapInterpreter, GradInterpreter, JvpInterpreter, FunctionalizeInterpreter | functions: temporarily_pop_interpreter_stack, temporarily_clear_interpreter_stack, temporarily_restore_interpreter_stack, nested, coerce_cinterpreter, retrieve_current_functor",
    "role": "src",
    "loc": 201
  },
  {
    "id": "torch\\_functorch\\python_key.py",
    "summary": "No description | imports: torch | [torch _functorch python_key.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\_functorch\\pytree_hacks.py",
    "summary": "No description | imports: torch | [torch _functorch pytree_hacks.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\_functorch\\top_operators_github_usage.py",
    "summary": "From https://docs.google.com/spreadsheets/d/12R3nCOLskxPYjjiNkdqy4OdQ65eQp_htebXGODsjSeA/edit#gid=0 | functions: get_nn_functional_top_list | imports: operator | [torch _functorch top_operators_github_usage.py]",
    "role": "src",
    "loc": 619
  },
  {
    "id": "torch\\_functorch\\utils.py",
    "summary": "No description | functions: enable_single_level_autograd_function, unwrap_dead_wrappers | imports: torch | [torch _functorch utils.py]",
    "role": "src",
    "loc": 30
  },
  {
    "id": "torch\\_functorch\\vmap.py",
    "summary": "No description | functions: doesnt_support_saved_tensors_hooks, fn, _validate_and_get_batch_size, _num_outputs, _as_tuple, _process_batched_inputs | imports: functools, threading, torch | [torch _functorch vmap.py]",
    "role": "src",
    "loc": 377
  },
  {
    "id": "torch\\_functorch\\__init__.py",
    "summary": "Package initializer | [torch _functorch __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_functorch\\_activation_checkpointing\\ac_logging_utils.py",
    "summary": "No description | functions: create_joint_graph_node_information, create_joint_graph_edges, create_activation_checkpointing_logging_structure_payload, create_structured_trace_for_min_cut_info | imports: json, torch | [torch _functorch _activation_checkpointing ac_logging_utils.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "torch\\_functorch\\_activation_checkpointing\\graph_info_provider.py",
    "summary": "This class provides information about the graph, such as the nodes, edges, and their runtime and memory requirements. | classes: GraphInfoProvider | imports: networkx, torch, matplotlib | [torch _functorch _activation_checkpointing graph_info_provider.py]",
    "role": "src",
    "loc": 280
  },
  {
    "id": "torch\\_functorch\\_activation_checkpointing\\knapsack.py",
    "summary": "No description | functions: greedy_knapsack, ilp_knapsack, dp_knapsack | imports: torch, numpy, scipy | [torch _functorch _activation_checkpointing knapsack.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\_functorch\\_activation_checkpointing\\knapsack_evaluator.py",
    "summary": "This class evaluates the theoretical runtime and peak memory usage of a given checkpointing strategy. | classes: KnapsackEvaluator | imports: networkx, numpy, torch | [torch _functorch _activation_checkpointing knapsack_evaluator.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\_functorch\\_activation_checkpointing\\__init__.py",
    "summary": "Package initializer | [torch _functorch _activation_checkpointing __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\autograd_cache.py",
    "summary": "Utils for caching the outputs of AOTAutograd | classes: BypassAOTAutogradCache, FXGraphCacheMiss, AOTAutogradCacheDetails, AOTAutogradCachePickler, FXGraphCacheLoadable, CompiledForward | functions: should_use_remote_autograd_cache, should_use_local_autograd_cache, check_node_safe, is_public_torch_a",
    "role": "src",
    "loc": 727
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\collect_metadata_analysis.py",
    "summary": "This module is one of the analysis modules - it takes as input a function or graph | functions: coerce_tangent_and_suggest_memory_format, run_functionalized_fw_and_collect_metadata, _to_fun, inner, view_avoid_dupes_with_primals, _plain_fake_tensor_like_subclass | imports: functools, torch, functiona",
    "role": "src",
    "loc": 462
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\dispatch_and_compile_graph.py",
    "summary": "This module dispatches the graphs to either the forward-only or joint compilation | functions: _create_graph, _detach_and_copy_item_memo, aot_dispatch_base_graph, aot_dispatch_autograd_graph | imports: dataclasses, torch, torchgen, functional_utils | [torch _functorch _aot_autograd dispatch_and_comp",
    "role": "src",
    "loc": 245
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\functional_utils.py",
    "summary": "This file contains utilities related to functionalization in AOTAutograd: | classes: MetadataKey, FunctionalTensorMetadataEq | functions: to_fun, sync_functional_tensor, from_fun, is_fun, has_data_mutation, are_all_mutations_hidden_from_autograd | imports: dataclasses, torch | [torch _functorch _aot",
    "role": "src",
    "loc": 357
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\input_output_analysis.py",
    "summary": "This module is one of the analysis modules - it takes as input a function or graph | functions: remove_dupe_metadata, create_synthetic_base_metadata, compute_overlapping_inputs, _graph_input_names, _graph_output_names, create_graph_signature | imports: torch, collect_metadata_analysis, schemas, util",
    "role": "src",
    "loc": 321
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py",
    "summary": "Functions in this module do most of the \"work\" of AOTAutograd. | functions: _create_wrappers_for_dispatch, aot_dispatch_export, aot_dispatch_base, collect_fw_donated_buffer_idxs, collect_bw_donated_buffer_idxs, aot_dispatch_autograd | imports: traceback, torch, torchgen, autograd_cache | [torch _fun",
    "role": "src",
    "loc": 676
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\logging_utils.py",
    "summary": "Contains utils for logging in AOTAutograd, including managing the names of the graphs under | functions: set_model_name, get_aot_compilation_context, get_aot_graph_name, track_graph_compiling, setup_stacktrace_preservation_hooks, iter_graph | imports: torch | [torch _functorch _aot_autograd logging_",
    "role": "src",
    "loc": 103
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\runtime_wrappers.py",
    "summary": "This module defines runtime wrappers, which, based on previous analysis attempts to: | classes: CompilerWrapper, RuntimeWrapper, NoopAliasHandler, AliasOfInputHandler, IsInputHandler, AliasOfIntermediateHandler | functions: _unwrap_tensoralias, _identity, make_output_handler, _create_runtime_wrapper",
    "role": "src",
    "loc": 1459
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\schemas.py",
    "summary": "The various dataclasses, Enums, namedtuples etc used in AOTAutograd. This includes | classes: OutputAliasInfo, MutationType, InputAliasInfo, PlainTensorMeta, SubclassCreationMeta, ViewAndMutationMeta | imports: dataclasses, functools, torch, functional_utils | [torch _functorch _aot_autograd schemas",
    "role": "src",
    "loc": 542
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\subclass_parametrization.py",
    "summary": "Model transformation that replaces all the parameters that are subclasses to plain tensors. | classes: SubclassCreationMeta, UnwrapTensorSubclass | functions: unwrap_tensor_subclass_parameters | imports: dataclasses, torch | [torch _functorch _aot_autograd subclass_parametrization.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\subclass_utils.py",
    "summary": "This file contains utilities for tracing through __torch_dispatch__ based tensor subclasses and modes. | functions: requires_subclass_dispatch, maybe_suggest_memory_format, get_subclass_typing_container, _get_types_for_subclass, create_subclass_metadata, create_subclass_meta | imports: torch, schema",
    "role": "src",
    "loc": 331
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\traced_function_transforms.py",
    "summary": "This module is responsible for transforming functions to be traced into a form | functions: fn_input_mutations_to_outputs, inner_fn, fn_prepped_for_autograd, create_joint, inner_fn_with_anomaly, create_functionalized_rng_ops_wrapper | imports: functools, unittest, torch, collect_metadata_analysis | ",
    "role": "src",
    "loc": 568
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\utils.py",
    "summary": "Contains various utils for AOTAutograd, including those for handling collections. | classes: PytreeThunk | functions: strict_zip, _get_symint_hints, partial_flatten_asdict, normalize_as_list, _get_autocast_states, make_boxed_func | imports: dataclasses, operator, functools, torch | [torch _functorch",
    "role": "src",
    "loc": 344
  },
  {
    "id": "torch\\_functorch\\_aot_autograd\\__init__.py",
    "summary": "Package initializer | [torch _functorch _aot_autograd __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_higher_order_ops\\aoti_call_delegate.py",
    "summary": "aoti_call_delegate is a HOP for calling AOTInductor lowered submodule in ExportedProgram. | classes: AOTICallDelegate | functions: call_delegate_cpu, call_delegate_fake_tensor_mode | imports: torch | [torch _higher_order_ops aoti_call_delegate.py]",
    "role": "src",
    "loc": 78
  },
  {
    "id": "torch\\_higher_order_ops\\associative_scan.py",
    "summary": "No description | classes: AssociativeScanOp | functions: wrap_combine_fn_flat, _interleave, safe_map, nf, associative_scan, generic_associative_scan | imports: functools, torch | [torch _higher_order_ops associative_scan.py]",
    "role": "src",
    "loc": 342
  },
  {
    "id": "torch\\_higher_order_ops\\auto_functionalize.py",
    "summary": "No description | classes: ViewInfo, AsStridedViewInfo, SliceViewInfo, AliasViewInfo, NotView, AutoFunctionalized | functions: get_base, is_alias, try_use_slice, write_view_information_to_args, write_single_view, use_as_strided | imports: abc, dataclasses, torch | [torch _higher_order_ops auto_functi",
    "role": "src",
    "loc": 622
  },
  {
    "id": "torch\\_higher_order_ops\\base_hop.py",
    "summary": "This is the \"Base\" HOP implementation for a HOP that looks like: | classes: BaseHOP, BaseHOPFunction, FunctionWithNoFreeVars | imports: abc, torch, invoke_subgraph, utils | [torch _higher_order_ops base_hop.py]",
    "role": "src",
    "loc": 137
  },
  {
    "id": "torch\\_higher_order_ops\\cond.py",
    "summary": "Conditionally applies `true_fn` or `false_fn`. | classes: CondOp, CondAutogradOp | functions: cond, _validate_input, _cond_op_wrapper, create_fw_bw_graph_branches, trace_cond, cond_op_dense | imports: torch, utils | [torch _higher_order_ops cond.py]",
    "role": "src",
    "loc": 553
  },
  {
    "id": "torch\\_higher_order_ops\\effects.py",
    "summary": "No description | classes: _EffectType, WithEffects | functions: _register_effectful_op, _deregister_effectful_op, has_aliasing, has_effects, get_effect_key, new_token_tensor | imports: weakref, torch | [torch _higher_order_ops effects.py]",
    "role": "src",
    "loc": 213
  },
  {
    "id": "torch\\_higher_order_ops\\executorch_call_delegate.py",
    "summary": "No description | classes: ExecutorchCallDelegate | functions: trace_call_delegate, _unwrap_proxy, call_delegate_cpu, call_delegate_autograd, fake_requires_grad, call_delegate_proxy_torch_dispatch_mode | imports: torch | [torch _higher_order_ops executorch_call_delegate.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\_higher_order_ops\\flat_apply.py",
    "summary": "Definition: a graphable type is a type that that is an acceptable input/output type to a FX node. | classes: _ConstantFunction, FlatApply | functions: is_graphable, is_graphable_type, to_graphable, from_graphable, func_to_graphable, impl | imports: dataclasses, torch | [torch _higher_order_ops flat_",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torch\\_higher_order_ops\\flex_attention.py",
    "summary": "From a list of sizes and a fill order, construct the strides of the permuted tensor. | classes: FlexAttentionHOP, FlexAttentionBackwardHOP, FlexAttentionAutogradOp | functions: _construct_strides, _permute_strides, _math_attention_inner, math_attention, sdpa_dense, trace_flex_attention | imports: to",
    "role": "src",
    "loc": 1028
  },
  {
    "id": "torch\\_higher_order_ops\\foreach_map.py",
    "summary": "No description | classes: ForeachMap | functions: foreach_map | imports: torch | [torch _higher_order_ops foreach_map.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\_higher_order_ops\\hints_wrap.py",
    "summary": "No description | classes: HintsWrapper | functions: hints_wrapper_dense, hints_wrapper_fake_tensor_mode, hints_wrapper_functionalize, trace_hints_wrapper, inner | imports: torch | [torch _higher_order_ops hints_wrap.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\_higher_order_ops\\invoke_subgraph.py",
    "summary": "No description | classes: InvokeSubgraphHOP, InvokeSubgraphAutogradOp | functions: invoke_subgraph_placeholder, mark_compile_region, wrap, inner, get_invoke_subgraph_cache, trace_joint_graph | imports: torch | [torch _higher_order_ops invoke_subgraph.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "torch\\_higher_order_ops\\map.py",
    "summary": "No description | classes: MapWrapper, MapImpl, MapAutogradOp | functions: create_fw_bw_graph, joint_f, map_wrapper, flat_fn, trace_map, expand_tensor | imports: torch, utils | [torch _higher_order_ops map.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "torch\\_higher_order_ops\\out_dtype.py",
    "summary": "The out_dtype operator takes an existing ATen functional operator, an | classes: OutDtypeOperator | functions: trace_out_dtype, out_dtype_dense, is_int_mm, out_dtype_fallback, out_dtype_proxy, out_dtype_fake_tensor_mode | imports: torch | [torch _higher_order_ops out_dtype.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\_higher_order_ops\\run_const_graph.py",
    "summary": "No description | classes: RunConstGraph | functions: run_const_graph_dispatch_mode, run_const_graph_functional, run_const_graph_fake_tensor_mode, run_const_graph_cpu | imports: torch | [torch _higher_order_ops run_const_graph.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\_higher_order_ops\\scan.py",
    "summary": "No description | classes: ScanOp | functions: wrap_combine_fn_flat, _extract_carry_and_out, scan, _check_new_carry_match_init, run_flattened_scan, generic_scan | imports: functools, torch | [torch _higher_order_ops scan.py]",
    "role": "src",
    "loc": 402
  },
  {
    "id": "torch\\_higher_order_ops\\strict_mode.py",
    "summary": "No description | classes: StrictMode | functions: strict_mode, strict_mode_op_dense, inner, trace_strict_mode, strict_mode_fake_tensor_mode, strict_mode_func | imports: torch | [torch _higher_order_ops strict_mode.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\_higher_order_ops\\torchbind.py",
    "summary": "No description | classes: CallTorchBind | functions: torchbind_method_redispatch, enable_torchbind_tracing, call_torchbind_impl, inner, call_torchbind_fake, call_torchbind_func | imports: torch | [torch _higher_order_ops torchbind.py]",
    "role": "src",
    "loc": 104
  },
  {
    "id": "torch\\_higher_order_ops\\triton_kernel_wrap.py",
    "summary": "Uses Triton's internal code generation to create TTIR | classes: Autotuner, JITFunction, KernelSideTable, Param, Intermediate, Op | functions: generate_ttir, _get_specialization, get_signature_value, ttir_to_functions, reindex, mlir_to_functions | imports: copy, dataclasses, inspect, threading | [to",
    "role": "src",
    "loc": 1303
  },
  {
    "id": "torch\\_higher_order_ops\\utils.py",
    "summary": "If autograd is enabled and any of the arguments require grad this will either | classes: UnsupportedAliasMutationException | functions: autograd_not_implemented_inner, fake_requires_grad, autograd_not_implemented, inner, _maybe_run_with_interpreter, graph_with_interpreter | imports: functools, datac",
    "role": "src",
    "loc": 438
  },
  {
    "id": "torch\\_higher_order_ops\\while_loop.py",
    "summary": "Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn  | classes: WhileLoopOp | functions: while_loop, flat_cond_fn, flat_body_fn, _validate_input, _while_loop_op_wrapper, while_loop_dense | imports: torch | [torch _higher_order_ops w",
    "role": "src",
    "loc": 324
  },
  {
    "id": "torch\\_higher_order_ops\\wrap.py",
    "summary": "No description | classes: Wrap, WrapWithSetGradEnabled, WrapWithAutocast, WrapActivationCheckpoint, TagActivationCheckpoint | imports: inspect, torch | [torch _higher_order_ops wrap.py]",
    "role": "src",
    "loc": 168
  },
  {
    "id": "torch\\_higher_order_ops\\_invoke_quant.py",
    "summary": "No description | classes: InvokeQuantTracer, InvokeQuantUnpacked, InvokeQuant | imports: dataclasses, torch | [torch _higher_order_ops _invoke_quant.py]",
    "role": "src",
    "loc": 43
  },
  {
    "id": "torch\\_higher_order_ops\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch _higher_order_ops __init__.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\_inductor\\analyze_preserves_zero_mask.py",
    "summary": "For prologue kernels where the loads are masked, does the final store of this kernel preserve | classes: PreservesZeros, DTypeContainer, RecordLowPrecisionOps | functions: construct_symbol, prologue_preserves_zero_mask, low_prec_float, can_codegen_without_upcasts | imports: dataclasses, sympy, torch",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\_inductor\\aoti_eager.py",
    "summary": "No description | functions: aoti_eager_cache_dir, aoti_eager_op_conf_lock, load_aoti_eager_cache, supported_builtin_dtype_torch_dtype, supported_scalar_types, extract_tensor_metadata | imports: json, unittest, torch, runtime | [torch _inductor aoti_eager.py]",
    "role": "src",
    "loc": 241
  },
  {
    "id": "torch\\_inductor\\async_compile.py",
    "summary": "In memory cache for storing compiled triton kernels. | classes: CompiledTritonKernels, AsyncCompile | functions: pre_fork_setup, caching_device_properties, _compile_start, _compile_end, shutdown_compile_workers, after_fork | imports: atexit, functools, multiprocessing, concurrent | [torch _inductor ",
    "role": "src",
    "loc": 343
  },
  {
    "id": "torch\\_inductor\\autotune_process.py",
    "summary": "Context manager to set the CUDA_VISIBLE_DEVICES environment variable to the | classes: Ping, Pong, NonzeroWorkspaceNotSupportedError, TuningProcess, TuningProcessPool, TensorMeta | functions: set_cuda_visible_device, benchmark_in_sub_process | imports: ctypes, dataclasses, functools, queue | [torch ",
    "role": "src",
    "loc": 767
  },
  {
    "id": "torch\\_inductor\\bounds.py",
    "summary": "Performs Value Range Analysis on LoopBody's fx graph by calling BoundVars.run() | classes: BoundVars, ValueRangeAnalysis | imports: operator, functools, sympy, torch | [torch _inductor bounds.py]",
    "role": "src",
    "loc": 197
  },
  {
    "id": "torch\\_inductor\\choices.py",
    "summary": "Anything that can be used as a list.sort() key (int/tuple/etc) | classes: Sortable, InductorChoices | imports: sympy, codecache, metrics, runtime | [torch _inductor choices.py]",
    "role": "src",
    "loc": 275
  },
  {
    "id": "torch\\_inductor\\codecache.py",
    "summary": "No description | classes: CacheBase, LocalCache, PersistentCache, TensorMetadataAndValues, FxGraphCachePickler, OrderedSetHolder | functions: log_global_cache_errors, log_global_cache_stats, log_global_cache_vals, use_global_cache, get_cpp_wrapper_cubin_path_name, get_global_cache_path_impl | import",
    "role": "src",
    "loc": 2535
  },
  {
    "id": "torch\\_inductor\\comms.py",
    "summary": "Greedily schedules waits as late as possible. | classes: Runnable | functions: sink_waits, raise_comms, reorder_compute_for_overlap, _schedule_for_comm, schedule, get_overlapping_candidate | imports: heapq, operator, torch, dependencies | [torch _inductor comms.py]",
    "role": "src",
    "loc": 662
  },
  {
    "id": "torch\\_inductor\\comm_analysis.py",
    "summary": "No description | classes: NCCL_COLL, NVIDIA_GPU_TYPE, NCCL_HW, NCCL_ALGO, NCCL_PROTO | functions: get_gpu_type, get_collective_type, get_collective_input_size_bytes, get_collective_group_size, estimate_nccl_collective_runtime | imports: functools, sympy, torch, utils | [torch _inductor comm_analysis",
    "role": "src",
    "loc": 166
  },
  {
    "id": "torch\\_inductor\\comm_lowering.py",
    "summary": "Check if an input can be realized as a comm buffer of the specified | functions: can_realize_as_comm_buffer, realize_as_comm_buffer, _get_data, mark_as_skip_wait, should_skip_wait, _should_lower_as_one_shot_all_reduce | imports: torch, virtualized, lowering | [torch _inductor comm_lowering.py]",
    "role": "src",
    "loc": 259
  },
  {
    "id": "torch\\_inductor\\compiler_bisector.py",
    "summary": "No description | classes: Subsystem, BisectSubsystem, BinarySubsystem, ConfigChange, BisectionResult, DisableBisect | functions: reset_counters, get_env_val, command_line_usage, test_function, get_is_bisection_enabled | imports: atexit, dataclasses, functools, shutil | [torch _inductor compiler_bise",
    "role": "src",
    "loc": 514
  },
  {
    "id": "torch\\_inductor\\compile_fx.py",
    "summary": "No description | classes: FxCompileMode, _CompileFxKwargs, _CompileFxCallable, FxCompile, _InProcessFxCompile, _WireProtocolInput | functions: time_and_log, log_optimus_to_scuba, _fx_compile_mode_default, get_static_input_idxs, record_original_output_strides, _step_logger | imports: functools, io, j",
    "role": "src",
    "loc": 1849
  },
  {
    "id": "torch\\_inductor\\config.py",
    "summary": "No description | classes: _collective, cpp, triton, aot_inductor, cuda, rocm | functions: fx_graph_remote_cache_default, vec_isa_ok_default, autotune_remote_cache_default, bundled_autotune_remote_cache_default, bundle_triton_into_fx_graph_cache_default, prologue_fusion_enabled | imports: torch, libf",
    "role": "src",
    "loc": 578
  },
  {
    "id": "torch\\_inductor\\constant_folding.py",
    "summary": "No description | classes: ConstantFolder | functions: replace_node_with_constant, is_const_source, constant_fold, constant_graph_tag, run_and_get_constant_graph, untag | imports: torch | [torch _inductor constant_folding.py]",
    "role": "src",
    "loc": 292
  },
  {
    "id": "torch\\_inductor\\cpp_builder.py",
    "summary": "This is the Base class for store cxx build options, as a template. | classes: BuildOptionsBase, CppOptions, CppTorchOptions, CppTorchDeviceOptions, CppBuilder | functions: log_global_cache_errors, log_global_cache_stats, log_global_cache_vals, use_global_cache, cpp_compiler_search, install_gcc_via_c",
    "role": "src",
    "loc": 1309
  },
  {
    "id": "torch\\_inductor\\cpu_vec_isa.py",
    "summary": "No description | classes: VecISA, VecNEON, VecSVE256, VecAVX512, VecAMX, VecAVX2 | functions: _get_isa_dry_compile_fingerprint, x86_isa_checker, _check_and_append_supported_isa, get_isa_from_cpu_capability, valid_vec_isa_list, pick_vec_isa | imports: dataclasses, functools, platform, subprocess | [t",
    "role": "src",
    "loc": 306
  },
  {
    "id": "torch\\_inductor\\cudagraph_trees.py",
    "summary": "CUDA graph trees are a safety abstraction over CUDAGraphs, similar to make_graph_callables, | classes: AllocatorState, GraphID, TreeManagerContainer, MarkStepBox, StorageWeakRefWrapper, CUDAWarmupNode | functions: _set_cached_tensors_enabled, clear_cublass_cache, clear_cublas_manager, disable_conv_c",
    "role": "src",
    "loc": 1775
  },
  {
    "id": "torch\\_inductor\\cudagraph_utils.py",
    "summary": "Unique counter of a function wrapped in cudagraphify_impl | classes: FunctionID, PlaceholderInfo, WrappedFunction, BoxedDeviceIndex, CheckInvariantStatus, CudagraphCachedInfo | functions: get_mutating_use_stack_trace_from_node, get_mutating_use_stack_trace, to_placeholder_info, get_placeholder_info,",
    "role": "src",
    "loc": 247
  },
  {
    "id": "torch\\_inductor\\custom_graph_pass.py",
    "summary": "Implement this interface for custom Graph passes: | classes: CustomGraphPass | functions: get_hash_for_files | imports: hashlib, abc, functools, typing_extensions | [torch _inductor custom_graph_pass.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torch\\_inductor\\debug.py",
    "summary": "No description | classes: DebugContext, DebugFormatter, TensorMetadataHolder | functions: has_dot, draw_buffers, create_fx_from_snodes, get_fake_func, func1, in_output | imports: copy, dataclasses, functools, io | [torch _inductor debug.py]",
    "role": "src",
    "loc": 783
  },
  {
    "id": "torch\\_inductor\\decomposition.py",
    "summary": "No description | functions: register_decomposition, assert_async_msg_decomp, functional_assert_async_msg_decomp, sym_constrain_range_for_size, clamp, full | imports: functools, typing_extensions, torch, utils | [torch _inductor decomposition.py]",
    "role": "src",
    "loc": 865
  },
  {
    "id": "torch\\_inductor\\dependencies.py",
    "summary": "No description | classes: Dep, MemoryDep, StarDep, WeakDep, IndexExprDep, ReadWrites | functions: var_builder, add_var, index_vars_no_squeeze, index_vars_squeeze, extract_read_writes, extract_loop_body_with_args | imports: abc, dataclasses, typing_extensions, unittest | [torch _inductor dependencies",
    "role": "src",
    "loc": 611
  },
  {
    "id": "torch\\_inductor\\dtype_propagation.py",
    "summary": "No description | classes: DTypeVar, DtypePropagationOpsHandler, _typecheck_DtypePropagation | functions: get_promoted_dtype, construct_input, promote_types | imports: functools, sympy, torch, ops_handler | [torch _inductor dtype_propagation.py]",
    "role": "src",
    "loc": 287
  },
  {
    "id": "torch\\_inductor\\exc.py",
    "summary": "No description | classes: OperatorIssue, MissingOperatorWithoutDecomp, MissingOperatorWithDecomp, LoweringException, SubgraphLoweringException, InvalidCxxCompiler | functions: _record_missing_op | imports: tempfile, textwrap, functools, torch | [torch _inductor exc.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\_inductor\\extern_node_serializer.py",
    "summary": "No description | functions: serialize_extern_kernel_node, extern_node_json_serializer | imports: json, torch | [torch _inductor extern_node_serializer.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\_inductor\\freezing.py",
    "summary": "Replaces the parameters of a PyTorch GraphModule with constants wherever possible. | classes: ErasedTensor | functions: replace_params_with_constants, freeze, _freeze, invalidate_eager_modules, discard_traced_gm_params, enforce_output_layout | imports: weakref, torch | [torch _inductor freezing.py]",
    "role": "src",
    "loc": 216
  },
  {
    "id": "torch\\_inductor\\freezing_utils.py",
    "summary": "No description | functions: _freezing_active, enter_freezing, record_has_frozen_params, has_frozen_params, maybe_set_is_frozen_param, is_frozen_param | imports: threading, torch | [torch _inductor freezing_utils.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torch\\_inductor\\fuzzer.py",
    "summary": "A Dummy pass to be used by ConfigFuzzer | classes: DummyPass, TypeExemplars, Status, SamplingMethod, Default, ResultType | functions: is_type, is_optional_type, is_callable_type, check_halide_import, visualize_results | imports: importlib, pickle, random, signal | [torch _inductor fuzzer.py]",
    "role": "src",
    "loc": 814
  },
  {
    "id": "torch\\_inductor\\fx_utils.py",
    "summary": "The main idea here is that it's difficult to maintain accurate fake | classes: FakeTensorUpdater | functions: matches_module_function_pattern, get_storage, get_node_storage, get_fake, get_fake_args_kwargs, is_node_realized | imports: operator, sympy, torch, virtualized | [torch _inductor fx_utils.py",
    "role": "src",
    "loc": 188
  },
  {
    "id": "torch\\_inductor\\graph.py",
    "summary": "No description | classes: GraphLowering, SubgraphLowering | functions: log_module_code, may_get_constant_buffer_dtype, is_magic_method, getattr_recursive, get_user_visible_output_strides, mark_nodes_dislike_padding | imports: functools, json, operator, sympy | [torch _inductor graph.py]",
    "role": "src",
    "loc": 1689
  },
  {
    "id": "torch\\_inductor\\hooks.py",
    "summary": "No description | functions: intermediate_hook, run_intermediate_hooks | imports: torch | [torch _inductor hooks.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\_inductor\\index_propagation.py",
    "summary": "This file implements the IndexPropagation ops handler, which wraps an | classes: TypedExpr, SymPyOps, IndexPropVar, IndexPropagation | functions: _is_constant, upper_bound | imports: dataclasses, typing_extensions, sympy, torch | [torch _inductor index_propagation.py]",
    "role": "src",
    "loc": 277
  },
  {
    "id": "torch\\_inductor\\inductor_prims.py",
    "summary": "No description | functions: make_prim, meta, eager_force_stride, _low_memory_max_pool2d_with_offsets_aten, _low_memory_max_pool2d_offsets_to_indices_aten | imports: torch | [torch _inductor inductor_prims.py]",
    "role": "src",
    "loc": 147
  },
  {
    "id": "torch\\_inductor\\ir.py",
    "summary": "No description | classes: GraphPartitionSignature, IRNode, Operation, Loops, Pointwise, Scatter | functions: validate_ir, _check_tensorbox, ops_wrapper, fn, inverse_reorder, reindex | imports: dataclasses, functools, textwrap, traceback | [torch _inductor ir.py]",
    "role": "src",
    "loc": 6150
  },
  {
    "id": "torch\\_inductor\\jagged_lowerings.py",
    "summary": "No description | functions: dense_idx_to_jagged_idx, get_inverse_offsets, inner_fn, jagged_idx_to_dense_idx, register_jagged_ops, _jagged_to_padded_dense_forward | imports: sympy, torch, ir, lowering | [torch _inductor jagged_lowerings.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "torch\\_inductor\\loop_body.py",
    "summary": "No description | classes: InterpreterShim, LightTracer, MemoryEntry, MemoryUsageType, LoopBody, LoopBodyBlock | imports: functools, sympy, torch, codegen | [torch _inductor loop_body.py]",
    "role": "src",
    "loc": 542
  },
  {
    "id": "torch\\_inductor\\lowering.py",
    "summary": "No description | functions: cur_node_has_non_foreach_users, group_foreach_args, maybe_layout_constraints, handle_layout_constraint_tag, get_layout_constraint_tag, assert_nyi | imports: dataclasses, functools, operator, typing_extensions | [torch _inductor lowering.py]",
    "role": "src",
    "loc": 5380
  },
  {
    "id": "torch\\_inductor\\memory.py",
    "summary": "Create and keep track of all input buffers that can be freed during the program | classes: MemoryPlanningInfoForBuffer, MemoryPlanningInfoForNode, FreeableInputBuffer, BufferInfo, NodeInfo, NodeWithPriority | functions: get_freeable_input_buf, _dep_size_hint, compute_size_for_scheduler_buffer, _comp",
    "role": "src",
    "loc": 511
  },
  {
    "id": "torch\\_inductor\\metrics.py",
    "summary": "No description | classes: CppOuterLoopFusedCount, CachedMetricsDeltas, CachedMetricsHelper, MetricTable | functions: reset, get_metric_fields, _parse_kernel_fn_code, _parse_kernel_line_of_code, _parse_size_hints, _parse_reduction_hint | imports: csv, dataclasses, inspect, functools | [torch _inducto",
    "role": "src",
    "loc": 343
  },
  {
    "id": "torch\\_inductor\\mkldnn_ir.py",
    "summary": "This function is a helper function to prepare inputs, layout and constant args | classes: ConvolutionUnary, ConvolutionBinary, ConvolutionBinaryInplace, ConvolutionTransposeUnary, QConvPointWisePT2E, QConvPointWiseBinaryPT2E | functions: _prepare_convolution_fusion_create, _conv_input_size, _origina",
    "role": "src",
    "loc": 1077
  },
  {
    "id": "torch\\_inductor\\mkldnn_lowerings.py",
    "summary": "No description | functions: grouped_gemm_lowering, register_onednn_fusion_ops, convolution_unary, convolution_binary, convolution_binary_inplace, linear_unary | imports: functools, torch, codegen, ir | [torch _inductor mkldnn_lowerings.py]",
    "role": "src",
    "loc": 1092
  },
  {
    "id": "torch\\_inductor\\mock_cache.py",
    "summary": "No description | classes: Stats, _GlobalItemStats, _GlobalStats, MockBackend, PatchCaches | imports: dataclasses, threading, typing_extensions, unittest | [torch _inductor mock_cache.py]",
    "role": "src",
    "loc": 208
  },
  {
    "id": "torch\\_inductor\\ops_handler.py",
    "summary": "Protocol describing the set of valid operations on ``torch._inductor.virtualized.ops``, | classes: OpsHandler, DefaultHandler, NoopHandler, BasicMathOpsMixin, MockHandler, KernelFormatterHandler | functions: _arg_str, list_ops | imports: inspect, io, unittest, sympy | [torch _inductor ops_handler.py",
    "role": "src",
    "loc": 852
  },
  {
    "id": "torch\\_inductor\\optimize_indexing.py",
    "summary": "No description | functions: val_expressable_in_32_bits, range_expressable_in_32_bits, try_to_reduce_precision, skip_filter, indexing_dtype_strength_reduction | imports: sympy, torch, loop_body, utils | [torch _inductor optimize_indexing.py]",
    "role": "src",
    "loc": 86
  },
  {
    "id": "torch\\_inductor\\output_code.py",
    "summary": "This provides an abstract class which parametrizes over an \"output code\" concept | classes: OutputCode, CompiledFxGraphConstants, CompiledFxGraphConstantsWithGm, CompiledFxGraph, CompiledAOTI, MockFXGraphCacheOutput | functions: get_expanded_dims, index_expanded_dims, complex_memory_overlap, cudagra",
    "role": "src",
    "loc": 463
  },
  {
    "id": "torch\\_inductor\\pattern_matcher.py",
    "summary": "# Inductor Pattern Matcher | classes: SearchFn, ReplaceFn, TraceFn, Multiple, Match, FailedMatch | functions: _transfer_meta, is_match, _return_true, log_trace_failure, check_and_add_duplicate_pattern, register_replacement | imports: dataclasses, functools, importlib, inspect | [torch _inductor patt",
    "role": "src",
    "loc": 1677
  },
  {
    "id": "torch\\_inductor\\quantized_lowerings.py",
    "summary": "No description | functions: register_quantized_ops, register_woq_mm_ops, int8pack_mm, _mul_epilogue, int4pack_mm_cpu | imports: torch, codegen, lowering, select_algorithm | [torch _inductor quantized_lowerings.py]",
    "role": "src",
    "loc": 130
  },
  {
    "id": "torch\\_inductor\\remote_cache.py",
    "summary": "A backend implementation for accessing a remote/distributed cache.  Only | classes: RemoteCacheBackend, RemoteCacheSerde, RemoteCacheJsonSerde, RemoteCachePassthroughSerde, RemoteCache, RedisRemoteCacheBackend | functions: create_cache, dump_cache_stats | imports: atexit, dataclasses, functools, jso",
    "role": "src",
    "loc": 267
  },
  {
    "id": "torch\\_inductor\\scheduler.py",
    "summary": "No description | classes: SchedulerBuffer, SchedulerDonatedBuffer, BaseSchedulerNode, WhyNoFuse, OutputNode, ExternKernelSchedulerNode | functions: pformat, _prune_redundant_deps, should_prune, refresh_group_node_dependencies, init_group_node, pick_loop_order | imports: dataclasses, functools, inspe",
    "role": "src",
    "loc": 3593
  },
  {
    "id": "torch\\_inductor\\select_algorithm.py",
    "summary": "No description | classes: KernelNamespace, BenchmarkTensors, AutotuneArgs, PartialRender, SubgraphInfo, ModificationWrapper | functions: _jinja2_env, get_mm_log_filename, append_to_log, get_num_workers, create_inputs_key, create_precompile_key | imports: builtins, dataclasses, functools, inspect | [",
    "role": "src",
    "loc": 1907
  },
  {
    "id": "torch\\_inductor\\sizevars.py",
    "summary": "No description | classes: SizeVarAllocator, SimplifyIndexing | functions: evaluate_expr, join_dimensions, _join_dimensions_cached | imports: functools, sympy, torch, runtime | [torch _inductor sizevars.py]",
    "role": "src",
    "loc": 709
  },
  {
    "id": "torch\\_inductor\\subgraph_lowering.py",
    "summary": "Utilities for lowering subgraphs used by higher order operators | classes: PointwiseSubgraphLowering, InputDescriptor, TracingOpsHandler | functions: lower_pointwise_subgraph, fake_inner_fn, inner_fn | imports: functools, operator, dataclasses, typing_extensions | [torch _inductor subgraph_lowering.",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\_inductor\\test_case.py",
    "summary": "A base TestCase for inductor tests. Enables FX graph caching and isolates | classes: TestCase | functions: run_tests | imports: torch | [torch _inductor test_case.py]",
    "role": "src",
    "loc": 39
  },
  {
    "id": "torch\\_inductor\\test_operators.py",
    "summary": "No description | classes: Realize | functions: realize | imports: torch | [torch _inductor test_operators.py]",
    "role": "src",
    "loc": 21
  },
  {
    "id": "torch\\_inductor\\triton_bundler.py",
    "summary": "When we have compiled a triton kernel, we take note of that kernel by | classes: TritonBundleEntry, TritonKernelArtifact, TritonKernelArtifacts, TritonBundlerMetadata, TritonBundler | imports: dataclasses, shutil, uuid, torch | [torch _inductor triton_bundler.py]",
    "role": "src",
    "loc": 222
  },
  {
    "id": "torch\\_inductor\\utils.py",
    "summary": "Symbolically round up to the nearest multiple of ALIGN_BYTES | classes: align, CachedMethod, LineContext, IndentedBuffer, FakeIndentedBuffer, DeferredLineBase | functions: get_gpu_type, _align, _is_aligned, do_bench_using_profiling, has_torchvision_roi_align, decode_device | imports: dataclasses, fu",
    "role": "src",
    "loc": 2002
  },
  {
    "id": "torch\\_inductor\\virtualized.py",
    "summary": "This file provides a number of \"global\" variables/handlers that are actually | classes: NullHandler, Virtualized, NullKernelHandler, OpsValue, OpsWrapper, _V | functions: _choices_default | imports: threading, torch, ops_handler | [torch _inductor virtualized.py]",
    "role": "src",
    "loc": 285
  },
  {
    "id": "torch\\_inductor\\wrapper_benchmark.py",
    "summary": "Similar to get_kernel_category but use the source code. Call this API | classes: BenchmarkCallableType, ProfileEvent | functions: get_kernel_category_by_source_code, get_kernel_category, get_triton_kernel, benchmark_all_kernels, get_info_str, parse_profile_event_list | imports: dataclasses, datetime",
    "role": "src",
    "loc": 381
  },
  {
    "id": "torch\\_inductor\\__init__.py",
    "summary": "Compile a given FX graph with TorchInductor.  This allows compiling | functions: compile, aoti_compile_and_package, _aoti_compile_and_package_inner, aoti_load_package, aot_compile, list_mode_options | imports: io, torch, compile_fx, debug | [torch _inductor __init__.py]",
    "role": "src",
    "loc": 259
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\autoheuristic.py",
    "summary": "To be able to collect data for a choice, a function providing feedback given a choice has to be provided. | classes: LocalFeedback, InconsistentMetadata, AutoHeuristic, AutoHeuristicSelectAlgorithm | imports: json, functools, torch | [torch _inductor autoheuristic autoheuristic.py]",
    "role": "src",
    "loc": 268
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\autoheuristic_utils.py",
    "summary": "The context, that AutoHeuristic stores, is a list of features. AutoHeuristic needs to know whether a feature is | classes: AHFeature, AHOperation, AHContext, AHMetadata | functions: get_metadata_str_from_log, check_minsize, pad_mm_precondition, get_mixedmm_precondition, get_mult_dims_ops, get_arith_",
    "role": "src",
    "loc": 260
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\learnedheuristic_interface.py",
    "summary": "LearnedHeuristic is a base class for all learned heuristics. | classes: LearnedHeuristic, LearnedHeuristicRegression, LearnedHeuristicDecision | imports: torch | [torch _inductor autoheuristic learnedheuristic_interface.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\learned_heuristic_controller.py",
    "summary": "Class that finds and instantiates all learned heuristics. It also provides | classes: LearnedHeuristicController | functions: find_and_instantiate_subclasses | imports: importlib, inspect, pkgutil, torch | [torch _inductor autoheuristic learned_heuristic_controller.py]",
    "role": "src",
    "loc": 96
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\__init__.py",
    "summary": "Package initializer | [torch _inductor autoheuristic __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\_MixedMMA100.py",
    "summary": "No description | classes: MixedMMA100 | imports: torch | [torch _inductor autoheuristic artifacts _MixedMMA100.py]",
    "role": "src",
    "loc": 136
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\_MixedMMH100.py",
    "summary": "No description | classes: MixedMMH100 | imports: torch | [torch _inductor autoheuristic artifacts _MixedMMH100.py]",
    "role": "src",
    "loc": 135
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\_MMRankingA100.py",
    "summary": "No description | classes: MMRankingA100 | imports: torch | [torch _inductor autoheuristic artifacts _MMRankingA100.py]",
    "role": "src",
    "loc": 282
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\_MMRankingH100.py",
    "summary": "No description | classes: MMRankingH100 | imports: torch | [torch _inductor autoheuristic artifacts _MMRankingH100.py]",
    "role": "src",
    "loc": 307
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\_PadMMA100.py",
    "summary": "No description | classes: PadMMA100 | imports: torch | [torch _inductor autoheuristic artifacts _PadMMA100.py]",
    "role": "src",
    "loc": 97
  },
  {
    "id": "torch\\_inductor\\autoheuristic\\artifacts\\__init__.py",
    "summary": "Package initializer | [torch _inductor autoheuristic artifacts __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\codegen\\aoti_hipify_utils.py",
    "summary": "No description | functions: maybe_hipify_code_wrapper, c2_repl | imports: torch | [torch _inductor codegen aoti_hipify_utils.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torch\\_inductor\\codegen\\block_analysis.py",
    "summary": "Matches block indexing expressions. | classes: BlockPatternMatcher | imports: functools, textwrap, sympy, torch | [torch _inductor codegen block_analysis.py]",
    "role": "src",
    "loc": 125
  },
  {
    "id": "torch\\_inductor\\codegen\\common.py",
    "summary": "No description | classes: WorkspaceZeroMode, WorkspaceArg, TensorArg, SizeArg, ConstexprArg, TMADescriptorArg | functions: data_type_logger, register_backend_for_device, get_backend_features, has_backend_feature, get_scheduling_for_device, get_wrapper_codegen_for_device | imports: dataclasses, funct",
    "role": "src",
    "loc": 2128
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp.py",
    "summary": "No description | classes: OuterLoopFusedSchedulerNode, RecordOptimizationContext, CppOverrides, CppVecOverrides, CppTile2DOverrides, CppKernel | functions: get_export_declaration, reduction_init, reduction_acc_type, reduction_combine, reduction_project, move_code_under_inner_loop | imports: dataclas",
    "role": "src",
    "loc": 4573
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_bmm_template.py",
    "summary": "No description | classes: CppBmmTemplate | imports: unittest, sympy, select_algorithm, virtualized | [torch _inductor codegen cpp_bmm_template.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_flex_attention_template.py",
    "summary": "No description | classes: CppFlexAttentionTemplate | imports: unittest, sympy, torch, utils | [torch _inductor codegen cpp_flex_attention_template.py]",
    "role": "src",
    "loc": 908
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_gemm_template.py",
    "summary": "No description | classes: CppGemmTemplate | functions: get_padded_n, transpose_w, expand_bias, prune_tensors, share_storage, get_candidates | imports: functools, unittest, torch, _dynamo | [torch _inductor codegen cpp_gemm_template.py]",
    "role": "src",
    "loc": 1217
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_grouped_gemm_template.py",
    "summary": "No description | classes: CppGroupedGemmTemplate | functions: get_deduplicated_act | imports: unittest, torch, _dynamo, kernel | [torch _inductor codegen cpp_grouped_gemm_template.py]",
    "role": "src",
    "loc": 464
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_micro_gemm.py",
    "summary": "No description | classes: LayoutType, CppMicroGemm, CppMicroGemmConfig, CppMicroGemmRef, CppMicroGemmFP32Vec, CppMicroGemmAMX | functions: get_restrict_keyword, register_micro_gemm, inner, generate_gemm_config, check_amx_extra, check_brgemm_extra | imports: dataclasses, sympy, torch, cpu_vec_isa | [",
    "role": "src",
    "loc": 924
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_template.py",
    "summary": "No description | classes: CppTemplate | imports: ctypes, functools, unittest, sympy | [torch _inductor codegen cpp_template.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_template_kernel.py",
    "summary": "No description | classes: CppTemplateKernel, CppTemplateCaller | functions: parse_expr_with_index_symbols, wrap_with_tensorbox | imports: sympy, torch, autotune_process, loop_body | [torch _inductor codegen cpp_template_kernel.py]",
    "role": "src",
    "loc": 518
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_utils.py",
    "summary": "No description | classes: CppCSEVariable, CppPrinter, LocalizeBufferHandler, LocalBufferContext | functions: get_promote_dtype, promote_args, promote_arg, get_opt_ctx, get_current_node_opt_ctx, deduce_dtype_for_cpp_cse_variable | imports: dataclasses, functools, unittest, sympy | [torch _inductor co",
    "role": "src",
    "loc": 678
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_wrapper_cpu.py",
    "summary": "Generates cpp wrapper for running on CPU and calls cpp kernels | classes: CppWrapperCpu | imports: functools, textwrap, sympy, torch | [torch _inductor codegen cpp_wrapper_cpu.py]",
    "role": "src",
    "loc": 1993
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_wrapper_cpu_array_ref.py",
    "summary": "Generates cpp wrapper for running on CPU and calls cpp kernels | classes: CppWrapperCpuArrayRef | imports: sympy, torch, utils, virtualized | [torch _inductor codegen cpp_wrapper_cpu_array_ref.py]",
    "role": "src",
    "loc": 817
  },
  {
    "id": "torch\\_inductor\\codegen\\cpp_wrapper_gpu.py",
    "summary": "When using cpp wrapper, GPU kernel load and launch needs to wait for Triton kernels | classes: DeferredGpuKernelLine, DeferredGpuDefaultGrid, DeferredGpuGridLine, CppWrapperGpu | imports: sympy, torch, codecache, ir | [torch _inductor codegen cpp_wrapper_gpu.py]",
    "role": "src",
    "loc": 552
  },
  {
    "id": "torch\\_inductor\\codegen\\cpu_device_op_overrides.py",
    "summary": "No description | classes: CpuDeviceOpOverrides | imports: textwrap, common | [torch _inductor codegen cpu_device_op_overrides.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda_combined_scheduling.py",
    "summary": "Scheduler for CUDA Kernels, which delegates calls as appropriate | classes: CUDACombinedScheduling | imports: scheduler, cuda, rocm, triton | [torch _inductor codegen cuda_combined_scheduling.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "torch\\_inductor\\codegen\\debug_utils.py",
    "summary": "No description | classes: IntermediateValueDebuggingLevel, DebugPrinterManager | functions: _print_debugging_tensor_value_info | imports: functools, torch, virtualized, multi_kernel | [torch _inductor codegen debug_utils.py]",
    "role": "src",
    "loc": 234
  },
  {
    "id": "torch\\_inductor\\codegen\\halide.py",
    "summary": "No description | classes: Unsupported, HalidePrinter, HalideOverrides, HalideCSEVariable, DimensionInfo, HalideKernel | functions: halide_constant, halide_type, halide_acc_type, eq, lt | imports: dataclasses, functools, sympy, torch | [torch _inductor codegen halide.py]",
    "role": "src",
    "loc": 1412
  },
  {
    "id": "torch\\_inductor\\codegen\\memory_planning.py",
    "summary": "A range where a given tensor is live.  Begin and end are both counters | classes: LiveRange, LiveRanges, AllocationTreeNode, Allocation, Empty, MemorySplitProtocol | imports: dataclasses, pprint, sympy, torch | [torch _inductor codegen memory_planning.py]",
    "role": "src",
    "loc": 627
  },
  {
    "id": "torch\\_inductor\\codegen\\mps.py",
    "summary": "No description | classes: MetalExprPrinter, MetalOverrides, MetalKernel, MetalScheduling | functions: value_to_metal | imports: sympy, torch, utils, virtualized | [torch _inductor codegen mps.py]",
    "role": "src",
    "loc": 502
  },
  {
    "id": "torch\\_inductor\\codegen\\mps_device_op_overrides.py",
    "summary": "No description | classes: MPSDeviceOpOverrides | imports: common | [torch _inductor codegen mps_device_op_overrides.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\_inductor\\codegen\\multi_kernel.py",
    "summary": "Maintain state of multi-kernel compilation so we don't define duplicated | classes: MultiKernelState, MultiKernel, MultiKernelCall | functions: get_kernel_argdefs, _get_all_args, get_all_kernel_argdefs, get_all_call_args, get_numel_argdefs | imports: functools, torch, codecache, runtime | [torch _in",
    "role": "src",
    "loc": 363
  },
  {
    "id": "torch\\_inductor\\codegen\\simd.py",
    "summary": "Each range tree represents multiple sets of iteration indexing | classes: IterationRanges, IterationRangesRoot, IterationRangesEntry, SIMDKernel, SIMDScheduling, CandidateTiling | functions: constant_repr | imports: dataclasses, functools, operator, textwrap | [torch _inductor codegen simd.py]",
    "role": "src",
    "loc": 1706
  },
  {
    "id": "torch\\_inductor\\codegen\\simd_kernel_features.py",
    "summary": "No description | classes: NodeScheduleMarker, DisableReduction, EnableReduction, SIMDKernelFeatures, MemoryEstimator, MemoryEstimate | imports: dataclasses, functools, sympy, torch | [torch _inductor codegen simd_kernel_features.py]",
    "role": "src",
    "loc": 497
  },
  {
    "id": "torch\\_inductor\\codegen\\triton.py",
    "summary": "Some Triton ops such as libdevice and tl.math only support float32 and float64. | classes: OpDtypeSupport, TritonSymbols, IndexingOptions, BlockPtrOptions, TritonPrinter, TritonCSEVariable | functions: gen_attr_descriptor_import, gen_common_triton_imports, triton_reshape, triton_compute_type, _get_p",
    "role": "src",
    "loc": 3433
  },
  {
    "id": "torch\\_inductor\\codegen\\triton_combo_kernel.py",
    "summary": "Horizontally partition the given list of nodes into a list of list of nodes where each sublist | classes: PartitionState, SequentialDispatch, RoundRobinDispatch, ComboKernel | functions: _default_custom_combo_kernel_horizontal_partition, set_custom_combo_kernel_horizontal_partition | imports: textwr",
    "role": "src",
    "loc": 1004
  },
  {
    "id": "torch\\_inductor\\codegen\\triton_split_scan.py",
    "summary": "Generates a triton kernel that supports ops.scan calls while also splitting | classes: TritonSplitScanKernel | imports: functools, sympy, torch, utils | [torch _inductor codegen triton_split_scan.py]",
    "role": "src",
    "loc": 175
  },
  {
    "id": "torch\\_inductor\\codegen\\triton_utils.py",
    "summary": "No description | functions: should_unwrap_unspec_arg, signature_of, non_constexpr_signature, signature_to_meta, is_unaligned_buffer, _arg_equals_1 | imports: sympy, torch, runtime, utils | [torch _inductor codegen triton_utils.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "torch\\_inductor\\codegen\\wrapper.py",
    "summary": "No description | classes: SymbolicCallArg, MemoryPlanningState, WrapperLine, EnterSubgraphLine, ExitSubgraphLine, EnterDeviceContextManagerLine | functions: buffer_reuse_key, can_match_buffer_size, convert_arg_type, convert_return_type, get_cpp_op_schema, user_defined_kernel_grid_fn_code | imports: ",
    "role": "src",
    "loc": 2225
  },
  {
    "id": "torch\\_inductor\\codegen\\__init__.py",
    "summary": "Package initializer | [torch _inductor codegen __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cuda_cpp_scheduling.py",
    "summary": "Partial Scheduling implementation for CUDA C++ Kernels. | classes: CUDACPPScheduling | imports: torch, _dynamo, codecache, ir | [torch _inductor codegen cuda cuda_cpp_scheduling.py]",
    "role": "src",
    "loc": 94
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cuda_env.py",
    "summary": "No description | functions: get_cuda_arch, get_cuda_version, nvcc_exist | imports: functools, torch, subprocess | [torch _inductor codegen cuda cuda_env.py]",
    "role": "src",
    "loc": 34
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cuda_kernel.py",
    "summary": "No description | classes: LayoutArg, CUDAKernel, CUDATemplateKernel, CUDATemplateCaller | functions: _normalize_idx | imports: dataclasses, sympy, torch, cuda_template | [torch _inductor codegen cuda cuda_kernel.py]",
    "role": "src",
    "loc": 462
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cuda_template.py",
    "summary": "No description | classes: ArgInfo, CUDATemplate, CUTLASSTemplate | imports: functools, dataclasses, typing_extensions, unittest | [torch _inductor codegen cuda cuda_template.py]",
    "role": "src",
    "loc": 224
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cutlass_utils.py",
    "summary": "CUTLASS args used to initialize a CUTLASS Manifest. | classes: CUTLASSArgs, CUDACompileSourceCapturingContext | functions: _rename_cutlass_import, try_import_cutlass, _normalize_cuda_arch, _gen_ops_cached, gen_ops, torch_dtype_to_cutlass_type | imports: functools, dataclasses, sympy, torch | [torch ",
    "role": "src",
    "loc": 326
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\device_op_overrides.py",
    "summary": "No description | classes: CUDADeviceOpOverrides | imports: torch, utils, common | [torch _inductor codegen cuda device_op_overrides.py]",
    "role": "src",
    "loc": 200
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\gemm_template.py",
    "summary": "CUTLASS GEMM Template, which is used to generate CUTLASS GEMM kernels | classes: CUTLASSGemmTemplate, CUTLASS3xGemmTemplate, CUTLASS2xGemmTemplate | imports: copy, abc, config, ir | [torch _inductor codegen cuda gemm_template.py]",
    "role": "src",
    "loc": 1390
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\__init__.py",
    "summary": "Package initializer | [torch _inductor codegen cuda __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cutlass_lib_extensions\\gemm_operation_extensions.py",
    "summary": "Responsible for emitting a CUTLASS 3.x template definition | classes: EmitGemmUniversal3xInstanceWithEVT | imports: cutlass_utils, cutlass_library | [torch _inductor codegen cuda cutlass_lib_extensions gemm_operation_extensions.py]",
    "role": "src",
    "loc": 160
  },
  {
    "id": "torch\\_inductor\\codegen\\cuda\\cutlass_lib_extensions\\__init__.py",
    "summary": "Package initializer | [torch _inductor codegen cuda cutlass_lib_extensions __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\ck_conv_template.py",
    "summary": "No description | classes: CKGroupedConvFwdTemplate | functions: gen_conv_ops_library, torch_layout_to_ck_layouts, torch_layout_to_ck_input_layout, torch_layout_to_ck_weight_layout, torch_layout_to_ck_output_layout | imports: copy, random, torch, ck4inductor | [torch _inductor codegen rocm ck_conv_te",
    "role": "src",
    "loc": 505
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\ck_template.py",
    "summary": "Base class for generating CK templates, has common, i.e. non-gemm-specific, code generation logic | classes: CKTemplate | imports: typing_extensions, torch, rocm_template | [torch _inductor codegen rocm ck_template.py]",
    "role": "src",
    "loc": 72
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\ck_universal_gemm_template.py",
    "summary": "No description | classes: CKGemmTemplate | functions: is_static_int, torch_layout_to_ck_layout | imports: copy, random, sympy, torch | [torch _inductor codegen rocm ck_universal_gemm_template.py]",
    "role": "src",
    "loc": 845
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\compile_command.py",
    "summary": "No description | functions: _rocm_include_paths, _rocm_lib_options, _rocm_compiler_options, rocm_compiler, rocm_compile_command | imports: torch, libfb | [torch _inductor codegen rocm compile_command.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\rocm_benchmark_request.py",
    "summary": "No description | classes: ROCmBenchmarkRequest | imports: functools, ctypes, torch | [torch _inductor codegen rocm rocm_benchmark_request.py]",
    "role": "src",
    "loc": 122
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\rocm_cpp_scheduling.py",
    "summary": "Partial Scheduling implementation for ROCm C++ Kernels. | classes: ROCmCPPScheduling | imports: codecache, scheduler, utils, virtualized | [torch _inductor codegen rocm rocm_cpp_scheduling.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\rocm_kernel.py",
    "summary": "Baseclass for ROCm based Kernels | classes: ROCmKernel, ROCmTemplateKernel, ROCmTemplateCaller | functions: _normalize_idx | imports: torch, ir, virtualized, common | [torch _inductor codegen rocm rocm_kernel.py]",
    "role": "src",
    "loc": 226
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\rocm_template.py",
    "summary": "No description | classes: ArgInfo, ROCmTemplate | imports: functools, dataclasses, unittest, autotune_process | [torch _inductor codegen rocm rocm_template.py]",
    "role": "src",
    "loc": 153
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\rocm_template_buffer.py",
    "summary": "No description | classes: ROCmTemplateBuffer | imports: typing_extensions, ir | [torch _inductor codegen rocm rocm_template_buffer.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\_inductor\\codegen\\rocm\\__init__.py",
    "summary": "Package initializer | [torch _inductor codegen rocm __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\codegen\\xpu\\device_op_overrides.py",
    "summary": "No description | classes: XPUDeviceOpOverrides | imports: common | [torch _inductor codegen xpu device_op_overrides.py]",
    "role": "src",
    "loc": 53
  },
  {
    "id": "torch\\_inductor\\codegen\\xpu\\__init__.py",
    "summary": "Package initializer | [torch _inductor codegen xpu __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\compile_worker\\subproc_pool.py",
    "summary": "Carries exception info from subprocesses across the wire. traceback | classes: _SubprocExceptionInfo, SubprocException, SubprocPickler, SubprocKind, SubprocPool, SubprocMain | functions: _pack_msg, _unpack_msg, _send_msg, _recv_msg, _get_ld_library_path, _warm_process_pool | imports: functools, mult",
    "role": "src",
    "loc": 275
  },
  {
    "id": "torch\\_inductor\\compile_worker\\watchdog.py",
    "summary": "No description | functions: _async_compile_initializer, run, has_parent_changed | imports: signal, threading | [torch _inductor compile_worker watchdog.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torch\\_inductor\\compile_worker\\__init__.py",
    "summary": "Package initializer | [torch _inductor compile_worker __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\compile_worker\\__main__.py",
    "summary": "Given a base type and qualified name: import & lookup that name, check | functions: _lookup_and_create_type, main | imports: argparse, functools, importlib, torch | [torch _inductor compile_worker __main__.py]",
    "role": "src",
    "loc": 57
  },
  {
    "id": "torch\\_inductor\\fx_passes\\b2b_gemm.py",
    "summary": "No description | functions: b2b_gemm_grid, load_ratio_left, load_ratio_right, is_b2b_gemm_good_on, check_all_attr_true, unoptimized_b2b_gemm | imports: functools, torch, _dynamo, ir | [torch _inductor fx_passes b2b_gemm.py]",
    "role": "src",
    "loc": 608
  },
  {
    "id": "torch\\_inductor\\fx_passes\\binary_folding.py",
    "summary": "No description | functions: mark_mixed_dtype, mark_mixed_dtype_allowed_computation_ops, recover_original_precision_folded_computation_ops, binary_folding_init, _op_not_broadcasting_with_conv, _op_not_broadcasting_with_linear | imports: functools, torch, _dynamo, pattern_matcher | [torch _inductor fx",
    "role": "src",
    "loc": 446
  },
  {
    "id": "torch\\_inductor\\fx_passes\\ddp_fusion.py",
    "summary": "No description | classes: CommBlock | functions: move_block_after, move_block_before, call_function, get_comm_block, get_all_comm_blocks, always_true | imports: inspect, operator, dataclasses, functools | [torch _inductor fx_passes ddp_fusion.py]",
    "role": "src",
    "loc": 457
  },
  {
    "id": "torch\\_inductor\\fx_passes\\decompose_mem_bound_mm.py",
    "summary": "No description | functions: check_device, realize_inputs, should_decompose_bmm, should_decompose_mm, is_node_meta_valid, print_decompose_pattern | imports: torch, pattern_matcher, split_cat | [torch _inductor fx_passes decompose_mem_bound_mm.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\_inductor\\fx_passes\\dedupe_symint_uses.py",
    "summary": "Hash for a py_sym_types that will use the underlying sympy expression | classes: _SymExprHash, _SymHashingDict | functions: dedupe_symints | imports: dataclasses, torch | [torch _inductor fx_passes dedupe_symint_uses.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\_inductor\\fx_passes\\efficient_conv_bn_eval.py",
    "summary": "Implementation based on https://arxiv.org/abs/2305.11624 | functions: efficient_conv_bn_eval, efficient_conv_bn_eval_decomposed, efficient_conv_bn_eval_graph_transform_inlined, efficient_conv_bn_eval_graph_transform_decomposed, efficient_conv_bn_eval_graph_transform | imports: torch, pattern_matcher",
    "role": "src",
    "loc": 307
  },
  {
    "id": "torch\\_inductor\\fx_passes\\freezing_patterns.py",
    "summary": "Passes that are applied to the graph to freeze pass. | functions: freezing_passes, lazy_init, register_freezing_graph_pattern, register_binary_folding_pattern, addmm_patterns_init, check_concat_weights | imports: functools, torch, _dynamo, pattern_matcher | [torch _inductor fx_passes freezing_patter",
    "role": "src",
    "loc": 177
  },
  {
    "id": "torch\\_inductor\\fx_passes\\fuse_attention.py",
    "summary": "No description | functions: _sfdp_pattern_1, _sfdp_replacement_1, _sfdp_pattern_2, _sfdp_replacement_2, _sfdp_pattern_3, _sfdp_replacement_3 | imports: functools, inspect, torch, _dynamo | [torch _inductor fx_passes fuse_attention.py]",
    "role": "src",
    "loc": 740
  },
  {
    "id": "torch\\_inductor\\fx_passes\\group_batch_fusion.py",
    "summary": "Update the example value of the node in the graph to enable followup split cat opt. | classes: GroupBatchFusionBase, GroupFusion, BatchFusion, BatchPointwiseOpsFusionFactory, PostGradBatchLinearFusion, GroupLinearFusion | functions: update_stack_example_value, update_pointwise_example_value, registe",
    "role": "src",
    "loc": 1164
  },
  {
    "id": "torch\\_inductor\\fx_passes\\joint_graph.py",
    "summary": "Runs constant folding and replaces tensors that have a unifrom value | classes: UniformValueConstantFolder | functions: lazy_init, remove_no_ops, fake_tensors_eq, replace_no_op, visit, remove_redundant_views | imports: functools, operator, torch, pattern_matcher | [torch _inductor fx_passes joint_gr",
    "role": "src",
    "loc": 615
  },
  {
    "id": "torch\\_inductor\\fx_passes\\micro_pipeline_tp.py",
    "summary": "No description | classes: _AllGatherMatch, _ReduceScatterMatch, _Matmul, _ScaledMatmul | functions: _is_backward, _compute_mm_arithmetic_intensity, _filter_nodes_by_target, _find_ancestors, _get_tensor, find_all_gather_patterns | imports: operator, dataclasses, torch, pattern_matcher | [torch _induc",
    "role": "src",
    "loc": 692
  },
  {
    "id": "torch\\_inductor\\fx_passes\\misc_patterns.py",
    "summary": "No description | classes: NumpyCompatNormalization | functions: _misc_patterns_init, randperm_index_add_pattern, randperm_index_add_replacement, randperm_index_pattern, randperm_index_replacement | imports: functools, torch, pattern_matcher, joint_graph | [torch _inductor fx_passes misc_patterns.py]",
    "role": "src",
    "loc": 98
  },
  {
    "id": "torch\\_inductor\\fx_passes\\mkldnn_fusion.py",
    "summary": "Here we check: | classes: UnaryAttr | functions: _is_valid_grouped_gemm_fusion, grouped_gemm_pass, _conv_call, _linear_call, _conv_transpose_call, _to_float | imports: functools, operator, torch, lowering | [torch _inductor fx_passes mkldnn_fusion.py]",
    "role": "src",
    "loc": 1268
  },
  {
    "id": "torch\\_inductor\\fx_passes\\numeric_utils.py",
    "summary": "Make torch manual seed deterministic. | functions: set_deterministic, clean_memory, compare_dict_tensors, compare_tuple_tensors, compare_parameters, compare_forward_output | imports: gc, random, traceback, numpy | [torch _inductor fx_passes numeric_utils.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "torch\\_inductor\\fx_passes\\pad_mm.py",
    "summary": "No description | functions: fetch_fake_tensors, unwrap_fake_args, decorator, wrapper, get_alignment_size, get_alignment_size_dtype | imports: functools, operator, torch, utils | [torch _inductor fx_passes pad_mm.py]",
    "role": "src",
    "loc": 725
  },
  {
    "id": "torch\\_inductor\\fx_passes\\post_grad.py",
    "summary": "Passes that run on after grad.  This is called once on the forwards | classes: ConstructorMoverPass | functions: post_grad_passes, lazy_init, reorder_for_locality, visit, register_lowering_pattern, is_valid_mm_plus_mm | imports: functools, operator, typing_extensions, torch | [torch _inductor fx_pas",
    "role": "src",
    "loc": 918
  },
  {
    "id": "torch\\_inductor\\fx_passes\\pre_grad.py",
    "summary": "No description | classes: IdentityRemover, ConvBNFusion, NormalizedLinearNode, NormalizedMatmulNode | functions: save_inductor_dict, is_same_dict, shape_prop, normalize_node_kwargs_pass, fuse_parallel_linear_pass, remove_split_ops | imports: copy, types, torch, fx_utils | [torch _inductor fx_passes ",
    "role": "src",
    "loc": 661
  },
  {
    "id": "torch\\_inductor\\fx_passes\\quantization.py",
    "summary": "Get the pattern's output dtype from node's meta | classes: PostOpAttr | functions: _get_pattern_output_dtype, _may_generate_pattern_with_dtype_convert, _may_generate_pattern_with_reshape, _generate_linear_t_pattern, _unary_fusion_pattern, get_dequantize_per_tensor_activation_pattern | imports: copy,",
    "role": "src",
    "loc": 2968
  },
  {
    "id": "torch\\_inductor\\fx_passes\\reinplace.py",
    "summary": "No description | classes: InplaceableOp, ViewOp | functions: graph_call_function, _inplace_generalized_scatter, _generalized_scatter, _decompose_scatter_functional_helper, _decompose_scatter_functional, _decompose_scatter_mutating | imports: operator, dataclasses, torch, triton | [torch _inductor fx",
    "role": "src",
    "loc": 560
  },
  {
    "id": "torch\\_inductor\\fx_passes\\replace_random.py",
    "summary": "Modify the given FX graph to use backend-native random ops | functions: replace_random_passes, fuse_seed_creation_pass, default_kwargs, get_device, replace_random, replacement | imports: torch, pattern_matcher, virtualized | [torch _inductor fx_passes replace_random.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\_inductor\\fx_passes\\split_cat.py",
    "summary": "Matches a call to torch.split if it is in a normalized form. Ensures that all users of | classes: TorchSplit, SplitCatSimplifier, UnbindCatRemover, GetItem | functions: construct_pattern_matcher_pass, _get_split_args_default, _get_dim, normalize_split_base, normalize_split_default, remove_split_with",
    "role": "src",
    "loc": 2179
  },
  {
    "id": "torch\\_inductor\\fx_passes\\__init__.py",
    "summary": "Package initializer | [torch _inductor fx_passes __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\addmm_pattern.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns addmm_pattern.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\bmm_pattern.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns bmm_pattern.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\mm_pattern.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns mm_pattern.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_1.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_1.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_10.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_10.py]",
    "role": "src",
    "loc": 189
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_11.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_11.py]",
    "role": "src",
    "loc": 188
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_12.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_12.py]",
    "role": "src",
    "loc": 204
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_13.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_13.py]",
    "role": "src",
    "loc": 114
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_14.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_14.py]",
    "role": "src",
    "loc": 194
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_15.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_15.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_16.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_16.py]",
    "role": "src",
    "loc": 567
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_17.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_17.py]",
    "role": "src",
    "loc": 228
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_18.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_18.py]",
    "role": "src",
    "loc": 429
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_19.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_19.py]",
    "role": "src",
    "loc": 193
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_2.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_2.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_3.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_3.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_4.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_4.py]",
    "role": "src",
    "loc": 174
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_5.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_5.py]",
    "role": "src",
    "loc": 162
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_6.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_6.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_7.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_7.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_8.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_8.py]",
    "role": "src",
    "loc": 189
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\_sfdp_pattern_9.py",
    "summary": "No description | imports: torch | [torch _inductor fx_passes serialized_patterns _sfdp_pattern_9.py]",
    "role": "src",
    "loc": 205
  },
  {
    "id": "torch\\_inductor\\fx_passes\\serialized_patterns\\__init__.py",
    "summary": "Package initializer | [torch _inductor fx_passes serialized_patterns __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_inductor\\kernel\\bmm.py",
    "summary": "No description | functions: bmm_grid, _is_large_block_for_cpu, bmm_configs, tuned_bmm, is_valid_to_require_contiguous, is_preferred_layout_as_bmm_input | imports: torch, select_algorithm, utils, virtualized | [torch _inductor kernel bmm.py]",
    "role": "src",
    "loc": 171
  },
  {
    "id": "torch\\_inductor\\kernel\\conv.py",
    "summary": "No description | classes: ConvLayoutParams | functions: conv2d_grid, conv3d_grid, _is_large_block_for_cpu, conv_configs, conv1x1_via_mm, conv_layout | imports: torch, lowering, select_algorithm, utils | [torch _inductor kernel conv.py]",
    "role": "src",
    "loc": 599
  },
  {
    "id": "torch\\_inductor\\kernel\\flex_attention.py",
    "summary": "Triton Implementation of the flex_attention Kernel | classes: Mode, JointOutputResult | functions: construct_strides, infer_dense_strides, flex_attention_grid, create_placeholder, maybe_realize, get_float32_precision | imports: copy, dataclasses, sympy, torch | [torch _inductor kernel flex_attention",
    "role": "src",
    "loc": 2041
  },
  {
    "id": "torch\\_inductor\\kernel\\flex_decoding.py",
    "summary": "Triton Implementation of the flex_attention Kernel for short query length (FlexDecoding) | functions: flex_decoding_grid, get_split_k, _get_decoding_default_config, create_flex_decoding_kernel | imports: sympy, torch, ir, lowering | [torch _inductor kernel flex_decoding.py]",
    "role": "src",
    "loc": 460
  },
  {
    "id": "torch\\_inductor\\kernel\\mm.py",
    "summary": "No description | functions: lazy_register_extern_choice, _is_int8_mat, _is_large_block_for_cpu, mm_config_kwargs, bias_addmm, tuned_mm | imports: functools, torch, codegen, ir | [torch _inductor kernel mm.py]",
    "role": "src",
    "loc": 715
  },
  {
    "id": "torch\\_inductor\\kernel\\mm_common.py",
    "summary": "No description | functions: triton_config, build_rocm_gemm_configs, filtered_configs, should_fallback_to_aten, mm_grid, persistent_mm_grid | imports: functools, sympy, torch, codegen | [torch _inductor kernel mm_common.py]",
    "role": "src",
    "loc": 497
  },
  {
    "id": "torch\\_inductor\\kernel\\mm_plus_mm.py",
    "summary": "No description | functions: mm_configs, tuned_mm_plus_mm | imports: functools, torch, lowering, select_algorithm | [torch _inductor kernel mm_plus_mm.py]",
    "role": "src",
    "loc": 203
  },
  {
    "id": "torch\\_inductor\\kernel\\mm_scaled.py",
    "summary": "No description | functions: are_compatible_scales, check_supported_striding, is_row_major, is_col_major, has_zero_dim, scaled_mm_options_device_tma | imports: sympy, torch, config, ir | [torch _inductor kernel mm_scaled.py]",
    "role": "src",
    "loc": 468
  },
  {
    "id": "torch\\_inductor\\kernel\\__init__.py",
    "summary": "Package initializer | [torch _inductor kernel __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\_inductor\\package\\build_package.py",
    "summary": "No description | [torch _inductor package build_package.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\_inductor\\package\\package.py",
    "summary": "No description | classes: PT2ArchiveWriter, PT2ArchiveReader, AOTICompiledModel | functions: _run_command_and_check, compile_so, get_aoti_file_with_suffix, package_aoti, load_package | imports: io, json, shlex, subprocess | [torch _inductor package package.py]",
    "role": "src",
    "loc": 239
  },
  {
    "id": "torch\\_inductor\\package\\pt2_archive_constants.py",
    "summary": "No description | [torch _inductor package pt2_archive_constants.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\_inductor\\package\\__init__.py",
    "summary": "Package initializer | imports: package | [torch _inductor package __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\_inductor\\runtime\\autotune_cache.py",
    "summary": "No description | classes: AutotuneCache, _AutotuneCacheBundlerImpl, AutotuneCacheBundler, _LocalAutotuneCacheBackend, LocalAutotuneCache | functions: inductor_meta_from_config, _comment_stripped_hash, _should_use_remote_autotune_cache, _load_cached_autotuning, _splitext_nodot | imports: dataclasses,",
    "role": "src",
    "loc": 372
  },
  {
    "id": "torch\\_inductor\\runtime\\benchmarking.py",
    "summary": "Wraps `fn` with `dynamo_timed` context, and increments the appropriate dynamo | classes: Benchmarker, TritonBenchmarker, InductorBenchmarker | functions: time_and_count, wrapper | imports: inspect, functools, statistics, typing_extensions | [torch _inductor runtime benchmarking.py]",
    "role": "src",
    "loc": 229
  },
  {
    "id": "torch\\_inductor\\runtime\\cache_dir_utils.py",
    "summary": "No description | functions: cache_dir, default_cache_dir, triton_cache_dir | imports: getpass, tempfile | [torch _inductor runtime cache_dir_utils.py]",
    "role": "src",
    "loc": 24
  },
  {
    "id": "torch\\_inductor\\runtime\\compile_tasks.py",
    "summary": "No description | functions: _reload_triton_kernel_in_subproc, _module_to_triton_kernel, _reload_python_module_in_subproc, _reload_python_module, _set_triton_ptxas_path, _worker_compile_triton | imports: functools, types, torch | [torch _inductor runtime compile_tasks.py]",
    "role": "src",
    "loc": 62
  },
  {
    "id": "torch\\_inductor\\runtime\\coordinate_descent_tuner.py",
    "summary": "The coordinate descent tuner. Tune one field/coordinate at a time. | classes: CoordescTuner | functions: get_field, set_field | imports: copy, hints, runtime_utils, triton_compat | [torch _inductor runtime coordinate_descent_tuner.py]",
    "role": "src",
    "loc": 235
  },
  {
    "id": "torch\\_inductor\\runtime\\halide_helpers.py",
    "summary": "Box-Muller transform | functions: _pair_uniform_to_normal, _uint_to_uniform_float, philox_impl, umulhi, halide_philox, randint4x | imports: halide | [torch _inductor runtime halide_helpers.py]",
    "role": "src",
    "loc": 82
  },
  {
    "id": "torch\\_inductor\\runtime\\hints.py",
    "summary": "No description | classes: ReductionHint, TileHint, HeuristicType, AutotuneHint, DeviceProperties, HalideInputSpec | functions: AttrsDescriptorWrapper | imports: functools, torch, triton | [torch _inductor runtime hints.py]",
    "role": "src",
    "loc": 158
  },
  {
    "id": "torch\\_inductor\\runtime\\runtime_utils.py",
    "summary": "No description | functions: conditional_product, ceildiv, is_power_of_2, next_power_of_2, get_num_bytes, triton_config_to_hashable | imports: functools, operator, torch, triton_compat | [torch _inductor runtime runtime_utils.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "torch\\_inductor\\runtime\\triton_compat.py",
    "summary": "No description | classes: PTXASError, OutOfResources, triton, tl, autograd_profiler | functions: GPUTarget, _log2, _raise_error, cc_warp_size | imports: torch, triton | [torch _inductor runtime triton_compat.py]",
    "role": "src",
    "loc": 106
  },
  {
    "id": "torch\\_inductor\\runtime\\triton_helpers.py",
    "summary": "No description | functions: set_driver_to_cpu, set_driver_to_gpu, get_backend_options, promote_to_tensor, div_floor_integer, remainder_integer | imports: triton_compat | [torch _inductor runtime triton_helpers.py]",
    "role": "src",
    "loc": 513
  },
  {
    "id": "torch\\_inductor\\runtime\\triton_heuristics.py",
    "summary": "No description | classes: NoTritonConfigsError, CachingAutotuner, _ConstRepr, TritonCompileResult, DebugAutotuner | functions: get_total_reduction_numel, autotune_hints_to_configs, disable_pointwise_autotuning, _dump_launch_params, _find_names, start_graph | imports: builtins, copy, functools, hashl",
    "role": "src",
    "loc": 1901
  },
  {
    "id": "torch\\_inductor\\runtime\\__init__.py",
    "summary": "Package initializer | [torch _inductor runtime __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_lazy\\closure.py",
    "summary": "Adds a closure to the list of the ones to be run at the end of the step. | classes: ClosureHandler, AsyncClosureHandler | functions: add_step_closure, run_step_closures | imports: threading, queue, torch | [torch _lazy closure.py]",
    "role": "src",
    "loc": 115
  },
  {
    "id": "torch\\_lazy\\computation.py",
    "summary": "Return tensor ids and eager tensors for DeviceData nodes in the | functions: get_tensors_ts_device_data_node, get_graph_hash, run_cached_graph | imports: torch | [torch _lazy computation.py]",
    "role": "src",
    "loc": 18
  },
  {
    "id": "torch\\_lazy\\config.py",
    "summary": "Get the config used to force LTC fallback | functions: get_force_fallback, set_force_fallback, set_reuse_ir | imports: torch | [torch _lazy config.py]",
    "role": "src",
    "loc": 10
  },
  {
    "id": "torch\\_lazy\\debug.py",
    "summary": "Return a text dump of the LTC IR graph in dot format for the tensors. | functions: render_ir_graph, dump_ir | imports: torch | [torch _lazy debug.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torch\\_lazy\\device_context.py",
    "summary": "No description | classes: DeviceContext | functions: get_device_context | imports: threading, torch | [torch _lazy device_context.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torch\\_lazy\\extract_compiled_graph.py",
    "summary": "The GraphInputMatcher class setup the graph inputs for future calls after lazy tracing. | classes: GraphInputMatcher, ReturnValueHandler | functions: force_lazy_device, tolazydevice, hasDeviceArg, get_fallback_ops, extract_compiled_graph, optimized_mod | imports: copy, dataclasses, torch | [torch _l",
    "role": "src",
    "loc": 157
  },
  {
    "id": "torch\\_lazy\\ir_cache.py",
    "summary": "Dump TrieCache in the dot format | functions: dump, reset | imports: torch | [torch _lazy ir_cache.py]",
    "role": "src",
    "loc": 9
  },
  {
    "id": "torch\\_lazy\\metrics.py",
    "summary": "Resets all metric counters. | functions: reset, counter_names, counter_value, metrics_report | imports: torch | [torch _lazy metrics.py]",
    "role": "src",
    "loc": 13
  },
  {
    "id": "torch\\_lazy\\tensor_factory_functions.py",
    "summary": "No description | imports: torch | [torch _lazy tensor_factory_functions.py]",
    "role": "src",
    "loc": 38
  },
  {
    "id": "torch\\_lazy\\ts_backend.py",
    "summary": "Initializes the lazy Torchscript backend | functions: init | imports: torch | [torch _lazy ts_backend.py]",
    "role": "src",
    "loc": 4
  },
  {
    "id": "torch\\_lazy\\__init__.py",
    "summary": "Triggers a mark step, which amounts to | functions: mark_step, wait_device_ops, sync_multi, get_tensor_id, to_cpu, save | imports: torch, closure | [torch _lazy __init__.py]",
    "role": "src",
    "loc": 37
  },
  {
    "id": "torch\\_library\\autograd.py",
    "summary": "No description | classes: InfoProtocol, Info, Metadata | functions: make_autograd_impl, forward_no_grad, forward, backward, autograd_impl, supports_tensorlist | imports: dataclasses, torch | [torch _library autograd.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "torch\\_library\\custom_ops.py",
    "summary": "CustomOpDef is a wrapper around a function that turns it into a custom op. | classes: CustomOpDef | functions: custom_op, inner, _cast, increment_version, get_library_allowing_overwrite, _maybe_get_opdef | imports: inspect, weakref, torch | [torch _library custom_ops.py]",
    "role": "src",
    "loc": 762
  },
  {
    "id": "torch\\_library\\fake_class_registry.py",
    "summary": "No description | classes: FakeScriptObject, FakeScriptMethod, HasStaticMethodFromReal, FakeClassRegistry | functions: _check_valid_flat_script_obj, tracing_with_real, maybe_to_fake_obj, register_fake_class, inner, deregister_fake_class | imports: copy, torch | [torch _library fake_class_registry.py]",
    "role": "src",
    "loc": 246
  },
  {
    "id": "torch\\_library\\fake_impl.py",
    "summary": "A holder where one can register an fake impl to. | classes: FakeImplHolder, FakeImplCtx | functions: construct_meta_kernel, meta_kernel, error_on_ctx, get_none, set_ctx_getter, allocate_size | imports: functools, typing_extensions, torch | [torch _library fake_impl.py]",
    "role": "src",
    "loc": 169
  },
  {
    "id": "torch\\_library\\infer_schema.py",
    "summary": "Parses the schema of a given function with type hints. The schema is inferred from the | functions: infer_schema, error_fn, convert_type_string, unstringify_types, unstringify_type, derived_types | imports: inspect, types, torch | [torch _library infer_schema.py]",
    "role": "src",
    "loc": 260
  },
  {
    "id": "torch\\_library\\simple_registry.py",
    "summary": "Registry for the \"simple\" torch.library APIs | classes: SimpleLibraryRegistry, SimpleOperatorEntry, GenericTorchDispatchRuleHolder | functions: find_torch_dispatch_rule | imports: fake_impl, utils | [torch _library simple_registry.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\_library\\triton.py",
    "summary": "Create a custom operator whose implementation is backed by 1+ triton kernels. | functions: triton_op, dec, backend_fn, functional_decomp, set_wrap_triton_enabled, is_wrap_triton_enabled | imports: threading, torch, custom_ops, infer_schema | [torch _library triton.py]",
    "role": "src",
    "loc": 213
  },
  {
    "id": "torch\\_library\\utils.py",
    "summary": "Models a (function, source location) | classes: Kernel, RegistrationHandle, MutationChecker | functions: warn_deploy, get_source, parse_namespace, lookup_op, is_builtin, is_functional_schema | imports: dataclasses, inspect, torch, torchgen | [torch _library utils.py]",
    "role": "src",
    "loc": 376
  },
  {
    "id": "torch\\_library\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch _library __init__.py]",
    "role": "src",
    "loc": 6
  },
  {
    "id": "torch\\_logging\\scribe.py",
    "summary": "No description | functions: make_scribe_logger, inner | imports: typing_extensions, fbscribelogger | [torch _logging scribe.py]",
    "role": "src",
    "loc": 35
  },
  {
    "id": "torch\\_logging\\structured.py",
    "summary": "Utilities for converting data types into structured JSON for dumping. | functions: intern_string, dump_file, from_traceback, get_user_stack, get_framework_stack | imports: inspect, traceback, torch | [torch _logging structured.py]",
    "role": "src",
    "loc": 81
  },
  {
    "id": "torch\\_logging\\_internal.py",
    "summary": "Sets the log level for individual components and toggles individual log | classes: LogRegistry, LogState, TorchLogsFormatter, LazyTraceHandler, LazyString | functions: set_logs, _set_logs, get_loggers, register_log, register_artifact, getArtifactLogger | imports: functools, hashlib, importlib, json ",
    "role": "src",
    "loc": 966
  },
  {
    "id": "torch\\_logging\\_registrations.py",
    "summary": "No description | imports: _internal | [torch _logging _registrations.py]",
    "role": "src",
    "loc": 212
  },
  {
    "id": "torch\\_logging\\__init__.py",
    "summary": "Package initializer | imports: torch, _internal | [torch _logging __init__.py]",
    "role": "src",
    "loc": 12
  },
  {
    "id": "torch\\_numpy\\fft.py",
    "summary": "NumPy fft casts inputs to 64 bit and *returns 64-bit results*. | functions: upcast, wrapped, fft, ifft, rfft, irfft | imports: functools, torch, _normalizations | [torch _numpy fft.py]",
    "role": "src",
    "loc": 85
  },
  {
    "id": "torch\\_numpy\\linalg.py",
    "summary": "No description | classes: LinAlgError | functions: _atleast_float_1, _atleast_float_2, linalg_errors, wrapped, matrix_power, multi_dot | imports: functools, torch, _normalizations | [torch _numpy linalg.py]",
    "role": "src",
    "loc": 154
  },
  {
    "id": "torch\\_numpy\\random.py",
    "summary": "Wrapper to mimic (parts of) np.random API surface. | functions: use_numpy_random, deco_stream, inner, seed, random_sample, rand | imports: functools, torch, _normalizations, numpy | [torch _numpy random.py]",
    "role": "src",
    "loc": 129
  },
  {
    "id": "torch\\_numpy\\_binary_ufuncs_impl.py",
    "summary": "Export torch work functions for binary ufuncs, rename/tweak to match numpy. | functions: matmul, divmod | imports: torch | [torch _numpy _binary_ufuncs_impl.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torch\\_numpy\\_casting_dicts.py",
    "summary": "No description | imports: torch | [torch _numpy _casting_dicts.py]",
    "role": "src",
    "loc": 1359
  },
  {
    "id": "torch\\_numpy\\_dtypes.py",
    "summary": "Define analogs of numpy dtypes supported by pytorch. | classes: generic, number, integer, inexact, signedinteger, unsignedinteger | functions: sctype_from_string, sctype_from_torch_dtype, dtype, set_default_dtype, issubclass_, issubdtype | imports: builtins, torch, _ndarray | [torch _numpy _dtypes.p",
    "role": "src",
    "loc": 302
  },
  {
    "id": "torch\\_numpy\\_dtypes_impl.py",
    "summary": "Dtypes/scalar type implementaions with torch dtypes. | functions: default_dtypes, get_default_dtype_for, can_cast_impl, result_type_impl, python_type_for_torch, is_scalar | imports: torch | [torch _numpy _dtypes_impl.py]",
    "role": "src",
    "loc": 142
  },
  {
    "id": "torch\\_numpy\\_funcs.py",
    "summary": "Written by Konrad Hinsen <hinsen@cnrs-orleans.fr> | classes: IndexExpression | functions: _public_functions, is_public_function | imports: inspect, _normalizations | [torch _numpy _funcs.py]",
    "role": "src",
    "loc": 41
  },
  {
    "id": "torch\\_numpy\\_funcs_impl.py",
    "summary": "A thin pytorch / numpy compat layer. | functions: copy, copyto, atleast_1d, atleast_2d, atleast_3d, _concat_check | imports: builtins, operator, torch, _normalizations | [torch _numpy _funcs_impl.py]",
    "role": "src",
    "loc": 1377
  },
  {
    "id": "torch\\_numpy\\_getlimits.py",
    "summary": "No description | functions: finfo, iinfo | imports: torch | [torch _numpy _getlimits.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torch\\_numpy\\_ndarray.py",
    "summary": "No description | classes: Flags, _Unspecified, ndarray | functions: create_method, f, _upcast_int_indices, _tolist, array, asarray | imports: builtins, operator, torch, _normalizations | [torch _numpy _ndarray.py]",
    "role": "src",
    "loc": 432
  },
  {
    "id": "torch\\_numpy\\_normalizations.py",
    "summary": "\"Normalize\" arguments: convert array_likes to tensors, dtypes to torch dtypes and so on. | functions: normalize_array_like, normalize_array_like_or_scalar, normalize_optional_array_like_or_scalar, normalize_optional_array_like, normalize_seq_array_like, normalize_dtype | imports: functools, inspect,",
    "role": "src",
    "loc": 166
  },
  {
    "id": "torch\\_numpy\\_reductions_impl.py",
    "summary": "Implementation of reduction operations, to be wrapped into arrays, dtypes etc | functions: _deco_axis_expand, wrapped, _atleast_float, count_nonzero, argmax, argmin | imports: functools, torch, _normalizations | [torch _numpy _reductions_impl.py]",
    "role": "src",
    "loc": 343
  },
  {
    "id": "torch\\_numpy\\_ufuncs.py",
    "summary": "No description | functions: _ufunc_postprocess, deco_binary_ufunc, wrapped, cast, matmul, ldexp | imports: torch, _normalizations | [torch _numpy _ufuncs.py]",
    "role": "src",
    "loc": 254
  },
  {
    "id": "torch\\_numpy\\_unary_ufuncs_impl.py",
    "summary": "Export torch work functions for unary ufuncs, rename/tweak to match numpy. | functions: cbrt, positive, absolute | imports: torch | [torch _numpy _unary_ufuncs_impl.py]",
    "role": "src",
    "loc": 58
  },
  {
    "id": "torch\\_numpy\\_util.py",
    "summary": "Assorted utilities, which do not need anything other then torch and stdlib. | classes: AxisError, UFuncTypeError | functions: is_sequence, cast_if_needed, cast_int_to_float, normalize_axis_index, normalize_axis_tuple, allow_only_single_axis | imports: operator, torch, _ndarray | [torch _numpy _util.",
    "role": "src",
    "loc": 191
  },
  {
    "id": "torch\\_numpy\\__init__.py",
    "summary": "Package initializer | imports: _dtypes, _funcs, _getlimits, _ndarray | [torch _numpy __init__.py]",
    "role": "src",
    "loc": 25
  },
  {
    "id": "torch\\_numpy\\testing\\utils.py",
    "summary": "Utility function to facilitate testing. | classes: _Dummy, IgnoreException, clear_and_catch_warnings, suppress_warnings | functions: assert_, gisnan, gisfinite, gisinf, build_err_msg, assert_equal | imports: gc, operator, platform, pprint | [torch _numpy testing utils.py]",
    "role": "src",
    "loc": 1911
  },
  {
    "id": "torch\\_numpy\\testing\\__init__.py",
    "summary": "Package initializer | imports: utils | [torch _numpy testing __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\_prims\\context.py",
    "summary": "Switches the interpretation of torch.* functions and Tensor methods to | classes: TorchRefsMode | functions: torch_to_refs_map, all_prims | imports: functools, torch | [torch _prims context.py]",
    "role": "src",
    "loc": 119
  },
  {
    "id": "torch\\_prims\\debug_prims.py",
    "summary": "No description | functions: load_tensor_reader, register_debug_prims, load_tensor_factory | imports: torch | [torch _prims debug_prims.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torch\\_prims\\executor.py",
    "summary": "Prototype ATen executor. | functions: execute, make_traced, _traced | imports: typing_extensions, torch | [torch _prims executor.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torch\\_prims\\rng_prims.py",
    "summary": "No description | classes: RunAndSaveRngState, RunWithRngState | functions: throw_on_non_cuda, register_rng_prim, philox_rand_offset_meta, philox_rand_offset, register_philox_rand, _philox_rand_meta | imports: torch | [torch _prims rng_prims.py]",
    "role": "src",
    "loc": 250
  },
  {
    "id": "torch\\_prims\\__init__.py",
    "summary": "Package initializer | classes: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND | functions: TensorMeta, _make_prim, _prim_impl, _autograd_impl, _backend_select_impl, _prim_elementwise_meta | imports: operator, functools, torch | [torch _prims __init__.py]",
    "role": "src",
    "loc": 2192
  },
  {
    "id": "torch\\_prims_common\\wrappers.py",
    "summary": "Adds elementwise type promotion to a Python reference implementation. | classes: elementwise_type_promotion_wrapper, BackwardsNotSupported | functions: _maybe_convert_to_dtype, _maybe_convert_to_type, _annotation_has_type, _resize_output_check, _maybe_resize_out, is_cpu_scalar | imports: inspect, ty",
    "role": "src",
    "loc": 330
  },
  {
    "id": "torch\\_prims_common\\__init__.py",
    "summary": "Package initializer | classes: _WorksWithInt, K, ELEMENTWISE_TYPE_PROMOTION_KIND, REDUCTION_OUTPUT_TYPE_KIND, RETURN_TYPE, CUDARngStateHelper | functions: same_shape, _maybe_get_pytype, compare_tensor_meta, _check_strides_helper, check_significant_strides, check_all_strides | imports: operator, func",
    "role": "src",
    "loc": 1438
  },
  {
    "id": "torch\\_refs\\fft.py",
    "summary": "Apply normalization to the un-normalized FFT result | classes: _ShapeAndDims, _CanonicalizeC2rReturn | functions: _apply_norm, _promote_type_fft, _maybe_promote_tensor_fft, _resize_fft_input, _fft_c2r, _fft_r2c | imports: torch | [torch _refs fft.py]",
    "role": "src",
    "loc": 477
  },
  {
    "id": "torch\\_refs\\_conversions.py",
    "summary": "No description | functions: _make_conversion_method, fn, complex, polar | imports: torch | [torch _refs _conversions.py]",
    "role": "src",
    "loc": 79
  },
  {
    "id": "torch\\_refs\\__init__.py",
    "summary": "Package initializer | functions: is_noncontiguous_supported, handle_noncontiguous_outputs, _broadcast_shapes, _maybe_broadcast, __maybe_broadcast, _make_elementwise_unary_reference | imports: builtins, inspect, operator, functools | [torch _refs __init__.py]",
    "role": "src",
    "loc": 5101
  },
  {
    "id": "torch\\_refs\\linalg\\__init__.py",
    "summary": "Checks related to the dtype kwarg in `linalg.*norm` functions | functions: _check_norm_dtype, cross, diagonal, vector_norm, _backshift_permutation, _inverse_permutation | imports: functools, torch, operator | [torch _refs linalg __init__.py]",
    "role": "src",
    "loc": 249
  },
  {
    "id": "torch\\_refs\\nn\\__init__.py",
    "summary": "Package initializer | [torch _refs nn __init__.py]",
    "role": "src",
    "loc": 1
  },
  {
    "id": "torch\\_refs\\nn\\functional\\__init__.py",
    "summary": "Helper function for all dropout-type operators. During training, | functions: _dropout_helper, alpha_dropout, _inplace_wrapper, _fn, celu, dropout | imports: functools, typing_extensions, torch | [torch _refs nn functional __init__.py]",
    "role": "src",
    "loc": 1003
  },
  {
    "id": "torch\\_refs\\special\\__init__.py",
    "summary": "Package initializer | functions: bessel_j0, bessel_j1, entr, erfcx, i0e, i1 | imports: torch | [torch _refs special __init__.py]",
    "role": "src",
    "loc": 184
  },
  {
    "id": "torch\\_strobelight\\cli_function_profiler.py",
    "summary": "Raised when an error happens during strobelight profiling | classes: StrobelightCLIProfilerError, StrobelightCLIFunctionProfiler | functions: _pid_namespace_link, _pid_namespace, _command_to_string, strobelight, strobelight_inner, wrapper_function | imports: functools, subprocess, threading, timeit ",
    "role": "src",
    "loc": 255
  },
  {
    "id": "torch\\_strobelight\\compile_time_profiler.py",
    "summary": "No description | classes: StrobelightCompileTimeProfiler | functions: get_fburl, get_strobelight_url | imports: json, subprocess, datetime, socket | [torch _strobelight compile_time_profiler.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "torch\\_strobelight\\__init__.py",
    "summary": "Package initializer | [torch _strobelight __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_subclasses\\fake_impls.py",
    "summary": "No description | functions: ordered_set, is_noncontiguous_supported, contains_tensor_types, _is_tensor_constructor, register_op_impl, impl_decorator | imports: functools, torch | [torch _subclasses fake_impls.py]",
    "role": "src",
    "loc": 739
  },
  {
    "id": "torch\\_subclasses\\fake_tensor.py",
    "summary": "No description | classes: _Unassigned, IncrementRecursionCount, UnsupportedFakeTensorException, DynamicOutputShapeException, DataDependentOutputException, UnsupportedOperatorException | functions: ordered_set, unset_fake_temporarily, get_plain_tensors, is_fake, maybe_get_fake_mode, get_schema_info |",
    "role": "src",
    "loc": 2062
  },
  {
    "id": "torch\\_subclasses\\fake_utils.py",
    "summary": "No description | classes: CrossRefFakeMode | functions: outputs_alias_inputs, outputs_are_inputs, output_alias_each_other, _check_alias_info, is_sdpa_error, try_convert_fake_to_real | imports: functools, torch | [torch _subclasses fake_utils.py]",
    "role": "src",
    "loc": 257
  },
  {
    "id": "torch\\_subclasses\\functional_tensor.py",
    "summary": "Functional tensors represent tensors that will remove mutations | classes: FunctionalTensor, FunctionalTensorMode, BaseFunctionalizeAPI, PythonFunctionalizeAPI, CppFunctionalizeAPI, FunctorchFunctionalizeAPI | functions: _conversion_method_template, _, disable_functional_mode, dispatch_functionalize",
    "role": "src",
    "loc": 541
  },
  {
    "id": "torch\\_subclasses\\meta_utils.py",
    "summary": "Given a Tensor/Storage, generate a MetaTensorDesc/MetaStorageDesc | classes: MetaTensorDescriber, MetaStorageDesc, ViewFunc, _FakeTensorViewFunc, _CustomViewFunc, _MetaTensorCallback | functions: _is_fake_tensor, safe_is_leaf, safe_grad, _expect_safe_grad, assert_eq, assert_metadata_eq | imports: da",
    "role": "src",
    "loc": 1308
  },
  {
    "id": "torch\\_subclasses\\schema_check_mode.py",
    "summary": "No description | classes: SchemaCheckMode | functions: is_iterable_of_tensors, clone_inputs | imports: copy, torch | [torch _subclasses schema_check_mode.py]",
    "role": "src",
    "loc": 177
  },
  {
    "id": "torch\\_subclasses\\_fake_tensor_utils.py",
    "summary": "Represents a SymNode without the associated ShapeEnv | classes: _DeconstructedSymNode, _DeconstructedSymType, _InputBackref, _PySymInputStub, _SymIntOutputStub, _CacheKeyState | imports: dataclasses, torch, sympy, fake_tensor | [torch _subclasses _fake_tensor_utils.py]",
    "role": "src",
    "loc": 178
  },
  {
    "id": "torch\\_subclasses\\__init__.py",
    "summary": "Package initializer | imports: torch | [torch _subclasses __init__.py]",
    "role": "src",
    "loc": 15
  },
  {
    "id": "torch\\_vendor\\__init__.py",
    "summary": "Package initializer | [torch _vendor __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torch\\_vendor\\packaging\\version.py",
    "summary": ".. testsetup:: | classes: _Version, InvalidVersion, _BaseVersion, Version | functions: parse, _parse_letter_version, _parse_local_version, _cmpkey | imports: _structures | [torch _vendor packaging version.py]",
    "role": "src",
    "loc": 404
  },
  {
    "id": "torch\\_vendor\\packaging\\_structures.py",
    "summary": "No description | classes: InfinityType, NegativeInfinityType | [torch _vendor packaging _structures.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torch\\_vendor\\packaging\\__init__.py",
    "summary": "Package initializer | [torch _vendor packaging __init__.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torchgen\\code_template.py",
    "summary": "No description | classes: CodeTemplate | [torchgen code_template.py]",
    "role": "src",
    "loc": 77
  },
  {
    "id": "torchgen\\context.py",
    "summary": "No description | functions: native_function_manager, with_native_function, wrapper, with_native_function_and, method_with_native_function, method_with_nested_native_function | imports: functools, torchgen | [torchgen context.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torchgen\\gen.py",
    "summary": "No description | classes: LineLoader, RegisterSchema, ComputeOperators, ComputeFunction, ComputeTensorMethod, ComputeRedispatchFunction | functions: file_manager_from_dispatch_key, parse_native_yaml_struct, parse_tags_yaml_struct, parse_tags_yaml, parse_native_yaml, error_check_native_functions | im",
    "role": "src",
    "loc": 2439
  },
  {
    "id": "torchgen\\gen_aoti_c_shim.py",
    "summary": "No description | classes: ShimGenerator | functions: convert_arg_type_and_name, zip_type_and_name, gen_arguments, gen_returns, convert_return, gen_declaration_and_definition | imports: textwrap, dataclasses, torchgen | [torchgen gen_aoti_c_shim.py]",
    "role": "src",
    "loc": 420
  },
  {
    "id": "torchgen\\gen_backend_stubs.py",
    "summary": "No description | functions: parse_backend_yaml, create_backend_index, error_on_missing_kernels, create_decl, main, gen_dispatchkey_nativefunc_headers | imports: argparse, yaml, torchgen | [torchgen gen_backend_stubs.py]",
    "role": "src",
    "loc": 523
  },
  {
    "id": "torchgen\\gen_executorch.py",
    "summary": "A wrapper function to basically get `sig.decl(include_context=True)`. | classes: ComputeFunction, ComputeCodegenUnboxedKernels | functions: _sig_decl_wrapper, static_dispatch, gen_unboxing, key_func, compute_native_function_declaration, gen_decl | imports: argparse, dataclasses, yaml, torchgen | [to",
    "role": "src",
    "loc": 895
  },
  {
    "id": "torchgen\\gen_functionalization_type.py",
    "summary": "No description | classes: GenCompositeViewCopyKernel | functions: return_str, modifies_arguments, wrapper_name, is_tensor_like, get_owning_type, unwrap_tensor_args | imports: dataclasses, torchgen | [torchgen gen_functionalization_type.py]",
    "role": "src",
    "loc": 654
  },
  {
    "id": "torchgen\\gen_lazy_tensor.py",
    "summary": "No description | classes: default_args | functions: parse_native_functions_keys, validate_shape_inference_header, get_ltc_helper_fns, main, run_gen_lazy_tensor, make_file_manager | imports: argparse, yaml, torchgen | [torchgen gen_lazy_tensor.py]",
    "role": "src",
    "loc": 456
  },
  {
    "id": "torchgen\\gen_schema_utils.py",
    "summary": "No description | classes: TypeGen, ReturnGen, ArgumentGen, FunctionSchemaGen | imports: torchgen, torch | [torchgen gen_schema_utils.py]",
    "role": "src",
    "loc": 80
  },
  {
    "id": "torchgen\\gen_vmap_plumbing.py",
    "summary": "No description | classes: ComputeBatchRulePlumbing | functions: is_tensor, is_optional_tensor, is_tensor_list, unwrap_tensor, unwrap_optional_tensor, gen_unwraps | imports: textwrap, dataclasses, torchgen | [torchgen gen_vmap_plumbing.py]",
    "role": "src",
    "loc": 207
  },
  {
    "id": "torchgen\\local.py",
    "summary": "No description | classes: Locals | functions: use_const_ref_for_mutable_tensors, use_ilistref_for_tensor_lists, parametrize | imports: threading | [torchgen local.py]",
    "role": "src",
    "loc": 36
  },
  {
    "id": "torchgen\\model.py",
    "summary": "No description | classes: Location, Variant, DispatchKey, _TorchDispatchModeKey, ScalarType, UfuncKey | functions: codegen_per_backend_entries, is_generic_dispatch_key, is_cuda_dispatch_key, is_xpu_dispatch_key, is_structured_dispatch_key, is_ufunc_dispatch_key | imports: dataclasses, torchgen | [to",
    "role": "src",
    "loc": 1974
  },
  {
    "id": "torchgen\\native_function_generation.py",
    "summary": "No description | functions: pre_group_native_functions, get_expected_out_variant_overload_name, self_to_out_signature, functional_to_out_signature, generate_out_args_from_schema, mutable_to_out_signature | imports: string, torchgen | [torchgen native_function_generation.py]",
    "role": "src",
    "loc": 424
  },
  {
    "id": "torchgen\\utils.py",
    "summary": "No description | classes: Target, FileManager, NamespaceHelper, OrderedSet | functions: split_name_params, mapMaybe, concatMap, context, assert_never, _read_template | imports: functools, hashlib, textwrap, dataclasses | [torchgen utils.py]",
    "role": "src",
    "loc": 398
  },
  {
    "id": "torchgen\\yaml_utils.py",
    "summary": "No description | classes: YamlLoader | imports: yaml | [torchgen yaml_utils.py]",
    "role": "src",
    "loc": 20
  },
  {
    "id": "torchgen\\__init__.py",
    "summary": "torchgen | [torchgen __init__.py]",
    "role": "src",
    "loc": 8
  },
  {
    "id": "torchgen\\aoti\\fallback_ops.py",
    "summary": "No description | [torchgen aoti fallback_ops.py]",
    "role": "src",
    "loc": 147
  },
  {
    "id": "torchgen\\aoti\\__init__.py",
    "summary": "Package initializer | [torchgen aoti __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\api\\autograd.py",
    "summary": "No description | classes: SavedAttribute, Derivative, ForwardDerivative, DifferentiabilityInfo, DifferentiableInput, DifferentiableOutput | functions: uses_ident, uses_retain_variables, uses_single_grad, dispatch_strategy, is_foreach_func, is_reference_for_foreach | imports: dataclasses, torchgen | ",
    "role": "src",
    "loc": 567
  },
  {
    "id": "torchgen\\api\\cpp.py",
    "summary": "No description | functions: name, valuetype_type, argumenttype_type, argument_type, returntype_type, return_type | imports: torchgen | [torchgen api cpp.py]",
    "role": "src",
    "loc": 370
  },
  {
    "id": "torchgen\\api\\dispatcher.py",
    "summary": "No description | functions: name, argumenttype_type, argument_type, returns_type, jit_arguments, to_argument | imports: torchgen | [torchgen api dispatcher.py]",
    "role": "src",
    "loc": 84
  },
  {
    "id": "torchgen\\api\\functionalization.py",
    "summary": "No description | functions: name, reverse_name, capture_arguments, returns_type, outer_arguments, inner_call_index | imports: torchgen | [torchgen api functionalization.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "torchgen\\api\\lazy.py",
    "summary": "No description | classes: LazyArgument, LazyIrProperties, LazyIrSchema | functions: getValueT, setValueT, process_ir_type, isValueType, isSymIntType, isWrappedScalarType | imports: torchgen | [torchgen api lazy.py]",
    "role": "src",
    "loc": 348
  },
  {
    "id": "torchgen\\api\\meta.py",
    "summary": "No description | functions: name | imports: torchgen | [torchgen api meta.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torchgen\\api\\native.py",
    "summary": "No description | functions: name, argumenttype_type, returns_type, argument_type, argument, arguments | imports: torchgen | [torchgen api native.py]",
    "role": "src",
    "loc": 121
  },
  {
    "id": "torchgen\\api\\python.py",
    "summary": "No description | classes: PythonReturns, PythonArgument, PythonOutArgument, PythonSignature, PythonSignatureDeprecated, PythonSignatureNativeFunctionPair | functions: _cpp_signature, has_tensor_options, argument_type_str, argument_type_size, argument, signature | imports: dataclasses, torchgen | [to",
    "role": "src",
    "loc": 879
  },
  {
    "id": "torchgen\\api\\structured.py",
    "summary": "No description | functions: argumenttype_type, argument_type, argument, impl_arguments, meta_arguments, out_arguments | imports: torchgen | [torchgen api structured.py]",
    "role": "src",
    "loc": 108
  },
  {
    "id": "torchgen\\api\\translate.py",
    "summary": "No description | classes: UnsatError | functions: translate, unsat, solve, direct_solve | imports: torchgen | [torchgen api translate.py]",
    "role": "src",
    "loc": 278
  },
  {
    "id": "torchgen\\api\\ufunc.py",
    "summary": "No description | classes: UfunctorBindings | functions: schema_kernel_name, kernel_name, dispatchstub_type, opmath_type, ufunctor_ctor_type, ufunctor_apply_type | imports: dataclasses, torchgen | [torchgen api ufunc.py]",
    "role": "src",
    "loc": 124
  },
  {
    "id": "torchgen\\api\\unboxing.py",
    "summary": "No description | functions: name, convert_arguments, argumenttype_ivalue_convert, _gen_code_base_type, _gen_code_optional_type, _gen_code_list_type | imports: torchgen | [torchgen api unboxing.py]",
    "role": "src",
    "loc": 132
  },
  {
    "id": "torchgen\\api\\__init__.py",
    "summary": "Package initializer | [torchgen api __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\api\\types\\signatures.py",
    "summary": "A CppSignature represents a single overload in the C++ API.  For | classes: CppSignature, CppSignatureGroup, DispatcherSignature, NativeSignature, ViewInverseSignature, FunctionalizationLambda | functions: kernel_signature | imports: dataclasses, torchgen | [torchgen api types signatures.py]",
    "role": "src",
    "loc": 304
  },
  {
    "id": "torchgen\\api\\types\\types.py",
    "summary": "Where should I add a new type? `types_base.py` vs `types.py` | classes: OptionalCType, ListCType, ArrayRefCType, VectorizedCType | imports: dataclasses, torchgen | [torchgen api types types.py]",
    "role": "src",
    "loc": 140
  },
  {
    "id": "torchgen\\api\\types\\types_base.py",
    "summary": "Where should I add a new type? `types_base.py` vs `types.py` | classes: SpecialArgName, BaseCppType, CType, BaseCType, ConstRefCType, VectorCType | imports: abc, dataclasses, torchgen | [torchgen api types types_base.py]",
    "role": "src",
    "loc": 142
  },
  {
    "id": "torchgen\\api\\types\\__init__.py",
    "summary": "Package initializer | imports: torchgen | [torchgen api types __init__.py]",
    "role": "src",
    "loc": 3
  },
  {
    "id": "torchgen\\decompositions\\gen_jit_decompositions.py",
    "summary": "No description | functions: gen_serialized_decompisitions, gen_decomposition_mappings, write_decomposition_util_file, main | imports: torch | [torchgen decompositions gen_jit_decompositions.py]",
    "role": "src",
    "loc": 63
  },
  {
    "id": "torchgen\\dest\\lazy_ir.py",
    "summary": "Given a LazyArgument, | classes: GenLazyIR, GenTSLazyIR, GenLazyNativeFuncDefinition, ComputeShapeSignature, GenLazyShapeInferenceDefinition | functions: node_ctor_arg_rvalue_string, node_ctor_inputs, gen_fallback_code, aten_symbol, convert_to_meta_tensors, generate_non_native_lazy_ir_nodes | import",
    "role": "src",
    "loc": 585
  },
  {
    "id": "torchgen\\dest\\lazy_ts_lowering.py",
    "summary": "No description | functions: ts_lowering_body, get_value | imports: torchgen | [torchgen dest lazy_ts_lowering.py]",
    "role": "src",
    "loc": 40
  },
  {
    "id": "torchgen\\dest\\native_functions.py",
    "summary": "No description | functions: torch_api_key_word_prefix, gen_unstructured, gen_structured, compute_native_function_declaration | imports: torchgen | [torchgen dest native_functions.py]",
    "role": "src",
    "loc": 65
  },
  {
    "id": "torchgen\\dest\\register_dispatch_key.py",
    "summary": "No description | classes: RegisterDispatchKey, StructuredRegisterDispatchKey | functions: gen_registration_headers, gen_empty_impl_names, gen_create_out_helper, gen_maybe_create_proxy_helper, gen_resize_out_helper, gen_check_inplace_helper | imports: textwrap, dataclasses, torchgen | [torchgen dest ",
    "role": "src",
    "loc": 777
  },
  {
    "id": "torchgen\\dest\\ufunc.py",
    "summary": "No description | classes: UfunctorSignature, UfuncSignature, BinaryScalarSpecializationConfig, StubSignature | functions: eligible_for_binary_scalar_specialization, compute_ufunc_cuda_functors, compute_ufunc_cuda_dtype_body, compute_ufunc_cuda, compute_ufunc_cpu, compute_ufunc_cpu_dtype_body | impor",
    "role": "src",
    "loc": 389
  },
  {
    "id": "torchgen\\dest\\__init__.py",
    "summary": "Package initializer | imports: torchgen | [torchgen dest __init__.py]",
    "role": "src",
    "loc": 19
  },
  {
    "id": "torchgen\\executorch\\model.py",
    "summary": "No description | classes: ScalarType, ETKernelKeyOpArgMeta, ETKernelKey, ETKernelIndex | imports: dataclasses, torchgen | [torchgen executorch model.py]",
    "role": "src",
    "loc": 172
  },
  {
    "id": "torchgen\\executorch\\parse.py",
    "summary": "Given a loaded yaml representing kernel assignment information, extract the | functions: parse_from_yaml, parse_et_yaml_struct, extract_kernel_fields, parse_et_yaml, strip_et_fields | imports: yaml, torchgen | [torchgen executorch parse.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torchgen\\executorch\\__init__.py",
    "summary": "Package initializer | [torchgen executorch __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\executorch\\api\\custom_ops.py",
    "summary": "Generate custom ops registration code for dest.RegisterDispatchKey. | classes: ComputeNativeFunctionStub | functions: gen_custom_ops_registration | imports: dataclasses, torchgen | [torchgen executorch api custom_ops.py]",
    "role": "src",
    "loc": 126
  },
  {
    "id": "torchgen\\executorch\\api\\et_cpp.py",
    "summary": "No description | functions: valuetype_type, argumenttype_type, argument_type, returntype_type, return_type, returns_type | imports: torchgen | [torchgen executorch api et_cpp.py]",
    "role": "src",
    "loc": 286
  },
  {
    "id": "torchgen\\executorch\\api\\unboxing.py",
    "summary": "Takes a sequence of Bindings and unbox EValues to these Bindings. Return generated code that performs correct unboxing. | classes: Unboxing | functions: name | imports: dataclasses, torchgen | [torchgen executorch api unboxing.py]",
    "role": "src",
    "loc": 180
  },
  {
    "id": "torchgen\\executorch\\api\\__init__.py",
    "summary": "Package initializer | [torchgen executorch api __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\executorch\\api\\types\\signatures.py",
    "summary": "This signature is merely a CppSignature with Executorch types (optionally | classes: ExecutorchCppSignature | imports: dataclasses, torchgen | [torchgen executorch api types signatures.py]",
    "role": "src",
    "loc": 54
  },
  {
    "id": "torchgen\\executorch\\api\\types\\types.py",
    "summary": "No description | classes: OptionalCType, ArrayRefCType | imports: dataclasses, torchgen | [torchgen executorch api types types.py]",
    "role": "src",
    "loc": 60
  },
  {
    "id": "torchgen\\executorch\\api\\types\\__init__.py",
    "summary": "Package initializer | imports: torchgen | [torchgen executorch api types __init__.py]",
    "role": "src",
    "loc": 2
  },
  {
    "id": "torchgen\\fuse\\gen_patterns.py",
    "summary": "No description | imports: torch | [torchgen fuse gen_patterns.py]",
    "role": "src",
    "loc": 11
  },
  {
    "id": "torchgen\\operator_versions\\gen_mobile_upgraders.py",
    "summary": "No description | classes: ByteCode | functions: construct_instruction, construct_constants, construct_operators, construct_types, construct_register_size, construct_version_maps | imports: operator, torch, torchgen | [torchgen operator_versions gen_mobile_upgraders.py]",
    "role": "src",
    "loc": 320
  },
  {
    "id": "torchgen\\operator_versions\\gen_mobile_upgraders_constant.py",
    "summary": "No description | [torchgen operator_versions gen_mobile_upgraders_constant.py]",
    "role": "src",
    "loc": 7
  },
  {
    "id": "torchgen\\operator_versions\\__init__.py",
    "summary": "Package initializer | [torchgen operator_versions __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\selective_build\\operator.py",
    "summary": "No description | classes: SelectiveBuildOperator | functions: merge_debug_info, combine_operators, merge_operator_dicts, strip_operator_overload_name | imports: dataclasses | [torchgen selective_build operator.py]",
    "role": "src",
    "loc": 100
  },
  {
    "id": "torchgen\\selective_build\\selector.py",
    "summary": "No description | classes: SelectiveBuilder | functions: merge_kernel_metadata, merge_et_kernel_metadata, combine_selective_builders, op_name_from_native_function | imports: dataclasses, yaml, torchgen | [torchgen selective_build selector.py]",
    "role": "src",
    "loc": 253
  },
  {
    "id": "torchgen\\selective_build\\__init__.py",
    "summary": "Package initializer | [torchgen selective_build __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\shape_functions\\gen_jit_shape_functions.py",
    "summary": "No description | functions: gen_serialized_decompisitions, gen_shape_mappings, gen_bounded_mappings, write_decomposition_util_file, main | imports: importlib | [torchgen shape_functions gen_jit_shape_functions.py]",
    "role": "src",
    "loc": 127
  },
  {
    "id": "torchgen\\static_runtime\\config.py",
    "summary": "No description | functions: func_name_base_str, is_hand_written, override_test_values | imports: torchgen | [torchgen static_runtime config.py]",
    "role": "src",
    "loc": 379
  },
  {
    "id": "torchgen\\static_runtime\\generator.py",
    "summary": "No description | classes: GenOpDispatcher, GenOpTestCase | functions: has_alias, is_supported, ivalue_type_conversion_method, should_use_int_tensor, should_use_complex_tensor, test_tensor_dim | imports: json, torchgen | [torchgen static_runtime generator.py]",
    "role": "src",
    "loc": 710
  },
  {
    "id": "torchgen\\static_runtime\\gen_static_runtime_ops.py",
    "summary": "No description | functions: group_functions_by_op_name, is_supported, clang_format, write_cpp, write_test_cpp, main | imports: argparse, libfb, torchgen, subprocess | [torchgen static_runtime gen_static_runtime_ops.py]",
    "role": "src",
    "loc": 145
  },
  {
    "id": "torchgen\\static_runtime\\__init__.py",
    "summary": "Package initializer | [torchgen static_runtime __init__.py]",
    "role": "src",
    "loc": 0
  },
  {
    "id": "torchgen\\_autoheuristic\\ah_tree.py",
    "summary": "No description | classes: DecisionTreeNode, DecisionTree | imports: numpy, sklearn | [torchgen _autoheuristic ah_tree.py]",
    "role": "src",
    "loc": 215
  },
  {
    "id": "torchgen\\_autoheuristic\\benchmark_runner.py",
    "summary": "BenchmarkRunner is a base class for all benchmark runners. It provides an interface to run benchmarks in order to | classes: BenchmarkRunner | imports: argparse, random, abc, tqdm | [torchgen _autoheuristic benchmark_runner.py]",
    "role": "src",
    "loc": 70
  },
  {
    "id": "torchgen\\_autoheuristic\\benchmark_utils.py",
    "summary": "No description | functions: transpose_tensors, fits_in_memory, get_mm_tensors, set_precision, get_random_between_pow2 | imports: random, torch | [torchgen _autoheuristic benchmark_utils.py]",
    "role": "src",
    "loc": 48
  },
  {
    "id": "torchgen\\_autoheuristic\\merge_data.py",
    "summary": "No description | functions: merge_txt_files | [torchgen _autoheuristic merge_data.py]",
    "role": "src",
    "loc": 47
  },
  {
    "id": "torchgen\\_autoheuristic\\test_utils.py",
    "summary": "No description | functions: read_file_to_string, run_bash | imports: subprocess | [torchgen _autoheuristic test_utils.py]",
    "role": "src",
    "loc": 14
  },
  {
    "id": "torchgen\\_autoheuristic\\train.py",
    "summary": "Base class for AutoHeuristic training. | classes: AHTrain | imports: argparse, json, pandas, torch | [torchgen _autoheuristic train.py]",
    "role": "src",
    "loc": 147
  },
  {
    "id": "torchgen\\_autoheuristic\\train_decision.py",
    "summary": "No description | classes: AHTrainDecisionTree, AccuracyMetrics, WrongSpeedupMetrics, RankingMetrics, DefaultComparisonMetrics, EvalResults | imports: json, dataclasses, numpy, pandas | [torchgen _autoheuristic train_decision.py]",
    "role": "src",
    "loc": 777
  },
  {
    "id": "torchgen\\_autoheuristic\\train_regression.py",
    "summary": "This class is responsible for generating a heuristic by using data collected with AutoHeuristic. It will learn a | classes: AHTrainRegressionTree | imports: numpy, pandas, scipy, sklearn | [torchgen _autoheuristic train_regression.py]",
    "role": "src",
    "loc": 378
  },
  {
    "id": "torchgen\\_autoheuristic\\mixed_mm\\gen_data_mixed_mm.py",
    "summary": "BenchmarkRunner for mixed mm. Used to generate collect training data with AutoHeuristic to learn a heuristic. | classes: BenchmarkRunnerMixedMM | imports: random, benchmark_runner, benchmark_utils, torch | [torchgen _autoheuristic mixed_mm gen_data_mixed_mm.py]",
    "role": "src",
    "loc": 112
  },
  {
    "id": "torchgen\\_autoheuristic\\mixed_mm\\test_mixed_mm.py",
    "summary": "No description | classes: TestMixedMM | imports: unittest, expecttest, test_utils | [torchgen _autoheuristic mixed_mm test_mixed_mm.py]",
    "role": "src",
    "loc": 300
  },
  {
    "id": "torchgen\\_autoheuristic\\mixed_mm\\train_decision_mixedmm.py",
    "summary": "No description | classes: AHTrainDecisionTreeMixedMM | imports: train_decision, torch | [torchgen _autoheuristic mixed_mm train_decision_mixedmm.py]",
    "role": "src",
    "loc": 33
  },
  {
    "id": "torchgen\\_autoheuristic\\mm\\gen_data_mm.py",
    "summary": "BenchmarkRunner for mm. | classes: BenchmarkRunnerMM | imports: random, benchmark_runner, benchmark_utils, torch | [torchgen _autoheuristic mm gen_data_mm.py]",
    "role": "src",
    "loc": 90
  },
  {
    "id": "torchgen\\_autoheuristic\\mm\\train_decision_mm.py",
    "summary": "No description | classes: AHTrainDecisionTreeMM | imports: pandas, train_decision, torch | [torchgen _autoheuristic mm train_decision_mm.py]",
    "role": "src",
    "loc": 45
  },
  {
    "id": "torchgen\\_autoheuristic\\pad_mm\\gen_data_pad_mm.py",
    "summary": "BenchmarkRunner for pad_mm. Used to generate collect training data with AutoHeuristic to learn a heuristic. | classes: BenchmarkRunnerPadMM | imports: random, benchmark_runner, benchmark_utils, torch | [torchgen _autoheuristic pad_mm gen_data_pad_mm.py]",
    "role": "src",
    "loc": 117
  },
  {
    "id": "torchgen\\_autoheuristic\\pad_mm\\test_pad_mm.py",
    "summary": "No description | classes: TestPadMM | imports: unittest, expecttest, test_utils | [torchgen _autoheuristic pad_mm test_pad_mm.py]",
    "role": "src",
    "loc": 116
  },
  {
    "id": "torchgen\\_autoheuristic\\pad_mm\\train_decision_pad_mm.py",
    "summary": "No description | classes: AHTrainDecisionTreePadMM | imports: train_decision, torch | [torchgen _autoheuristic pad_mm train_decision_pad_mm.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torchgen\\_autoheuristic\\pad_mm\\train_pad_mm.py",
    "summary": "No description | classes: AHTrainPadMM | imports: train_regression, torch | [torchgen _autoheuristic pad_mm train_pad_mm.py]",
    "role": "src",
    "loc": 17
  },
  {
    "id": "torchgen\\_autoheuristic\\pad_mm\\train_regression_pad_mm.py",
    "summary": "No description | classes: AHTrainPadMM | imports: train_regression, torch | [torchgen _autoheuristic pad_mm train_regression_pad_mm.py]",
    "role": "src",
    "loc": 17
  }
]