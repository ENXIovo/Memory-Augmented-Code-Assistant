#!/usr/bin/env python3
"""
Generate a README.md for a GitHub repo using indexed gist and detail embeddings
in Pinecone, with the directory tree structure included in the prompt.
"""

import os
import json
import tiktoken
from typing import Any, Dict, List

from pinecone import Pinecone
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

import config as C

# ====== Configuration ======
PINECONE_API_KEY = C.PINECONE_API_KEY
GIST_INDEX       = C.GIST_INDEX_NAME
DETAIL_INDEX     = C.DETAIL_INDEX_NAME
REPO_NAME        = "scikit-learn"
TOP_K_GIST       = 5
TOP_K_DETAIL     = 10

# Path to the analysis JSON file generated by repo-analyzer
ANALYSIS_JSON = os.path.join("analysis_results", "analysis_results", REPO_NAME, f"{REPO_NAME}_analysis.json")

# Output README file
OUTPUT_DIR = os.path.join("analysis_results", "analysis_results", REPO_NAME)
os.makedirs(OUTPUT_DIR, exist_ok=True)
OUTPUT_FILE = os.path.join(OUTPUT_DIR, f"{REPO_NAME}_new_readme.md")

# Questions used to retrieve relevant gist and detail chunks
SUB_QUESTIONS = [
    f"Overview of the {REPO_NAME} project and its purpose",
    f"What are the main components or modules in {REPO_NAME}?",
    f"Installation or setup instructions for {REPO_NAME}",
    f"Key functions, classes or APIs exposed by {REPO_NAME}",
    f"Examples or usage patterns in {REPO_NAME}"
]

# Token limits to avoid exceeding GPT context length
MAX_GIST_TOKENS   = 4000
MAX_DETAIL_TOKENS = 6000


# ====== Utility functions ======
def get_token_length(text: str) -> int:
    enc = tiktoken.encoding_for_model(C.LLM_MODEL)
    return len(enc.encode(text))

def truncate_chunks(chunks: List[str], max_tokens: int) -> List[str]:
    result, total = [], 0
    for chunk in chunks:
        tokens = get_token_length(chunk)
        if total + tokens > max_tokens:
            break
        result.append(chunk)
        total += tokens
    return result


# ====== Directory Tree Formatting ======
def load_directory_structure(json_path: str) -> Dict[str, Any]:
    """Load the nested directory structure from a JSON analysis file."""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} not found")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data.get("directory_structure", {})

def format_directory_structure(
    structure: Dict[str, Any],
    indent: int = 0,
    max_depth: int = 3,
    max_items_per_level: int = 10,
) -> List[str]:
    """Format a nested directory dict into a visual tree structure (as a list of lines)."""
    if indent > max_depth:
        return [f"{'    ' * indent}â””â”€â”€ ..."]

    dirs  = {k: v for k, v in structure.items() if isinstance(v, dict)}
    files = {k: v for k, v in structure.items() if v is None}
    items = sorted(dirs.items()) + sorted(files.items())

    truncated = False
    if len(items) > max_items_per_level:
        items = items[: max_items_per_level - 1]
        truncated = True

    lines: List[str] = []
    for idx, (name, content) in enumerate(items):
        prefix = "    " * indent
        connector = "â””â”€â”€ " if idx == len(items) - 1 and not truncated else "â”œâ”€â”€ "
        if content is None:
            lines.append(f"{prefix}{connector}{name}")
        else:
            lines.append(f"{prefix}{connector}{name}/")
            lines.extend(
                format_directory_structure(
                    content,
                    indent + 1,
                    max_depth,
                    max_items_per_level,
                )
            )

    if truncated:
        lines.append(f"{'    ' * indent}â””â”€â”€ ...")
    return lines

def build_directory_tree_fs(repo_root: str, max_depth: int = 3) -> str:
    """Fallback: scan the filesystem to generate directory tree string."""
    tree: List[str] = []
    for root, dirs, files in os.walk(repo_root):
        level = root.replace(repo_root, "").count(os.sep)
        if level > max_depth:
            continue
        indent = "    " * level
        tree.append(f"{indent}â”œâ”€â”€ {os.path.basename(root)}/")
        subindent = "    " * (level + 1)
        for f in files:
            tree.append(f"{subindent}â””â”€â”€ {f}")
    return "\n".join(tree)


# ====== Pinecone Setup ======
pc = Pinecone(api_key=PINECONE_API_KEY)

gist_index   = pc.Index(GIST_INDEX)
detail_index = pc.Index(DETAIL_INDEX)

embeddings = OpenAIEmbeddings(model=C.EMBED_MODEL)

gist_store = PineconeVectorStore(index=gist_index,   embedding=embeddings, namespace=C.GIST_NAMESPACE)
detail_store = PineconeVectorStore(index=detail_index, embedding=embeddings, namespace=C.DETAIL_NAMESPACE)


# ====== Vector Retrieval ======
def retrieve_top_chunks(repo_name: str) -> tuple[list[str], list[str]]:
    seen_gist = set()
    seen_detail = set()
    gist_chunks = []
    detail_chunks = []

    for q in SUB_QUESTIONS:
        # Gist retrieval
        gist_docs = gist_store.similarity_search(query=q, k=TOP_K_GIST, filter={"repo": repo_name})
        for d in gist_docs:
            key = (d.metadata.get("path", ""), d.page_content.strip())
            if key not in seen_gist:
                seen_gist.add(key)
                gist_chunks.append(f"### {key[0]}\n{key[1]}")

        # Detail retrieval
        detail_docs = detail_store.similarity_search(query=q, k=TOP_K_DETAIL, filter={"repo": repo_name})
        for d in detail_docs:
            key = (d.metadata.get("type", ""), d.metadata.get("name", ""), d.metadata.get("path", ""), d.page_content.strip())
            if key not in seen_detail:
                seen_detail.add(key)
                label = f"### [{key[0]} {key[1]} in {key[2]}]"
                detail_chunks.append(f"{label}\n{key[3]}")

    gist_chunks = truncate_chunks(gist_chunks, MAX_GIST_TOKENS)
    detail_chunks = truncate_chunks(detail_chunks, MAX_DETAIL_TOKENS)
    return gist_chunks, detail_chunks



# ====== README Prompt & Generation ======
EXAMPLE_README_STYLE = """
# Project Name
A short and clear description of what this project does, its purpose, and key goals.

## Installation
Step-by-step instructions for setting up the project.

## Components
An outline of the important files or modules.

## Usage
Code examples or typical workflows.

## API Overview
Key functions, modules or classes with descriptions.
""".strip()

def generate_readme(gist_chunks: list[str], detail_chunks: list[str], dir_tree: str) -> str:
    """Generate the README using GPT and the given context."""
    llm = ChatOpenAI(model=C.LLM_MODEL)
    prompt = (
        "You are an expert technical writer. "
        "Use the directory structure and retrieved documentation below to write a polished README.\n\n"
        f"## Directory Structure\n```\n{dir_tree}\n```\n\n"
        "Follow this README style template:\n\n"
        f"{EXAMPLE_README_STYLE}\n\n"
        "Now generate the README based on:\n\n"
        f"## File-level Summaries (Gists):\n\n" + "\n\n".join(gist_chunks) +
        f"\n\n## Detail-level Code Units:\n\n" + "\n\n".join(detail_chunks)
    )
    print(prompt)
    return llm.invoke(prompt).content


def save_readme(text: str, path: str) -> None:
    """Save the generated README to file."""
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"âœ… README saved to {path}")


# ====== Main Process ======
if __name__ == "__main__":
    print("ğŸ” Retrieving gist & detail chunks ...")
    gist_chunks, detail_chunks = retrieve_top_chunks(REPO_NAME)
    print(f"âœ… Retrieved {len(gist_chunks)} gist and {len(detail_chunks)} detail chunks.")

    print("ğŸ“ Loading directory structure ...")
    try:
        dir_struct = load_directory_structure(ANALYSIS_JSON)
        dir_tree_str = "\n".join(format_directory_structure(dir_struct))
        print("âœ… Directory tree loaded from analysis JSON.")
    except Exception as e:
        print(f"âš ï¸  Failed to load analysis JSON ({e}), fallback to os.walk.")
        dir_tree_str = build_directory_tree_fs(C.REPO_PATH)
        print("âœ… Directory tree built from file system.")

    print("ğŸ§  Generating README via GPT ...")
    readme_text = generate_readme(gist_chunks, detail_chunks, dir_tree_str)

    save_readme(readme_text, OUTPUT_FILE)
